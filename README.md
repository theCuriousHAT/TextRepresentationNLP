# ðŸ§  Text Representation in NLP

This project delves into various techniques for representing text data in Natural Language Processing (NLP). It covers foundational methods like Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), as well as advanced approaches such as Word2Vec and BERT embeddings. The goal is to provide a comprehensive understanding of how different text representation methods can be implemented and compared.

# ðŸ“‚ Project Structure

```
ðŸ“‚ TextRepresentationNLP/
â”œâ”€â”€ Text_representation/
â”‚   â”œâ”€â”€ 01_BoW_TFIDF.ipynb
â”‚   â”œâ”€â”€ 02_Word2Vec.ipynb
â”‚   â”œâ”€â”€ 03_BERT_Embeddings.ipynb
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_corpus.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


ðŸš€ Getting Started

## Prerequisites

- Python 3.7 or higher
- Jupyter Notebook

## Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/theCuriousHAT/TextRepresentationNLP.git
    cd TextRepresentationNLP
    ```

2. Create a virtual environment (optional but recommended):

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. Install the required packages:

    ```bash
    pip install -r requirements.txt
    ```

4. Launch Jupyter Notebook:

    ```bash
    jupyter notebook
    ```




