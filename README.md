# ðŸ§  Text Representation in NLP

This project delves into various techniques for representing text data in Natural Language Processing (NLP). It covers foundational methods like Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), as well as advanced approaches such as Word2Vec and BERT embeddings. The goal is to provide a comprehensive understanding of how different text representation methods can be implemented and compared.

# ðŸ“‚ Project Structure

```
ðŸ“‚ TextRepresentationNLP/
â”œâ”€â”€ Text_representation/
â”‚   â”œâ”€â”€ 01_BoW_TFIDF.ipynb
â”‚   â”œâ”€â”€ 02_Word2Vec.ipynb
â”‚   â”œâ”€â”€ 03_BERT_Embeddings.ipynb
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_corpus.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

ðŸš€ Getting Started

# Prerequisites

Python 3.7 or higher
Jupyter Notebook

# Installation
Clone the repository:
git clone https://github.com/theCuriousHAT/TextRepresentationNLP.git
cd TextRepresentationNLP

# Create a virtual environment (optional but recommended):
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install the required packages:
pip install -r requirements.txt

# Launch Jupyter Notebook:
jupyter notebook



