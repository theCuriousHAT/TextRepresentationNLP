link,title,sub_title,author,reading_time,text,id
https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205?source=tag_archive---------5-----------------------,"Ensemble methods: bagging, boosting and stacking",Understanding the key concepts of ensemble learning.,Joseph Rocca,20,"This post was co-written with Baptiste Rocca.
""Unity is strength"". This old saying expresses pretty well the underlying idea that rules the very powerful ""ensemble methods"" in machine learning. Roughly, ensemble learning methods, that often trust the top rankings of many machine learning competitions (including Kaggle's competitions), are based on the hypothesis that combining multiple models together can often produce a much more powerful model.
The purpose of this post is to introduce various notions of ensemble learning. We will give the reader some necessary keys to well understand and use related methods and be able to design adapted solutions when needed. We will discuss some well known notions such as boostrapping, bagging, random forest, boosting, stacking and many others that are the basis of ensemble learning. In order to make the link between all these methods as clear as possible, we will try to present them in a much broader and logical framework that, we hope, will be easier to understand and remember.
In the first section of this post we will present the notions of weak and strong learners and we will introduce three main ensemble learning methods: bagging, boosting and stacking. Then, in the second section we will be focused on bagging and we will discuss notions such that bootstrapping, bagging and random forests. In the third section, we will present boosting and, in particular, its two most popular variants: adaptative boosting (adaboost) and gradient boosting. Finally in the fourth section we will give an overview of stacking.
Ensemble learning is a machine learning paradigm where multiple models (often called ""weak learners"") are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.
In machine learning, no matter if we are facing a classification or a regression problem, the choice of the model is extremely important to have any chance to obtain good results. This choice can depend on many variables of the problem: quantity of data, dimensionality of the space, distribution hypothesis...
A low bias and a low variance, although they most often vary in opposite directions, are the two most fundamental features expected for a model. Indeed, to be able to ""solve"" a problem, we want our model to have enough degrees of freedom to resolve the underlying complexity of the data we are working with, but we also want it to have not too much degrees of freedom to avoid high variance and be more robust. This is the well known bias-variance tradeoff.
In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for designing more complex models by combining several of them. Most of the time, these basics models perform not so well by themselves either because they have a high bias (low degree of freedom models, for example) or because they have too much variance to be robust (high degree of freedom models, for example). Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a strong learner (or ensemble model) that achieves better performances.
In order to set up an ensemble learning method, we first need to select our base models to be aggregated. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways. The ensemble model we obtain is then said to be ""homogeneous"". However, there also exist some methods that use different type of base learning algorithms: some heterogeneous weak learners are then combined into an ""heterogeneous ensembles model"".
One important point is that our choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it should be with an aggregating method that tends to reduce variance whereas if we choose base models with low variance but high bias, it should be with an aggregating method that tends to reduce bias.
This brings us to the question of how to combine these models. We can mention three major kinds of meta-algorithms that aims at combining weak learners:
Very roughly, we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).
In the following sections, we will present in details bagging and boosting (that are a bit more widely used than stacking and will allow us to discuss some key notions of ensemble learning) before giving a brief overview of stacking.
In parallel methods we fit the different considered learners independently from each others and, so, it is possible to train them concurrently. The most famous such approach is ""bagging"" (standing for ""bootstrap aggregating"") that aims at producing an ensemble model that is more robust than the individual models composing it.
Let's begin by defining bootstrapping. This statistical technique consists in generating samples of size B (called bootstrap samples) from an initial dataset of size N by randomly drawing with replacement B observations.
Under some assumptions, these samples have pretty good statistical properties: in first approximation, they can be seen as being drawn both directly from the true underlying (and often unknown) data distribution and independently from each others. So, they can be considered as representative and independent samples of the true data distribution (almost i.i.d. samples). The hypothesis that have to be verified to make this approximation valid are twofold. First, the size N of the initial dataset should be large enough to capture most of the complexity of the underlying distribution so that sampling from the dataset is a good approximation of sampling from the real distribution (representativity). Second, the size N of the dataset should be large enough compared to the size B of the bootstrap samples so that samples are not too much correlated (independence). Notice that in the following, we will sometimes make reference to these properties (representativity and independence) of bootstrap samples: the reader should always keep in mind that this is only an approximation.
Bootstrap samples are often used, for example, to evaluate variance or confidence intervals of a statistical estimators. By definition, a statistical estimator is a function of some observations and, so, a random variable with variance coming from these observations. In order to estimate the variance of such an estimator, we need to evaluate it on several independent samples drawn from the distribution of interest. In most of the cases, considering truly independent samples would require too much data compared to the amount really available. We can then use bootstrapping to generate several bootstrap samples that can be considered as being ""almost-representative"" and ""almost-independent"" (almost i.i.d. samples). These bootstrap samples will allow us to approximate the variance of the estimator, by evaluating its value for each of them.
When training a model, no matter if we are dealing with a classification or a regression problem, we obtain a function that takes an input, returns an output and that is defined with respect to the training dataset. Due to the theoretical variance of the training dataset (we remind that a dataset is an observed sample coming from a true unknown underlying distribution), the fitted model is also subject to variability: if another dataset had been observed, we would have obtained a different model.
The idea of bagging is then simple: we want to fit several independent models and ""average"" their predictions in order to obtain a model with a lower variance. However, we can't, in practice, fit fully independent models because it would require too much data. So, we rely on the good ""approximate properties"" of bootstrap samples (representativity and independence) to fit models that are almost independent.
First, we create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution. Then, we can fit a weak learner for each of these samples and finally aggregate them such that we kind of ""average"" their outputs and, so, obtain an ensemble model with less variance that its components. Roughly speaking, as the bootstrap samples are approximatively independent and identically distributed (i.i.d.), so are the learned base models. Then, ""averaging"" weak learners outputs do not change the expected answer but reduce its variance (just like averaging i.i.d. random variables preserve expected value but reduce variance).
So, assuming that we have L bootstrap samples (approximations of L independent datasets) of size B denoted
we can fit L almost independent weak learners (one on each dataset)
and then aggregate them into some kind of averaging process in order to get an ensemble model with a lower variance. For example, we can define our strong model such that
There are several possible ways to aggregate the multiple models fitted in parallel. For a regression problem, the outputs of individual models can literally be averaged to obtain the output of the ensemble model. For classification problem the class outputted by each model can be seen as a vote and the class that receives the majority of the votes is returned by the ensemble model (this is called hard-voting). Still for a classification problem, we can also consider the probabilities of each classes returned by all the models, average these probabilities and keep the class with the highest average probability (this is called soft-voting). Averages or votes can either be simple or weighted if any relevant weights can be used.
Finally, we can mention that one of the big advantages of bagging is that it can be parallelised. As the different models are fitted independently from each others, intensive parallelisation techniques can be used if required.
Learning trees are very popular base models for ensemble methods. Strong learners composed of multiple trees can be called ""forests"". Trees that compose a forest can be chosen to be either shallow (few depths) or deep (lot of depths, if not fully grown). Shallow trees have less variance but higher bias and then will be better choice for sequential methods that we will described thereafter. Deep trees, on the other side, have low bias but high variance and, so, are relevant choices for bagging method that is mainly focused at reducing variance.
The random forest approach is a bagging method where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance. However, random forests also use another trick to make the multiple fitted trees a bit less correlated with each others: when growing each tree, instead of only sampling over the observations in the dataset to generate a bootstrap sample, we also sample over features and keep only a random subset of them to build the tree.
Sampling over features has indeed the effect that all trees do not look at the exact same information to make their decisions and, so, it reduces the correlation between the different returned outputs. Another advantage of sampling over the features is that it makes the decision making process more robust to missing data: observations (from the training dataset or not) with missing data can still be regressed or classified based on the trees that take into account only features where data are not missing. Thus, random forest algorithm combines the concepts of bagging and random feature subspace selection to create more robust models.
In sequential methods the different combined weak models are no longer fitted independently from each others. The idea is to fit models iteratively such that the training of model at a given step depends on the models fitted at the previous steps. ""Boosting"" is the most famous of these approaches and it produces an ensemble model that is in general less biased than the weak learners that compose it.
Boosting methods work in the same spirit as bagging methods: we build a family of models that are aggregated to obtain a strong learner that performs better. However, unlike bagging that mainly aims at reducing variance, boosting is a technique that consists in fitting sequentially multiple weak learners in a very adaptative way: each model in the sequence is fitted giving more importance to observations in the dataset that were badly handled by the previous models in the sequence. Intuitively, each new model focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias (even if we can notice that boosting can also have the effect of reducing variance). Boosting, like bagging, can be used for regression as well as for classification problems.
Being mainly focused at reducing bias, the base models that are often considered for boosting are models with low variance but high bias. For example, if we want to use trees as our base models, we will choose most of the time shallow decision trees with only a few depths. Another important reason that motivates the use of low variance but high bias models as weak learners for boosting is that these models are in general less computationally expensive to fit (few degrees of freedom when parametrised). Indeed, as computations to fit the different models can't be done in parallel (unlike bagging), it could become too expensive to fit sequentially several complex models.
Once the weak learners have been chosen, we still need to define how they will be sequentially fitted (what information from previous models do we take into account when fitting current model?) and how they will be aggregated (how do we aggregate the current model to the previous ones?). We will discuss these questions in the two following subsections, describing more especially two important boosting algorithms: adaboost and gradient boosting.
In a nutshell, these two meta-algorithms differ on how they create and aggregate the weak learners during the sequential process. Adaptive boosting updates the weights attached to each of the training dataset observations whereas gradient boosting updates the value of these observations. This main difference comes from the way both methods try to solve the optimisation problem of finding the best model that can be written as a weighted sum of weak learners.
In adaptative boosting (often called ""adaboost""), we try to define our ensemble model as a weighted sum of L weak learners
Finding the best ensemble model with this form is a difficult optimisation problem. Then, instead of trying to solve it in one single shot (finding all the coefficients and weak learners that give the best overall additive model), we make use of an iterative optimisation process that is much more tractable, even if it can lead to a sub-optimal solution. More especially, we add the weak learners one by one, looking at each iteration for the best possible pair (coefficient, weak learner) to add to the current ensemble model. In other words, we define recurrently the (s_l)'s such that
where c_l and w_l are chosen such that s_l is the model that fit the best the training data and, so, that is the best possible improvement over s_(l-1). We can then denote
where E(.) is the fitting error of the given model and e(.,.) is the loss/error function. Thus, instead of optimising ""globally"" over all the L models in the sum, we approximate the optimum by optimising ""locally"" building and adding the weak learners to the strong model one by one.
More especially, when considering a binary classification, we can show that the adaboost algorithm can be re-written into a process that proceeds as follow. First, it updates the observations weights in the dataset and train a new weak learner with a special focus given to the observations misclassified by the current ensemble model. Second, it adds the weak learner to the weighted sum according to an update coefficient that expresse the performances of this weak model: the better a weak learner performs, the more it contributes to the strong learner.
So, assume that we are facing a binary classification problem, with N observations in our dataset and we want to use adaboost algorithm with a given family of weak models. At the very beginning of the algorithm (first model of the sequence), all the observations have the same weights 1/N. Then, we repeat L times (for the L learners in the sequence) the following steps:
Repeating these steps, we have then build sequentially our L models and aggregate them into a simple linear combination weighted by coefficients expressing the performance of each learner. Notice that there exists variants of the initial adaboost algorithm such that LogitBoost (classification) or L2Boost (regression) that mainly differ by their choice of loss function.
In gradient boosting, the ensemble model we try to build is also a weighted sum of weak learners
Just as we mentioned for adaboost, finding the optimal model under this form is too difficult and an iterative approach is required. The main difference with adaptative boosting is in the definition of the sequential optimisation process. Indeed, gradient boosting casts the problem into a gradient descent one: at each iteration we fit a weak learner to the opposite of the gradient of the current fitting error with respect to the current ensemble model. Let's try to clarify this last point. First, theoretical gradient descent process over the ensemble model can be written
where E(.) is the fitting error of the given model, c_l is a coefficient corresponding to the step size and
is the opposite of the gradient of the fitting error with respect to the ensemble model at step l-1. This (pretty abstract) opposite of the gradient is a function that can, in practice, only be evaluated for observations in the training dataset (for which we know inputs and outputs): these evaluations are called pseudo-residuals attached to each observations. Moreover, even if we know for the observations the values of these pseudo-residuals, we don't want to add to our ensemble model any kind of function: we only want to add a new instance of weak model. So, the natural thing to do is to fit a weak learner to the pseudo-residuals computed for each observation. Finally, the coefficient c_l is computed following a one dimensional optimisation process (line-search to obtain the best step size c_l).
So, assume that we want to use gradient boosting technique with a given family of weak models. At the very beginning of the algorithm (first model of the sequence), the pseudo-residuals are set equal to the observation values. Then, we repeat L times (for the L models of the sequence) the following steps:
Repeating these steps, we have then build sequentially our L models and aggregate them following a gradient descent approach. Notice that, while adaptative boosting tries to solve at each iteration exactly the ""local"" optimisation problem (find the best weak learner and its coefficient to add to the strong model), gradient boosting uses instead a gradient descent approach and can more easily be adapted to large number of loss functions. Thus, gradient boosting can be considered as a generalization of adaboost to arbitrary differentiable loss functions.
Stacking mainly differ from bagging and boosting on two points. First stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners. Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms.
As we already mentioned, the idea of stacking is to learn several different weak learners and combine them by training a meta-model to output predictions based on the multiple predictions returned by these weak models. So, we need to define two things in order to build our stacking model: the L learners we want to fit and the meta-model that combines them.
For example, for a classification problem, we can choose as weak learners a KNN classifier, a logistic regression and a SVM, and decide to learn a neural network as meta-model. Then, the neural network will take as inputs the outputs of our three weak learners and will learn to return final predictions based on it.
So, assume that we want to fit a stacking ensemble composed of L weak learners. Then we have to follow the steps thereafter:
In the previous steps, we split the dataset in two folds because predictions on data that have been used for the training of the weak learners are not relevant for the training of the meta-model. Thus, an obvious drawback of this split of our dataset in two parts is that we only have half of the data to train the base models and half of the data to train the meta-model. In order to overcome this limitation, we can however follow some kind of ""k-fold cross-training"" approach (similar to what is done in k-fold cross-validation) such that all the observations can be used to train the meta-model: for any observation, the prediction of the weak learners are done with instances of these weak learners trained on the k-1 folds that do not contain the considered observation. In other words, it consists in training on k-1 fold in order to make predictions on the remaining fold and that iteratively so that to obtain predictions for observations in any folds. Doing so, we can produce relevant predictions for each observation of our dataset and then train our meta-model on all these predictions.
A possible extension of stacking is multi-level stacking. It consists in doing stacking with multiple layers. As an example, let's consider a 3-levels stacking. In the first level (layer), we fit the L weak learners that have been chosen. Then, in the second level, instead of fitting a single meta-model on the weak models predictions (as it was described in the previous subsection) we fit M such meta-models. Finally, in the third level we fit a last meta-model that takes as inputs the predictions returned by the M meta-models of the previous level.
From a practical point of view, notice that for each meta-model of the different levels of a multi-levels stacking ensemble model, we have to choose a learning algorithm that can be almost whatever we want (even algorithms already used at lower levels). We can also mention that adding levels can either be data expensive (if k-folds like technique is not used and, then, more data are needed) or time expensive (if k-folds like technique is used and, then, lot of models need to be fitted).
The main takeaways of this post are the following:
In this post we have given a basic overview of ensemble learning and, more especially, of some of the main notions of this field: bootstrapping, bagging, random forest, boosting (adaboost, gradient boosting) and stacking. Among the notions that were left aside we can mention for example the Out-Of-Bag evaluation technique for bagging or also the very popular ""XGBoost"" (that stands for eXtrem Gradient Boosting) that is a library that implements Gradient Boosting methods along with a great number of additional tricks that make learning much more efficient (and tractable for big dataset).
Finally, we would like to conclude by reminding that ensemble learning is about combining some base models in order to obtain an ensemble model with better performances/properties. Thus, even if bagging, boosting and stacking are the most commonly used ensemble methods, variants are possible and can be designed to better adapt to some specific problems. This mainly requires two things: fully understand the problem we are facing... and be creative!
Thanks for reading!
Our last articles with Baptiste Rocca:
towardsdatascience.com
towardsdatascience.com
",1
https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5?source=tag_archive---------1-----------------------,Understanding AUC - ROC Curve,"In Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance...",Sarang Narkhede,5,"In Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance of the multi-class classification problem, we use the AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model's performance. It is also written as AUROC (Area Under the Receiver Operating Characteristics)
Note: For better understanding, I suggest you read my article about Confusion Matrix.
This blog aims to answer the following questions:
1. What is the AUC - ROC Curve?
2. Defining terms used in AUC and ROC Curve.
3. How to speculate the performance of the model?
4. Relation between Sensitivity, Specificity, FPR, and Threshold.
5. How to use AUC - ROC curve for the multiclass model?
AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.
The ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis.
An excellent model has AUC near to the 1 which means it has a good measure of separability. A poor model has an AUC near 0 which means it has the worst measure of separability. In fact, it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means the model has no class separation capacity whatsoever.
Let's interpret the above statements.
As we know, ROC is a curve of probability. So let's plot the distributions of those probabilities:
Note: Red distribution curve is of the positive class (patients with disease) and the green distribution curve is of the negative class(patients with no disease).
This is an ideal situation. When two curves don't overlap at all means model has an ideal measure of separability. It is perfectly able to distinguish between positive class and negative class.
When two distributions overlap, we introduce type 1 and type 2 errors. Depending upon the threshold, we can minimize or maximize them. When AUC is 0.7, it means there is a 70% chance that the model will be able to distinguish between positive class and negative class.
This is the worst situation. When AUC is approximately 0.5, the model has no discrimination capacity to distinguish between positive class and negative class.
When AUC is approximately 0, the model is actually reciprocating the classes. It means the model is predicting a negative class as a positive class and vice versa.
Sensitivity and Specificity are inversely proportional to each other. So when we increase Sensitivity, Specificity decreases, and vice versa.
Sensitivity, Specificity and Sensitivity, Specificity
When we decrease the threshold, we get more positive values thus it increases the sensitivity and decreasing the specificity.
Similarly, when we increase the threshold, we get more negative values thus we get higher specificity and lower sensitivity.
As we know FPR is 1 - specificity. So when we increase TPR, FPR also increases and vice versa.
TPR, FPR and TPR, FPR
In a multi-class model, we can plot the N number of AUC ROC Curves for N number classes using the One vs ALL methodology. So for example, If you have three classes named X, Y, and Z, you will have one ROC for X classified against Y and Z, another ROC for Y classified against X and Z, and the third one of Z classified against Y and X.
Thanks for Reading.
I hope I've given you some understanding of what exactly is the AUC - ROC Curve. If you like this post, a tad of extra motivation will be helpful by giving this post some claps . I am always open to your questions and suggestions. You can share this on Facebook, Twitter, Linkedin, so someone in need might stumble upon this.
You can reach me at:
LinkedIn: https://www.linkedin.com/in/narkhedesarang/
Twitter: https://twitter.com/narkhede_sarang
Github: https://github.com/TheSarang
",2
https://towardsdatascience.com/how-to-work-with-object-detection-datasets-in-coco-format-9bf4fb5848a4?source=tag_archive---------7-----------------------,How to work with object detection datasets in COCO format,"A comprehensive guide to defining, loading, exploring, and evaluating object detection datasets in COCO format using FiftyOne",Eric Hofesmann,10,"Microsoft's Common Objects in Context dataset (COCO) is the most popular object detection dataset at the moment. It is widely used to benchmark the performance of computer vision methods.
Due to the popularity of the dataset, the format that COCO uses to store annotations is often the go-to format when creating a new custom object detection dataset. While the COCO dataset also supports annotations for other tasks like segmentation, I will leave that to a future blog post. For now, we will focus only on object detection data.
The ""COCO format"" is a specific JSON structure dictating how labels and metadata are saved for an image dataset.
Many blog posts exist that describe the basic format of COCO, but they often lack detailed examples of loading and working with your COCO formatted data. This post will walk you through:
In order to do all of this, I'll be using the open-source machine learning developer tool, FiftyOne, that I have been working on. It's designed to let researchers and engineers easily work with and visualize image and video datasets with annotations and model predictions stored in various formats.
You can easily install FiftyOne through pip:
As of 06/29/2021: With support from the COCO team, COCO has been integrated into FiftyOne to make it easy to download and evaluate on the dataset. You can now specify and download the exact subset of the dataset that you want, load your own COCO-formatted data into FiftyOne, and evaluate your models with COCO-style evaluation enhanced by the visualization capabilities of FiftyOne.
See this post or this documentation for more details!
If you are new to the object detection space and are tasked with creating a new object detection dataset, then following the COCO format is a good choice due to its relative simplicity and widespread usage. This section will explain what the file and folder structure of a COCO formatted object detection dataset actually looks like.
At a high level, the COCO format defines exactly how your annotations (bounding boxes, object classes, etc) and image metadata (like height, width, image sources, etc) are stored on disk.
The folder structure of a COCO dataset looks like this:
The dataset is stored in a directory containing your raw image data and a single json file that contains all of the annotations, metadata, categories, and other information that you could possibly want to store about your dataset. If you have multiple splits of data, they would be stored in different directories with different json files.
If you were to download the COCO dataset from their website, this would be the instances_train2017.json and instances_val2017.json files. (Note: The official test set annotations are unavailable to the public)
This section will outline how to take your raw or annotated dataset and convert it to the COCO format depending on what data you currently have and the format it is in.
In this case, you already have a dataset with images and annotations but want to convert it to the COCO format.
If your dataset happens to follow a different common format that is supported by FiftyOne, like CVAT, YOLO, KITTI, Pascal VOC, TF Object detection, or others, then you can load and convert it to COCO format in a single command.
If your data is not stored in a supported format, then it is still easy to load it into FiftyOne using Python and export it in COCO format. The idea is to load each image and associated labels as a FiftyOne Sample and add them to a FiftyOne Dataset:
You can then export this dataset in COCO format with one line:
And there you have it! /path/to/coco-detection-dataset now contains your images and labels in COCO format. Check out the next section to see how to easily load it back into Python.
If you only have unlabeled images, then you will first need to generate object labels. You can generate either ground truth labels with an annotation tool or provider (like CVAT, Labelbox, MTurk, or one of many others) or predicted labels with an existing pretrained model.
If, for example, you used CVAT to annotate your raw data, then you can now convert it to COCO format using the FiftyOne command just like in the above section:
Alternatively, if you want to use a model to generate predictions, you can load your unlabeled data into FiftyOne and generate predictions with the FiftyOne Model Zoo, then save your dataset in COCO format.
This section assumes that you have gathered images and annotated them, storing your dataset in the COCO format, either following the previous section or manually building the labels JSON through custom scripting.
In order to load your COCO formatted dataset, you could write a parser for the JSON labels file, but really you should just use one of the various tools out there that will load it for you. Two of the best tools for this are the official COCO APIs and FiftyOne.
There are official COCO APIs for Python, Lua, and Matlab. These APIs are commonly used and provide basic functionality to load and compute dataset-wide evaluation on your dataset.
If you are using Python, I would recommend trying out FiftyOne, since it provides similar functionality to the cocoapi, along with a powerful API and GUI designed specifically to make it as easy as possible for you to explore, analyze, and work with your data.
If your dataset correctly follows the COCO format outlined in the previous sections, you can load it into a FiftyOne Dataset in Python with a single command:
Now that your dataset is in Python, you can use the FiftyOne API to easily access all of the different information and labels associated with your data and visualize it in the App.
To visualize your dataset, launch the FiftyOne App:
With the API, you can use aggregations to get statistics about your dataset, like the number of detections for each category:
The primary way of interacting with your dataset is through views. Every query you make will give you a different view into your dataset, like sorting by samples with the most number of objects:
You can also make a view that filters a label field based on a more complex value like small bounding box area:
The FiftyOne Brain contains various methods that allow you to analyze the quality of your ground truth data. For example, you can find the most unique samples in your dataset which can help you get a better idea of what kind of additional data you should add:
Other Brain methods can help you find possible annotation mistakes and identify hard samples you may want to train on. All of these will assist you in training better models since better models generally stem from better data.
The main reason that you want to create a COCO formatted dataset is to use it to train and test models.
Most models these days rely on your data being loaded into Python. Especially if you are using TensorFlow or PyTorch since these libraries are primarily Python-based. Using the COCO API or FiftyOne to get your dataset into Python makes it much easier to write up a PyTorch dataloader, for example, than if you had to parse the labels JSON yourself. Actually training a model on your data is out of the scope of this post but there are plenty of examples for both PyTorch object detection training and even a TensorFlow Object Detection API to help you along.
If you are just starting and want to see how some pretrained models would behave on your dataset, the easiest way to generate some predictions is with the FiftyOne Model Zoo. It contains over 70 models, many of which are object detection models.
Note: If you're tired of configuring TensorFlow/PyTorch models to use your GPU, check out my blog post on Conda.
Since your data is stored in COCO format, it can be loaded into FiftyOne, model predictions can be generated on it, and then visualized in the App:
The primary evaluation metric for object detection models is mean average precision (mAP). This is a fairly complex metric that is explained in more detail in other posts. In summary, it is computed by:
The COCO evaluation protocol introduces one additional step: mAPs are averaged across a range of 10 IoU thresholds. Additionally, COCO object detection evaluation also includes calculating the mAP for things like small, medium, and large bounding boxes, and varying thresholds of detections per image.
Note: When evaluating an object detection model, the categories of the predictions must match those of the dataset. This means you must either follow the labels of the COCO dataset to use an out-of-the-box model trained on COCO, or you must fine-tune a model on your dataset so it will predict your custom categories.
This COCO mAP value can be computed with either the COCO API or with FiftyOne. Assuming you have a dataset in COCO format with model predictions stored in the predictions field of the dataset, the following will compute the COCO mAP in FiftyOne:
Even though mAP is the most popular single value to compare model performances, this metric does have drawbacks. If you really want to know how well your model is performing, you need to dig into your data and look at model predictions on individual samples.
The best way to build intuition about how your model performs is by looking at predictions that it was confident about but got wrong. With FiftyOne, this is easy. For example, let's create a view into our dataset looking at the samples with the most false positives:
The example above is in this view and shows a crowd of objects with the false positives not annotated in the ground truth! In the COCO format, ground truth objects can have an iscrowd attribute that specifies that the bounding box is drawn around a crowd of objects. This is one of many examples where this iscrowd box is either missing or incorrectly labeled resulting in false positives.
This is something that would have been impossible to find just by looking at the mAP of the model and shows the importance of sample-level analysis to understand dataset quality.
High-quality, intentionally-curated data is critical to training great computer vision models. At Voxel51, we have over 25 years of CV/ML experience and care deeply about enabling the community to bring their AI solutions to life. That's why we developed FiftyOne, an open-source tool that helps engineers and scientists to build high-quality datasets and models.
Want to learn more? Check us out at fiftyone.ai.
Machine learning engineer at Voxel51, Masters in Computer Science from the University of Michigan. https://www.linkedin.com/in/eric-hofesmann/
",3
https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b?source=tag_archive---------6-----------------------,11 Dimensionality reduction techniques you should know in 2021,Reduce the size of your dataset while keeping as much of the variation as possible,Rukshan Pramoditha,16,"In both Statistics and Machine Learning, the number of attributes, features or input variables of a dataset is referred to as its dimensionality. For example, let's take a very simple dataset containing 2 attributes called Height and Weight. This is a 2-dimensional dataset and any observation of this dataset can be plotted in a 2D plot.
If we add another dimension called Age to the same dataset, it becomes a 3-dimensional dataset and any observation lies in the 3-dimensional space.
Likewise, real-world datasets have many attributes. The observations of those datasets lie in high-dimensional space which is hard to imagine. The following is a general geometric interpretation of a dataset related to dimensionality considered by data scientists, statisticians and machine learning engineers.
In a tabular dataset containing rows and columns, the columns represent the dimensions of the n-dimensional feature space and the rows are the data points lying in that space.
Dimensionality reduction simply refers to the process of reducing the number of attributes in a dataset while keeping as much of the variation in the original dataset as possible. It is a data preprocessing step meaning that we perform dimensionality reduction before training the model. In this article, we will discuss 11 such dimensionality reduction techniques and implement them with real-world datasets using Python and Scikit-learn libraries.
When we reduce the dimensionality of a dataset, we lose some percentage (usually 1%-15% depending on the number of components or features that we keep) of the variability in the original data. But, don't worry about losing that much percentage of the variability in the original data because dimensionality reduction will lead to the following advantages.
There are several dimensionality reduction methods that can be used with different types of data for different requirements. The following chart summarizes those dimensionality reduction methods.
There are mainly two types of dimensionality reduction methods. Both methods reduce the number of dimensions but in different ways. It is very important to distinguish between those two types of methods. One type of method only keeps the most important features in the dataset and removes the redundant features. There is no transformation applied to the set of features. Backward elimination, Forward selection and Random forests are examples of this method. The other method finds a combination of new features. An appropriate transformation is applied to the set of features. The new set of features contains different values instead of the original values. This method can be further divided into Linear methods and Non-linear methods. Non-linear methods are well known as Manifold learning. Principal Component Analysis (PCA), Factor Analysis (FA), Linear Discriminant Analysis (LDA) and Truncated Singular Value Decomposition (SVD) are examples of linear dimensionality reduction methods. Kernel PCA, t-distributed Stochastic Neighbor Embedding (t-SNE), Multidimensional Scaling (MDS) and Isometric mapping (Isomap) are examples of non-linear dimensionality reduction methods.
Let's discuss each method in detail. Please note that, for PCA and FA, I include the links to the contents of my previous work which best describe the theory and implementation of those two methods. For all other methods, I'll include the theory, Python code and visualizations within this article.
Linear methods involve linearly projecting the original data onto a low-dimensional space. We'll discuss PCA, FA, LDA and Truncated SVD under linear methods. These methods can be applied to linear data and do not perform well on non-linear data.
PCA is one of my favorite machine learning algorithms. PCA is a linear dimensionality reduction technique (algorithm) that transforms a set of correlated variables (p) into a smaller k (k<p) number of uncorrelated variables called principal components while retaining as much of the variation in the original dataset as possible. In the context of Machine Learning (ML), PCA is an unsupervised machine learning algorithm that is used for dimensionality reduction.
As this is one of my favorite algorithms, I have previously written several contents for PCA. If you're interested to learn more about the theory behind PCA and its Scikit-learn implementation, you may read the following contents written by me.
Factor Analysis (FA) and Principal Component Analysis (PCA) are both dimensionality reduction techniques. The main objective of Factor Analysis is not to just reduce the dimensionality of the data. Factor Analysis is a useful approach to find latent variables which are not directly measured in a single variable but rather inferred from other variables in the dataset. These latent variables are called factors.
If you're interested to learn more about the theory behind FA and its Scikit-learn implementation, you may read the following content written by me.
LDA is typically used for multi-class classification. It can also be used as a dimensionality reduction technique. LDA best separates or discriminates (hence the name LDA) training instances by their classes. The major difference between LDA and PCA is that LDA finds a linear combination of input features that optimizes class separability while PCA attempts to find a set of uncorrelated components of maximum variance in a dataset. Another key difference between the two is that PCA is an unsupervised algorithm whereas LDA is a supervised algorithm where it takes class labels into account.
There are some limitations of LDA. To apply LDA, the data should be normally distributed. The dataset should also contain known class labels. The maximum number of components that LDA can find is the number of classes minus 1. If there are only 3 class labels in your dataset, LDA can find only 2 (3-1) components in dimensionality reduction. It is not needed to perform feature scaling to apply LDA. On the other hand, PCA needs scaled data. However, class labels are not needed for PCA. The maximum number of components that PCA can find is the number of input features in the original dataset.
LDA for dimensionality reduction should not be confused with LDA for multi-class classification. Both cases can be implemented using the Scikit-learn LinearDiscriminantAnalysis() function. After fitting the model using fit(X, y), we use the predict(X) method of the LDA object for multi-class classification. This will assign new instances to the classes in the original dataset. We can use the transform(X) method of the LDA object for dimensionality reduction. This will find a linear combination of new features that optimizes class separability.
The following Python code describes the implementation of LDA and PCA techniques to the Iris dataset and shows the difference between the two. The original Iris dataset has four features. LDA and PCA reduce that number of features into two and enable a 2D visualization.
This method performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). It works well with sparse data in which many of the row values are zero. In contrast, PCA works well with dense data. Truncated SVD can also be used with dense data. Another key difference between truncated SVD and PCA is that factorization for SVD is done on the data matrix while factorization for PCA is done on the covariance matrix.
The Scikit-learn implementation of truncated SVD is much easy. It can be done using the TruncatedSVD() function. The following Python code describes the implementation of truncated SVD and PCA techniques to the Iris dataset.
If we're dealing with non-linear data which are frequently used in real-world applications, linear methods discussed so far do not perform well for dimensionality reduction. In this section, we'll discuss four non-linear dimensionality reduction methods that can be used with non-linear data.
Kernel PCA is a non-linear dimensionality reduction technique that uses kernels. It can also be considered as the non-linear form of normal PCA. Kernel PCA works well with non-linear datasets where normal PCA cannot be used efficiently.
The intuition behind Kernel PCA is something interesting. The data is first run through a kernel function and temporarily projects them into a new higher-dimensional feature space where the classes become linearly separable (classes can be divided by drawing a straight line). Then the algorithm uses the normal PCA to project the data back onto a lower-dimensional space. In this way, Kernel PCA transforms non-linear data into a lower-dimensional space of data which can be used with linear classifiers.
In the Kernel PCA, we need to specify 3 important hyperparameters  the number of components we want to keep, the type of kernel and the kernel coefficient (also known as the gamma). For the type of kernel, we can use 'linear', 'poly', 'rbf', 'sigmoid', 'cosine'. The rbf kernel which is known as the radial basis function kernel is the most popular one.
Now, we are going to implement an RBF kernel PCA to non-linear data which can be generated by using the Scikit-learn make_moons() function.
We can clearly see that the two classes of the above non-linear data cannot be separated by drawing a straight line.
Let's perform both PCA and Kernel PCA on the above data and see what will happen!
As you can see in the above graphs, the normal PCA cannot be able to transform non-linear data into a linear form. After applying Kernel PCA to the same data, the two classes are linearly well separated (now, classes can be divided by drawing a vertical straight line).
Here, the original data has a dimension of 2 and the plotted data also has a dimension of 2. So, has Kernel PCA actually reduced the dimensionality of data? The answer is 'Yes' because the RBF kernel function temporarily projects the 2-dimensional data into a new higher-dimensional feature space where the classes become linearly separable and then the algorithm projects that higher-dimensional data back into the 2-dimensional data which can be plotted in a 2D plot. The dimensionality reduction process has happened behind the scenes while classes become linearly separable.
One limitation of using the Kernel PCA for dimensionality reduction is that we have to specify a value for the gamma hyperparameter before running the algorithm. It requires implementing a hyperparameter tuning technique such as Grid Search to find an optimal value for the gamma. It is beyond the scope of this article. But you can get help with the hyperparameter tuning process by reading my ""k-fold cross-validation explained in plain English"".
This is also a non-linear dimensionality reduction method mostly used for data visualization. In addition to that, it is widely used in image processing and NLP. The Scikit-learn documentation recommends you to use PCA or Truncated SVD before t-SNE if the number of features in the dataset is more than 50. The following is the general syntax to perform t-SNE after PCA. Also, note that feature scaling is required before PCA.
The above code can be simplified using a Scikit-learn pipeline.
Now, we apply t-SNE to the Iris dataset. It has only 4 features. Therefore, we do not need to run PCA before t-SNE.
MDA is another non-linear dimensionality reduction technique that tries to preserve the distances between instances while reducing the dimensionality of non-linear data. There are two types of MDS algorithms: Metric and Non-metric. The MDS() class in the Scikit-learn implements both by setting the metric hyperparameter to True (for Metric type) or False (for Non-metric type).
The following code implements the MDS to the Iris dataset.
This method performs non-linear dimensionality reduction through Isometric mapping. It is an extension of MDS or Kernel PCA. It connects each instance by calculating the curved or geodesic distance to its nearest neighbors and reduces dimensionality. The number of neighbors to consider for each point can be specified through the n_neighbors hyperparameter of the Isomap() class which implements the Isomap algorithm in the Scikit-learn.
The following code implements the Isomap to the Iris dataset.
Under this category, we'll discuss 3 methods. Those methods only keep the most important features in the dataset and remove the redundant features. So, they are mainly used for feature selection. But, dimensionality reduction happens automatically while selecting the best features! Therefore, they can also be considered dimensionality reduction methods. These methods will improve models' accuracy scores or boost performance on very high-dimensional datasets.
This method eliminates (removes) features from a dataset through a recursive feature elimination (RFE) process. The algorithm first attempts to train the model on the initial set of features in the dataset and calculates the performance of the model (usually, accuracy score for a classification model and RMSE for a regression model). Then, the algorithm drops one feature (variable) at a time, trains the model on the remaining features and calculates the performance scores. The algorithm repeats eliminating features until it detects a small (or no) change in the performance score of the model and stops there!
In the Scikit-learn, backward elimination can be implemented by using the RFE() class which is in the sklearn.feature_selection module. The first parameter of that class should be a supervised learning estimator with a fit() method and a coef_ or feature_importances_ attribute. The second one should be the number of features to select. According to the Scikit-learn documentation, half of the features are selected if we do not specify the number of features to select (n_features_to_select parameter). A major limitation of this method is that we do not know the number of features to select. In those situations, it is better to run this algorithm multiple times by specifying different values for n_features_to_select.
Now, we train a Logistic Regression model on the Iris data and identify the most important features through backward feature elimination.
From the output, we can see that the recursive feature elimination (RFE) algorithm has eliminated the sepal length (cm) from the logistic regression model. sepal length (cm) is the least important feature. The remaining features contain the original values as in the initial dataset. As the plot shows, it is better to keep the other 3 features in the model.
This method can be considered as the opposite process of backward elimination. Instead of eliminating features recursively, the algorithm attempts to train the model on a single feature in the dataset and calculates the performance of the model (usually, accuracy score for a classification model and RMSE for a regression model). Then, the algorithm adds (selects) one feature (variable) at a time, trains the model on those features and calculates the performance scores. The algorithm repeats adding features until it detects a small (or no) change in the performance score of the model and stops there!
In Scikit-learn, there is no direct function for forward feature selection. Instead, we can use f_regression (for regression tasks) and f_classif (for classification tasks) classes together with the SelectKBest class. f_regression returns F-value between label/feature for regression tasks. f_classif returns ANOVA F-value between label/feature for classification tasks. Those F-values can be used in the feature selection process. The SelectKBest automatically selects features according to the highest scores based on the F-values of features. The score_func parameter of SelectKBest should be specified to f_classif or f_regression. The k parameter defines the number of top features to select.
Let's perform forward feature selection on the Iris data and identify the most important features.
From the output, we can see that the forward feature selection process has selected the sepal length (cm), petal length (cm) and petal width (cm) which have higher F-values.
To define the value for the k parameter based on a cut-off F-value, we can use the following two lines of code.
This will calculate the number of F-values greater than 50 and assign it to k. This is exactly the same as the above implementation.
Random forests is a tree-based model which is widely used for regression and classification tasks on non-linear data. It can also be used for feature selection with its built-in feature_importances_ attribute which calculates feature importance scores for each feature based on the 'gini' criterion (a measure of the quality of a split of internal nodes) while training the model. If you're interested to learn more about how random forests make predictions, you can read my ""Random forests  An ensemble of decision trees"" article.
The following Python code implements a Random Forest Classifier to the Iris data, calculates and visualizes the feature importances.
By looking at the feature importance, we can decide to drop the sepal width (cm) feature because it does not contribute enough to making the model. Let's see how!
Scikit-learn SelectFromModel selects only the features whose importance is greater or equal to the specified threshold value. The values returned by SelectFromModel can be used as the new input X for the Random Forest Classifier which is now trained only on the selected features!
This is the end of today's post. My readers can sign up for a membership through the following link to get full access to every story I write and I will receive a portion of your membership fee.
Sign-up link: https://rukshanpramoditha.medium.com/membership
Thank you so much for your continuous support! See you in the next story. Happy learning to everyone!
Special credit goes to Nika Benedictova on Unsplash, who provides me with a nice cover image for this post.
Rukshan Pramoditha2021-04-14
",4
https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3?source=tag_archive---------4-----------------------,The Time Series Transformer,Attention Is All You Need they said. Is it a more robust convolution? Is it just a hack to squeeze more learning capacity out of fewer parameters? Is it supposed to be sparse? How did the original...,Theodoros Ntakouris,6,"Attention Is All You Need they said. Is it a more robust convolution? Is it just a hack to squeeze more learning capacity out of fewer parameters? Is it supposed to be sparse? How did the original authors come up with this architecture?
It is pretty easy to switch from an existing RNN model to the Attention architecture. Inputs are of the same shape!
Using Transformers for Time Series Tasks is different than using them for NLP or Computer Vision. We neither tokenize data, nor cut them into 16x16 image chunks. Instead, we follow a more classic / old school way of preparing data for training.
One thing that is definitely true is that we have to feed data in the same value range as input, to eliminate bias. This is typically on the [0, 1] or [-1, 1] range. In general, it is recommended to apply the same kind of preprocessing pipeline on all of your input features to eliminate this bias. Individual use cases may be exempt from this, different models and data are unique! Think about the origin of your data for a moment.
Again, preprocessing decisions are tightly coupled to the problem and data at hand, but this is a nice list to get your started.
If your time series can become stationary by doing preprocessing such as seasonal decomposition, you could get good quality predictions by using smaller models (that also get trained way faster and require less code and effort), such as NeuralProphet or Tensorflow Probability.
Deep Neural Networks can learn linear and periodic components on their own, during training (we will use Time 2 Vec later). That said, I would advise against seasonal decomposition as a preprocessing step.
Other decisions such as calculating aggregates and pairwise differences, depend on the nature of your data, and what you want to predict.
Treating sequence length as a hyper parameter, this leads us to an input tensor shape that is similar to RNNs: (batch size, sequence length, features) .
Here is a drawing for all the dimensions set to 3.
For Attention to work, you need to attach the meaning of time to your input features. In the original NLP model, a collection of superimposed sinusoidal functions were added to each input embedding. We need a different representation now that our inputs are scalar values and not distinct words/tokens.
The Time 2 Vec paper comes in handy. It's a learnable and complementary, model-agnostic represetation of time. If you've studied Fourier Transforms in the past, this should be easy to understand.
Just break down each input feature to a linear component ( a line ) and as many periodic (sinusoidal) components you wish. By defining the decomposition as a function, we can make the weights learnable through back propagation.
For each input feature, we apply the same layer in a time-independent (time-distributed layer) manner. This learnable embedding does not depend on time! Finally, concatenate the original inputs.
Here is a an illustration of the learned time embeddings, which are different, for each input feature category (1 learned linear component and 1 learned periodic component per feature).
This does not mean that each time  step will carry the same embedding value, because the computation of the time2vec embeddings depend on the input values!
And, in the end, we concatenate these all together to form the input for the attention blocks.
We are going to use Multi-Head Self-Attention (setting Q, K and V to depend on the input through different dense layers/matrices). The next part is optional and depends on the scale of your model and data, but we are also going to ditch the decoder part completely. This means, that we are only going to use one or more attention block layers.
In the last part, we are going to use a few (one or more) Dense layers to predict whatever we want to predict.
Each Attention Block consists of Self Attention, Layer Normalizations and a Feed  Forward Block. The input dimensions of each block are equal to it's output dimensions.
Optionally, before the head part, you can apply some sort of pooling (Global Average 1D for example).
Things to consider when using Transformers and Attention, to get the most out of your model.
Don't go crazy with hyperparameters. Start with a single, humble attention layer, a couple of heads and a low dimension. Observe results and adjust hyper parameters accordingly  don't overfit! Scale your model along with your data. Nevertheless, nothing is stopping you from scheduling a huge hyperparameter search job :).
A crucial part of the attention mechanism that leads to greater stability is learning-rate warmup. Start with a small learnign rate and gradually increase it till you reach the base one, then decrease again. You can go crazy with exponential  decaying schedules and sophisticated formulas, but I will just give you a simple example that you should be able to understand just by reading the following code out loud:
Non-accelerated gradient descent optimization methods do not work well with Transformers. Adam is a good initial optimizer choice to train with. Keep an eye out for newer (and possibly better) optimization techniques like AdamW or NovoGrad!
Thanks for reading all the way to the end!
[1] Attention Is All You Need, Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin, 2017
[2] Time2Vec: Learning a Vector Representation of Time, Seyed Mehran Kazemi and Rishab Goel and Sepehr Eghbali and Janahan Ramanan and Jaspreet Sahota and Sanjay Thakur and Stella Wu and Cathal Smyth and Pascal Poupart and Marcus Brubaker, 2019
",5
https://netflixtechblog.com/learning-a-personalized-homepage-aa8ec670359a?source=tag_archive---------5-----------------------,Learning a Personalized Homepage,"how to best tailor each member's homepage to make it relevant, cover their interests and intents, and allow for exploration of our catalog",Netflix Technology Blog,15,"by Chris Alvino and Justin Basilico
As we've described in our previous blog posts, at Netflix we use personalization extensively and treat every situation as an opportunity to present the right content to each of our over 57 million members. The main way a member interacts with our recommendations is via the homepage, which they see when they log into Netflix on any supported device. The primary function of the homepage is to help each member easily find something to watch that they will enjoy. A problem we face is that our catalog contains many more videos than can be displayed on a single page and each member comes with their own unique set of interests. Thus, a general algorithmic challenge becomes how to best tailor each member's homepage to make it relevant, cover their interests and intents, and still allow for exploration of our catalog.
This type of problem is not unique to Netflix, it is faced by others such as news sites, search engines, and online stores. Any site that needs to choose items from a large number of available possibilities and then present them in a coherent and easy-to-navigate manner will face the same general challenges. Of course, the problem of optimizing Netflix homepages has its own unique aspects, ranging from interface constraints to differences with how movies and TV are consumed compared to other media.
Currently, the Netflix homepage on most devices is structured with videos (movies and TV shows) organized into thematically coherent rows presented in a two-dimensional layout. Members can scroll either horizontally on a row to see more videos in that row or vertically to see other rows. Thus, a key part of our personalization approach is how we choose rows to display on the homepage. This involves figuring out how to select the rows most relevant to each member, how to populate those rows with videos, and how to arrange them on the limited page area such that selecting a video to watch is intuitive. In the rest of this post, we will highlight what we think are the most relevant and interesting aspects of this problem and how we can go about solving some of them.
We organize our homepage into a series of rows to make it easy for members to navigate through a large portion of our catalog. By presenting coherent groups of videos in a row, providing a meaningful name for each row, and presenting rows in a useful order, members can quickly decide whether a whole set of videos in a row is likely to contain something that they are interested in watching. This allows members to either dive deeper and look for more videos in the theme or to skip them and look at another row. This would not be the case if, for example, the page contained a large, unorganized collection of relevant videos.
One natural way to group videos is by genre or sub-genre or other video metadata dimensions like release date. Of course, the relationship between videos in a row does not have to be due to metadata alone, but can also be formed from behavioral information (for example from collaborative filtering algorithms), videos we think a member is likely to watch, or even groups of videos watched by a friend. Thus, each row can offer a unique and personalized slice of the catalog for a member to navigate. Part of the challenge and fun of creating a personalized homepage is figuring out new ways to create useful groupings of videos, which we are constantly experimenting with (e.g., rows of titles that might be watched by one of our Netflix original characters shown above).
Once we have a set of possible video groups to consider for a page, we can begin to assemble the homepage from them. To do this, we start by finding candidate groupings that are likely relevant for a member based on the information we know about them. This also involves coming up with the evidence (or explanations) to support the presentation of a row, for example the movies that the member has previously watched in a genre. Next, we filter each group to handle concerns like maturity rating or to remove some previously watched videos. After filtering, we rank the videos in each group according to a row-appropriate ranking algorithm, which produces an ordering of videos such that the most relevant videos for the member in a group are at the front of the row. From this set of row candidates we can then apply a row selection algorithm to assemble the full page. As the page is assembled, we do additional filtering like deduplication to remove repeat videos and format rows to the appropriate size for the device.
To algorithmically create a good personalized homepage means assembling one page per member profile and device from thousands of videos that may be relevant for a member and from easily tens of thousands of potential rows, each with a variable number of videos. On top of that, we need to balance several factors that often compete for precious screen real estate. Our approach to personalization and recommendation largely focuses on helping our members find something new to watch, which we call discovery. However, we also want to make it easy for a member to watch the next episode of a show or re-watch something that they watched in the past, which normally falls outside the realm of recommendation. We want our recommendations to be accurate in that they are relevant to the tastes of our members, but they also need to be diverse so that we can address the spectrum of a member's interests versus only focusing on one. We want to be able to highlight the depth in the catalog we have in those interests and also the breadth we have across other areas to help our members explore and even find new interests. We want our recommendations to be fresh and responsive to the actions a member takes, such as watching a show, adding to their list, or rating; but we also want some stability so that people are familiar with their homepage and can easily find videos they've been recommended in the recent past. Finally, we need to be able to place task-oriented rows, such as ""My List,"" in amongst the more discovery-oriented rows.
Each device has different hardware capabilities that can limit the number of videos or rows displayed at any one time and how big the whole page can be. As such, the page generation process must be aware of the constraints of the device for which it is creating the page, including the number of rows, the minimum and maximum length of a row, the size of the visible portion of the page, and whether or not certain rows are required or are not applicable for a certain device.
While there are many challenges to page generation, tackling recommendation problems at this level also opens up new solutions. As mentioned before, selecting a diverse set of items is important in a recommendation system. However, it can be challenging to navigate a diverse ranking since the relevant items may be blended with other items that do not match someone's current intent. However, by presenting a two-dimensional navigation layout, a member can scroll vertically to easily skip over entire groups of content that may not match their current intent and then find a more relevant set, which they can then scroll horizontally to see more recommendations in that set. This allows for coherent, meaningful individual rows to be selected while maintaining the diversity of the videos shown on the whole page, and thus lets the member have both relevance and diversity.
There are several approaches for how we can build our homepage algorithmically. The most basic is a rule-based approach, which we used for a long time. Here a set of rules define a template that dictates for all members what types of rows can go in certain positions on the page. For example, the rules could specify that the first row would be Continue Watching (if any), then Top Picks (if any), then Popular on Netflix, then 5 personalized genre rows, and so on. The only personalization in this approach was from selecting candidate rows in a personalized way, such as including ""Because you watched <video>"" rows for videos someone has watched in the past and genre rows based on known genre preferences. To choose specific rows within each type, simple heuristics and sampling were used. We evolved this template using A/B testing to understand where to place rows for all members.
This approach served us well, but it ignored many aspects we consider important for the quality of the page, such as the quality of the videos in the row, the amount of diversity on the page, the affinity of members for specific kinds of rows, and the quality of the evidence we can surface for each video. It also made it hard to add new types of rows, because for a new row to succeed it would need to not only contain a relevant set of videos in a good order but also be placed appropriately in the template. Because of this, the rules for the template grew over time and became too complex to handle the variety of rows and how they should all be placed, which represented a local optimum for the member experience.
To address these issues, we can instead think of personalizing the ordering of rows on the homepage. The simplest approach for doing this is to treat rows as items in a ranking problem, which we call a row-ranking approach. For this approach, we could leverage a lot of existing recommendation or learning-to-rank approaches by developing a scoring function for rows, applying it to all the candidate rows independently, sorting by that function, and then picking the top ones to fill the page. Even though the space of rows may be relatively big, this type of approach could be relatively fast and may result in reasonable accuracy. However, doing this would lack any notion of diversity, so someone could easily get a page full of slight variations of their interests, such as many rows each with different variants of comedies: late-night, family, romantic, action, etc.
A simple way to add in diversity is to switch from a row-ranking approach to a stage-wise approach using a scoring function that considers both a row as well as its relationship to both the previous rows and the previous videos already chosen for the page. In this case, one can take a simple greedy approach and pick the row that maximizes this function as the next row to use and then re-score all the rows for the next position taking that selection into account. Depending on the diversity function, this greedy selection may not lead to an optimal page. Using a stage-wise approach with k-row lookahead could result in a more optimal page than greedy selection, but it comes with increased computational cost. Other approaches to greedily add diversity based on submodular function maximization can also be used.
However, even the stage-wise algorithm is not guaranteed to produce an optimal page because a fixed horizon may limit the ability to fill in better rows further down the page. Thus, if we can instead take a page-wise approach by defining a full-page scoring function, we can try to optimize it by choosing rows and videos appropriately to fill the page. Of course, the space of possible pages is huge, even larger than the space of possible rows. Since a page layout is defined in a discrete space, directly optimizing a function that defines the quality of the whole page is a computationally prohibitive integer programming problem.
When solving a page optimization problem with any of these approaches, there are also various constraints that need to be taken into account that were mentioned before, like deduping, filtering, and device-specific constraints. Each of these constraints add to the complexity of the optimization problem.
When forming the homepage it is also important to consider how members navigate the page, i.e., to consider which positions on the page they are likely to pay attention to and interact with in a session. Placing the most relevant videos in the positions that are most likely to be seen, which tends to be the upper-left corner, should reduce the time for a member to find something relevant to watch. However, modeling navigation on a two-dimensional page is difficult, especially taking into account that different people may navigate differently, people's navigation patterns may change over time, there are differences in navigation across different device types based on the interaction design, and that navigation is clearly dependent on the relevance of the content shown. With an accurate navigation model, we can inform better placement of videos and rows and where on the page to focus on relevance as opposed to diversity.
At the core of building a personalized page is a scoring function that can evaluate the quality of a row or a page. While we could use heuristics or intuition for building such a scoring function and tune it using A/B testing, we prefer to learn a good function from the data so that we can easily incorporate new data sources and balance the various different aspects of a homepage. To do this, we can use a machine learning approach to create the scoring function by training it using historical information of which homepages we have created for our members, what they actually see, how they interact, and what they play.
There is a large set of features that we could potentially use to represent a row for our learning algorithms. Since rows contain a set of videos, we can use any features of those videos in the row representation, either by aggregating across the row or indexing them by position. These features can be simple metadata or more useful model-based features that represent how good of a recommendation we believe a specific video is for a member. Of course, we have many different recommendation approaches, so we can include them as different features to learn an ensemble of them at the page level. We can also look at the quality of the evidence associated with the row, such as how much support there is for a member being interested in a specific genre. We can also look at past interactions with the row to see if that row or similar such rows have been consumed in the past by the member. We can also add simple descriptive features like how many videos are in a row, in what position a row is being placed on a page, or how often we've shown the row in the past. Diversity can also be additionally incorporated into the scoring model when considering the features of a row compared to the rest of the page by looking at how similar the row is to the rest of the rows or the videos in the row to the videos on the rest of the page.
While the space of potentially useful features is quite large, there are several challenges with training machine learning models for scoring rows. One challenge is dealing with presentation bias, where a member can only play from a row on the homepage that we've chosen to display, which can have a huge impact on the training data. To further complicate things, the position of a row on the page can greatly affect whether a member actually sees the row and then chooses to play from it. To handle these presentation and position biases, we need to be extremely careful about how we select training data for our algorithms. There is also a challenge around how attribution is allowed in the model; a video may have been played in a certain row in the past, but does that mean the member would have chosen that same video if it was placed in a different row but in the first position? Perhaps the title of a row being ""Critically Acclaimed Documentaries"" was responsible for play where it may not have been selected without that additional evidence, for example, in a ""New Releases"" row, even if it was in a better position. Learning over features to represent diversity can also be challenging because while the space of potential rows at different positions on the page is large, when the rest of the page (or the already chosen rows) is taken into account for diversity, the space of possible pages is even larger.
To deal with these challenges, as with any algorithmic approach, choosing a good metric is important. Of fundamental importance in page generation is how to evaluate the quality of the pages produced by a specific algorithm during offline experimentation. While we ultimately will test any potential algorithmic improvement online in an A/B test, we would like to be able to focus our precious A/B testing resources on algorithms that we have evidence are likely to improve the quality of the pages. We also need to be able to tune the parameters of those algorithms before A/B testing. To do this, we can use historical data to generate hypothetical pages from new algorithmic approaches, provided we can choose a good metric for page quality.
To come up with page-level quality metrics, we took inspiration from ranking metrics that are common in information retrieval (many of which exist in the literature) for a one-dimensional list and created ones that work over a two-dimensional layout. For instance, consider a simple metric like Recall@n, which measures the number of relevant items in the top n divided by the total number of relevant items. We can extend it in two dimensions to be Recall@m-by-n, where now we count the number of relevant items in first m rows and n columns on the page divided by the total number of relevant items. Thus, Recall@3-by-4 may represent quality of videos displayed in the viewport on a device that initially can show 3 rows and 4 videos at a time. One nice property of recall defined this way is that it automatically can handle corner-cases like duplicate videos or short rows. We can also hold one of the values n (or m) fixed and sweep across the other to calculate, for instance, how the recall increases in the viewport as the member would scroll down the page.
Of course, Recall is a basic metric and requires choosing values for m and n, but we can likewise extend metrics that assign a score or likelihood for a member seeing a position, like NDCG or MRR, to the two-dimensional case. We can also adapt navigation models like Expected Reciprocal Rank to incorporate two-dimensional navigation through the page and take into account the cascading aspect of browsing. With such page-level metrics defined, we can use them to evaluate changes in any of the algorithmic approaches used to generate the page, not just the algorithms for ordering the rows, but also the selection, filtering, and ranking algorithms, or any of the input data that they use.
There is no shortage of challenging questions that come up in engineering the homepage. For example: When is it appropriate to take into account other context variables such as the time of the day or device, in how we populate the homepages? How do we find the appropriate trade-off between finding the optimal page and computational cost? How do we form the home pages during the critical first few sessions of a member, precisely at the time when we have the least information about them? We need to think about and weigh the importance of each of these questions every day in order to continually improve the Netflix homepages.
While Netflix may be most famous in the recommendations community for the Netflix prize, we think of personalized page generation as the next step in the evolution of our personalization approach from rating prediction to video ranking to now page generation. We have taken the initial step of coming up with our first algorithm for personalized page generation that showed significantly better online performance than our existing template, and deployed it last year. However, personalized page generation is a challenging problem that involves balancing a multitude of factors, and we think that this is just the beginning. There is a lot of potential to improve the homepages for all of our members and help them easily find content they will love.
We are always looking for talented researchers and engineers to join our team. So if you are interested in helping us solve these types of problems and increasing global happiness, please take a look at some of our open positions on the Netflix jobs page.
Originally published at techblog.netflix.com on April 9, 2015.
Learn about Netflix's world class engineering efforts...
1.2K 
6
1.2K claps
1.2K 
",6
https://towardsdatascience.com/6-data-science-certificates-to-level-up-your-career-275daed7e5df?source=tag_archive---------7-----------------------,6 Data Science Certificates To Level Up Your Career,Pump up your portfolio and get closer to your dream job,Sara A. Metwalli,6,"Because of the appeal of the field of data science and the premise of high incomes, more and more people decide to join the field every day. Some may come from a technical background, while others just join in due to curiosity; regardless of the reason you decide to join the field, your no.1 goal will probably be to have a strong, solid portfolio that can help you land the job you want.
So, how can you increase the appeal of your portfolio?
Although getting into data science doesn't necessarily require any degrees or certificates, sometimes having some could help make you stand out in the applicants pool when applying for a job.
towardsdatascience.com
What makes a good data science portfolio is collecting projects that show your skills, prove your knowledge, and demonstrate your ability to build solid data science projects. That's the core of a good portfolio, but you can also add some certificates to prove that you put in the time, effort, and money to hone your skills and become a more qualified data scientist.
Luckily, not all certificates you can get require you to go to a testing center. In fact, most of the desirable data science certificates can be taken from the comfort of your couch.
This article will present you with 6 highly desirable certificates that you can obtain to increase your chances of landing an internship or your dream job.
Microsoft is one of the leading names of technology and software; they offer a certificate that aims to measure your ability to run experiments, train machine learning models, optimize your model's performance, and deploy it using the Azure Machine Learning workspace.
To obtain this certificate, you will need to pass one exam, and you can prepare for this exam in one of two ways. Microsoft offers free online materials that you can self-study to prepare for the exam. If you prefer having an instructor, they also offer a paid option where an Azure machine learning instructor can tutor you.
This exam will cost around 165$. The price varies based on the country you will proctor the test from.
towardsdatascience.com
This certificate comes from IBM and is offered at the end of a course series that takes you from being a complete data science beginner to a professional data scientist online and at your own pace.
IBM Data science professional certificate is offered on both Coursera and edX. On either platform, you have to complete a set of courses covering all the core knowledge of data science to get the certificate and an IBM badge once you're done.
To get the certificate from Coursera, you will need to pay a fee of 39$ per month, so the sooner you can finish the series, the less you will need to pay. On the other hand, edX requires 793$ for the full course experience regardless of how long you will talk to complete it.
Google's professional data engineer certification is aimed to examine the skills you need to be qualified as a data engineer. A data engineer can make data-driven decisions, build reliable models, train, test and optimize them.
You can get this certificate by applying directly through the official Google certificate page, or you can take a course series and the certificate on Coursera. The courses will teach you all you need to know about machine learning and AI fundamentals and build efficient data pipelines and analytics.
To access the course series on Coursera, you will need to have Coursera Plus or pay a fee of 49$ per month for as long as you need to finish the series and obtain your certificate.
towardsdatascience.com
Cloudera targets open-source developers and offers the CCP Data Engineer certificate for developers to test their ability to collect, process, and analyze data efficiently on the Cloudera CDH environment.
To pass this exam, you will be given 5~10 data science problems, each with its own large dataset and CDH cluster. Your task will be to find a high-precision solution for each of these problems and implement it correctly.
To take this exam, you will need to score at least 70% in the exam. The exam will be 4 hours long and will cost you 400$. You can take this exam anywhere online.
Unlike the certificates we discussed so far, the SAS AI & Machine Learning Professional certificate is acquired by passing three exams that test three different skill sets. The three exams you will need to pass to get the certificate are:
SAS offers a free 30 days worth of preparation materials that will get you ready to take and pass each of these three exams.
towardsdatascience.com
TensorFlow is one of the widely used packages for machine learning, AI, and deep learning applications. The TensorFlow Developer Certificate is given to a developer to demonstrate their ability to use TensorFlow to develop solutions for machine learning and deep learning problems.
You can prepare for this certificate by finishing the DeepLearning.AI TensorFlow Developer Professional Certificate Coursera course series. Once you have earned this certificate, your name and picture will be added to the Google Developers webpage.
The TensorFlow Developer Certificate is valid for 3 years. Afterward, you will need to retake the test to keep your skill level synced with the TensorFlow package's recent updates.
If you ask any data scientist whether they needed their degree or certificate to land their job roles, most will tell you that they got into data science from a non-technical background with a curious mind that only wanted to learn more.
And even though you can become a data scientist and get a good job by self-studying the core concepts of data science and building real-life-sized projects or projects that can be applied easily to real-life data, sometimes having a certificate can help make your portfolio stand out and attract the eyes of recruiters to you.
Because data science is one of the popular fields today, you will find a redundant amount of tutorials and guides online on what you need to do to become a ""good data scientist"" or ""how to land your dream data science role?"". Not to mention the tons of certificates that you can get and free courses you can take to improve your skills.
towardsdatascience.com
I have been where you are, overwhelmed by the amount of information out there about data science and how to get into the field. But, I always appreciated simple, straightforward articles that get to the point without dragging the topic too long.
My purpose in writing this article is to give anyone looking to obtain a data science certificate to prove their ability to start and some ideas of what certificates are considered valuable. It will definitely add some value to your resume or portfolio.
",7
https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34?source=tag_archive---------9-----------------------,"Transformers Explained Visually (Part 2): How it works, step-by-step","A Gentle Guide to the Transformer under the hood, and its end-to-end operation.",Ketan Doshi,11,"This is the second article in my series on Transformers. In the first article, we learned about the functionality of Transformers, how they are used, their high-level architecture, and their advantages.
In this article, we can now look under the hood and study exactly how they work in detail. We'll see how data flows through the system with their actual matrix representations and shapes and understand the computations performed at each stage.
Here's a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.
And if you're interested in NLP applications in general, I have some other articles you might like.
As we saw in Part 1, the main components of the architecture are:
Data inputs for both the Encoder and Decoder, which contains:
The Encoder stack contains a number of Encoders. Each Encoder contains:
The Decoder stack contains a number of Decoders. Each Decoder contains:
Output (top right)  generates the final output, and contains:
To understand what each component does, let's walk through the working of the Transformer while we are training it to solve a translation problem. We'll use one sample of our training data which consists of an input sequence ('You are welcome' in English) and a target sequence ('De nada' in Spanish).
Like any NLP model, the Transformer needs two things about each word  the meaning of the word and its position in the sequence.
The Transformer combines these two encodings by adding them.
The Transformer has two Embedding layers. The input sequence is fed to the first Embedding layer, known as the Input Embedding.
The target sequence is fed to the second Embedding layer after shifting the targets right by one position and inserting a Start token in the first position. Note that, during Inference, we have no target sequence and we feed the output sequence to this second layer in a loop, as we learned in Part 1. That is why it is called the Output Embedding.
The text sequence is mapped to numeric word IDs using our vocabulary. The embedding layer then maps each input word into an embedding vector, which is a richer representation of the meaning of that word.
Since an RNN implements a loop where each word is input sequentially, it implicitly knows the position of each word.
However, Transformers don't use RNNs and all words in a sequence are input in parallel. This is its major advantage over the RNN architecture, but it means that the position information is lost, and has to be added back in separately.
Just like the two Embedding layers, there are two Position Encoding layers. The Position Encoding is computed independently of the input sequence. These are fixed values that depend only on the max length of the sequence. For instance,
These constants are computed using the formula below, where
In other words, it interleaves a sine curve and a cos curve, with sine values for all even indexes and cos values for all odd indexes. As an example, if we encode a sequence of 40 words, we can see below the encoding values for a few (word position, encoding_index) combinations.
The blue curve shows the encoding of the 0th index for all 40 word-positions and the orange curve shows the encoding of the 1st index for all 40 word-positions. There will be similar curves for the remaining index values.
As we know, deep learning models process a batch of training samples at a time. The Embedding and Position Encoding layers operate on matrices representing a batch of sequence samples. The Embedding takes a (samples, sequence length) shaped matrix of word IDs. It encodes each word ID into a word vector whose length is the embedding size, resulting in a (samples, sequence length, embedding size) shaped output matrix. The Position Encoding uses an encoding size that is equal to the embedding size. So it produces a similarly shaped matrix that can be added to the embedding matrix.
The (samples, sequence length, embedding size) shape produced by the Embedding and Position Encoding layers is preserved all through the Transformer, as the data flows through the Encoder and Decoder Stacks until it is reshaped by the final Output layers.
This gives a sense of the 3D matrix dimensions in the Transformer. However, to simplify the visualization, from here on we will drop the first dimension (for the samples) and use the 2D representation for a single sample.
The Input Embedding sends its outputs into the Encoder. Similarly, the Output Embedding feeds into the Decoder.
The Encoder and Decoder Stacks consists of several (usually six) Encoders and Decoders respectively, connected sequentially.
The first Encoder in the stack receives its input from the Embedding and Position Encoding. The other Encoders in the stack receive their input from the previous Encoder.
The Encoder passes its input into a Multi-head Self-attention layer. The Self-attention output is passed into a Feed-forward layer, which then sends its output upwards to the next Encoder.
Both the Self-attention and Feed-forward sub-layers, have a residual skip-connection around them, followed by a Layer-Normalization.
The output of the last Encoder is fed into each Decoder in the Decoder Stack as explained below.
The Decoder's structure is very similar to the Encoder's but with a couple of differences.
Like the Encoder, the first Decoder in the stack receives its input from the Output Embedding and Position Encoding. The other Decoders in the stack receive their input from the previous Decoder.
The Decoder passes its input into a Multi-head Self-attention layer. This operates in a slightly different way than the one in the Encoder. It is only allowed to attend to earlier positions in the sequence. This is done by masking future positions, which we'll talk about shortly.
Unlike the Encoder, the Decoder has a second Multi-head attention layer, known as the Encoder-Decoder attention layer. The Encoder-Decoder attention layer works like Self-attention, except that it combines two sources of inputs  the Self-attention layer below it as well as the output of the Encoder stack.
The Self-attention output is passed into a Feed-forward layer, which then sends its output upwards to the next Decoder.
Each of these sub-layers, Self-attention, Encoder-Decoder attention, and Feed-forward, have a residual skip-connection around them, followed by a Layer-Normalization.
In Part 1, we talked about why Attention is so important while processing sequences. In the Transformer, Attention is used in three places:
The Attention layer takes its input in the form of three parameters, known as the Query, Key, and Value.
The Transformer calls each Attention processor an Attention Head and repeats it several times in parallel. This is known as Multi-head attention. It gives its Attention greater power of discrimination, by combining several similar Attention calculations.
The Query, Key, and Value are each passed through separate Linear layers, each with their own weights, producing three results called Q, K, and V respectively. These are then combined together using the Attention formula as shown below, to produce the Attention Score.
The important thing to realize here is that the Q, K, and V values carry an encoded representation of each word in the sequence. The Attention calculations then combine each word with every other word in the sequence, so that the Attention Score encodes a score for each word in the sequence.
When discussing the Decoder a little while back, we briefly mentioned masking. The Mask is also shown in the Attention diagrams above. Let's see how it works.
While computing the Attention Score, the Attention module implements a masking step. Masking serves two purposes:
In the Encoder Self-attention and in the Encoder-Decoder-attention: masking serves to zero attention outputs where there is padding in the input sentences, to ensure that padding doesn't contribute to the self-attention. (Note: since input sequences could be of different lengths they are extended with padding tokens like in most NLP applications so that fixed-length vectors can be input to the Transformer.)
Similarly for the Encoder-Decoder attention.
In the Decoder Self-attention: masking serves to prevent the decoder from 'peeking' ahead at the rest of the target sentence when predicting the next word.
The Decoder processes words in the source sequence and uses them to predict the words in the destination sequence. During training, this is done via Teacher Forcing, where the complete target sequence is fed as Decoder inputs. Therefore, while predicting a word at a certain position, the Decoder has available to it the target words preceding that word as well as the target words following that word. This allows the Decoder to 'cheat' by using target words from future 'time steps'.
For instance, when predicting 'Word 3', the Decoder should refer only to the first 3 input words from the target but not the fourth word 'Ketan'.
Therefore, the Decoder masks out input words that appear later in the sequence.
When calculating the Attention Score (refer to the picture earlier showing the calculations) masking is applied to the numerator just before the Softmax. The masked out elements (white squares) are set to negative infinity, so that Softmax turns those values to zero.
The last Decoder in the stack passes its output to the Output component which converts it into the final output sentence.
The Linear layer projects the Decoder vector into Word Scores, with a score value for each unique word in the target vocabulary, at each position in the sentence. For instance, if our final output sentence has 7 words and the target Spanish vocabulary has 10000 unique words, we generate 10000 score values for each of those 7 words. The score values indicate the likelihood of occurrence for each word in the vocabulary in that position of the sentence.
The Softmax layer then turns those scores into probabilities (which add up to 1.0). In each position, we find the index for the word with the highest probability, and then map that index to the corresponding word in the vocabulary. Those words then form the output sequence of the Transformer.
During training, we use a loss function such as cross-entropy loss to compare the generated output probability distribution to the target sequence. The probability distribution gives the probability of each word occurring in that position.
Let's assume our target vocabulary contains just four words. Our goal is to produce a probability distribution that matches our expected target sequence ""De nada END"".
This means that the probability distribution for the first word-position should have a probability of 1 for ""De"" with probabilities for all other words in the vocabulary being 0. Similarly, ""nada"" and ""END"" should have a probability of 1 for the second and third word-positions respectively.
As usual, the loss is used to compute gradients to train the Transformer via backpropagation.
Hopefully, this gives you a feel for what goes on inside the Transformer during Training. As we discussed in the previous article, it runs in a loop during Inference but most of the processing remains the same.
The Multi-head Attention module is what gives the Transformer its power. In the next article, we will continue our journey and go one step deeper to really understand the details of how Attention is computed.
And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Let's keep learning!
",8
https://medium.com/coders-camp/60-python-projects-with-source-code-919cd8a6e512?source=tag_archive---------0-----------------------,60 Python Projects with Source Code,60 Python Projects with Source code solved and explained for free,Aman Kharwal,2,"Python has been in the top 10 popular programming languages for a long time, as the community of Python programmers has grown a lot due to its easy syntax and library support. In this article, I will introduce you to 60 amazing Python projects with source code solved and explained for free.
If you're a newbie to Python where you've just learned lists, tuples, dictionaries, and some basic Python modules like the random module, here are some Python projects with source code for beginners for you:
If you have learned the fundamental Python libraries and some of the external libraries, you should now know how to install external libraries and work with them. So if you are at that level now, you can work on all the advanced Python projects with source code mentioned below:
So these were some very useful Python projects with source code for both a beginner and someone in advance level of Python. I hope you liked this article on Python Projects with source code solved and explained. Feel free to ask your valuable questions in the comments section below.
From Hello, World to Building Robots
1.2K 
9
",9
https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d?source=tag_archive---------0-----------------------,Geometric foundations of Deep Learning,Geometric Deep Learning is an attempt to unify a broad class of ML problems from the perspectives of symmetry and invariance.,Michael Bronstein,13,"This blog post was co-authored with Joan Bruna, Taco Cohen, and Petar Velickovic and is based on the new ""proto-book"" M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic, Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (2021), Petar's talk at Cambridge and Michael's keynote talk at ICLR 2021.
In October 1872, the philosophy faculty of a small university in the Bavarian city of Erlangen appointed a new young professor. As customary, he was requested to deliver an inaugural research programme, which he published under the somewhat long and boring title Vergleichende Betrachtungen uber neuere geometrische Forschungen (""A comparative review of recent researches in geometry""). The professor was Felix Klein, only 23 years of age at that time, and his inaugural work has entered the annals of mathematics as the ""Erlangen Programme"" [1].
The nineteenth century had been remarkably fruitful for geometry. For the first time in nearly two thousand years after Euclid, the construction of projective geometry by Poncelet, hyperbolic geometry by Gauss, Bolyai, and Lobachevsky, and elliptic geometry by Riemann showed that an entire zoo of diverse geometries was possible. However, these constructions had quickly diverged into independent and unrelated fields, with many mathematicians of that period questioning how the different geometries are related to each other and what actually defines a geometry.
The breakthrough insight of Klein was to approach the definition of geometry as the study of invariants, or in other words, structures that are preserved under a certain type of transformations (symmetries). Klein used the formalism of group theory to define such transformations and use the hierarchy of groups and their subgroups in order to classify different geometries arising from them. Thus, the group of rigid motions leads to the traditional Euclidean geometry, while affine or projective transformations produce, respectively, the affine and projective geometries. Importantly, the Erlangen Programme was limited to homogeneous spaces [2] and initially excluded Riemannian geometry.
The impact of the Erlangen Program on geometry and mathematics broadly was very profound. It also spilled to other fields, especially physics, where symmetry considerations allowed to derive conservation laws from the first principles  an astonishing result known as Noether's Theorem [3]. It took several decades until this fundamental principle  through the notion of gauge invariance (in its generalised form developed by Yang and Mills in 1954)  proved successful in unifying all the fundamental forces of nature with the exception of gravity. This is what is called the Standard Model and it describes all the physics we currently know. We can only repeat the words of a Nobel-winning physicist, Philip Anderson [4], that
""it is only slightly overstating the case to say that physics is the study of symmetry.''
We believe that the current state of affairs in the field of deep (representation) learning is reminiscent of the situation of geometry in the nineteenth century: on the one hand, in the past decade, deep learning has brought a revolution in data science and made possible many tasks previously thought to be beyond reach  whether computer vision, speech recognition, natural language translation, or playing Go. On the other hand, we now have a zoo of different neural network architectures for different kinds of data, but few unifying principles. As a consequence, it is difficult to understand the relations between different methods, which inevitably leads to the reinvention and re-branding of the same concepts.
Geometric Deep Learning is an umbrella term we introduced in [5] referring to recent attempts to come up with a geometric unification of ML similar to Klein's Erlangen Programme. It serves two purposes: first, to provide a common mathematical framework to derive the most successful neural network architectures, and second, give a constructive procedure to build future architectures in a principled way.
Supervised machine learning in its simplest setting is essentially a function estimation problem: given the outputs of some unknown function on a training set (e.g. labelled dog and cat images), one tries to find a function f from some hypothesis class that fits well the training data and allows to predict the outputs on previously unseen inputs. In the past decade, the availability of large, high-quality datasets such as ImageNet coincided with growing computational resources (GPUs), allowing to design rich function classes that have the capacity to interpolate such large datasets.
Neural networks appear to be a suitable choice to represent functions, because even the simplest architecture like the Perceptron can produce a dense class of functions when using just two layers, allowing to approximate any continuous function to any desired accuracy  a property known as Universal Approximation [6].
The setting of this problem in low-dimensions is a classical problem in approximation theory that has been studied extensively, with precise mathematical control of estimation errors. But the situation is entirely different in high dimensions: one can quickly see that in order to approximate even a simple class of e.g. Lipschitz continuous functions the number of samples grows exponentially with the dimension  a phenomenon known colloquially as the ""curse of dimensionality"". Since modern machine learning methods need to operate with data in thousands or even millions of dimensions, the curse of dimensionality is always there behind the scenes making such a naive approach to learning impossible.
This is perhaps best seen in computer vision problems like image classification. Even tiny images tend to be very high-dimensional, but intuitively they have a lot of structure that is broken and thrown away when one parses the image into a vector to feed it into the Perceptron. If the image is now shifted by just one pixel, the vectorised input will be very different, and the neural network will need to be shown a lot of examples in order to learn that shifted inputs must be classified in the same way [7].
Fortunately, in many cases of high-dimensional ML problems we have an additional structure that comes from the geometry of the input signal. We call this structure a ""symmetry prior"" and it is a general powerful principle that gives us optimism in dimensionality-cursed problems. In our example of image classification, the input image x is not just a d-dimensional vector, but a signal defined on some domain , which in this case is a two-dimensional grid. The structure of the domain is captured by a symmetry group G  the group of 2D translations in our example  which acts on the points on the domain. In the space of signals X(), the group actions (elements of the group, gG) on the underlying domain are manifested through what is called the group representation (g)  in our case, it is simply the shift operator, a dd matrix that acts on a d-dimensional vector [8].
The geometric structure of the domain underlying the input signal imposes structure on the class of functions f that we are trying to learn. One can have invariant functions that are unaffected by the action of the group, i.e., f((g)x)=f(x) for any gG and x. On the other hand, one may have a case where the function has the same input and output structure and is transformed in the same way as the inputsuch functions are called equivariant and satisfy f((g)x)=(g)f(x) [9]. In the realm of computer vision, image classification is a good illustration of a setting where one would desire an invariant function (e.g. no matter where a cat is located in the image, we still want to classify it as a cat), while image segmentation, where the output is a pixel-wise label mask, is an example of an equivariant function (the segmentation mask should follow the transformation of the input image).
Another powerful geometric prior is ""scale separation"". In some cases, we can construct a multiscale hierarchy of domains ( and ' in the figure below) by ""assimilating"" nearby points and producing also a hierarchy of signal spaces that are related by a coarse-graining operator P. On these coarse scales, we can apply coarse-scale functions. We say that a function f is locally stable if it can be approximated as the composition of the coarse-graining operator P and the coarse-scale function, ff'P. While f might depend on long-range dependencies, if it is locally stable, these can be separated into local interactions that are then propagated towards the coarse scales [10].
These two principles give us a very general blueprint of Geometric Deep Learning that can be recognised in the majority of popular deep neural architectures used for representation learning: a typical design consists of a sequence of equivariant layers (e.g. convolutional layers in CNNs), possibly followed by an invariant global pooling layer aggregating everything into a single output. In some cases, it is also possible to create a hierarchy of domains by some coarsening procedure that takes the form of local pooling.
This is a very general design that can be applied to different types of geometric structures, such as grids, homogeneous spaces with global transformation groups, graphs (and sets, as a particular case), and manifolds, where we have global isometry invariance and local gauge symmetries. The implementation of these principles leads to some of the most popular architectures that exist today in deep learning: Convolutional Networks (CNNs), emerging from translational symmetry, Graph Neural Networks, DeepSets [11], and Transformers [12], implementing permutation invariance, gated RNNs (such as LSTM networks) that are invariant to time warping [13], and Intrinsic Mesh CNNs [14] used in computer graphics and vision, that can be derived from gauge symmetry.
In future posts, we will be exploring in further detail the instances of the Geometric Deep Learning blueprint on the ""5G"" [15]. As a final note, we should emphasize that symmetry has historically been a key concept in many fields of science, of which physics, as already mentioned in the beginning, is key. In the machine learning community, the importance of symmetry has long been recognised in particular in the applications to pattern recognition and computer vision, with early works on equivariant feature detection dating back to Shun'ichi Amari [16] and Reiner Lenz [17]. In the neural networks literature, the Group Invariance Theorem for Perceptrons by Marvin Minsky and Seymour Papert [18] put fundamental limitations on the capabilities of (single-layer) perceptrons to learn invariants. This was one of the primary motivations for studying multi-layer architectures [19-20], which had ultimately led to deep learning.
[1] According to a popular belief, repeated in many sources including Wikipedia, the Erlangen Programme was delivered in Klein's inaugural address in October 1872. Klein indeed gave such a talk (though on December 7, 1872), but it was for a non-mathematical audience and concerned primarily his ideas of mathematical education. What is now called the ""Erlangen Programme"" was actually the aforementioned brochure Vergleichende Betrachtungen, subtitled Programm zum Eintritt in die philosophische Fakultat und den Senat der k. Friedrich-Alexanders-Universitat zu Erlangen (""Program for entry into the Philosophical Faculty and the Senate of the Emperor Friedrich-Alexander University of Erlangen"", see an English translation). While Erlangen claims the credit, Klein stayed there for only three years, moving in 1875 to the Technical University of Munich (then called Technische Hochschule), followed by Leipzig (1880), and finally settling down in Gottingen from 1886 until his retirement. See R. Tobies Felix Klein  Mathematician, Academic Organizer, Educational Reformer (2019) In: H. G. Weigand et al. (eds) The Legacy of Felix Klein, Springer.
[2] A homogeneous space is a space where ""all points are the same"" and any point can be transformed into another by means of group action. This is the case for all geometries proposed before Riemann, including Euclidean, affine, and projective, as well as the first non-Euclidean geometries on spaces of constant curvature such as the sphere or hyperbolic space. It took substantial effort and nearly 50 years, notably by Elie Cartan and the French geometry school, to extend Klein's ideas to manifolds.
[3] Klein himself has probably anticipated the potential of his ideas in physics, complaining of ""how persistently the mathematical physicist disregards the advantages afforded him in many cases by only a moderate cultivation of the projective view"". By that time, it was already common to think of physical systems through the perspective of the calculus of variation, deriving the differential equations governing such systems from the ""least action principle"", i.e. as the minimiser of some functional (action). In a paper published in 1918, Emmy Noether showed that every (differentiable) symmetry of the action of a physical system has a corresponding conservation law. This, by all means, was a stunning result: beforehand, meticulous experimental observation was required to discover fundamental laws such as the conservation of energy, and even then, it was an empirical result not coming from anywhere. For historical notes, see C. Quigg, Colloquium: A Century of Noether's Theorem (2019), arXiv:1902.01989.
[4] P. W. Anderson, More is different (1972), Science 177(4047):393-396.
[5] M. M. Bronstein et al. Geometric deep learning: going beyond Euclidean data (2017), IEEE Signal Processing Magazine 34(4):18-42 attempted to unify learning on grids, graphs, and manifolds from the spectral analysis perspective. The term ""geometric deep learning"" was actually coined earlier, in Michael's ERC grant proposal.
[6] There are multiple versions of the Universal Approximation theorem. It is usually credited to G. Cybenko, Approximation by superpositions of a sigmoidal function (1989) Mathematics of Control, Signals, and Systems 2(4):303-314 and K. Hornik, Approximation capabilities of multilayer feedforward networks (1991), Neural Networks 4(2):251-257.
[7] The remedy for this problem in computer vision came from classical works in neuroscience by Hubel and Wiesel, the winners of the Nobel prize in medicine for the study of the visual cortex. They showed that brain neurons are organised into local receptive fields, which served as an inspiration for a new class of neural architectures with local shared weights, first the Neocognitron in K. Fukushima, A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position (1980), Biological Cybernetics 36(4):193-202, and then the Convolutional Neural Networks, the seminar work of Y. LeCun et al., Gradient-based learning applied to document recognition (1998), Proc. IEEE 86(11):2278-2324, where weight sharing across the image effectively solved the curse of dimensionality.
[8] Note that a group is defined as an abstract object, without saying what the group elements are (e.g. transformations of some domain), only how they compose. Hence, very different kinds of objects may have the same symmetry group.
[9] These results can be generalised for the case of approximately invariant and equivariant functions, see e.g. J. Bruna and S. Mallat, Invariant scattering convolution networks (2013), Trans. PAMI 35(8):1872-1886.
[10] Scale separation is a powerful principle exploited in physics, e.g. in the Fast Multipole Method (FMM), a numerical technique originally developed to speed up the calculation of long-range forces in n-body problems. FMM groups sources that lie close together and treats them as a single source.
[11] M. Zaheer et al., Deep Sets (2017), NIPS. In the computer graphics community, a similar architecture was proposed in C. R. Qi et al., PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017), CVPR.
[12] A. Vaswani et al., Attention is all you need (2017), NIPS, introduced the now popular Transformer architecture. It can be considered as a graph neural network with a complete graph.
[13] C. Tallec and Y. Ollivier, Can recurrent neural networks warp time? (2018), arXiv:1804.11188.
[14] J. Masci et al., Geodesic convolutional neural networks on Riemannian manifolds (2015), arXiv:1501.06297 was the first convolutional-like neural network architecture with filters applied in local coordinate charts on meshes. It is a particular case of T. Cohen et al., Gauge Equivariant Convolutional Networks and the Icosahedral CNN (2019), arXiv:1902.04615.
[15] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic, Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (2021)
[16] S.-l. Amari, Feature spaces which admit and detect invariant signal transformations (1978), Joint Conf. Pattern Recognition. Amari is also famous as the pioneer of the field of information geometry, which studies statistical manifolds of probability distributions using tools of differential geometry.
[17] R. Lenz, Group theoretical methods in image processing (1990), Springer.
[18] M. Minsky and S. A Papert. Perceptrons: An introduction to computational geometry (1987), MIT Press. This is the second edition of the (in)famous book blamed for the first ""AI winter"", which includes additional results and responds to some of the criticisms of the earlier 1969 version.
[19] T. J. Sejnowski, P. K. Kienker, and G. E. Hinton, Learning symmetry groups with hidden units: Beyond the perceptron (1986), Physica D:Nonlinear Phenomena 22(1-3):260-275
[20] J. Shawe-Taylor, Building symmetries into feedforward networks (1989), ICANN. The first work that can be credited with taking a representation-theoretical view on invariant and equivariant neural networks is J. Wood and J. Shawe-Taylor, Representation theory and invariant neural networks (1996), Discrete Applied Mathematics 69(1-2):33-60. In the ""modern era"" of deep learning, building symmetries into neural networks was done by R. Gens and P. M. Domingos, Deep symmetry networks (2014), NIPS (see also Pedro Domingos' invited talk at ICLR 2014)
We are grateful to Ben Chamberlain for proofreading this post and to Yoshua Bengio, Charles Blundell, Andreea Deac, Fabian Fuchs, Francesco di Giovanni, Marco Gori, Raia Hadsell, Will Hamilton, Maksym Korablyov, Christian Merkwirth, Razvan Pascanu, Bruno Ribeiro, Anna Scaife, Jurgen Schmidhuber, Marwin Segler, Corentin Tallec, Ngan Vu, Peter Wirnsberger, and David Wong for their feedback on different parts of the text on which this post is based. We also that Xiaowen Dong and Pietro Lio for helping us break the ""stage fright"" and present early versions of our work.
See additional information on the project webpage, Towards Data Science Medium posts, and follow Michael, Joan, Taco, and Petar on Twitter.
",10
https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761?source=tag_archive---------7-----------------------,Machine Learning Basics with the K-Nearest Neighbors Algorithm,"The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both...",Onel Harrison,9,"The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. Pause! Let us unpack that.
A supervised machine learning algorithm (as opposed to an unsupervised machine learning algorithm) is one that relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data.
Imagine a computer is a child, we are its supervisor (e.g. parent, guardian, or teacher), and we want the child (computer) to learn what a pig looks like. We will show the child several different pictures, some of which are pigs and the rest could be pictures of anything (cats, dogs, etc).
When we see a pig, we shout ""pig!"" When it's not a pig, we shout ""no, not pig!"" After doing this several times with the child, we show them a picture and ask ""pig?"" and they will correctly (most of the time) say ""pig!"" or ""no, not pig!"" depending on what the picture is. That is supervised machine learning.
Supervised machine learning algorithms are used to solve classification or regression problems.
A classification problem has a discrete value as its output. For example, ""likes pineapple on pizza"" and ""does not like pineapple on pizza"" are discrete. There is no middle ground. The analogy above of teaching a child to identify a pig is another example of a classification problem.
This image shows a basic example of what classification data might look like. We have a predictor (or set of predictors) and a label. In the image, we might be trying to predict whether someone likes pineapple (1) on their pizza or not (0) based on their age (the predictor).
It is standard practice to represent the output (label) of a classification algorithm as an integer number such as 1, -1, or 0. In this instance, these numbers are purely representational. Mathematical operations should not be performed on them because doing so would be meaningless. Think for a moment. What is ""likes pineapple"" + ""does not like pineapple""? Exactly. We cannot add them, so we should not add their numeric representations.
A regression problem has a real number (a number with a decimal point) as its output. For example, we could use the data in the table below to estimate someone's weight given their height.
Data used in a regression analysis will look similar to the data shown in the image above. We have an independent variable (or set of independent variables) and a dependent variable (the thing we are trying to guess given our independent variables). For instance, we could say height is the independent variable and weight is the dependent variable.
Also, each row is typically called an example, observation, or data point, while each column (not including the label/dependent variable) is often called a predictor, dimension, independent variable, or feature.
An unsupervised machine learning algorithm makes use of input data without any labels in other words, no teacher (label) telling the child (computer) when it is right or when it has made a mistake so that it can self-correct.
Unlike supervised learning that tries to learn a function that will allow us to make predictions given some new unlabeled data, unsupervised learning tries to learn the basic structure of the data to give us more insight into the data.
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
""Birds of a feather flock together.""
Notice in the image above that most of the time, similar data points are close to each other. The KNN algorithm hinges on this assumption being true enough for the algorithm to be useful. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood calculating the distance between points on a graph.
Note: An understanding of how we calculate the distance between points on a graph is necessary before moving on. If you are unfamiliar with or need a refresher on how this calculation is done, thoroughly read ""Distance Between 2 Points"" in its entirety, and come right back.
There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.
3. For each example in the data
3.1 Calculate the distance between the query example and the current example from the data.
3.2 Add the distance and the index of the example to an ordered collection
4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances
5. Pick the first K entries from the sorted collection
6. Get the labels of the selected K entries
7. If regression, return the mean of the K labels
8. If classification, return the mode of the K labels
To select the K that's right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm's ability to accurately make predictions when it's given data it hasn't seen before.
Here are some things to keep in mind:
KNN's main disadvantage of becoming significantly slower as the volume of data increases makes it an impractical choice in environments where predictions need to be made rapidly. Moreover, there are faster algorithms that can produce more accurate classification and regression results.
However, provided you have sufficient computing resources to speedily handle the data you are using to make predictions, KNN can still be useful in solving problems that have solutions that depend on identifying similar objects. An example of this is using the KNN algorithm in recommender systems, an application of KNN-search.
At scale, this would look like recommending products on Amazon, articles on Medium, movies on Netflix, or videos on YouTube. Although, we can be certain they all use more efficient means of making recommendations due to the enormous volume of data they process.
However, we could replicate one of these recommender systems on a smaller scale using what we have learned here in this article. Let us build the core of a movies recommender system.
What question are we trying to answer?
Given our movies data set, what are the 5 most similar movies to a movie query?
Gather movies data
If we worked at Netflix, Hulu, or IMDb, we could grab the data from their data warehouse. Since we don't work at any of those companies, we have to get our data through some other means. We could use some movies data from the UCI Machine Learning Repository, IMDb's data set, or painstakingly create our own.
Explore, clean, and prepare the data
Wherever we obtained our data, there may be some things wrong with it that we need to correct to prepare it for the KNN algorithm. For example, the data may not be in the format that the algorithm expects, or there may be missing values that we should fill or remove from the data before piping it into the algorithm.
Our KNN implementation above relies on structured data. It needs to be in a table format. Additionally, the implementation assumes that all columns contain numerical data and that the last column of our data has labels that we can perform some function on. So, wherever we got our data from, we need to make it conform to these constraints.
The data below is an example of what our cleaned data might resemble. The data contains thirty movies, including data for each movie across seven genres and their IMDB ratings. The labels column has all zeros because we aren't using this data set for classification or regression.
Additionally, there are relationships among the movies that will not be accounted for (e.g. actors, directors, and themes) when using the KNN algorithm simply because the data that captures those relationships are missing from the data set. Consequently, when we run the KNN algorithm on our data, similarity will be based solely on the included genres and the IMDB ratings of the movies.
Use the algorithm
Imagine for a moment. We are navigating the MoviesXb website, a fictional IMDb spin-off, and we encounter The Post. We aren't sure we want to watch it, but its genres intrigue us; we are curious about other similar movies. We scroll down to the ""More Like This"" section to see what recommendations MoviesXb will make, and the algorithmic gears begin to turn.
The MoviesXb website sends a request to its back-end for the 5 movies that are most similar to The Post. The back-end has a recommendation data set exactly like ours. It begins by creating the row representation (better known as a feature vector) for The Post, then it runs a program similar to the one below to search for the 5 movies that are most similar to The Post, and finally sends the results back to the MoviesXb website.
When we run this program, we see that MoviesXb recommends 12 Years A Slave, Hacksaw Ridge, Queen of Katwe, The Wind Rises, and A Beautiful Mind. Now that we fully understand how the KNN algorithm works, we are able to exactly explain how the KNN algorithm came to make these recommendations. Congratulations!
The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It's easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.
KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).
In the case of classification and regression, we saw that choosing the right K for our data is done by trying several Ks and picking the one that works best.
Finally, we looked at an example of how the KNN algorithm could be used in recommender systems, an application of KNN-search.
[1] The KNN movies recommender implemented in this article does not handle the case where the movie query might be part of the recommendation data set for the sake of simplicity. This might be unreasonable in a production system and should be dealt with appropriately.
Software Engineer  Data & Machine Learning
",11
https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b?source=tag_archive---------9-----------------------,"Building RNN, LSTM, and GRU for time series using PyTorch",Revisiting the decade-long problem with a new toolkit,Kaan Kuguoglu,17,"Historically, time-series forecasting has been dominated by linear and ensemble methods since they are well-understood and highly effective on various problems when supported with feature engineering. Partly for this reason, Deep Learning has been somewhat neglected; in other words, it had less impact on time-series forecasting compared to other domains, such as image recognition, speech recognition, and NLP.
With the emergence of Recurrent Neural Networks (RNN) in the '80s, followed by more sophisticated RNN structures, namely Long-Short Term Memory (LSTM) in 1997 and, more recently, Gated Recurrent Unit (GRU) in 2014, Deep Learning techniques enabled learning complex relations between sequential inputs and outputs with limited feature engineering. In short, these RNN techniques and the like hold great potential for analyzing large-scale time series in ways that were not previously practical.
In this post, I'd like to give you a bit of an introduction to some of the RNN structures, such as RNN, LSTM, and GRU, and help you get started building your deep learning models for time-series forecasting. Though not the focus of this article, I'll provide some of the feature engineering techniques that are widely applied in time-series forecasting, such as one-hot encoding, lagging, and cyclical time features. I'll use Scikit-learn, Pandas, and PyTorch, an open-source machine learning library primarily developed by Facebook's AI Research lab. While the former two have long been a sweetheart of data scientists and machine learning practitioners, PyTorch is relatively new but steadily growing in popularity. Due to its recency, though, it has been somewhat difficult for me to find the relevant pieces of information and code samples from the get-go, which is usually a bit easier with frameworks that have been around for a while, say TensorFlow. So, I decided to put together the things I would have liked to know earlier. Less talk, more work: where do we start?
Well, I suppose we need some time-series data to start with. Be it payment transactions or stock exchange data, time-series data is everywhere. One such public dataset is PJM's Hourly Energy Consumption data, a univariate time-series dataset of 10+ years of hourly observations collected from different US regions. I'll be using the PJM East region data, which originally has the hourly energy consumption data from 2001 to 2018, but any of the datasets provided in the link should work.
Given that there are heaps of blogs on data visualizations out there, I'll keep the exploratory data analysis (EDA) part very short. For those interested, I can recommend using Plotly to create interactive graphs. The following method will plot a simple interactive figure that allows you, well, to play with the dataset interactively.
The next step is to generate feature columns to transform our univariate dataset into a multivariate dataset. We will convert this time series into a supervised learning problem, if you will. In some datasets, such features as hourly temperature, humidity, or precipitation, are readily available. However, in our dataset, no extra information could help us predict the energy consumption is given. So, it falls to our lot to create such predictors, i.e., feature columns.
I'll show you two popular ways to generate features: passing lagged observations as features and creating date time features from the DateTime index. Both approaches have their advantages and disadvantages, and each may prove more useful depending on the task at hand.
Let's start with using time steps as features. In other words, we're trying to predict the next value, X(t+n), from the previous n observations Xt, X+1, ..., and X(t+n-1). Then, what we need to do is simply creating n columns with the preceding observations. Luckily, Pandas provides the method shift() to shift the values in a column. So, we can write a for loop to create such lagged observations by shifting the values in a column by n times and removing the first n columns. Lagging is simple yet a good starting point, especially if you don't have many features to work with at the start.
After setting the number of input features, i.e., lagged observations, to 100, we get the following DataFrame with 101 columns, one for the actual value, and the rest for the preceding 100 observations at each row.
Despite its name, feature engineering is generally more art than science. Nonetheless, some rules of thumb can guide data scientists and the like. My goal in this section is not to go through all such practices here but to demonstrate a couple of them and let you experiment on your own. In effect, feature engineering is very much dependent on the domain that you're working in, possibly requiring the creation of a different set of features for the task at hand.
Having a univariate time-series dataset at hand, it seems only logical to generate date and time features. As we have already converted the dataset's index into Pandas' DatetimeIndex type, a series of DateTime objects, we can easily create new features from the index values, like the hour of the day, the day of the month, the month, the day of the week and the week of the year, as follows.
Although passing date and time features to the model without any touch may work in practice, it would be harder for the model to learn interdependencies between these features. For us humans, it is rather straightforward to see the hours, days, weeks, and months follow somewhat cyclical patterns. While it is trivial for us to say that December is followed by January, it may not be apparent that the algorithms to understand the first month of the year come after the 12th one. One can easily come up with many more examples, for that matter. This makes good feature engineering crucial for building deep learning models, even more so for traditional machine learning models.
One way to encode DateTime features is to treat them as categorical variables and add a new binary variable for each unique value, widely known as one-hot encoding. Suppose you applied one-hot encoding on your month column, which ranges from 1 to 12. In this case, 12 new month columns are created, say [Jan, Feb, ... Dec], and only one of such columns has the value 1 while the rest being zeroed. For instance, some DateTime value from February should have the second of these encoded columns as 1, as in [0, 1, ... 0].
Using Pandas' get_dummies method, we can easily create one-hot encoded columns from a given dataset.
Alternatively, you may wish to use Scikit-learn's OneHotEncoder to encode a column in your DataFrame, by using the ColumnTransformer class. Unlike the Pandas way, ColumnTranformer outputs a Numpy array when it is called to fit the DataFrame.
Though quite useful to encode categorical features, one-hot encoding does not fully capture the cyclical patterns in DateTime features. It simply creates categorical buckets, if you will, and lets the model learn from these seemingly independent features. Encoding the day of the week similarly, for instance, loses the information that Monday is closer to Tuesday than Wednesday.
For some use cases, this may not matter too much, indeed. In fact, with enough data, training time, and model complexity, the model may learn such relationships between such features independently. But there is also another way.
As with all the data we have worked on until now, some data is inherently cyclical. Be it hours, days, weeks, or months, they all follow periodic cycles. Again, this is trivial for us to see, but not so much for machine learning models. How can we tell algorithms that hours 23 and 0 are as close as hour 1 is to hour 2?
The gist is to create two new cyclical features, calculating sine and cosine transform of the given DateTime feature, say the hour of the day. Instead of using the hour's original value, the model then uses the hour's sine transform, preserving its cyclicality. To see how and why it works, feel free to refer to Pierre-Louis'or David's blog post on the matter, which explains the concept more in detail.
A good thought exercise is probably to think about how separating one time feature into two would perform in decision tree-based models (Random Forest, Gradient Boosted Trees, and XGBoost). These features form their splits according to one feature at a time, meaning that it will fail to use two-time features, e.g., a sine and cosine transform, simultaneously. Usually, these models are robust enough to handle such splits, but it's certainly food for thought.
Considering we are now working with the energy consumption data, one might ask whether holidays in a year affect the energy consumption patterns. Indeed, very likely. For such binary variables, i.e., 0 or 1, we can generate extra columns with binary values to denote if a given date is actually a holiday. Remembering all the holidays or manually defining them is a tedious task, to say the least. Fortunately, a package, called holidays, does what it promises to do.
When it comes to feature engineering, possibilities are seemingly limitless, and there is certainly room for some experimentation and creativity. The good news is there are already quite a few packages out there that do the job for us, like meteostat for historical weather data or yfinance for stock market data. The bad one is, there is generally no clear answer to which additional features may improve the model performance without actually trying them out.
Just an idea, one can also try to include weather data, e.g., temperature, humidity, precipitation, wind, rain, snow, and so on, to learn how the weather affects the energy consumption in a given hour, day, week, and month.
After creating feature columns, be it time-lagged observations or date/time features, we split the dataset into three different datasets: training, validation, and test sets. Since we're dealing with time-dependent data, it is crucial to keep the time sequences intact, unshuffled if you will. You can easily do so by setting the parameter shuffle to false, avoiding shuffling while splitting into sets.
Scaling the values in your dataset is a highly recommended practice for neural networks, as it is for other machine learning techniques. It speeds up the learning by making it easier for the model to update the weights. You can easily do that by using Scikit-learn's scalers, MinMaxScaler, RobustScaler, Standard Scaler, and so on. For more information on the effects of each scaler, please refer to the official documentation.
And, here is a cool trick if you're looking for a way to switch between scalers quickly. Get yourself comfortable with the switcher function; we may use it again later on.
After you standardize your data, you are usually good to go. Not so fast, this time. After spending quite some time working with PyTorch and going through others' code on the internet, I noticed most people ended up doing the matrix operations for mini-batch training, i.e., slicing the data into smaller batches, using NumPy. You may think that's what NumPy is for; I get it. But there is also a more elegant PyTorch way of doing it, which certainly gets much less attention than it should, in my opinion.
PyTorch's DataLoader class, a Python iterable over Dataset, loads the data and splits them into batches for you to do mini-batch training. The most important argument for the DataLoader constructor is the Dataset, which indicates a dataset object to load data from. There are mainly two types of datasets, one being map-style datasets and the other iterable-style datasets.
In this tutorial, I'll use the latter, but feel free to check them out in the official documentation. It is also possible to write your own Dataset or DataLoader classes for your requirements, but that's definitely beyond the scope of this post as the built-in constructors would do more than suffice. But here's a link to the official tutorial on the topic.
For now, I'll be using the class called TensorDataset, a dataset class wrapping the tensors. Since Scikit-learn's scalers output NumPy arrays, I need to convert them into Torch tensors to load them into TensorDatasets. After creating Tensor datasets for each dataset, I'll use them to create my DataLoaders.
You may notice an extra DataLoader with the batch size of 1 and wonder why the hell we need it. The short answer, it's not a must-have but a nice-to-have. Like mini-batch training, you can also do mini-batch testing to evaluate the model's performance, which is likely much faster due to tensor operations. However, doing so will drop the last time steps that couldn't make up a batch, leading to losing these data points. This will not likely result in a significant change in your error metrics unless your batch size is huge.
From a more aesthetical standpoint, if one is interested in making predictions into the future, discarding the last time steps may cause a discontinuity from the test set to forecasted values. As for the training and validation DataLoaders, this effect can be tolerated due to significant performance improvement batching provides in training and such dropped time steps being less visible.
I don't think I can ever do justice to RNNs if I try to explain the nitty-gritty of how they work in just a few sentences here. Fortunately, there are several well-written articles on these networks for those who are looking for a place to start, Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks, Chris Olah's Understanding LSTM networks, and Michael Phi's Illustrated Guide to LSTM's and GRU's: A step by step explanation are a few that come to mind.
If you've watched the movie Memento -definitely a great watch btw- you may already have an idea about how hard it could be to make predictions with memory loss. While your memory is being reset every couple of minutes, it can easily become impossible to tell what is happening, where you are going, or why -let alone tracking down your wife's murderer. The same can also be said for working with sequential data, be it words or retail sales data. Generally speaking, having the preceding data points helps you understand patterns, build a complete picture and make better predictions.
However, traditional neural networks can't do this, and they start from scratch every time they are given a task, pretty much like Leonard, you see. RNN addresses this shortcoming. To make a gross oversimplification, they do so by looping the information from one step of the network to the next, allowing information to persist within the network. This makes them a pretty strong candidate to solve various problems involving sequential data, such as speech recognition, language translation, or time-series forecasting, as we will see in a bit.
By extending PyTorch's nn.Module, a base class for all neural network modules, we define our RNN module as follows. Our RNN module will have one or more RNN layers connected by a fully connected layer to convert the RNN output into desired output shape. We also need to define the forward propagation function as a class method, called forward(). This method is executed sequentially, passing the inputs and the zero-initialized hidden state. Nonetheless, PyTorch automatically creates and computes the backpropagation function backward().
Vanilla RNN has one shortcoming, though. Simple RNNs can connect previous information to the current one, where the temporal gap between the relevant past information and the current one is small. As that gap grows, RNNs become less capable of learning the long-term dependencies. This is where LSTM comes for help.
Long Short-Term Memory, LSTM for short, is a special type of recurrent network capable of learning long-term dependencies and tends to work much better than the standard version on a wide variety of tasks. RNNs on steroids, so to speak.
The standard version's main difference is that, in addition to the hidden state, LSTMs have the cell state, which works like a conveyor belt that carries the relevant information from the earlier steps to later steps. Along the way, the new information is added to or removed from the cell state via input and forget gates, two neural networks that determine which information is relevant.
From the implementation standpoint, you don't really have to bother with such details. All you need to add is a cell state in your forward() method.
Gated Recurrent Units (GRU) is a slightly more streamlined variant that provides comparable performance and considerably faster computation. Like LSTMs, they also capture long-term dependencies, but they do so by using reset and update gates without any cell state.
While the update gate determines how much of the past information needs to be kept, the reset gate decides how much of the past information to forget. Doing fewer tensor operations, GRUs are often faster and require less memory than LSTMs. As you see below, its model class is almost identical to the RNN's.
Similar to the trick we do with scalers, we can also easily switch between these models we just created.
Now, it seems like we got everything ready to train our RNN models. But where do we start?
Let's start by creating the main framework for training the models. There are probably heaps of ways to do this, and one of them is to use a helper, or a wrapper, class that holds the training, validation, and evaluation methods. First, we need to have a model class, a loss function to calculate the losses, and an optimizer to update the network's weights.
If you're familiar with neural networks, you already know that training them is a rather repetitive process, looping back and forth between forward-prop and back-prop. I find it useful to have one level of abstraction, a train step function or wrapper, to combine these repetitive steps.
After defining one proper training step, we can now move onto writing the training loop where this step function will be called at each epoch. During each epoch in training, there are two stages: training and validation. After each training step, the network's weights are tweaked a bit to minimize the loss function. Then, the validation step will evaluate the current state of the model to see if there has been any improvement after the most recent update.
I'll be using mini-batch training, a training technique where only a portion of data is used at each epoch. Given a large enough batch size, the model can learn and update its weights more efficiently by only learning a sample of the data. This usually requires reshaping each batch tensor into the correct input dimensions so that the network can use it as an input. In order to reap the computational benefits of tensor operations, I defined our RNN models to operate with 3D input tensors earlier, unless you haven't noticed already. So, you may think of each batch as packages of data, like boxes in a warehouse, with dimensions of batch size, sequence length, and input_dim.
There are also two for loops for each stage where a model is trained and validated batch by batch. It is important to activate the train() mode during training and the eval() mode during the validation. While the train() mode allows the network's weights to be updated, the eval() mode signals the model that there is no need to calculate the gradients. Hence, the weights get updated or stay the same depending on the operation.
Now, we can finally train our model. However, without evaluating these models with a separate test set, i.e., a hold-out set, it would be impossible to tell how the model performs compared to other models we're building. Much similar to the validation loop in the train() method, we'll define a testing method to evaluate our models as follows.
During the training, the loss function outputs are generally a good indicator of whether the model is learning, overfitting, or underfitting. For this reason, we'll be plotting simple loss figures by using the following method.
So far, we have prepared our dataset, defined our model classes and the wrapper class. We need to put all of them together. You can find some of the hyperparameters defined in the code snippet below, and I encourage you to play with them as you wish. The code below will build an LSTM model using the module we defined earlier. You can also build RNN or GRU models by quickly changing the input of the function get_model from ""lstm"" to a model of your choice. Without further ado, let's start training our model.
As you may recall, we trained our network with standardized inputs; therefore, all the model's predictions are also scaled. Also, after using batching in our evaluation method, all of our predictions are now in batches. To calculate error metrics and plot these predictions, we need first to reduce these multi-dimensional tensors to a one-dimensional vector, i.e., flatten, and then apply inverse_transform() to get the predictions' real values.
After flattening and de-scaling the values, we can now calculate error metrics, such as mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE).
0.64 R2-score... Not great, not terrible. As you can also see in the next section, there is room for improvement, which you can achieve by engineering better features and trying out different hyper-parameters. I'll leave that challenge to you.
Having some sort of baseline model helps us compare how our models actually do at prediction. For this task, I've chosen good old linear regression, good enough to generate a reasonable baseline but simple enough to do it fast.
Last but not least, visualizing your results helps you better understand how your model performs and adds what features would likely improve it. I'll be using Plotly again, but feel free to use a package that you are more comfortable with.
I'd like to say that was all, but there's and will be certainly more. Deep learning has been one of if not, the most fruitful, research areas in machine learning. The research on the sequential deep learning models is growing and will likely keep growing in the future. You may consider this post as the first step into exploring what these techniques have to offer for time series forecasting.
Here's a link to Google Colab if you'd like to have a look at the complete notebook and play with it. If there are things that don't add up or you disagree, please reach out to me or let me know in the comments. That also goes for all sorts of feedback you may have.
There are still a few more topics that I'd like to write about, like forecasting into the future time steps using time-lagged and DateTime features, regularization techniques, some of which we have already used in this post, and more advanced deep learning architectures for time series. And, the list goes on. Let's hope my motivation lives up to such ambitions.
But, for now, that's a wrap.
",12
https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4?source=tag_archive---------1-----------------------,Algorithms of the Mind,What Machine Learning Teaches Us About Ourselves,Christopher Nguyen,8,"What Machine Learning Teaches Us About Ourselves
Originally published at blog.arimo.com.Follow me on Twitter to keep informed of interesting developments on these topics.
""Science often follows technology, because inventions give us new ways to think about the world and new phenomena in need of explanation.""
Or so Aram Harrow, an MIT physics professor, counter-intuitively argues in ""Why now is the right time to study quantum computing"".
He suggests that the scientific idea of entropy could not really be conceived until steam engine technology necessitated understanding of thermodynamics. Quantum computing similarly arose from attempts to simulate quantum mechanics on ordinary computers.
So what does all this have to do with machine learning?
Much like steam engines, machine learning is a technology intended to solve specific classes of problems. Yet results from the field are indicating intriguingpossibly profoundscientific clues about how our own brains might operate, perceive, and learn. The technology of machine learning is giving us new ways to think about the science of human thought ... and imagination.
Five years ago, deep learning pioneer Geoff Hinton (who currently splits his time between the University of Toronto and Google) published the following demo.
Hinton had trained a five-layer neural network to recognize handwritten digits when given their bitmapped images. It was a form of computer vision, one that made handwriting machine-readable.
But unlike previous works on the same topic, where the main objective is simply to recognize digits, Hinton's network could also run in reverse. That is, given the concept of a digit, it can regenerate images corresponding to that very concept.
We are seeing, quite literally, a machine imagining an image of the concept of ""8"".
The magic is encoded in the layers between inputs and outputs. These layers act as a kind of associative memory, mapping back-and-forth from image and concept, from concept to image, all in one neural network.
""Is this how human imagination might work?
But beyond the simplistic, brain-inspired machine vision technology here, the broader scientific question is whether this is how human imagination  visualization  works. If so, there's a huge a-ha moment here.
After all, isn't this something our brains do quite naturally? When we see the digit 4, we think of the concept ""4"". Conversely, when someone says ""8"", we can conjure up in our minds' eye an image of the digit 8.
Is it all a kind of ""running backwards"" by the brain from concept to images (or sound, smell, feel, etc.) through the information encoded in the layers? Aren't we watching this network create new pictures  and perhaps in a more advanced version, even new internal connections  as it does so?
If visual recognition and imagination are indeed just back-and-forth mapping between images and concepts, what's happening between those layers? Do deep neural networks have some insight or analogies to offer us here?
Let's first go back 234 years, to Immanuel Kant's Critique of Pure Reason, in which he argues that ""Intuition is nothing but the representation of phenomena"".
Kant railed against the idea that human knowledge could be explained purely as empirical and rational thought. It is necessary, he argued, to consider intuitions. In his definitions, ""intuitions"" are representations left in a person's mind by sensory perceptions, where as ""concepts"" are descriptions of empirical objects or sensory data. Together, these make up human knowledge.
Fast forwarding two centuries later, Berkeley CS professor Alyosha Efros, who specializes in Visual Understanding, pointed out that ""there are many more things in our visual world than we have words to describe them with"". Using word labels to train models, Efros argues, exposes our techniques to a language bottleneck. There are many more un-namable intuitions than we have words for.
In training deep networks, such as the seminal ""cat-recognition"" work led by Quoc Le at Google/Stanford, we're discovering that the activations in successive layers appear to go from lower to higher conceptual levels. An image recognition network encodes bitmaps at the lowest layer, then apparent corners and edges at the next layer, common shapes at the next, and so on. These intermediate layers don't necessarily have any activations corresponding to explicit high-level concepts, like ""cat"" or ""dog"", yet they do encode a distributed representation of the sensory inputs. Only the final, output layer has such a mapping to human-defined labels, because they are constrained to match those labels.
""Is this Intuition staring at us in the face?
Therefore, the above encodings and labels seem to correspond to exactly what Kant referred to as ""intuitions"" and ""concepts"".
In yet another example of machine learning technology revealing insights about human thought, the network diagram above makes you wonder whether this is how the architecture of Intuition  albeit vastly simplified  is being expressed.
If  as Efros has pointed out  there are a lot more conceptual patterns than words can describe, then do words constrain our thoughts? This question is at the heart of the Sapir-Whorf or Linguistic Relativity Hypothesis, and the debate about whether language completely determines the boundaries of our cognition, or whether we are unconstrained to conceptualize anything  regardless of the languages we speak.
In its strongest form, the hypothesis posits that the structure and lexicon of languages constrain how one perceives and conceptualizes the world.
One of the most striking effects of this is demonstrated in the color test shown here. When asked to pick out the one square with a shade of green that's distinct from all the others, the Himba people of northern Namibia  who have distinct words for the two shades of green  can find it almost instantly.
The rest of us, however, have a much harder time doing so.
The theory is that  once we have words to distinguish one shade from another, our brains will train itself to discriminate between the shades, so the difference would become more and more ""obvious"" over time. In seeing with our brain, not with our eyes, language drives perception.
""We see with our brains, not with our eyes.
With machine learning, we also observe something similar. In supervised learning, we train our models to best match images (or text, audio, etc.) against provided labels or categories. By definition, these models are trained to discriminate much more effectively between categories that have provided labels, than between other possible categories for which we have not provided labels. When viewed from the perspective of supervised machine learning, this outcome is not at all surprising. So perhaps we shouldn't be too surprised by the results of the color experiment above, either. Language does indeed influence our perception of the world, in the same way that labels in supervised machine learning influence the model's ability to discriminate among categories.
And yet, we also know that labels are not strictly required to discriminate between cues. In Google's ""cat-recognizing brain"", the network eventually discovers the concept of ""cat"", ""dog"", etc. all by itself  even without training the algorithm against explicit labels. After this unsupervised training, whenever the network is fed an image belonging to a certain category like ""Cats"", the same corresponding set of ""Cat"" neurons always gets fired up. Simply by looking at the vast set of training images, this network has discovered the essential patterns of each category, as well as the differences of one category vs. another.
In the same way, an infant who is repeatedly shown a paper cup would soon recognize the visual pattern of such a thing, even before it ever learns the words ""paper cup"" to attach that pattern to a name. In this sense, the strong form of the Sapir-Whorf hypothesis cannot be entirely correct  we can, and do, discover concepts even without the words to describe them.
Supervised and unsupervised machine learning turn out to represent the two sides of the controversy's coin. And if we recognized them as such, perhaps Sapir-Whorf would not be such a controversy, and more of a reflection of supervised and unsupervised human learning.
I find these correspondences deeply fascinating  and we've only scratched the surface. Philosophers, psychologists, linguists, and neuroscientists have studied these topics for a long time. The connection to machine learning and computer science is more recent, especially with the advances in big data and deep learning. When fed with huge amounts of text, images, or audio data, the latest deep learning architectures are demonstrating near or even better-than-human performance in language translation, image classification, and speech recognition.
Every new discovery in machine learning demystifies a bit more of what may be going on in our brains. We're increasingly able to borrow from the vocabulary of machine learning to talk about our minds.
Thanks to Sonal Chokshi and Vu Pham for extensive review & edits. Also, chrisjagers, chickamade.
Fundamentals and Latest Developments in #DeepLearning
1.1K 
8
",13
https://medium.com/@metjush/4-reasons-why-economists-make-great-data-scientists-and-why-no-one-tells-them-524478845ec2?source=tag_archive---------3-----------------------,4 Reasons Why Economists Make Great Data Scientists (And Why No One Tells Them),"As far as job titles go, data scientist is kind of the biggest buzzwords of the last few years. It's also one of the more nebulous ones...",Matus Luptak,7,"Matus Luptak
As far as job titles go, data scientist is kind of the biggest buzzwords of the last few years. It's also one of the more nebulous ones. What actually is data science? Can you even study this? What do data scientists do?
Yes, you can now study data science at some universities (Edinburgh's Data Science program is one of the better ones), but most data scientists come from other fields. Mathematics. Computer Science. Statistics. Physics.
You know, the usual suspects  math-heavy courses that also expose you to a lot of programming and algorithms.
But I want to suggest that economics is  surprisingly perhaps  a great background for data science.
Yes yes yes. Please, hear me out. I know I am biased, but I really believe there aren't many degrees that give you better training for working in data science than economics.
As a graduate of economics, I've committed possibly the greatest sin of the profession. I switched sides. To machine learning and data science.
Gasp.
I don't think I really switched sides, but the world  at least the economics world  around you would have you believe that econometricians and data scientists are sets without an intersection. Data mining is somewhat of a bad word in econometrics, a field almost religiously seeking causal inference and interpretability of results.
But when you actually look into what data science usually is, the boundaries between more traditional econometrics/statistics and the hip and cool machine learning become less and less clear (this infographic is a great illustration of it: source).
Reading through common data science job descriptions, you may get the idea that economics is the worst training to have. Most economics programs don't teach programming and databases, neither do they come even close to machine learning. WTF is Hadoop? And Hive and Pig? Is this a joke?
Specific skills aren't the most important, though. Solid background is  a background that will let you learn the specific skills quickly. And good economics education is indeed a solid background to have.
So here's 4 reasons why economists make great data scientists:
Before you stop reading, thinking that I must've gone to a very weird economics school to have learned machine learning there, read this:
Machine learning is really just a very fancy term for statistical/predictive modelling that programmers invented to keep away the uninitiated from their elite club (hey, they do know some economics after all  scarcity drives prices up!).
In fact, the first two modules in the most popular machine learning course on Coursera are, wait for it, linear regression and logistic regression.
For the 99% percent of economists who took introductory econometrics, this may surprise you. But you probably have deeper knowledge of linear regression than the average data scientist. Just as you may be freaked out by names like ""neural networks"" or ""support vector machines"", you'd have to work very hard to find the term ""heteroskedasticity"" anywhere in machine learning syllabi.
And even the terms you may not know, they are often just examples of skilful copywriting. Neural networks are a great example. It's something that sounds incredibly complicated (are we modelling the brain or what?), but on a (basic) fundamental level, they just combine layers of logistic(-like) regressions to model more complex non-linear relationships that a single regression may not capture (for great primers on neural nets, see http://karpathy.github.io/neuralnets/ or http://iamtrask.github.io/2015/07/12/basic-python-network/).
Granted, neural networks can go deep, far deeper than what I've just described. Recurrent nets, convolutional nets, deep learning are all much more complex topics  and much more powerful algorithms. But for most machine learning applications, you should do just fine with far simpler models: basic neural nets, decision trees, regressions, SVMs... And with statistical background from most econometric courses, you are not going to have any trouble grasping these concepts quickly (I highly recommend that Coursera course).
Hands up if you can still recite all OLS assumptions. And all the possible threats to internal and external validity of your analysis.
Of course you can, you nerds.
At least in my experience, econometrics was obsessed with finding causal relationships  and making it really clear how difficult this is without randomized controlled trials. And how sensitive most models are to their basic assumptions. A lecture wouldn't pass without someone mentioning yet another possible source of bias. Attenuation bias. Survivorship bias. Selection bias. Measurement error. Reverse causality. Truncation. Censoring.
For every problem there was another  more complicated  model that was to deal with it. A model that also introduced its own bag of assumptions and issues.
The world of econometrics was messy, uncertain and frustratingly limiting.
Warning: gross exaggeration ahead.
Compared to this, machine learning is beautifully straightforward. Instead of solving models explicitly  relying on strict assumptions to be able to do so  models are estimated iteratively with gradient descent (and its derivatives). Instead of figuring out what the theory is behind the relationship you are trying to study, and carefully selecting explanatory variables and the appropriate model, you try all you can think of and see if it sticks. Get used to cross-validation and testing. Instead of t-statistics, why not try some bootstrapping?
To econometricians, this may seem blasphemous. But that's only because you are expecting the same from ML that you expected from econometrics. Inference and causal interpretation. For the most part, ML strives for prediction and discovering patterns, not causality. For some models, you can't even say which variables are the most important in predicting the results.
Yes, neural networks may not be used in explaining the causal effect of minimum wage on unemployment. But neither can you really expect (multinomial) logit to be used to recognize hand-writing. It's all about using the right tools in the right applications  and I think econometrics taught you a lot about that.
Data science isn't just fancy algorithms, though. Unless you are an academic researcher who only writes theoretical papers (in which case you probably wouldn't be reading this anyway), presentation and writing are big parts of data science. Just as they are in economics.
If you work as a data scientist anywhere in the ""real world"", you'll have to present your results to non-technical audiences  managers, marketers and copywriters, customers and clients. And you'll have to be able to show why your results matter and how normal folk can use it and act on it.
As economists, I'd wager you've written your fair share of papers, essays, reports, presentations and dissertations in your time at university. Don't underestimate this skill. In fact, it probably puts you well ahead of most of computer scientists and mathematicians when it comes to presenting and explaining your work clearly  and putting together longer pieces of texts that have structure and logic behind them.
Alas, you will probably also have to write code, not just words, if you want to work in data science. But it's not like economists don't have to write code, too. True, Stata isn't a ""proper"" programming language, but it's a great introduction to statistical computing. And if you go on to graduate studies, many economics programs have you learn other languages anyway  Python is very common, as is R and Matlab.
Fortunately, Python's become the programming ""lingua franca"" of data science. Not only has it got a great selection of libraries (Numpy, Scipy, Scikit-learn, Statsmodels, Pandas, Matplotlib, Seaborn...), but it's also a very legible and easy-to-learn language and you've probably come across it anyway.
And if you haven't, just learn Python. R may be powerful too, but the syntax is an abomination and it's kind of slow with bigger datasets. Matlab is a commercial software, and while it is great (and fast) at mathematical computing and it has an open-source alternative (Octave), it's not that common. Julia is too obscure and still a bit too young.
Apparently, economists should make great data scientists. So why no one tells them in university that this is a very real career choice? For one, it's all relatively young. And course prospectuses are slow to change  favoring more traditional options in finance, academia, government...
But I also think there is a bit of prejudice in the economics world against data science. That it's beneath an economist to go into data science. That they are concerned with greater issues.
Which is a shame. Because economics gives its graduates a very unique blend of technical/statistical and soft/human skills that are much harder to come by in the mathematic and CS departments. And perhaps data science positions would benefit from having careful econometricians do the job  people aware of all the possible shortcomings of data mining and just trying all that might work. Just as econometricians might learn from ML when it comes to testing and cross-validation and algorithmic approaches to estimation.
So give it a try. Follow the links in this article. See if it catches your fancy. And don't think that just because you don't know what Hessians are, you can't go into machine learning.
(This isn't meant to be a guide for economists on how to become data scientists. But it should give you plenty of things to think about  and expand your range of possible career options. I may write more specific ""tutorial"" articles later.)
",14
https://towardsdatascience.com/how-to-create-a-chatbot-with-python-deep-learning-in-less-than-an-hour-56a063bdfc44?source=tag_archive---------3-----------------------,How To Create A Chatbot with Python & Deep Learning In Less Than An Hour,Obviously don't expect it to be Siri or Alexa...,Jere Xu,8,"Some people genuinely dislike human interaction. Whenever they are forced to socialize or go to events that involve lots of people, they feel detached and awkward. Personally, I believe that I'm most extroverted because I gain energy from interacting with other people. There are plenty of people on this Earth who are the exact opposite, who get very drained from social interaction.
I'm reminded of a very unique film called Her (2013). The basic premise of the film is that a man who suffers from loneliness, depression, a boring job, and an impending divorce, ends up falling in love with an AI (artificial intelligence) on his computer's operating system. Maybe at the time this was a very science-fictiony concept, given that AI back then wasn't advanced enough to become a surrogate human, but now? 2020? Things have changed a LOT. I fear that people will give up on finding love (or even social interaction) among humans and seek it out in the digital realm. Don't believe me? I won't tell you what it means, but just search up the definition of the term waifu and just cringe.
Now isn't this an overly verbose introduction to a simple machine learning project? Possibly. Now that I've detailed an issue that has grounds for actual concern for many men (and women) in this world, let's switch gears and build something simple and fun!
Here's what the finished product will look like.
If you want a more in-depth view of this project, or if you want to add to the code, check out the GitHub repository.
All of the necessary components to run this project are on the GitHub repository. Feel free to fork the repository and clone it to your local machine. Here's a quick breakdown of the components:
The full code is on the GitHub repository, but I'm going to walk through the details of the code for the sake of transparency and better understanding.
Now let's begin by importing the necessary libraries. (When you run the python files on your terminal, be sure to make sure they are installed properly. I use pip3 to install the packages.)
We have a whole bunch of libraries like nltk (Natural Language Toolkit), which contains a whole bunch of tools for cleaning up text and preparing it for deep learning algorithms, json, which loads json files directly into Python, pickle, which loads pickle files, numpy, which can perform linear algebra operations very efficiently, and keras, which is the deep learning framework we'll be using.
Now it's time to initialize all of the lists where we'll store our natural language data. We have our json file I mentioned earlier which contains the ""intents"". Here's a snippet of what the json file actually looks like.
We use the json module to load in the file and save it as the variable intents.
If you look carefully at the json file, you can see that there are sub-objects within objects. For example, ""patterns"" is an attribute within ""intents"". So we will use a nested for loop to extract all of the words within ""patterns"" and add them to our words list. We then add to our documents list each pair of patterns within their corresponding tag. We also add the tags into our classes list, and we use a simple conditional statement to prevent repeats.
Next, we will take the words list and lemmatize and lowercase all the words inside. In case you don't already know, lemmatize means to turn a word into its base meaning, or its lemma. For example, the words ""walking"", ""walked"", ""walks"" all have the same lemma, which is just ""walk"". The purpose of lemmatizing our words is to narrow everything down to the simplest level it can be. It will save us a lot of time and unnecessary error when we actually process these words for machine learning. This is very similar to stemming, which is to reduce an inflected word down to its base or root form.
Next we sort our lists and print out the results. Alright, looks like we're set to build our deep learning model!
Let's initialize our training data with a variable training. We're creating a giant nested list which contains bags of words for each of our documents. We have a feature called output_row which simply acts as a key for the list. We then shuffle our training set and do a train-test-split, with the patterns being the X variable and the intents being the Y variable.
Now that we have our training and test data ready, we will now use a deep learning model from keras called Sequential. I don't want to overwhelm you with all of the details about how deep learning models work, but if you are curious, check out the resources at the bottom of the article.
The Sequential model in keras is actually one of the simplest neural networks, a multi-layer perceptron. If you don't know what that is, I don't blame you. Here's the documentation in keras.
This particular network has 3 layers, with the first one having 128 neurons, the second one having 64 neurons, and the third one having the number of intents as the number of neurons. Remember, the point of this network is to be able to predict which intent to choose given some data.
The model will be trained with stochastic gradient descent, which is also a very complicated topic. Stochastic gradient descent is more efficient than normal gradient descent, that's all you need to know.
After the model is trained, the whole thing is turned into a numpy array and saved as chatbot_model.h5.
We will use this model to form our chatbot interface!
Once again, we need to extract the information from our files.
Here are some functions that contain all of the necessary processes for running the GUI and encapsulates them into units. We have the clean_up_sentence() function which cleans up any sentences that are inputted. This function is used in the bow() function, which takes the sentences that are cleaned up and creates a bag of words that are used for predicting classes (which are based off the results we got from training our model earlier).
In our predict_class() function, we use an error threshold of 0.25 to avoid too much overfitting. This function will output a list of intents and the probabilities, their likelihood of matching the correct intent. The function getResponse() takes the list outputted and checks the json file and outputs the most response with the highest probability.
Finally our chatbot_response() takes in a message (which will be inputted through our chatbot GUI), predicts the class with our predict_class() function, puts the output list into getResponse(), then outputs the response. What we get is the foundation of our chatbot. We can now tell the bot something, and it will then respond back.
Here comes the fun part (if the other parts weren't fun already). We can create our GUI with tkinter, a Python library that allows us to create custom interfaces.
We create a function called send() which sets up the basic functionality of our chatbot. If the message that we input into the chatbot is not an empty string, the bot will output a response based on our chatbot_response() function.
After this, we build our chat window, our scrollbar, our button for sending messages, and our textbox to create our message. We place all the components on our screen with simple coordinates and heights.
Finally it's time to run our chatbot!
Because I run my program on a Windows 10 machine, I had to download a server called Xming. If you run your program and it gives you some weird errors about the program failing, you can download Xming.
Before you run your program, you need to make sure you install python or python3 with pip (or pip3). If you are unfamiliar with command line commands, check out the resources below.
Once you run your program, you should get this.
Congratulations on completing this project! Building a simple chatbot exposes you to a variety of useful skills for data science and general programming. I feel that the best way (for me, at least) to learn anything is to just build and tinker around. If you want to become good at something, you need to get in lots of practice, and the best way to practice is to just get your hands dirty and build!
Thank you for taking the time to read through this article! Feel free to check out my portfolio site or my GitHub.
We used the simplest keras neural network, so there is a LOT of room for improvement. Feel free to try out convolutional networks or recurrent networks for your projects.
Our json file was extremely tiny in terms of the variety of possible intents and responses. Human language is billions of times more complex than this, so creating JARVIS from scratch will require a lot more.
There are many more deep learning frameworks than just keras. There's tensorflow, Apache Spark, PyTorch, Sonnet, and more. Don't limit yourself to just one tool!
",15
https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa?source=tag_archive---------7-----------------------,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You...",Adam Geitgey,16,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in , , , Tieng Viet,  or Italiano.
Giant update: I've written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
We all know and love Google Translate, the website that can instantly translate between 100 different human languages as if by magic. It is even available on our phones and smartwatches:
The technology behind Google Translate is called Machine Translation. It has changed the world by allowing people to communicate when it wouldn't otherwise be possible.
But we all know that high school students have been using Google Translate to... umm... assist with their Spanish homework for 15 years. Isn't this old news?
It turns out that over the past two years, deep learning has totally rewritten our approach to machine translation. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world.
The technology behind this breakthrough is called sequence-to-sequence learning. It's very powerful technique that be used to solve many kinds problems. After we see how it is used for translation, we'll also learn how the exact same algorithm can be used to write AI chat bots and describe pictures.
Let's go!
So how do we program a computer to translate human language?
The simplest approach is to replace every word in a sentence with the translated word in the target language. Here's a simple example of translating from Spanish to English word-by-word:
This is easy to implement because all you need is a dictionary to look up each word's translation. But the results are bad because it ignores grammar and context.
So the next thing you might do is start adding language-specific rules to improve the results. For example, you might translate common two-word phrases as a single group. And you might swap the order nouns and adjectives since they usually appear in reverse order in Spanish from how they appear in English:
That worked! If we just keep adding more rules until we can handle every part of grammar, our program should be able to translate any sentence, right?
This is how the earliest machine translation systems worked. Linguists came up with complicated rules and programmed them in one-by-one. Some of the smartest linguists in the world labored for years during the Cold War to create translation systems as a way to interpret Russian communications more easily.
Unfortunately this only worked for simple, plainly-structured documents like weather reports. It didn't work reliably for real-world documents.
The problem is that human language doesn't follow a fixed set of rules. Human languages are full of special cases, regional variations, and just flat out rule-breaking. The way we speak English more influenced by who invaded who hundreds of years ago than it is by someone sitting down and defining grammar rules.
After the failure of rule-based systems, new translation approaches were developed using models based on probability and statistics instead of grammar rules.
Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages. This double-translated text is called parallel corpora. In the same way that the Rosetta Stone was used by scientists in the 1800s to figure out Egyptian hieroglyphs from Greek, computers can use parallel corpora to guess how to convert text from one language to another.
Luckily, there's lots of double-translated text already sitting around in strange places. For example, the European Parliament translates their proceedings into 21 languages. So researchers often use that data to help build translation systems.
The fundamental difference with statistical translation systems is that they don't try to generate one exact translation. Instead, they generate thousands of possible translations and then they rank those translations by likely each is to be correct. They estimate how ""correct"" something is by how similar it is to the training data. Here's how it works:
First, we break up our sentence into simple chunks that can each be easily translated:
Next, we will translate each of these chunks by finding all the ways humans have translated those same chunks of words in our training data.
It's important to note that we are not just looking up these chunks in a simple translation dictionary. Instead, we are seeing how actual people translated these same chunks of words in real-world sentences. This helps us capture all of the different ways they can be used in different contexts:
Some of these possible translations are used more frequently than others. Based on how frequently each translation appears in our training data, we can give it a score.
For example, it's much more common for someone to say ""Quiero"" to mean ""I want"" than to mean ""I try."" So we can use how frequently ""Quiero"" was translated to ""I want"" in our training data to give that translation more weight than a less frequent translation.
Next, we will use every possible combination of these chunks to generate a bunch of possible sentences.
Just from the chunk translations we listed in Step 2, we can already generate nearly 2,500 different variations of our sentence by combining the chunks in different ways. Here are some examples:
I love | to leave | at | the seaside | more tidy.I mean | to be on | to | the open space | most lovely.I like | to be |on | per the seaside | more lovely.I mean | to go | to | the open space | most tidy.
But in a real-world system, there will be even more possible chunk combinations because we'll also try different orderings of words and different ways of chunking the sentence:
I try | to run | at | the prettiest | open space.I want | to run | per | the more tidy | open space.I mean | to forget | at | the tidiest | beach.I try | to go | per | the more tidy | seaside.
Now need to scan through all of these generated sentences to find the one that is that sounds the ""most human.""
To do this, we compare each generated sentence to millions of real sentences from books and news stories written in English. The more English text we can get our hands on, the better.
Take this possible translation:
I try | to leave | per | the most lovely | open space.
It's likely that no one has ever written a sentence like this in English, so it would not be very similar to any sentences in our data set. We'll give this possible translation a low probability score.
But look at this possible translation:
I want | to go | to | the prettiest | beach.
This sentence will be similar to something in our training set, so it will get a high probability score.
After trying all possible sentences, we'll pick the sentence that has the most likely chunk translations while also being the most similar overall to real English sentences.
Our final translation would be ""I want to go to the prettiest beach."" Not bad!
Statistical machine translation systems perform much better than rule-based systems if you give them enough training data. Franz Josef Och improved on these ideas and used them to build Google Translate in the early 2000s. Machine Translation was finally available to the world.
In the early days, it was surprising to everyone that the ""dumb"" approach to translating based on probability worked better than rule-based systems designed by linguists. This led to a (somewhat mean) saying among researchers in the 80s:
""Every time I fire a linguist, my accuracy goes up.""
 Frederick Jelinek
Statistical machine translation systems work well, but they are complicated to build and maintain. Every new pair of languages you want to translate requires experts to tweak and tune a new multi-step translation pipeline.
Because it is so much work to build these different pipelines, trade-offs have to be made. If you are asking Google to translate Georgian to Telegu, it has to internally translate it into English as an intermediate step because there's not enough Georgain-to-Telegu translations happening to justify investing heavily in that language pair. And it might do that translation using a less advanced translation pipeline than if you had asked it for the more common choice of French-to-English.
Wouldn't it be cool if we could have the computer do all that annoying development work for us?
The holy grail of machine translation is a black box system that learns how to translate by itself just by looking at training data. With Statistical Machine Translation, humans are still needed to build and tweak the multi-step statistical models.
In 2014, KyungHyun Cho's team made a breakthrough. They found a way to apply deep learning to build this black box system. Their deep learning model takes in a parallel corpora and and uses it to learn how to translate between those two languages without any human intervention.
Two big ideas make this possible  recurrent neural networks and encodings. By combining these two ideas in a clever way, we can build a self-learning translation system.
We've already talked about recurrent neural networks in Part 2, but let's quickly review.
A regular (non-recurrent) neural network is a generic machine learning algorithm that takes in a list of numbers and calculates a result (based on previous training). Neural networks can be used as a black box to solve lots of problems. For example, we can use a neural network to calculate the approximate value of a house based on attributes of that house:
But like most machine learning algorithms, neural networks are stateless. You pass in a list of numbers and the neural network calculates a result. If you pass in those same numbers again, it will always calculate the same result. It has no memory of past calculations. In other words, 2 + 2 always equals 4.
A recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!
Why in the world would we want to do this? Shouldn't 2 + 2 always equal 4 no matter what we last calculated?
This trick allows neural networks to learn patterns in a sequence of data. For example, you can use it to predict the next most likely word in a sentence based on the first few words:
RNNs are useful any time you want to learn patterns in data. Because human language is just one big, complicated pattern, RNNs are increasingly used in many areas of natural language processing.
If you want to learn more about RNNs, you can read Part 2 where we used one to generate a fake Ernest Hemingway book and then used another one to generate fake Super Mario Brothers levels.
The other idea we need to review is Encodings. We talked about encodings in Part 4 as part of face recognition. To explain encodings, let's take a slight detour into how we can tell two different people apart with a computer.
When you are trying to tell two faces apart with a computer, you collect different measurements from each face and use those measurements to compare faces. For example, we might measure the size of each ear or the spacing between the eyes and compare those measurements from two pictures to see if they are the same person.
You're probably already familiar with this idea from watching any primetime detective show like CSI:
The idea of turning a face into a list of measurements is an example of an encoding. We are taking raw data (a picture of a face) and turning it into a list of measurements that represent it (the encoding).
But like we saw in Part 4, we don't have to come up with a specific list of facial features to measure ourselves. Instead, we can use a neural network to generate measurements from a face. The computer can do a better job than us in figuring out which measurements are best able to differentiate two similar people:
This is our encoding. It lets us represent something very complicated (a picture of a face) with something simple (128 numbers). Now comparing two different faces is much easier because we only have to compare these 128 numbers for each face instead of comparing full images.
Guess what? We can do the same thing with sentences! We can come up with an encoding that represents every possible different sentence as a series of unique numbers:
To generate this encoding, we'll feed the sentence into the RNN, one word at time. The final result after the last word is processed will be the values that represent the entire sentence:
Great, so now we have a way to represent an entire sentence as a set of unique numbers! We don't know what each number in the encoding means, but it doesn't really matter. As long as each sentence is uniquely identified by it's own set of numbers, we don't need to know exactly how those numbers were generated.
Ok, so we know how to use an RNN to encode a sentence into a set of unique numbers. How does that help us? Here's where things get really cool!
What if we took two RNNs and hooked them up end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again:
Of course being able to encode and then decode the original sentence again isn't very useful. But what if (and here's the big idea!) we could train the second RNN to decode the sentence into Spanish instead of English? We could use our parallel corpora training data to train it to do that:
And just like that, we have a generic way of converting a sequence of English words into an equivalent sequence of Spanish words!
This is a powerful idea:
Note that we glossed over some things that are required to make this work with real-world data. For example, there's additional work you have to do to deal with different lengths of input and output sentences (see bucketing and padding). There's also issues with translating rare words correctly.
If you want to build your own language translation system, there's a working demo included with TensorFlow that will translate between English and French. However, this is not for the faint of heart or for those with limited budgets. This technology is still new and very resource intensive. Even if you have a fast computer with a high-end video card, it might take about a month of continuous processing time to train your own language translation system.
Also, Sequence-to-sequence language translation techniques are improving so rapidly that it's hard to keep up. Many recent improvements (like adding an attention mechanism or tracking context) are significantly improving results but these developments are so new that there aren't even wikipedia pages for them yet. If you want to do anything serious with sequence-to-sequence learning, you'll need to keep with new developments as they occur.
So what else can we do with sequence-to-sequence models?
About a year ago, researchers at Google showed that you can use sequence-to-sequence models to build AI bots. The idea is so simple that it's amazing it works at all.
First, they captured chat logs between Google employees and Google's Tech Support team. Then they trained a sequence-to-sequence model where the employee's question was the input sentence and the Tech Support team's response was the ""translation"" of that sentence.
When a user interacted with the bot, they would ""translate"" each of the user's messages with this system to get the bot's response.
The end result was a semi-intelligent bot that could (sometimes) answer real tech support questions. Here's part of a sample conversation between a user and the bot from their paper:
Human: connection refused or something like thatMachine: may i know the version of network connect you connect?Human: i am not sure i know thatMachine: is the network connect prompting for any user certificate?Human: yesMachine: are you at home or at work at the moment? office?Human: i am at homeMachine: try this. goto <URL redacted> page and enter your name and passwordHuman: doneMachine: try logging in with <NAME redacted> and check if its working fine nowHuman: yes, now it works!Machine: great. anything else that i can help?
They also tried building a chat bot based on millions of movie subtitles. The idea was to use conversations between movie characters as a way to train a bot to talk like a human. The input sentence is a line of dialog said by one character and the ""translation"" is what the next character said in response:
This produced really interesting results. Not only did the bot converse like a human, but it displayed a small bit of intelligence:
Human: my name is david. what is my name?Machine: david.Human: my name is john. what is my name?Machine: john.Human: are you a leader or a follower?Machine: i'm a leader.Human: are you a follower or a leader?Machine: i'm a leader.
This is only the beginning of the possibilities. We aren't limited to converting one sentence into another sentence. It's also possible to make an image-to-sequence model that can turn an image into text!
A different team at Google did this by replacing the first RNN with a Convolutional Neural Network (like we learned about in Part 3). This allows the input to be a picture instead of a sentence. The rest works basically the same way:
And just like that, we can turn pictures into words (as long as we have lots and lots of training data)!
Andrej Karpathy expanded on these ideas to build a system capable of describing images in great detail by processing multiple regions of an image separately:
This makes it possible to build image search engines that are capable of finding images that match oddly specific search queries:
There's even researchers working on the reverse problem, generating an entire picture based on just a text description!
Just from these examples, you can start to imagine the possibilities. So far, there have been sequence-to-sequence applications in everything from speech recognition to computer vision. I bet there will be a lot more over the next year.
If you want to learn more in depth about sequence-to-sequence models and translation, here's some recommended resources:
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I'll only email you when I have something new and awesome to share. It's the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I'd love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun! Part 6!
",16
https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21?source=tag_archive---------9-----------------------,Illustrated Guide to LSTM's and GRU's: A step by step explanation,"Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I'm Michael, and I'm a Machine...",Michael Phi,10,"Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I'm Michael, and I'm a Machine Learning Engineer in the AI voice assistant space.
In this post, we'll start with the intuition behind LSTM 's and GRU's. Then I'll explain the internal mechanisms that allow LSTM's and GRU's to perform so well. If you want to understand what's happening under the hood for these two networks, then this post is for you.
You can also watch the video version of this post on youtube if you prefer.
Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they'll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN's may leave out important information from the beginning.
During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn't contribute too much learning.
So in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually the earlier layers. So because these layers don't learn, RNN's can forget what it seen in longer sequences, thus having a short-term memory. If you want to know more about the mechanics of recurrent neural networks in general, you can read my previous post here.
towardsdatascience.com
LSTM 's and GRU's were created as the solution to short-term memory. They have internal mechanisms called gates that can regulate the flow of information.
These gates can learn which data in a sequence is important to keep or throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions. Almost all state of the art results based on recurrent neural networks are achieved with these two networks. LSTM's and GRU's can be found in speech recognition, speech synthesis, and text generation. You can even use them to generate captions for videos.
Ok, so by the end of this post you should have a solid understanding of why LSTM's and GRU's are good at processing long sequences. I am going to approach this with intuitive explanations and illustrations and avoid as much math as possible.
Ok, Let's start with a thought experiment. Let's say you're looking at reviews online to determine if you want to buy Life cereal (don't ask me why). You'll first read the review then determine if someone thought it was good or if it was bad.
When you read the review, your brain subconsciously only remembers important keywords. You pick up words like ""amazing"" and ""perfectly balanced breakfast"". You don't care much for words like ""this"", ""gave"", ""all"", ""should"", etc. If a friend asks you the next day what the review said, you probably wouldn't remember it word for word. You might remember the main points though like ""will definitely be buying again"". If you're a lot like me, the other words will fade away from memory.
And that is essentially what an LSTM or GRU does. It can learn to keep only relevant information to make predictions, and forget non relevant data. In this case, the words you remembered made you judge that it was good.
To understand how LSTM's or GRU's achieves this, let's review the recurrent neural network. An RNN works like this; First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one.
While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.
Let's look at a cell of the RNN to see how you would calculate the hidden state. First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.
The tanh activation is used to help regulate the values flowing through the network. The tanh function squishes values to always be between -1 and 1.
When vectors are flowing through a neural network, it undergoes many transformations due to various math operations. So imagine a value that continues to be multiplied by let's say 3. You can see how some values can explode and become astronomical, causing other values to seem insignificant.
A tanh function ensures that the values stay between -1 and 1, thus regulating the output of the neural network. You can see how the same values from above remain between the boundaries allowed by the tanh function.
So that's an RNN. It has very few operations internally but works pretty well given the right circumstances (like short sequences). RNN's uses a lot less computational resources than it's evolved variants, LSTM's and GRU's.
An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM's cells.
These operations are used to allow the LSTM to keep or forget information. Now looking at these operations can get a little overwhelming so we'll go over this step by step.
The core concept of LSTM's are the cell state, and it's various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the ""memory"" of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it's way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get's added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.
Gates contains sigmoid activations. A sigmoid activation is similar to the tanh activation. Instead of squishing values between -1 and 1, it squishes values between 0 and 1. That is helpful to update or forget data because any number getting multiplied by 0 is 0, causing values to disappears or be ""forgotten."" Any number multiplied by 1 is the same value therefore that value stay's the same or is ""kept."" The network can learn which data is not important therefore can be forgotten or which data is important to keep.
Let's dig a little deeper into what the various gates are doing, shall we? So we have three different gates that regulate information flow in an LSTM cell. A forget gate, input gate, and output gate.
First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.
To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.
Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.
Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.
To review, the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.
For those of you who understand better through seeing the code, here is an example using python pseudo code.
1. First, the previous hidden state and the current input get concatenated. We'll call it combine.2. Combine get's fed into the forget layer. This layer removes non-relevant data.4. A candidate layer is created using combine. The candidate holds possible values to add to the cell state.3. Combine also get's fed into the input layer. This layer decides what data from the candidate should be added to the new cell state.5. After computing the forget layer, candidate layer, and the input layer, the cell state is calculated using those vectors and the previous cell state.6. The output is then computed.7. Pointwise multiplying the output and the new cell state gives us the new hidden state.
That's it! The control flow of an LSTM network are a few tensor operations and a for loop. You can use the hidden states for predictions. Combining all those mechanisms, an LSTM can choose which information is relevant to remember or forget during sequence processing.
So now we know how an LSTM work, let's briefly look at the GRU. The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU's got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.
The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.
The reset gate is another gate is used to decide how much past information to forget.
And that's a GRU. GRU's has fewer tensor operations; therefore, they are a little speedier to train then LSTM's. There isn't a clear winner which one is better. Researchers and engineers usually try both to determine which one works better for their use case.
To sum this up, RNN's are good for processing sequence data for predictions but suffers from short-term memory. LSTM's and GRU's were created as a method to mitigate short-term memory using mechanisms called gates. Gates are just neural networks that regulate the flow of information flowing through the sequence chain. LSTM's and GRU's are used in state of the art deep learning applications like speech recognition, speech synthesis, natural language understanding, etc.
If you're interested in going deeper, here are links of some fantastic resources that can give you a different perspective in understanding LSTM's and GRU's. This post was heavily inspired by them.
http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://www.youtube.com/watch?v=WCUNPb-5EYI
I had a lot of fun making this post so let me know in the comments if this was helpful or what you would like to see in the next one. And as always, thanks for reading!
Check out michaelphi.com for more content like this.
 Want more Content? Check out my blog at https://www.michaelphi.com
 Like to watch project-based videos? Check out my Youtube!
 Stay up to date on articles and videos by signing up for my email newsletter!
",17
https://towardsdatascience.com/how-to-go-from-a-python-newbie-to-a-google-certified-tensorflow-developer-under-two-months-3d1535432284?source=tag_archive---------4-----------------------,How to go from a Python newbie to a Google Certified TensorFlow Developer under two months,A learning journey (with tips and tricks) on how I got certified as a TensorFlow Developer by Google within two months.,Grady Matthias Oktavian,11,"I still remember the day I finalized my thesis submission to my university. I sighed in relief as my bachelor studies came to an end. However, boredom quickly overcame me. With nothing to do, and the world swallowed by a pandemic, I desperately seek for a new activity to fill my empty days.
In this post, I'm going to tell you how this pandemic boredom led me to becoming a Google Certified TensorFlow developer in under two months, despite having never coded in Python before. I've provided a list of linked study materials that I use to prepare for this exam.
As someone who enjoys learning, my curiosity has led me to read extensive news and articles about the pandemic when I stumbled upon an article about a group of researchers developing a new system which can distinguish pneumonia from COVID-19 in X-ray images.
The article mentioned that they use 'artificial intelligence' and 'neural network' to make the system. This immediately piqued my interest  how could they train a system which can differentiate X-rays scans? The AI doesn't even have a medical degree to begin with, and yet it has more than 90% of accuracy! Thus begin my journey into the rabbit hole that is deep learning.
Long story short, a brief conversation with a friend opened my eyes to TensorFlow (and Keras). Another day of curious browsing led me to read a blog post by Daniel Bourke about how he got certified as a TensorFlow Developer.
I challenged myself to get certified too, but I was worried that I don't have enough time as soon I was going to be employed and start my master's degree classes at roughly the same time. Furthermore, I am blind to Python. Can I really accomplish this feat?
I studied applied mathematics for actuarial science for my bachelor's degree, which means that I quite familiar with calculus, regression, time series, and statistics. However, my coding skills in Python were slim to none, as the only programming language I know is R. While I find that R is a very versatile language that compliments the needs of data-related jobs, unfortunately at this moment the language R is not supported for the TensorFlow Developer Certification exam.
Taking this certification would be a milestone in my journey as a self-proclaimed data and AI enthusiast. Okay, enough of my story. Let's talk about TensorFlow.
TL;DR version: TensorFlow is a widely available software library for machine learning.
A slightly less TL;DR version: TensorFlow is a free and open source framework which enables users to develop end-to-end machine learning and deep learning projects, starting from pre-processing to model training and deployment. It is initially developed by the Google Brain team for internal use within Google, but now its usage has been widespread.
Now, why should you learn TensorFlow? Because it is capable of a lot of things, and it is more widespread than you think. Chances are, you are using services made using TensorFlow without knowing it.
Have you ever used Gmail's Smartreply? It is AI-powered and suggests you three responses based on what's written on your email. It is built using TensorFlow.
Your Twitter timeline's sorting method? WPS Office's OCR (image-to-text recognition)? VSCO's preset suggestion for your photos? TensorFlow.
When this article was written, TensorFlow has been around for only 4 years old, and it has seen widespread usage in so many services and products that we use daily. While not explicitly written, there is a possibility that the researchers who develop an image recognition system to differentiate regular pneumonia from COVID-19 pneumonia use TensorFlow in their system.
In the future, as the field of deep learning and artificial intelligence improves, we may see more and more products, services, as well as scientific breakthroughs which are powered by TensorFlow to assist in their deep learning aspect.
Practitioners in these fields are benefited if they are familiar with this platform, and this line of thought is what made me interested in becoming a certified TensorFlow developer myself. Perhaps, you have similar thoughts prior to or during reading this article, or perhaps you have your own reasons too to study TensorFlow. Nevertheless, read on to the next part to know more about the exam.
The TensorFlow Developer Certificate exam is written and has to be completed in Python language. In the exam, we utilize Python's TensorFlow library as well as its API. The exam costs $100 per trial. If you fail the first trial, you may pay $100 again and retake the exam after two weeks. Further details about the exam payments and regulations can be found in the handbook here.
The exam syllabus comprises of four main points: building and training neural network using TensorFlow, image classification, natural language processing, as well as time series. The exam has to be taken in the Pycharm IDE.
After reviewing the handbook, I begin to plan out my learning path, which starts from learning the Python language itself, then familiarizing myself with TensorFlow.
If you're still with me, or if you skipped reading the article to get to this point, then let me refresh you briefly. A bored applied mathematics graduate with nothing to do and no prior experience in Python suddenly dreamed of becoming a TensorFlow developer under two months. Here's a recap my journey to achieve that goal.
During the first month, I familiarize myself with the Python language. How do I do this quickly? The first thing I did is going to hackerrank and immediately practiced with a lot of Python problems. When I get to a problem that I can't solve on my own, I try to look up solutions online on-the-go. If that too didn't help, I viewed the solution and tried to understand the concepts I can grasp in this problem.
That's all I do for two weeks, and by then, I am able to answer most questions, even the ones with higher difficulty without looking at any solutions.
What did I do for the remaining two weeks? I watch free YouTube Python tutorials. You heard it right.
Alright, disclaimer incoming. If you have the opportunity to take formal Python class in a more structured manner, by all means please do so. The three videos I list below are only my personal choice to accelerate my Python learning journey.
These videos are so underrated just because they are ""free"", and you don't get any certificates for completing them. Here are a few great choices:
While I would enroll in a 'formal' Python class in my next studies, these three YouTube videos suffice for now. Just make sure to take notes, write your own codes, as well as trying out different things while you watch along.
I spend the last month taking the DeepLearning.AI TensorFlow Developer Professional Certificate at Coursera. In this course, we are tutored by Laurence Moroney, Lead Artificial Intelligence Advocate at Google, and Andrew Ng, founder of deeplearning.ai.
There are four courses in the specialization covering the four key points of the exam syllabus mentioned earlier. Each course consists of four weeks of lessons, but I learned the lessons for one week in a day, as this has been on top of my priority list for that month.
After completing each course, I take a day off to rest my mind, and use that day to either toy around my practice codes, or to explore ideas related to the course in a leisure manner.
To recap, I take five days to complete each course. Four days to view the lesson materials, and the fifth day to rest and review. Thus, I am able to finish the whole courses in 20 days.
Each courses have its own coding projects, and I really explore the codes provided. I often find myself spending hours toying around the hyperparameters of the neural network (you'll know what I'm talking about when you start learning it) in order to try and gain the best validation accuracy. By doing so, you'll gain an 'instinct' on the trial and error processes of creating deep neural network models.
Sometimes, the lessons referenced an external source such as datasets, articles, and ungraded materials. These are not mandatory to pass the course, but in my curiosity I explore a lot of these external sources. As the lessons are mostly practical, often than not we are also given links to videos made by Andrew Ng in which he explains a more intuitive and theoretical approach of particular subjects.
You don't necessarily have to follow my study path and my learning materials in order to succeed in this exam. There are other alternatives to the coursera course if you don't want to spend $49 per month for the course, and I'll list them here:
I take four days to review my lessons and reread the handbook after finishing all the courses. On the 25th day of the second month, I started the exam.
Alright, here comes the D-Day. Okay so here is the answer key of the first questio  just kidding. While I can't go to the exam details for obvious reasons, here are a few points regarding the exam that I've compiled about preparing and taking the exam:
If you've studied well, and made sure that you've learned all things listed in the exam syllabus, you should pass the exam. I can tell you that the syllabus written in the handbook is not misleading, and you can really use the list provided there as a benchmark for your exam readiness.
Personally, I do the exam on my AMD notebook with no dedicated GPU, and yet I only need to use Google Colab once in a problem with large data set. Train a few practice models on your device and you'll know for yourself if your device is capable enough. I'd be more concerned on the internet speed and stability as you need to upload these models to complete the exam.
After ending the exam, I immediately received an email saying that I have passed the exam. Within 2 weeks, my official digital certificate is sent to my email, and I can link it to my LinkedIn profile.
It is only valid for three years, so I will have to take another certification exam in 2023. I could only wonder on the advancement of TensorFlow and the field of deep learning by then, and hopefully my journey in taking that exam is smoother than this first one.
This is not the end, in fact this is just the beginning for me. Having this exam as my first milestone really supercharged me and is my door to the world of data science, which is weird  as usually deep learning is the cherry on top for aspiring data scientists.
I'm glad that I managed to finish this certificate exam as well as writing this article merely days before my actuarial job and my master degree started. Through these two months, I have a newfound interest in the world of data science and artificial intelligence. The possibilities that this field might bring to solve real world problems are seemingly endless.
I have to write a reminder that I believe my learning path is not the best, and there are still a lot of room for improvements. To those who are not time-constrained, perhaps taking it slower as well as making projects along the way would be a better learning path. As for now, even when I am a certified TensorFlow developer, I have yet to make a single project on my GitHub account. This is what I aim to focus on after this article is published  improving my skills furthermore by making real personal projects and putting them on my GitHub.
As a quickly growing field filled with innovations, discoveries, and breakthroughs, I'm sure that the world of artificial intelligence, data science, machine learning, and deep learning is a new frontier waiting to be explored. Are you excited to venture into this world? Because I am. And for me, it all begins from a boredom during the pandemic.
",18
https://towardsdatascience.com/17-clustering-algorithms-used-in-data-science-mining-49dbfa5bf69a?source=tag_archive---------4-----------------------,17 Clustering Algorithms Used In Data Science and Mining,"This article covers various clustering algorithms used in machine learning, data science, and data mining, discusses their use cases, and...",Mahmoud Harmouch,35,"""if you want to go quickly, go alone; if you want to go far, go together.""  African Proverb.
Quick note: If you are reading this article through a chromium-based browser (e.g., Google Chrome, Chromium, Brave), the following TOC would work fine. However, it is not the case for other browsers like Firefox, in which you need to click each link twice to get to the intended section. Enjoy!
As always, everything written and visualized were created by the author unless it was specified.
As information becomes increasingly important and accessible to people all around the globe, more and more data science and machine learning methods have been developed. The cluster analysis model may look simple at first glance, but it is crucial to understand how to deal with enormous data. However, making a reasonable choice between plenty of clustering algorithms can sometimes seem daunting, and it requires a fair amount of understanding of various algorithms. Therefore, this article has compiled seventeen clustering algorithms to give the reader a good amount of information about most of them.
 Machine learning
The most simple yet straightforward definition for machine learning, a subfield of artificial intelligence, is how machines are taught from data(e.g., data collected from sensors, experiments...) by discovering statistical patterns to make decisions and do tasks on their own(automating data-driven models). It's that simple. However, the difficulty comes from the narrowed details and applications. It's all about analyzing data and learning from it. Moreover, machine learning provides the foundation for data science at its core, as the Drew Conway ven diagram shows.
Historically speaking, Machine learning arises from the connectionist in artificial intelligence where a group of individuals wanted to replicate the mechanism of the human brain with similar characteristics. Additionally, it has mainly benefited by incorporating ideas from psychology and other domains(e.g., statistics.). Moreover, statistics and machine learning are fundamentally different fields where the former aims to provide humans with the right tools to analyze and understand data. The latter focuses on automating the intervention of humans in analyzing data(AI singularity).
 Cluster analysis
Cluster analysis, clustering, or data segmentation can be defined as an unsupervised(unlabeled data) machine learning technique that aims to find patterns(e.g., many sub-groups, size of each group, common characteristics, data cohesion...) while gathering data samples and group them into similar records using predefined distance measures like the Euclidean distance and such. Data objects or observations that share similar characteristics are grouped into one cluster described by a distance that holds these data samples(e.g., the major axis of an ellipse.).
Cluster analysis is widely adopted by various applications like image processing, neuroscience, economics, network communication, medicine, recommendation systems, customer segmentation, to name a few. Additionally, clustering can be considered the initial step when dealing with a new dataset to extract insights and understand the data distribution. Cluster analysis can also be used to perform dimensionality reduction(e.g., PCA). It might also serve as a preprocessing or intermediate step for others algorithms like classification, prediction, and other data mining applications.
 Types of Clustering
There are many ways to group clustering methods into categories. For instance, based on the area of overlap, exists two types of clustering:
0. Hard clustering: Clusters don't overlap: k-means, k-means++. A data point belongs to one cluster only. It either belongs to a certain cluster or not.
1. Soft clustering: Clusters can overlap: Fuzzy c-means, EM. A data object can exist in more than one cluster with a certain probability or degree of membership.
Additionally, Clustering algorithms can be classified based on the purpose they are trying to achieve. Therefore, exists two types of Clustering techniques based on this criterion:
0.Monothetic: Exists some common properties between cluster members(e.g., 25% of patients show side effects due to vaccine A): the data are divided on values generated by a single feature.
1. Polythetic: Exists some degree of similarity between cluster members without having a common property(e.g., dissimilarity measure): the data are divided on values generated by all features.
Based on the clustering analysis technique being used, each cluster presents a centroid, a single observation representing the center of the data samples, and a boundary limit. The following illustration represents some common categories of clustering algorithms.
One of the major steps in this methodology is to initialize the number of clusters k, a hyperparameter that remains constant during the training phase of the model.
One of the most popular partitioning algorithms( with over a million citations on google scholar) used to cluster numerical data attributes. Using this polythetic hard clustering technique, n data objects are split into k partitions (k << n) where each partition represents a cluster. Each cluster must contain at least one data point. Additionally, each data object must belong to one group only. Further, observations of the same cluster should be similar or closed to each other. In contrast, objects of different groups must far apart or dissimilar from each other. In other terms, the goal of the k-means algorithm is to maximize the distance between each pair of clusters' centers and minimize the distance between observations within each cluster (e.g., minimize the sum of squared error, SSE, within a cluster.).
k- means clustering works well if the following conditions are met:
0. The distribution's variance of each attribute is spherical.
1. Clusters are linearly separable.
2. Clusters have similar numbers of observations(closer size.).
3. Variables present the same variance.
However, if one of these assumptions is broken, it doesn't necessarily mean that k- means would fail in clustering the observations since the only purpose of the algorithm is to minimize the sum of squared errors (SSE). Here a good discussion illustrates that k-means would work well if one of the previous assumptions is not satisfied.
In order to better understand the data(e.g., extract information and finding clusters), a rule of thumb is to plot the data in 2-d space. For instance, to find how many clusters are in the iris dataset, a basic correlation matrix would tell a lot.
As the graph shows, there are three major clusters in this dataset. Therefore k should equal to three for further training purposes. However, this is not the best method to choose the value of k.
In practice, the standard approach is to start with the elbow method where the algorithm ran for different values of k(e.g., k= 1, 2, 3, 4...) and use a robust method called WCSS(within-cluster sum of squares) that calculates the sum of distances between each cluster member and its centroid in order to minimize it to reach the optimum value for k.
There is another method for choosing the right value of k by computing the Silhouette coefficient for each cluster: the average distances between points of the same cluster. It gives an indicator of how similar data objects are according to their clusters. To illustrate this, a silhouette plot was performed on the iris dataset, where each cluster has a silhouette coefficient.
Using this method, the more the coefficient is closer to one, the better the value of k would fit the model. Therefore, the best values for k are two and three since they present a higher silhouette coefficient for each cluster than other values.
K can also be initialized using the shoulder method, which displays a plot of the percentage sum of squares(BSS/TSS) against the number of clusters.
As shown in the graph, the optimum number of clusters is where a shoulder (leap) starts to form. Therefore, k equals 3.
Besides, there are plenty of other methods that can be used to estimate the optimum value of k, such as the R-squared measure. However, the silhouette score has been proved to be the best way to find k.
 Explanation.
It all starts by randomly placing k points in the features space where each point represents a centroid for a unique cluster. Iteratively compute the distance, using a certain dissimilarity measure, between each observation of the dataset with each cluster center. Further, assign each observation to the cluster of the nearest centroid. After that, for each cluster, calculate the mean (numeric attributes) for each cluster's points and reassign the centroid to the resulted mean. This process will keep repeating until a predefined convergence condition is satisfied(e.g., max number of iterations has reached, means difference become unchanged, BSS becomes below a given minimum, a minimum value for SSE, minimize an objective function, distortions...)
 Algorithm.
0. Pick k random centroids from the dataset.
1. Compute the distances between each data point w.r.t clusters' centroids using a proper dissimilarity measure(e.g., Euclidean distance).
2. Assign each data point to the nearest cluster based on the computed distance.
3. Reposition the centroids by computing the mean, the average, of the data points. Therefore k-means works only on numerical data!
4. Repeat 1. until clusters become stable or an objective function J reaches its minimum.
 Advantages.
0. The learning curve is relatively steep.
1. Widely implemented by a variety of packages(Stats package in R, scikit-learn in python...)
2. Fast convergence for clustering small datasets.
3. Easy to implement.
 Drawbacks.
0. Computationally expensive for large datasets(k becomes large.).
1. Sometimes, it is difficult to choose an initial value for the number of clusters(k).
2. Doesn't guarantee to converge to a global minimum. It is sensitive to the centroids' initialization. Different setups may lead to different results.
3. Strong sensitivity to outliers.
4. Works only on numerical data.
5. Fails to give good quality of clustering for a group of points that have non-convex shapes.
However, Some disadvantages can be solved using the elbow method to initialize the number of clusters, using k-means++ to overcome the sensitivity in the initialization of the parameters, and using a technique like the genetic algorithm to find the global optimum solution.
 Applications.
k-means clustering is adopted by various real-world businesses such as search engines (e.g., document clustering, clustering similar articles), customer segmentation, spam/ham detection system, academic performance, faults diagnostic systems, wireless communications, and many more.
 Objective function minimization.
To find the optimum solution for k clusters, the derivative of the cost function J w.r.t  must equal zero.
For each cluster J, the previous equation would lead to:
After each iteration, the centroid of each cluster is updated to the empirical mean of all data points within the cluster.
Note that the problem of minimizing the Euclidean distances within each cluster is known as the Weber problem. Moreover, geometrically speaking, The mean is not the optimal solution. Thus the need for complex geometrical centers such as median, medoid to minimize Euclidean distances.
The idea behind k-means++ is that it tries to spread out the centers while allocating a new center per iteration. Therefore, the algorithm starts by randomly(uniform) picking an initial center from the dataset, which means that all points have an equal probability of getting selected. Then computes the distance squared from each data point to the previously chosen center. After that, it computes the probability for each data point by simply dividing the distance by the total distances. Further, assign a new cluster center to the point that has the highest probability or the highest distance. In other words, the likelihood of a data object being the center of a new cluster is proportional to the distance squared.
Once the centers have been assigned, the k-means algorithm will run with these clusters' centers, and it will converge much faster since the centroids have been chosen carefully and far away from each other.
 Algorithm
 Initialization step
Sample each centroid independently in a uniform fashion with a probability proportional to the distance squared for each data point from each centroid.
 Clustering step
Once the k centroids have been uniformly sampled, the K-means algorithm will run using these centroids.
 Advantages
0. Same advantages as K-means.
1. Converge faster than K-means in a fewer number of iterations.
 Drawbacks.
Same drawbacks as K-means.
The initialization step(choosing an initial value for K) can be considered one of the major drawbacks for kmeans++ like other flavors of the K-means algorithm. However, it is more likely to converge and faster than running K-means alone. Furthermore, this algorithm is still sensitive to outliers that can be solved using LOF, RANSAC, and other methods.
K-means parallel is another sufficient technique that updates the distribution of the samples less frequently after each iteration. It introduces an oversampling factor (L ~ order of k., e.g., k, k/2, ...) to the k-means algorithm. Using that factor, it will make the algorithm converge much faster for larger datasets.
 Algorithm.
 Initialization step
0. Initialize the value of the oversampling factor, L.
1. For a certain number of iterations(0  nb_iter  k), sample L centroids uniformly at random with a probability proportional to the distance squared for each data point from each centroid(L times bigger than the probability in the kmeans++ algorithm).
nb_iter = 0  k-means clustering.
nb_iter = k, and L = 1  k-means++ clustering.
 Clustering step
Once k centroids have been uniformly sampled, the K-means algorithm will run using these centroids.
 Advantages.
0. Scales well for large datasets. Runtime ~ log(k)
1. Faster than kmeans++ because it samples L centroids per iteration.
 Drawbacks.
0. It can lead to oversampling or undersampling based on the value of L.
The term fuzzy was used to stress the fact that there are various shades of clusters(e.g., disjoint, non disjoints...) that are allowed to form where a data point can exist in one or more clusters. For instance, the color orange is a mixture of red and yellow colors, which means that it belongs to each color group to some degree.
A membership degree function is used to measure the degree of belonging of a data point to each cluster. It describes the probability that a data point belongs to a certain cluster.
The algorithm aims to minimize the following cost function:
 Algorithm
0. Select k initial fuzzy pseudo centroids based on predefined weights aij^p, and an initial value for p.
1. Update the cluster centers using a fuzzy partition.
2. Updates the weights using the following formula.
3. Compute the objective function J.
4. repeat 1. until stabilizing the centroids or while the following criterion is satisfied: the difference between the newly computed cost function and the old one is smaller than a certain value.
Fuzzy k-means presents large real-world use cases such as image segmentation, anomaly detection. It is less computationally intensive compared to other image processing techniques like edge and object detection.
 Advantages.
0. Better results for overlapped data in contrast to k-means.
1. Low time complexity.
2. Convergence is guaranteed.
 Drawbacks.
0. Sensitive to the initial values of k and p.
1. Sensitive to outliers.
 Objective function minimization.
In order to find the optimum solution for k clusters, the derivative of the cost function J w.r.t  must equal zero.
For each cluster J, the previous equation would lead to:
Knowing that for each observation in the dataset, the sum of memberships for all clusters is equal to one; Therefore, each cluster's centroid is updated to its empirical mean after each iteration.
A modified version of the k-means algorithm where a medoid represents a data point with the lowest average dissimilarity among all points within a cluster. The purpose is to minimize the overall cost for each cluster. Unlike k-means, it uses a medoid as a metric to reassign the centroid of each cluster. Medoids are less sensitive to outliers. These medoids are actual observations from the dataset and not computed points(mean value) like in the case of k-means. It is preferred to use the manhattan distance as a metric because it is less sensitive to outliers.
 Algorithm.
0. Randomly pick k observations as initial medoids.
1. Compute the distances between the observations and medoids.
2. Assign each point to the nearest medoid.
3. Pick a new observation(non-medoid) in each cluster and swap it with the correspondent medoid.
4. Compute the swapping cost of each medoid and the new data point within each cluster.
5. Select the observation with the lowest cost(e.g., the minimum sum of dissimilarities) as a new medoid.
6. Repeat step 1. until a convergence condition is satisfied(e.g., minimize a cost function, a sum of squared error (SSE in PAM)).
 Advantages.
0. More robust than k-means in the presence of outliers(Less influenced by outliers.)
1. It can be Implemented Easily.
2. It converges in a fixed number of iterations.
3. It works effectively with a small dataset.
 Drawbacks.
0. It doesn't scale well for a large dataset.
1. The computational complexity is quite expensive.
2. The parameter k needs to be initialized to a certain value.
3. Doesn't guarantee to converge to a global minimum. It is sensitive to the centroids' initialization. Different setups lead to different results.
4. Works only on numerical data.
For efficiency improvement of PAM, the CLARA algorithm is used.
A modified version of the k-means algorithm uses the median which represents the middle point where other observations are evenly distributed around it. A median is less sensitive to outliers than the mean.
Additionally, it uses the Manhattan distance as a metric for computing distances between observations. Moreover, the algorithm aims to minimize the following cost function:
 Algorithm.
0. Randomly pick k observations as initial medians.
1. Compute the distances between the observations and medians.
2. Assign each point to the nearest median.
3. Compute the median for each cluster and assign it as a new centroid for the cluster
4. Repeat step 1. until a convergence condition is satisfied(e.g. minimize a cost function like SSE).
Since K-means handles only numerical data attributes, a modified version of the k-means algorithm has been developed to cluster categorical data. The mode replaces the mean in each cluster. However, someone could come with the idea of mapping between categorical and numerical attributes and then clustering using k-means. This could sometimes work on a small dimensional dataset. But, mapping between two different types of attributes cannot guarantee a high-quality clustering for high dimensional data. Therefore, it is recommended to use k-modes when clustering categorical data attributes.
One of the dissimilarity measures used in k-modes is the cosine dissimilarity measure, a frequency-based method that computes the distance between two observations(e.g., the distance between two sentences or two documents).
 Algorithm.
The K-Modes clustering process consists of the following steps:
0. Randomly pick k observations as initial centers(modes).
1. Compute the dissimilarity measure between each data point and the cluster center(mode)
2. Assign each observation to the nearest cluster center based on the dissimilarity measure(e.g. cosine dissimilarity function).
3. Reposition each centroid based on the mode value computed in each cluster.
4. Repeat step 2 until a convergence condition is satisfied(e.g. minimize a cost function like SSE).
 Advantages.
0. Able to Cluster categorical data attributes.
1. It Converges faster than K-prototypes.
 Drawbacks.
0. Computationally expensive for large datasets(k becomes large.).
1. Sometimes, it is difficult to choose the right initial value for the number of clusters(k).
2. Doesn't guarantee to converge to a global minimum. It is sensitive to the centroids' initialization. Different setups may lead to different results.
3. Efficiency depends on the dissimilarity measure used by the algorithm(e.g. Spearman correlation, cosine distance...).
4. Additional variable is added to the algorithm() that controls the weight of the distance from each observation to their clusters' centers.
The local optimum problem can be solved using a global optimization algorithm such as the Cuckoo Search algorithm.
 Applications.
k-modes is often used in text mining like document clustering, topic modeling where each cluster group represents a given topic(similar words...), fraud detection systems, marketing(e.g., customer segmentation.), clustering webpages, and many more.
This method works on a mixture of numerical and categorical data attributes. This algorithm can be thought of as a composition between k-means and k-modes algorithms.
Using this algorithm, each data point has a weight being a part of numerical and categorical clusters. Moreover, each type of observation can be treated in a separate fashion where centroids play the role of an attractor in each type of cluster. The membership to a given data point can be controlled using a fuzzy membership function aij like in FCM.
 is used to balance the influence between categorical and numerical data attributes.
 Algorithm.
The K-Prototypes clustering process consists of the following steps:
0. Randomly select k representative as initial prototypes of k clusters.
1. Compute the distance(e.g., Euclidean) and the dissimilarity measure(e.g., cosine.) between each data point and the corresponding cluster centers (prototype).
2. Assign each observation to the nearest cluster prototype based on the distance formula.
3. Reposition each cluster center based on the following formulas.
4. Repeat step 1. until a convergence condition is satisfied(e.g., minimum of a cost function).
 Advantages
0. Ability to cluster mixed types of attributes.
1. Converge in a reasonable number of iterations.
 Drawbacks.
0. Different dissimilarity measures can lead to different outcomes.
1. Sensitive to the initial values of k and .
2. Doesn't guarantee to converge to a global minimum. It is sensitive to the medoids' initialization. Different setups may lead to different results.
3. Efficiency depends on the dissimilarity measure used by the algorithm(e.g. Spearman correlation, cosine distance...).
4. Slower than k-modes in case of clustering categorical data.
It is a sample-based method that randomly selects a small subset of data points instead of considering the whole observations, which means that it works well on a large dataset. Moreover, k medoids are chosen from the previously selected sample. This will help towards improving the scalability of PAM(reduce computing time and memory allocation problem). It works sequentially on different batches of the dataset to find the most optimal result.
The upshot of the algorithm is a set of medoids with minimal cost.
 Algorithm
0. Randomly select multiple subsets from the data having a fixed size (size s).
1. Compute the k-medoid algorithm on a chunk of data and select the corresponding k medoids.
2. Assign each observation of the original dataset to the closest medoid.
3. Compute the mean of the dissimilarities of the observations to their nearest medoid.
4. Retain the subset of data for which the mean is minimal.
5. Repeat until finding the optimal medoids.
 Advantages.
0. Ability to handle large datasets.
1. Reduce the computation time while dealing with a large data set.
2. Ability to handle outliers.
 Drawbacks.
0. Efficiency is influenced by the value of k and the sample size.
1. The quality of clustering depends on the quality of the sampling method being used.
2. Hard to implement.
It is an extension to k-medoid used in data mining to cluster large datasets.
The outcome of the algorithm is a set of medoids with minimal cost.
 Algorithm.
0. Randomly select k-medoids from the dataset.
1. Pick an observation and a medoid from the previously chosen ones.
2. Compute the distance between the two-point and all other data points in the dataset.
3. Compute the cost of swapping the two data points and choose the one as medoid that has the minimal cost.
4. Repeat step 1. until convergence(finding the optimal choice of k-medoids).
 Advantages.
0. More effective than PAM and CLARA on large datasets.
1. Ability to handle outliers.
 Drawbacks.
0. Hard to implement.
1. The quality of clustering depends on the quality of the sampling method being used.
 Probabilistic Modeling.
A probabilistic model is a generative data model parameterized by a joint distribution over data variables: P(x1, x2, ..., xn, y1, y2, ...,yn|) where X is observed data, y: latent variables,  a parameter.
P(y1,...,yn|x1,...,xn, ) = P(x1,...,xn, y1,...,yn|)(joint) / P(x1,...,xn|)(marginal probability)
 Learning.
The Learning phase is carried out using the maximum likelihood:
ML = argmax  P(x1,...,xn|)
The purpose is to find a parameter  that maximized the probability of the observed data.
 Prediction.
P(xn+1, yn+1|x1,...,xn, )
The goal is to compute the conditional distribution of the latent attributes given the observed dataset.
 Classification:
the goal is to find a class that maximizes the probability of the future data given the learned parameters :
argmax c P(xn+1|c )
Some standard algorithms used in probabilistic modeling are the EM algorithm, MCMC sampling, junction tree, etc.
In 2-d variables space, a gaussian distribution is a bivariate normal distribution constructed using two random variables having a normal distribution, each parameterized by its mean and standard deviation.
In my opinion, Gaussian distribution is so important because it made the computation(e.g., linear algebra computation.) effortless to do. However, it is not the perfect model for real-world applications.
The Gaussian Mixture Model is a semi-parametric model (finite number of parameters that increases with data.) used as a soft clustering algorithm where each cluster corresponds to a generative model that aims to discover the parameters of a probability distribution (e.g., mean, covariance, density function...) for a given cluster(its own probability distribution governs each cluster). The process of learning is to fit a gaussian model to the data points. The Gaussian Mixture model assumes that the clusters are distributed in a normal distribution in n-dimensional space.
To illustrate mixture models in one-dimensional space, suppose there are two sources of information with a normal distribution where n samples have been collected from each source. To estimate the mean of each Gaussian distribution, take the sum of the values of observations and divide them by the number of collected samples(the empirical mean.), likewise for estimating other parameters.
The problem arises when there are k Gaussian models, and no information is given on where the observations are coming from; It's not easy to figure out how to divide the points into k clusters. Therefore, it is nearly impossible to estimate each of the Gaussian parameters. However, this problem can be solved if the gaussian parameters(mean, variance) are predefined.
That what the EM method is trying to solve.
It is a well-known algorithm for fitting mixture distributions that aims to estimate the parameters of a given distribution using the maximum likelihood principle(finding the optimum values) when some of the data points are not available(e.g., Unknown parameters, latent values...). In the context of GMM, The intuition is to randomly place k gaussian models in space and compute the degree of belonging of each data point to a certain gaussian model. Unlike hard clustering(e.g., k-means), the method computes the probabilities for each point to be a member of a certain cluster. Further, these values are used to reestimate the cluster parameters(e.g., mean, covariance)to fit the points assigned for each cluster.
EM is widely used to solve problems such as the ""hidden-data"" problems, the Hidden Markov Models, where there is a sequence of latent variables that depends on the state of the previously hidden variable. Additionally, each observation depends on the state of the corresponding hidden variable.
k is the transition probability given the previous state k. Arrows describe dependencies between the variables.
Algorithm.
EM algorithm consists of 2 steps, the Expectation step, and the Maximization step.
Step-0: Initialization of the parameters thetas.
E-Step: In this step, the observations are estimated to come from which distribution by calculating the normalized expected value Wij(Weight of a data point in each distribution) of each latent data point assuming the current hypothesis holds given the centroid j and covariance matrix j for the cluster J:
P(xi|K=j, ) is the conditional probability of the multivariate normal distribution Xi~N(i, i).
Each cluster has the probability (prior) that can be estimated based on the training dataset.
M-Step: Using the information gained from the previous step, the M-Step will update the estimated values of the mean j and the covariance j(or variance ) using the new maximum likelihood hypothesis assuming the value taken by each hidden variable is the expected value.
Repeat E and M steps until the log-likelihood function converges.
Using the fact that the likelihood is monotonically increasing after each iteration, the algorithm is more likely to converge to an optimum.
To demonstrate the EM algorithm, let's consider observations generated from three Gaussian models(a, b, c). Since each sample is unlabeled, the goal is to estimate the parameters of these three Gaussian models to label each point to certain gaussian distribution. To estimate these parameters, the three Gaussian models are placed randomly in the 1-d dataset space.
0. Compute the likelihood for each data point generated from the three Gaussian models having the following density functions.
1. E-Step: For each data point, compute its weight wi(ai, bi, ci).
2. M-Step: at this point, the mean and the variance for each model can be estimated.
3. Estimate the probabilities.
4. Repeat E and M steps until the log-likelihood function converges.
 Advantages.
0. It produces a valid estimation of the parameters for a mixture distribution.
1. Very simple to implement.
 Drawbacks.
0. Choosing an initial value for k (number of mixture models ) like in k-means.
1. Sensitive to the initial values, which leads to different results.
2. It may converge to a local optimum solution.
3. Convergence might be slow.
The Dirichlet process is a stochastic process that produces a distribution over a discrete distribution(probability measures) used for defining Bayesian non-parametric(unfixed set of parameters. e.g., ~ infinite number of parameters.) models. The Dirichlet distribution is a continuous multivariate density function parameterized by a concentration/precision parameter/vector (1, ..., k) with positive components and a base distribution H: DP(, H). It is like a Beta distribution(e.g., Coinflip) for more than two outcomes.
k-dimensional Dirichlet: (1, 2, ..., k) ~Dirichlet(1, 2,..., k)
Thetas are independent parameters and identically distributed over H, and the goal is to infer the parameters  and the latent variables given the observations xi.
One of the great properties of Dirichlet distribution is that when merging two different components(i, j), it will result in a marginal distribution that is a Dirichlet distribution parametrized by summing the parameters(i, j). It is similar to the idea of dimensionality reduction. This property is known as collapsing. Another property is that a random variable that has a gamma distribution can be proven to follow a Dirichlet distribution.
 are probabilities often described using the famous stick-breaking example. To explain these values, a stick of length one unit is used to randomly generate a number between zero and one(max length of the stick), at which the stick is going to be broken. Once that has been generated, the stick can be broken at a length  which represents a random value from a Beta distribution with 1 and  as parameters:   Beta(1,). By breaking that stick, it will generate a probability mass function(PMF) with two results having probabilities  and 1 each. Two sticks can be further broken similarly so that the sum of lengths for all pieces must equal one. And the process can be repeated indefinitely.
Dirichlet distribution is often explained in the context of topic modeling and LDA(Latent{Hidden topics} Dirichlet{Dirichlet Distribution} Allocation). LDA works by clustering many documents into topics containing similar words without prior knowledge of these topics. LDA builds a document by sampling from two distributions(Distribution of topics by document, distribution of words by topic).
In LDA, each topic has a multinomial distribution(H) over words, each document is sampled from a Dirichlet distribution() parametrized by , and each word(xi) is sampled from hidden topics(Zi) having a multinomial distribution parametrized by .
By classifying each document, LDA tends to make each document meaningful by maximizing its probability, which looks like the following:
However, maximizing this formula is quite expensive. Therefore Gibbs sampling is used to maximize each parameter of the equation(words: x, topics: z...).
 LDA Algorithm.
0. Initialize the number of topics k.
1. Randomly classify each word for each document into one topic.
2.Iterate on each document, and compute the following probabilities:
3. Reclassify each word to a given topic.
4. Repeat until the previous formula reaches its maximum.
 Advantages.
0. Very efficient and flexible for large datasets.
1. The workflow of the algorithm is independent of other tasks.
 Drawbacks.
0. The number of topics k must be defined in advance.
1. Uncorrelated topics.
In density-based clustering, dense regions in the data space are separated from those with lower density. Observations are assigned to a given cluster if its density in a certain location is larger than a predefined threshold.
For a given observation in one cluster, the local density around that point must exceed some threshold. The local density is defined by two parameters: the radius  of the circle that contains a certain number of neighbors around a given point and a minimum number of points around that radius: minPts.
 Definitions.
0. Eps-Neighborhood: The area of the circle of radius eps for a given point.
1.Density reachable: A point p is described as density reachable from point q with respect to Eps and MinPoints iff there is set of points(p1, p2, ..., pi,..., pn) in such a way pi+1 is directly reachable from pi.
2. Directly Density-reachable: A point p is described as directly density reachable from point q with respect to Eps and MinPoints iff p belongs to the circle of radius Eps and the radius of that circle is larger than or equal to MinPoints.
3. Density-Connected: A point p is described as density connected to point q with respect to Eps and MinPoints iff there is a point w that is density reachable from p and q.
 Advantages.
0. Doesn't require the number of clusters k.
1. Discovers more complex shapes of clusters(e.g. moons shape clusters.).
2. Outliers detections.
 Drawbacks.
0. Computationally infeasible to classify topologically connected objects.
1. Doesn't maintain scalability as K-means.
2. Sensitive to Eps, MinPts
3. The density measure is affected by sampling data points.
It is by far the most popular density-based clustering algorithm with more than 41k citations on Google Scholar. The central idea is to partition the observations into 3 types of points group:
0. Core points: There are more than minPts points in the -neighborhood.
1. Boundary points: Fewer than minPts within  but in the neighborhood of a core point.
2. Noise or Outlier points: All remaining points: Not a core point, and not close enough to be reachable from a core point.
 Explanation.
It starts by randomly choosing a point that has not been yet assigned to a cluster. Then the algorithm determines if it is a core point or outlier. Once finding a core point, all its density reachable observations will be added to a cluster. After that, the algorithm will perform a neighbor jumps to each directly reachable point and add them to the cluster. If an outlier has been added, it will be labeled as a boundary point. The algorithm then picks another core point and repeats the previous steps until all points have been assigned to clusters or labeled as outliers.
 Algorithm.
0. Randomly pick a point P.
1. Discover all the points that are density reachable from P given eps and minPts.
2. Test if P is a core point. A cluster will be formed with at least one core point, reachable core points, and all their borders.
3. Repeat the previous steps until all points have been traversed.
 Advantages.
0. Able to determine arbitrarily shaped clusters.
1. Less sensitivity to outliers.
2. Can be used as outliers detection.
3. Works effectively with any size of datasets.
 Drawbacks.
0. Doesn't scale well for a high-dimensional dataset.
1. Depends on several hyperparameters.
2. Problems in finding clusters of varying density.
3. Works only on numerical data.
 Applications.
It is widely used in anomaly detection, scientific literature, and other applications.
As the name suggests, this algorithm differs from the previous one by adapting the values of Eps and MinPts on behalf of the density distribution for each cluster. It automatically finds proper Eps and MinPts values.
It starts by randomly choosing a value for Eps. Then, it runs DBSCAN on the dataset, and if it fails to find a cluster, it increases the value of Eps by 0.5. When the algorithm finds a cluster(10% of similar data), it excludes the cluster from the dataset. And, the algorithm keeps increasing the value of Eps to find the next cluster. Once the algorithm successfully finishes scanning around 95% of the data, the remaining data points will be declared outliers.
However, ADBSCAN requires an initial value for the number clusters in the dataset. For more information, consider reading this paper.
DENCLUE applies the kernel density estimation method to estimate a random variable's undefined probability density function that generates the data sample. The estimation is based on a kernel density function(e.g., Gaussian density function.) representing the distribution of each data point. Then, the kernel density estimate of all the previous functions is computed by summing them up(or integral).
A kernel is a mathematical function that models the influence between data points and their neighbors. Additionally, a kernel density function has the following properties:
0. Non-negativity: K(x)  0
1. Symmetric: K(x) = K(-x)
2. The area under the kernel must equal one unit.
3. Decreasing: K'(x)  0
DENCLUE uses a concept of density attractors that serve as representatives for observations where clusters are formed around.
There are two types of clusters:
0. Center defined clusters: It is formed by assigning the density of the points attracted to a given density attractor.
1. Arbitrarily shaped cluster: It is formed by merging density attractors having high densities (> a given threshold)
 Algorithm.
0. Estimate the overall kernel density function of the data space by adding the density functions of all data points.
1. Clusters are formed by identifying density attractors that constitute the local maxima of the estimated density function.
2. The local maxima are computed using the Hill-climbing algorithm with the gradient of the estimated density function.
 Advantages.
0. Significantly faster than DBSCAN.
1. Flexible for any arbitrary shaped clusters.
2. Works effectively with any size of datasets.
 Drawbacks.
0. Doesn't scale well for a high-dimensional dataset.
1. Depends on several hyperparameters.
2. Works only on numerical data.
Since the performance of DBSCAN depends on its parameters setup, Optics has extended DBSCAN, makes it less sensitive to parameter setting, and finding structures among clusters. The intuition is that higher density regions will be processed first before the lower ones based on two parameters:
0. Core distance: The smallest radius eps that contains at least MinPts observations.
1. Reachability distance: The minimum distance that makes two observations density-reachable from each other.
With that been said, Optics forms ordered clusters of the observations based on their density structure. Moreover, it uses the computed values of reachability distance for all points as a threshold in order to separate the data and outliers(points that are located above the red line).
 Algorithm.
0. Pick a random data point from the dataset.
1. Determine whether the selected point is a core point or not by computing the core distance within the eps-neighborhood.
2. If the selected point is a core point, then for each other observations, update the reachability-distance from the previously selected point. Further, insert the new observation into an OrderSeeds which contains points sorted by their reachability distance.
3. If the selected point is not a core point, then moves to the next observation in the OrderSeeds or the next one in the initial data point if OrderSeeds is empty.
4. Repeat until all observations have been traversed.
 Advantages.
0. Able to discover intrinsic and hierarchically nested clustering structures.
1. Requires the same number of parameters as DBSCAN(eps and minPts) but eps is not required which reduces the runtime complexity of the algorithm.
2. Able to find clusters that have a varying density.
 Drawbacks.
0. Problems with clusters that don't have density drops.
1. Still sensitive to parameter minPts.
 Applications.
Optics can be used for anomaly detection(discovering outliers).
Throughout this article, you've learned how cluster analysis can be used as a powerful technique to discover patterns and extract insights from data. However, deciding whether to choose a given clustering algorithm depends on several criteria such as the clustering application's goal(e.g., topic modeling, recommendation systems ...), data type, etc. Moreover, it is the responsibility of the data mining team to decide to choose the best fit for their need.
Wohoooo! You've reached the end of today's blog, which is a little bit overwhelming, not gonna lie. However, you can relieve the stress by smashing the  button till you feel relieved .
I hope you enjoyed this post that took me ages(~ one month) to make it concise and simple as much as possible. I would appreciate your support by following me to stay tuned for the upcoming work and/or sharing this article so others can find it.
As you can tell from the illustrations, I have managed to implement and visualize most of the algorithms. And I am going to publish the first release on GitHub when it gets done.
As always, you can use any illustration and other information from this post by citing it as:
Mahmoud Harmouch, 17 clustering algorithms used in data science & mining, towards data science, April, 23, 2021.
If you have any word of wisdom that needs to impart, I am so pleased to read your thoughts down in the comments section.
If you've encountered any misinformation or mistake throughout this article, don't forget to mention them for the sake of content improvement.
See you in the next article.
Peace
a. k-means
[0] Notebook: 05.11-K-Means.ipynb
[1] Chandan K. Reddy, Bhanukiran Vinzamuri; A Survey of Partitional and Hierarchical Clustering Algorithms
[2] jeffp; L10: k-Means Clustering
[3] Chris Piech; K Means
b. k-means++
[0] David Arthur, Sergei Vassilvitskii; k-means++: The Advantages of Careful Seeding.
c. k-means||
[0] Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, Sergei Vassilvitskii; Scalable K-Means++
d. FCM
[0] Madhukumar, S. & Santhiyakumari, N.. (2015). Evaluation of k-Means and fuzzy C-Means segmentation of MR images of brain. The Egyptian Journal of Radiology and Nuclear Medicine. 1. 10.1016/j.ejrnm.2015.02.008.
e. k-medoids
[0] Wikipedia's article on K-medoids:https://en.wikipedia.org/wiki/K-medoids
[1] K-medoids implementation by Tri Nguyen: https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05
[2] Github repository of scikit learn_extra: https://github.com/scikit-learn-contrib/scikit-learn-extra/tree/master/sklearn_extra/cluster
f. k-medians
[0] Sanjoy Dasgupta, Nave Frost, Michal Moshkovitz, Cyrus Rashtchian; Explainable k-Means and k-Medians Clustering
[1] David Dohan, Stefani Karp, Brian Matejek; K-median Algorithms: Theory in Practice
[2] Li, Jinhua & Song, Shiji & Zhang, Yuli & Zhou, Zhen. (2016). Robust K-Median and K-Means Clustering Algorithms for Incomplete Data. Mathematical Problems in Engineering. 2016. 1-8. 10.1155/2016/4321928.
g. k-modes
[0] Zengyon He. Approximation Algorithms for K-modes Clustering
[1] Miguel A. Carreira-Perpinan, Weiran Wang. The K-modes algorithm for clustering
[2] ZHEXUE HUANG. Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values
[3] Sharma, N. and N. Gaud. ""K-modes Clustering Algorithm for Categorical Data."" International Journal of Computer Applications 127 (2015): 1-6.
h. k-prototypes
[0] Jia, Ziqi. (2020). Weighted k-Prototypes Clustering Algorithm Based on the Hybrid Dissimilarity Coefficient. Mathematical Problems in Engineering. Hindawi.
[1] ZHEXUE HUANG. (1998). Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values. Mathematical and Information Sciences, GPO Box 664, Canberra, ACT 2601, Australia.
[2] Huang, Zhexue. ""CLUSTERING LARGE DATA SETS WITH MIXED NUMERIC AND CATEGORICAL VALUES."" (1997).
[3] Byoungwook Kim. (2017). A Fast K-prototypes Algorithm Using Partial Distance Computation.
i. CLARA and CLARANS
[0] Erich Schubert, Peter J. Rousseeuw: Faster k-Medoids Clustering: Improving the PAM, CLARA, and CLARANS Algorithms. Similarity Search and Applications. SISAP 2019: 171-187 https://doi.org/ 10.1007/978-3-030-32047-8_16
[1] Ng, Raymond & Han, Jiawei. (2002). CLARANS: A method for clustering objects for spatial data mining. Knowledge and Data Engineering, IEEE Transactions on. 14. 1003- 1016. 10.1109/TKDE.2002.1033770.
[2] Vijaya Sagvekar , Vidya Sagvekar, Kalpana Deorukhkar.(2013). Performance assessment of CLARANS: A Method for Clustering Objects forSpatial Data Mining. G.J. E.D.T.,Vol.2(6):1-8
j GMM.
[0] Reynolds D. (2009) Gaussian Mixture Models. In: Li S.Z., Jain A. (eds) Encyclopedia of Biometrics. Springer, Boston, MA. https://doi.org/10.1007/978-0-387-73003-5_196
[1] Notebook: 1 Gaussian Mixture Model
[2] Notebook: Exercise - 1D Gaussian Mixture Model and Expectation Maximization
k. DMM
[0] Blog: Dirichlet Process Gaussian mixture model via the stick-breaking construction in various PPLs
[1] Slides: Memoized Online Variational Inference for Dirichlet Process Mixture Models
[2] Blog: Visualizing Dirichlet Distributions with Matplotlib
[3] Blog: Ritchie Vink, Clustering data with Dirichlet Mixtures in Edward and Pymc3.
l. DBSCAN
[0] Michael Hahsler, Matthew Piekenbrock, Derek Doran. dbscan: Fast Density-based Clustering with R
[1] Erich Schubert, Jorg Sander, Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. 2017. DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN. ACM Trans. Database Syst. 42, 3, Article 19 (July 2017), 21 pages. https://doi.org/10.1145/3068335
m. DENCLUE
[0] Hinneburg, A. and H. Gabriel. ""DENCLUE 2.0: Fast Clustering Based on Kernel Density Estimation."" IDA (2007).
n. OPTICS
[0] Ankerst, Mihael & Breunig, Markus & Kriegel, Hans-Peter & Sander, Joerg. (1999). OPTICS: Ordering Points to Identify the Clustering Structure. Sigmod Record. 28. 49-60. 10.1145/304182.304187.
[1] Slides: Optics ordering points to identify the clustering structure
",19
https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3?source=tag_archive---------7-----------------------,Introduction to Genetic Algorithms  Including Example Code,A genetic algorithm is a search heuristic that is inspired by Charles Darwin's theory of natural evolution. This algorithm reflects the...,Vijini Mallawaarachchi,4,"A genetic algorithm is a search heuristic that is inspired by Charles Darwin's theory of natural evolution. This algorithm reflects the process of natural selection where the fittest individuals are selected for reproduction in order to produce offspring of the next generation.
The process of natural selection starts with the selection of fittest individuals from a population. They produce offspring which inherit the characteristics of the parents and will be added to the next generation. If parents have better fitness, their offspring will be better than parents and have a better chance at surviving. This process keeps on iterating and at the end, a generation with the fittest individuals will be found.
This notion can be applied for a search problem. We consider a set of solutions for a problem and select the set of best ones out of them.
Five phases are considered in a genetic algorithm.
The process begins with a set of individuals which is called a Population. Each individual is a solution to the problem you want to solve.
An individual is characterized by a set of parameters (variables) known as Genes. Genes are joined into a string to form a Chromosome (solution).
In a genetic algorithm, the set of genes of an individual is represented using a string, in terms of an alphabet. Usually, binary values are used (string of 1s and 0s). We say that we encode the genes in a chromosome.
The fitness function determines how fit an individual is (the ability of an individual to compete with other individuals). It gives a fitness score to each individual. The probability that an individual will be selected for reproduction is based on its fitness score.
The idea of selection phase is to select the fittest individuals and let them pass their genes to the next generation.
Two pairs of individuals (parents) are selected based on their fitness scores. Individuals with high fitness have more chance to be selected for reproduction.
Crossover is the most significant phase in a genetic algorithm. For each pair of parents to be mated, a crossover point is chosen at random from within the genes.
For example, consider the crossover point to be 3 as shown below.
Offspring are created by exchanging the genes of parents among themselves until the crossover point is reached.
The new offspring are added to the population.
In certain new offspring formed, some of their genes can be subjected to a mutation with a low random probability. This implies that some of the bits in the bit string can be flipped.
Mutation occurs to maintain diversity within the population and prevent premature convergence.
The algorithm terminates if the population has converged (does not produce offspring which are significantly different from the previous generation). Then it is said that the genetic algorithm has provided a set of solutions to our problem.
The population has a fixed size. As new generations are formed, individuals with least fitness die, providing space for new offspring.
The sequence of phases is repeated to produce individuals in each new generation which are better than the previous generation.
Given below is an example implementation of a genetic algorithm in Java. Feel free to play around with the code.
Given a set of 5 genes, each gene can hold one of the binary values 0 and 1.
The fitness value is calculated as the number of 1s present in the genome. If there are five 1s, then it is having maximum fitness. If there are no 1s, then it has the minimum fitness.
This genetic algorithm tries to maximize the fitness function to provide a population consisting of the fittest individual, i.e. individuals with five 1s.
Note: In this example, after crossover and mutation, the least fit individual is replaced from the new fittest offspring.
Check out this awesome implementation of genetic algorithms with visualizations of the gene pool in each generation at https://github.com/memento/GeneticAlgorithm by mem ento.
Thank you very much mem ento for sharing this repo with me and letting me add the link to the article.
",20
https://medium.com/@voshart/photoreal-roman-emperor-project-236be7f06c8f?source=tag_archive---------2-----------------------,Photoreal Roman Emperor Project,54 Machine-learning assisted portraits,Daniel Voshart,3,"Using the neural-net tool Artbreeder, Photoshop and historical references, I have created photoreal portraits of Roman Emperors. For this project, I have transformed, or restored (cracks, noses, ears etc.) 800 images of busts to make the 54 emperors of The Principate (27 BC to 285 AD).
Update Sept 10th: New print available here (version 2) in a choice of languages. (Version 1: Gold / red marble, limited edition print sold out)
The main technology behind Artbreeder is it's generative adversarial network (GAN). Some call it Artificial Intelligence but it is more accurately described as Machine Learning.
Artistic interpretations are, by their nature, more art than science but I've made an effort to cross-reference their appearance (hair, eyes, ethnicity etc.) to historical texts and coinage. I've striven to age them according to the year of death  their appearance prior to any major illness.
My goal was not to romanticize emperors or make them seem heroic. In choosing bust / sculptures, my approach was to favor the bust that was made when the emperor was alive. Otherwise, I favored the bust made with the greatest craftsmanship and where the emperor was stereotypically uglier  my pet theory being that artists were likely trying to flatter their subjects.
Some emperors (latter dynasties, short reigns) did not have surviving busts. For this, I researched multiple coin depictions, family tree and birthplaces. Sometimes I created my own composites.
ABOUT THE PRINT
The working file for this print is enormous: 340 dpi at 24x36"". Available in English, Spanish, Italian, Latin, Polish and Russian.
ABOUT THE AUTHOR
Daniel Voshart is a designer from Canada. First edition print was a quarantine project. Second edition print made possible by the overwhelming support and important critical feedback to the first print.
",21
https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114?source=tag_archive---------6-----------------------,Fundamental Techniques of Feature Engineering for Machine Learning,All required methods for comprehensive data preprocessing with Pandas examples.,Emre Rencberoglu,14,"Emre Rencberoglu
What is a feature and why we need the engineering of it? Basically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for feature engineering arises. I think feature engineering efforts mainly have two goals:
The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.
 Luca Massaron
According to a survey in Forbes, data scientists spend 80% of their time on data preparation:
This metric is very impressive to show the importance of feature engineering in data science. Thus, I decided to write this article, which summarizes the main techniques of feature engineering with their short descriptions. I also added some basic python scripts for every technique. You need to import Pandas and Numpy library to run them.
Some techniques above might work better with some algorithms or datasets, while some of them might be beneficial in all cases. This article does not aim to go so much deep in this aspect. Tough, it is possible to write an article for every method above, I tried to keep the explanations brief and informative. I think the best way to achieve expertise in feature engineering is practicing different techniques on various datasets and observing their effect on model performances.
Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.
Some machine learning platforms automatically drop the rows which include missing values in the model training phase and it decreases the model performance because of the reduced training size. On the other hand, most of the algorithms do not accept datasets with missing values and gives an error.
The most simple solution to the missing values is to drop the rows or the entire column. There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing values with higher than this threshold.
Imputation is a more preferable option rather than dropping because it preserves the data size. However, there is an important selection of what you impute to the missing values. I suggest beginning with considering a possible default value of missing values in the column. For example, if you have a column that only has 1 and NA, then it is likely that the NA rows correspond to 0. For another example, if you have a column that shows the ""customer visit count in last month"", the missing values might be replaced with 0 as long as you think it is a sensible solution.
Another reason for the missing values is joining tables with different sizes and in this case, imputing 0 might be reasonable as well.
Except for the case of having a default value for missing values, I think the best imputation way is to use the medians of the columns. As the averages of the columns are sensitive to the outlier values, while medians are more solid in this respect.
Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like ""Other"" might be more sensible, because in such a case, your imputation is likely to converge a random selection.
Before mentioning how outliers can be handled, I want to state that the best way to detect the outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a decision with high precision. Anyway, I am planning to focus visualization deeply in another article and let's continue with statistical methodologies.
Statistical methodologies are less precise as I mentioned, but on the other hand, they have a superiority, they are fast. Here I will list two different ways of handling outliers. These will detect them using standard deviation, and percentiles.
If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?
There is no trivial solution for x, but usually, a value between 2 and 4 seems practical.
In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation.
Another mathematical method to detect outliers is to use percentiles. You can assume a certain percent of the value from the top or the bottom as an outlier. The key point is here to set the percentage value once again, and this depends on the distribution of your data as mentioned earlier.
Additionally, a common mistake is using the percentiles according to the range of the data. In other words, if your data ranges from 0 to 100, your top 5% is not the values between 96 and 100. Top 5% means here the values that are out of the 95th percentile of data.
Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.
On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it.
Binning can be applied on both categorical and numerical data:
The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized. (Please see regularization in machine learning)
The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.
However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like ""Other"".
Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. What are the benefits of log transform:
A critical note: The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.
Log(x+1)
One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.
This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. (For details please see the last part of Categorical Column Grouping)
Why One-Hot?: If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns. If all the columns in our hand are equal to 0, the missing value must be equal to 1. This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. This function maps all values in a column to multiple columns.
In most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance. This kind of data called ""Tidy"".
Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.
 Hadley Wickham
Datasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. In such a case, we group the data by the instances and then every instance is represented by only one row.
The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.
I suggest three different ways for aggregating categorical columns:
Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either.
Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:
Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let's introduce it with two examples. First, a simple split function for an ordinary name column:
The example above handles the names longer than two words by taking only the first and last elements and it makes the function robust for corner cases, which should be regarded when manipulating strings like that.
Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row.
In most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?
Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but it might be still nice to apply. However, the algorithms based on distance calculations such as k-NN or k-Means need to have scaled continuous features as model input.
Basically, there are two common ways of scaling:
Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.
Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.
In the following formula of standardization, the mean is shown as  and the standard deviation is shown as .
Though date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like ""01-01-2017"".
Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. Here, I suggest three types of preprocessing for dates:
If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them.
I tried to explain fundamental methods that can be beneficial in the feature engineering process. After this article, proceeding with other topics of data preparation such as feature selection, train/test splitting, and sampling might be a good option.
You can check my other article about Oversampling.
Lastly, I want to conclude the article with a reminder. These techniques are not magical tools. If your data tiny, dirty and useless, feature engineering may remain incapable. Do not forget ""garbage in, garbage out!""
Head of Data Science - Hepsiburada
",22
https://medium.com/@gordicaleksa/how-i-got-a-job-at-deepmind-as-a-research-engineer-without-a-machine-learning-degree-1a45f2a781de?source=tag_archive---------3-----------------------,How I Got a Job at DeepMind as a Research Engineer (without a Machine Learning Degree!),"In this blog I give you the tips on how to land a job at your dream AI company (like OpenAI, DeepMind, etc.).",Aleksa Gordic,31,"Aleksa Gordic
I recently landed a job at DeepMind as a Research Engineer! It's a dream come true for me, I still can't believe it! (if you feel like an imposter sometimes, trust me, you're not alone...)
I don't have a Ph.D. in ML.
I don't have a master's in ML.
In fact, I don't have any type of degree in ML  I did my bachelor's in EE.
That's it. That's my formal education. I dropped out of my EE master's as I figured out I could learn stuff much more effectively and efficiently on my own.
My first exposure to ML was back in 2018. And my first exposure to the world of programming was when I was 19 (I'm 27 now).
So how in the world did I pull it off? (hint: I'm not that smart, you can do it!)
In this blog, I'll try to tell you the whole story. I'll be very transparent in order to help you guys out as much as possible. This blog even contains a snapshot of my resume (very much suboptimal, I feel uncomfortable sharing it ), the information on how I got referred to DeepMind, and many more details that will hopefully make this blog much closer to your heart.
I'll structure this blog post into roughly 3 parts:
Note: for all practical purposes you can read DeepMind in this post as ""any top-notch AI lab/company like DeepMind, OpenAI, FAIR, AI2, etc."" because I'm fairly sure that the same (or at least similar) principles would apply there as well.
Well, let's go!
It's the summer of 2017. I'm doing my first ever internship at a small German startup called Telocate, as an Android dev. I'm about to graduate (in September, once I return back to Belgrade, Serbia) from an EE program that had a strong focus on digital/analog electronics and less so on CS.
Sometime around the end of my studies (end of 2016), I realized I actually want to completely pivot to the software industry. It's exciting, it's much more open compared to the hardware industry, there are all these hackathons and datathons, and it offers amazing salaries.
So earlier that year (2017) I started learning Android on my own and I landed this internship in Freiburg, Germany. I was so excited  the world was mine.
Snap back to the summer of 2017, Freiburg. I'm chatting with a friend whom I knew was a very successful software engineer. He's done multiple internships at companies such as MSFT, Facebook, Jane Street, etc. and he was my peer.
I realized I'm falling behind by a lot (along the SE dimension at least)! Some of my friends were doing competitive programming since their high school days. Algorithms and data structures were their second nature at this point.
I wanted to apply for big tech companies, and I suddenly realized that all of the skills I accumulated throughout the years are not that important when you're interviewing at FAANG.
Nobody cared that I speak 5 languages, that I know a bunch about how microcontrollers work in the tiniest of details, how an analog high-frequency circuit is built from bare metal, and how computers actually work. All of that is abstracted away. You only need...algorithms & data structures pretty much.
I felt confused and angry at the same time. Some kid who's 16 (I was 23 back then), could go and do some Leetcode problems for 9 months and he is already ahead of me in this respect. Even though he/she lacks so much of the fundamental engineering knowledge.
It's a nasty feeling, I very much remember it. I was very ambitious throughout whole my life, and hardworking, and yet I'm falling behind. I remember hating my life for not figuring out about this earlier on in my childhood.
But, what can you do, this world is obviously not fair. Some kids get to grow up in Silicon Valley and have rich and educated parents whereas some kids are born in a ""third world"" country, sometimes starving to death.
This strong feeling of injustice was giving me energy. I thought to myself  I'm going to hustle really hard over the next years. Warp speed on.
I came back from Germany. I successfully graduated 3 days after that. I remember some of my friends were celebrating their graduation for 3+ weeks, drinking  and everything that goes along. Me? I celebrated my success over the weekend and I immediately started planning ahead. I felt I was on a mission.
I enrolled in a master's program ""symbolically"" (you get some perks as a student) but I knew that I really wanted to land a job at a big tech company.
And so I made my own SE curriculum, plus, I started attending an algorithms course at my university without even being enrolled. I was sitting there with students that were 2 years younger than me.
Over the next couple of months I was working like crazy, and I immediately started applying. Soon after, I got myself an interview with Facebook in December 2017 and I failed miserably on my first interview.
I kept learning and I got an invitation from Microsoft  the tests are in March 2018 they said. Got it, I'm gonna work hard until then. I was learning, I was attending hackathons and datathons, and I started doing competitive programming (mostly Topcoder).
Then, in February 2018, I heard about this ML summer camp being organized by Microsoft people (we have an office in Belgrade, the only big tech company we have). I applied and after nailing a 3-day long entrance test I got accepted!!! (my decent digital image processing background helped)
I had a strong gut feeling that this may be my opportunity to show that I'm good enough to work at Microsoft! 
Around that same time, I landed an internship in Ouro Preto, Brazil via a student organization called IAESTE. It ended up being more of an amazing life experience (I lived in a fraternity with 11 Brazilians and I had to learn to speak Portugueses) rather than a tech internship. It was scheduled right after my ML summer camp. It's as if the stars aligned!
March 2018 came, and I did my first tests with Microsoft. I failed the written entrance tests. Luckily 2 months later they gave me another opportunity. I tried again, passed the written entrance tests, did a round of 4 back to back interviews on-site, and 3 weeks later...they told me that there currently are no positions opened for me. I interpreted that as a rejection. Again.
So if there is somebody that can empathize with newcomers to this field that's me. I know that it can be very mentally draining, this whole job hunt process.
Fast forward to the summer of 2018. I attended the ML summer camp and I nailed it. I was very engaged, I learned a lot and our final project was internally voted as the best one (solving Rubik's cube with deep RL).
Immediately after the camp, I flew to Brazil in early August 2018.
3 days in and I get a call from Microsoft. You got an offer. We need you asap.
I was super happy and super sad at the same moment. I had to leave this beautiful country of Brazil for a bit over a month instead of after 3 months. It was either that or I wouldn't get the job. They told me that the team that wants me is the ""HoloLens team""!!! The same team that ran the summer camp. The most famous, exciting team we had in Belgrade  I couldn't miss this opportunity. I took it.
September 2018, I'm in Belgrade working at Microsoft as a software engineer! I pulled it off. I was incredibly happy.
And that's where my SE and ML story really started.
But, ever since I attended that summer camp the idea of DeepMind and the feeling that it is almost unreachable for me was deeply in my subconsciousness. The only folks I knew there (some of them were lecturing at the camp) studied at Oxford or Cambridge.
But there is a part of me that's very attracted to impossible challenges. I thought to myself I nailed Microsoft even though I once thought that's very hard. Why should this be any different?
And so that idea was buried into my subconsciousness. I really enjoyed the fact that I'm at Microsoft now.
Throughout 2018 and 2019 I was focusing really hard on learning as much as I can about software engineering while working at Microsoft. I was reading coding books such as the C++ book from Scott Meyers, etc.
Other than that I started doing ML in my free time.
I finished all of Andrew Ng's Coursera courses before the end of 2018. I attended internal ML hackathons, I was heavily involved around the organization of the aforementioned ML camp, and I started reading research papers!
I still remember how they made me feel (and they still do!).  I felt very dumb. I was like, really? I studied EE for 4 years and I can still feel this way? (Little did I know; a semidefinite chuckle)
Somehow, around that same time, I also discovered Lex Fridman's podcast. That's 3 years ago before he even had 50k subs on his YouTube channel! I remember religiously watching all of the 20ish podcasts he had back then.
I remember I started feeling increasingly confident with my ML knowledge. And so beginning of 2019 I shared my first blog ever on how to get started with ML:
gordicaleksa.medium.com
(My writing was terrible back then, and it still probably is, I apologize for that. I'll try to update it over the next couple of days after publishing this blog (I won't change the content as I want to keep it as a historical document and also, the strategy that I'd take to learn ML hasn't changed, so it's still very much relevant))
End of 2019 I internally made a switch to an ML role (due to my religious engagement that my manager noticed). I was sent to ICCV 2019 conference.
I returned and I got the task of implementing a paper from scratch in PyTorch. I didn't know any PyTorch back then. Needless to say  I learned a lot.
And this is where another warp speed moment happened. But this time, I was smarter, this time I'm going to share my journey online as I progress.
So what happened? What made me change my mindset? What made a push from this old Aleksa, that was learning and working hard but kept everything for himself, to this new Aleksa?
Well, throughout 2019, as a ""side track"", I was reading a lot about entrepreneurship, marketing, and personal finance. And along the way, I found this guy called Gary Vaynerchuk who motivated me to start sharing stuff I do online. Thanks Gary! 
I know most of you probably don't like him (if you know of him that is), but he had a positive influence on my thinking. And for that I'm grateful. He definitely wasn't the only person that influenced my mindset (I was sharing my code even back in 2018) but he finally made it click.
You probably may say (if you don't know me): ""Well, this story is inspiring and all but how did you create your own SE/ML curriculum in the first place?""
Well, before all of this happened, I already had a strong track record of learning on my own. If it still feels like magic I strongly recommend you go ahead and read this blog:
gordicaleksa.medium.com
I'll leave a TL;DR here: through my workout routines, learning human languages throughout my life, and maths (all of that on my own) I got skillful at learning. Coursera's ""Learning how to learn course"" helped as well.
Now that you have all of the necessary context let me tell you about the ML curriculum I followed to land a job at DeepMind!
New Year 2020 came along. I just finished reading ""Crushing it"" from garyvee and decided I should start my own YouTube channel sharing my ML learning journey along the way.
The pandemic struck and I was in warp speed again. The stars aligned. 
I decided that over the next year I want to cover various subfields of AI, and I roughly decided that I want to dedicate 3 months to each one of these.
But things weren't that smooth in the beginning, I was still figuring stuff out. My first subfield, neural style transfer (NST), took longer than 3 months because I found it enjoyable and I thought why rush it?
Along the way, I found a perfect strategy for myself.
I structured my learning the following way. I had ""macro"" cycles (3-month periods where I tackle a new subfield) and I had multiple micro-cycles interspersed over macro.
The micro-cycles are of 2 types:
Now the tricky part was combining all of this with my full-time job at Microsoft! It took a significant amount of willpower. I'd hit the program as soon as I wake up for 2h, I'd go for a quick stroll, I'd do my job at Microsoft and a 30-minute power nap after I wrap it up, and finally I'd be working for additional 2-3 hours before going to sleep. The pandemic helped me maintain this crazy rhythm.
The time management and stress handling probably deserve a separate blog post, but it mostly boils down to willpower, correct mindset, as well as taking smart breaks (power naps are golden  let me know in the comments if you'd like a separate blog on the topic of time management).
Now let me go into the specifics of the macros I did on my way to DeepMind! Luckily I started writing blogs sometime around my transformers macro so if you want to know all the nitty-gritty details and the strategy I took, I'll link the blogs in every single macro section.
For the first 3 macros (NST, DeepDream, and GANs) I, unfortunately, didn't write a dedicated blog, but the intro section of the transformers blog below as well as the YouTube videos and projects I created can fill in the gaps:
gordicaleksa.medium.com
Having said that, here are the TL; DRs of every macro I did:
During this period I was learning about neural style transfer. I ended up reading a pile of research papers (though a lot less than later when I became proficient), implementing 3 different projects, which I then open-sourced on my GitHub, and I made an NST playlist on YouTube.
I think that this is a great topic to start with as it's highly visual and visceral. I recently wrote some of my thoughts on a similar topic in this LinkedIn post. Also, digital image processing and computer vision were the fields I felt the most comfortable with and so that's why this was a great starting point for me.
I perfected my PyTorch knowledge, learned a lot about CNNs and optimization methods, improved my presentation skills (just check out those README files ), became more proficient at reading research papers, and improved my SE skills in general  to name a few.
Here is an NST image synthesized using the code I wrote:
Ever since I first saw the pictures created with DeepDream I was fascinated. I knew I had to learn it. Every single detail. I couldn't just use online generators and not understand what was going on.
I was reading blogs, analyzing DeepDream subreddits, and exploring various codebases. Most of the original code was written in Torch & Lua combo. I remember losing a lot of time trying to set it up on Windows. Then I switched to Linux, got it to work, only to realize that I won't be using that repo either way. 
My output was a single video on the theory behind DeepDream and an open-source project. I learned a lot by doing this and I enjoyed the process. I used my code to generate these beautiful images, among them the current visual identity of The AI Epiphany. Here is an example:
GANs were still very popular in early 2020. I felt like I'm missing the necessary background and so I decided to tackle them next.
I've read all of the seminal papers (as well as other less prominent papers) and I decided to implement vanilla GAN, cGAN (conditional GAN), and DCGAN models. Here is an output from the DCGAN model I trained:
At this point in time, I refined my strategy. I realized that:
Unfortunately, I don't have GAN paper overviews from this period. Back then I was experimenting with videos such as ""PyTorch or Tensorflow"" which turned out to be the most popular ones (proving a hypothesis I had in my mind).
But I didn't seek popularity (not that I would mind in general), I was seeking a highly relevant audience. I'd much rather have 1 guy/gal from ""DeepMind"" following my work than 100 beginners  because I knew what my goals are.
I knew I wanted to learn more about BERT and the GPT family of models. Transformers were everywhere, and I didn't quite understand them.
This time I did everything correctly and executed the plan I sketched above. I ended up implementing the original transformer paper from scratch and learned a ton! I decided to create an English-German machine translation system. Since I speak those languages, I thought that'll make debugging easier, and it did.
I'm so grateful I did it back then  every now and then you need to understand the QKV attention, and it's second nature to me now. You can find out much more about this journey here:
gordicaleksa.medium.com
Even before I started doing transformers I knew I wanted to dig deeper into Graph ML (before it became as popular as it is nowadays).
And, in retrospect, I'm very glad I covered transformers/NLP first since the field of NLP had a huge influence on this field. GAT was directly inspired by the original transformer paper, DeepWalk by Word2Vec, and so on.
At this point in time, I was already chatting with Petar Velickovic on a fairly regular basis (I'll tell you a bit later more about how this came to be) and he knew that I'll be covering GNNs in the next period. He was very receptive and he told me that I can ping him whenever I felt stuck!
I realized that this is a great opportunity! I'll be learning Graph ML, a topic that fascinated me and I'll be communicating with Petar, a researcher at DeepMind, who was among the best researchers in this particular field.
Needless to say, I learned a lot! I made a ""popular"" YouTube GNN series that was shared by influential Graph ML researchers like Michael Bronstein, Petar, and others.
I also made a popular PyTorch implementation of GAT (Petar is the first author of GAT). The reason it became so popular is that it was, according to others, the most beginner-friendly resource out there. With 2 Jupyter notebooks, PPI and Cora datasets support, a nice README, and accompanying videos it filled the gap and made it easy for beginners to enter the field.
(It became a recommended repo for a Cambridge lecture on GNNs and I even got citations. )
I again massively used the fact that I could ping Petar if I had any problems and that this could be a great collaboration between the 2 of us!
For the whole story check out this blog:
gordicaleksa.medium.com
Drum roll. Surprise, surprise! I really wanted to learn RL either way but it so happens that DeepMind is famous for its RL breakthroughs. 
There were so many papers and projects I wanted to understand! Like AlphaGo, DQN, OpenAI's robotic hand that could solve Rubik's cube, etc.
This field proved to be very different compared to other ML subfields. The data point independence assumption doesn't hold anymore, a random seed can make your agent fail to converge, etc.
I'm very glad I already had a decent amount of experience before tackling RL, otherwise, I might have gotten demotivated. I started with stuff that's very close to my heart  computer vision  and I used it to ramp up my ML knowledge. Then, I slowly progressed toward topics I knew little about.
I again had some help here! Petar connected me with Charles Blundell (who's also a DeepMinder) and so from time to time, I used that opportunity, although a lot more frugally.
For the whole story check out this blog:
gordicaleksa.medium.com
And those are pretty much the main macros I had. Throughout the whole journey I was making notes in my OneNote:
(If there is enough interest I could maybe convert those to pdfs and share them on a public drive. Let me know in the comments!)
After RL, I started focusing on the latest and greatest papers. You can clearly notice that trend looking at my newest YouTube videos over the past 4 months.
And all of you who were following along my journey knew that these were my plans  I was very transparent throughout the whole process! (except for the fact that I wanted to apply for DeepMind the whole time)
Aside from this ""main"" track, already in the very beginning of 2020, when I started reading papers, I realized that I was having difficulties with mathematics so I took some time to read the ""Mathematics for Machine Learning"" book (and I went over 3B1B's awesome playlists).
That turned out to be a great move. I'm so happy I did it similarly to how I took a step back and took the ""Learning How to Learn"" course in September of 2019. All of these actions helped propel my learning speed.
Here are some other books I also read in this period:
Also, needless to say, all this time I was working at Microsoft on various SE-ML engineering projects. Unfortunately, I can't tell you a lot about all of that but here are some of the generic highlights I can share:
As a summary that's 1.5 years of warp-speed ML study, 1.5 years of non-warp ML study, 1-year of warp-speed SE study, and the rest is captured in my ""5 Tips to Boost Your Learning"" blog.
And that's my story.
Tip: an additional thing you could do to see more details of this whole period is to just scroll through all of my LinkedIn posts, that's where I was (and still am) regularly sharing my updates.
Middle of the year 2020 I reached out to Petar on LinkedIn. It turned out he was already following the content I was posting there and found it interesting, so that made things a bit easier! 
During my ICCV19 conference I met other cool DeepMinders, that were also from Serbia, like Relja Arandjelovic (who did his Ph.D. with professor Andrew Zisserman at Oxford) and Jovana Mitrovic (Oxford girl as well!).
Even though this was my first conference I immediately realized that the value of these conferences lies in:
IMHO, many people get this wrong. By the time you attend the conference most of the papers were already published to arXiv months ago (and chances are somebody already covered them and shared the summary online ).
That's why virtual conferences will never be a good replacement.
Then sometime around September of 2020, I made it clear to Petar and Relja that sometime next year, probably in April, I'll start applying for DeepMind and asked them for any tips on how I could prepare! In the meanwhile, both of them offered to refer me! 
Fast forward to Friday, April 2021. I was casually chatting with Petar, mentioned that I'm finally ready to apply for DeepMind, and while we were chatting he just said (without me even realizing): ""ok, I just finished submitting the referral!"" 
He then pinged a recruiter (Cameron Anderson, didn't know him back then but I can tell you now that he's amazing at his job) on LinkedIn, mentioned who I was, and asked whether it's fine for me to submit my LinkedIn/YouTube/GitHub instead of my resume. It turned out that Cameron was already following my work! And he said that it's fine! My God, I was so happy.
Everything happened so fast. I just sent my LinkedIn profile and I already had my interview scheduled for Monday.
So summarizing. Networking matters! A lot. BUT. Please don't spam people. Instead, create genuine connections. Ask what you can do for them and not what they can do for you. None of them, including me, won't recommend a person unless we're familiar with that person's work already and we can make some guarantees.
(Pro tip: contribute to an open-source project of the company you care about  that's a great way to get noticed! That way some of them will be familiar with your work and you can later ask for a referral that is if they don't already offer it themselves! Why is nobody using this strategy? It beats me.)
At one point an automated email came asking for my resume (although Cameron confirmed that it's not necessary). After some quick edits, I decided to send them my resume without investing too much effort into it (thinking that they probably won't even read it).
Which in retrospect could have been a big mistake. A well-written resume matters! And my interviewers did in fact use it. It's easier to ask questions looking at a 1/2 page resume than at a LinkedIn profile.
I'm leaving it here for legacy:
There are many things wrong with it. For a start, I should have written a lot more about my latest work at Microsoft. The thing is that my team at Microsoft was very secretive (for quite some time my other colleagues from Microsoft didn't know what I'm working on), and so I took the safest strategy  of not communicating anything to the outside world. 
Also, my resume was always a 1-pager, I should probably cut all of these older projects out. I have many other ideas on how to improve it.
Finally, let's see how my last preps for DeepMind looked like!
Ok, at this point I managed to build up solid foundations and I was referred for an RE position. You do need a solid background to land a job at DeepMind. I'm fairly sure I wouldn't even get a chance to interview if I didn't have all of my GitHub projects, prior experience, and other public artifacts.
(I don't have any paper publications but I compensated in different, original ways. REs don't necessarily need publications, as you can see. But the game may be different for RSs (research scientists). I'm fairly sure they need a strong record in publishing to top-tier conferences and a Ph.D. Take all of this with a grain of salt, I'll update this part when I start working at DeepMind and get myself better informed.)
But the battle wasn't won. As all of you know, the interviewing processes are...well, kind of dark magic.
Even when you're really good, big tech companies may reject you. It's safer for them to reject a great candidate (by a mistake) than to accept a really bad one (they want to reduce the false positives). Repeat after me  hiring pipelines are never perfect  this will save you from a lot of unnecessary stress!
Secondly, you're often asked stuff you won't actually be using in your daily job and that's why you have to explicitly prepare for the interviews.
Let me now tell you how I pulled it off.
First, here are some general tips:
For DeepMind I also did serious research around AGI. I read some of the seminal AI papers like:
I've previously also researched Alan Turing's seminal ""Computing Machinery and Intelligence"" paper as well as follow-up papers from John Searle, etc. You probably don't need these. (in case you want a summary here are my blogs, parts one and two)
And I also watched YouTube videos featuring some of the following people: Demis Hassabis, Shane Legg, Marcus Hutter, F. Chollet, Murray Shanahan.
(Flex: Demis liked a couple of my tweets )
The reason I did this research, is because it's DeepMind's mission  cracking AGI and using it to solve difficult scientific problems. You can also expect ""behavioral"" questions around these topics so I wanted to have a sound background.
Research as much as you can about DeepMind. For me, it came naturally since I was covering papers on YouTube, implemented some of them, knew people, culture, etc.
Those were some general tips. Replace DeepMind with your dream company and map a concept like ""AGI"" to ""whatever my dream company cares about"" and do your research on it.
Now let's see how DeepMind's hiring pipeline looks like for an RE position at the ""Core"" part of the team (more on DeepMind's org a bit later).
The pipeline consists of the following interviews (at the time I'm writing this, it may change later in the future but you should still have a general idea):
Let me now give you some tips on how to prepare for these!
My goal here is not to give you an unfair advantage or a way to game the hiring system, but rather guidelines on how you can get better at these topics, and thus hopefully a better engineer (and a better human being)! 
I'm writing this so that Aleksa of the past would be grateful to have had it!
Interview #1: A chat with the recruiter
Do your due diligence and understand what exactly it is that you want to do at DeepMind (as much as you can!). DeepMind has 2 ""teams"" (it's a soft type of a division, both ""teams"" are working towards a common mission):
Go over to DeepMind's website, read their blogs, and research all of the available positions you care about.
Also, I strongly recommend you read the first (behavioral) part of the ""Cracking The Coding Interview"" (CTCI) book. As I said every interviewer will be assessing your personality and whether you're fit for the team  so some preps in that regard definitely won't hurt!
Interviews #2 and #3: The ""quiz""
This is probably the hardest part to prepare for (although TBH, it wasn't for me, it really depends on where you're coming from). You need solid technical foundations to pass it.
I'll tell you how I prepared myself, although again, it might be overkill:
That should be enough. If you already feel confident with some of these  just skip it  treat this as a biased guideline rather than as ground truth.
My interview process already started at this point so this was more of a sprint rather than a month-long preparation. I strongly recommend you start preparing before you kick off the interviewing process.
Interview #4: The coding challenge
A couple of tips on how to read through CTCI:
You really don't need anything else other than CTCI! Just that. 
Interviews #5, #6 and #7: team lead interviews and people & culture
Just read the behavioral/first part of the CTCI book and follow my general tips (like be ready to talk about your projects and be familiar with your interviewer's research).
They'll ask you questions such as: ""What do you like about DeepMind?"", ""Why DeepMind and not some other company?"", ""Tell me something about your favorite project"". Stuff like that.
And that's it! You should be good to go now!
Finally, let me briefly tell you what happened to me. Some of you may recall from this video that I mentioned failing to land a job at DeepMind:
I failed the 6th (Senior Team Lead) interview. IMHO, it was literally just a misunderstanding more than anything else (I got the feedback via email they were very kind, and that's how I know that it was a misunderstanding), but life goes on!
As a ""technically strong"" candidate, I was rerouted to the Incubation/Applied part of the team. I again prepared myself following the general tips I gave you guys and got 4 times YES. I was accepted. Hurray!!!
The good thing is, after having interviewed with them, I realized that the Incubation team is probably an even better fit for me!
The interviews were very conversational in nature. They told me what the team does, there were some behavioral questions, some ML system design, and open-ended problems. The best way to prepare for those is to have genuine work experience.
And that's it. I guess the rest is history.
And these 2 viral posts on Twitter and LinkedIn. 
I'll be joining DeepMind later this year (December 2021) as a Research Engineer. I'm very excited about this next period of my life!
Many new things are waiting for me. I'll be moving to London, UK. I'll have to build up new social circles. I'll be exposed to a new culture, new languages (as London is quite multicultural), a new company, and even a new OS! 
As for what you can expect from me over the next period, I'll be:
On a personal note, I'll try and travel a bit more before December and I'll be doing some investing (I'll be reading the ""The Intelligent Investor"" book).
I got so many kind words and requests from my community at The AI Epiphany:
And so I felt obliged to share more about my journey to DeepMind in order to help you guys become better. And hopefully, by sharing this deeply personal journey I managed to help at least some of you.
If there is something you would like me to write about  write it down in the comment section or DM me. I'd be glad to write more about mathematics & statistics, ML/deep learning, CS/software engineering, landing a job at a big tech company, getting an invitation to prestigious ML camps, etc., anything that could help you!
Also feel free to drop me a message or:
And if you find the content I create useful consider becoming a Patreon!
Much love 
",23
https://towardsdatascience.com/towards-the-end-of-deep-learning-and-the-beginning-of-agi-d214d222c4cb?source=tag_archive---------0-----------------------,Towards the end of deep learning and the beginning of AGI,"How recent research points the way towards defeating adversarial examples and achieving a more resilient, consistent and flexible A.I",Javier Ideami,14,"Adversarial examples are a hot research topic in deep learning nowadays. Subtle, often invisible changes in the data can push our networks to make terrible mistakes. We, as human beings, seem to be way more resilient to these perturbations in our sensory inputs (though not totally immune).
There is a certain pattern in our deep learning systems. They achieve remarkable things, but they are also at times delicate and brittle. Like a rigid tree in the middle of a storm, they look majestic, but may crack at any time without warning. Why is this happening and how can we improve the situation?
Some clarity is starting to appear through new research arriving from the field of neuroscience. In this article we are going to explore it.
In his recently published book, ""A Thousand Brains: A New Theory of Intelligence"", a masterpiece that I really enjoyed, scientist and entrepreneur Jeff Hawkins dissects the latest research performed by his team on our neocortex, the part of the brain that occupies 70% of its volume and is responsible for our advanced intelligence. (the other 30% is occupied by the older, more primitive part of the brain).
In a fascinating journey, Jeff Hawkins takes us deep into the epicenter of our intelligence. He shares that:
Vernon Mountcastle was a leading american neurophysiologist and professor emeritus of neuroscience at Johns Hopkins university. He was the discoverer of the columnar structure of the cortex. And he proposed that through evolution, our neocortex became larger by basically copying over and over the very same thing, the same basic circuit.
When I read about Mountcastle's idea in Jeff's book, I got reminded of a fascinating talk by the great scientist Robert Sapolsky. Answering a question about what separates us from Chimps (https://www.youtube.com/watch?v=AzDLkPFjev4), Sapolsky explains that about half of the difference in gene expression between chimps and humans has to do with genes that are coding for olfactory receptors, other differences are related to the size of the pelvic arch, the amount of body hair, immune system recognition capabilities, some aspects of reproductive isolation, etc; those and others account for almost all the genetic differences between chimps and humans. So then, where are the differences in the genes that are relevant to the human brain? Sapolsky explains that there is hardly any, and the few identified are in genes having to do with the number of rounds of cell division during fetal brain development. Basically: we have 3 times more neurons than chimps. And this difference in scale seems to be key to our advanced intelligence.
This fits nicely with Mountcastle's idea of a single circuit that gets replicated many, many times (volume matters, but is volume enough to push today's deep learning systems towards AGI? let's keep exploring below).
That all parts of our neocortex are working based on a similar principle, on the same basic circuit, fits the flexibility our brain has demonstrated in different scenarios. And if volume matters, will this mean that GPT-11 may get us closer to AGI?
Unfortunately, it is not that simple. Because there is a massive elephant in the room that Jeff illuminates in his book and theory. One that we have been kind of ignoring for far too long.
Before we go to visit the elephant in the room, let's establish the context. According to the scientists, we have around 150000 cortical columns in our neocortex. Jeff tells us that we could think of these columns as if they were thin spaghetti. So, imagine 150000 thin spaghetti next to each other. That's your neocortex metaphorically speaking.
What is going on within these cortical columns? Over the last years scientists have come to realize that the brain is a predictive machine. It produces a model of the world and continuously predicts what will happen next.
When our brain's predictions are not correct, we realize that something is not right and our brain updates its model of the world. As time goes on, our model of the world gets richer and more sophisticated.
So in a way we indeed live in a simulation. For what we perceive is really the model that the brain constructs rather than the ""reality"" out there. This explains phantom limbs and other similar scenarios.
Jeff Hawkins points out that our brain learns a model of the world by paying attention to how the inputs it receives change as we move (or as those inputs move). And that takes us to the elephant in the room.
The world is changing constantly. Everything moves. And it makes sense that as things move and change, our brain keeps updating our model of the world (our many, many models as we will soon see).
And just as attention mechanisms have revolutionized the deep learning field in recent years, so is attention key in how our brain is learning these models.
But if our neocortex is making a very large amount of predictions constantly, and adapting to any misalignments between its models and what it perceives, why don't we notice all of those predictions and instead we perceive one continuous reality? Let's get there, step by step.
Through their latest research, Jeff and his team reach some fascinating insights:
Notice that the concept of movement is beginning to appear everywhere. Movement and the dynamic nature of systems are the elephant in the room. And we will soon discuss how that connects with the issue of adversarial examples and the limitations of much of what's going on in today's deep learning.
So, it is all about reference frames or maps, maps of physical spaces, maps of concepts, maps of anything. Jeff tells us that in the same way that reference frames in the old brain are learning maps of different environments, reference frames in the neocortex are learning maps of objects (in the case of what they call ""what"" columns), or the space around the body (in the case of ""where"" columns), or maps of concepts within the non-sensory columns.
I love the analogy that Jeff uses regarding how in order to be an expert in any domain we need to find a good way to organize our knowledge about that domain, we need to create internally a great reference frame or map of that domain. Think of the deep and complex reference frames that, for example, Leonardo Da Vinci or Einstein had in order to excel as they did within their respective areas of expertise.
All right, so each of our 150000 cortical columns is learning a predictive model of the world as it pays attention to how the inputs change throughout time. And each of these columns learns models of a large number of elements, objects, concepts, etc.
So our knowledge of anything, an object or a concept, is distributed across thousands of cortical columns, across thousands of complementary models. This relates to the name of Jeff's theory (A thousand brains).
And all of this connects with the flexible nature of our brain. Our neocortex doesn't depend on a single column. Knowledge is distributed across thousands of them. So the brain continues working even if an injury damages a set of columns (there are great examples about this in the academic literature).
The next thing to consider is: if the brain is creating new predictions everytime movement happens, where are these predictions being stored?
Jeff and his team propose that spikes that occur at different dendrites in a neuron are predictions (dendrites are branches in the neuron, and receive inputs through their synapses). Dendrite spikes put the cell connected to them into what Jeff calls a predictive state. Therefore, predictions happen inside neurons. And these predictions change the electrical properties of the neuron and make it fire sooner than it would otherwise, but the predictions are not sent through the axon to other neurons, which explains why we are not aware of most of them. The question now is: how do we settle on a specific prediction?
Our perception of reality is the result of a voting process. The different cortical columns reach consensus through voting and this is what produces a single perception that unifies different predictions coming from different parts of the system (that may also be related to a diversity of types of sensory inputs).
Only some cells need to vote, the ones that represent, for example, a specific object that we are perceiving. And how do they vote?
Most of the connections in our cortical columns are going up and down the different layers of the neocortex. But there are exceptions. Scientists have found that there are cells that send their axons (output connections) from side to side through the neocortex. Jeff and his team propose that these cells that have long distance connections are the ones responsible for the voting.
As we recognize an object, our cortical columns have reached a consensus on what it is that we are looking at. The voting cells (neurons) in each of our columns make a stable pattern that represents that object and where the object is located in relation to us.
And as long as we keep perceiving the same object, the state of those voting neurons doesn't change while we keep interacting with that element. Other neurons will change their state as we move or the object moves, but the voting neurons will remain stable.
This is why our perception is stable and we are not aware of the flurry of activity related to the moving predictions that are taking place. We are just aware of the final stable patterns that arise from the consensus reached by the cells that are voting.
Therefore:
It is time to return to the adversarial examples and the status of the deep learning field.
Human beings are not immune to adversarial examples. Perturbations in our sensory inputs can confuse us and make us interpret things incorrectly. Most of us have experienced a wide range of optical illusions. However, in general, our perception is consistent and quite resilient, and certainly way more consistent than the one we find in today's deep learning systems, where invisible changes can derail completely our results.
What is behind this resiliency, consistency and flexibility? Whatever it is, it may include some of the following:
So, ""the end"" of adversarial examples in deep learning, and by ""end"" I don't mean absolute end, just getting to a level of resiliency, consistency and flexibility similar to the one we have as human beings, will be possible with a combination of:
Researching new ways of detecting adversarial examples is an interesting area with much academic activity. What is missing now is to rethink our deep learning architectures and systems to transition from the current static paradigm to a dynamic one based on multi-modal, multi-model, consensus-based predictive systems that are resilient, consistent and flexible. When we reach that point, we will be able to hide or perturb parts of our systems and still maintain stable predictions.
As Jeff points out, this will become more and more crucial as we try to apply AI systems to scenarios that require a lot of flexibility and resilience.
Mountcastle's ideas, Sapolsky's thoughts and our fascination about the GPT architecture, all of those things indicate the importance of volume. Volume matters. Having 3 times more neurons, or thousands of copies of the same basic circuit, or hundreds of billions of parameters rather than 1 billion, all of that matters.
And that's good news for the current state of the deep learning field. With projects such as the GPT system, we are discovering and confirming that fact, that volume matters.
But, what we are also beginning to realize is that, as much as volume matters, it will not be enough to take us where we wanna go.
If you follow the latest conversations about systems like GPT-3 in a range of podcasts and venues, say for example at the machine learning street talk podcast, you will hear similar conclusions. GPT-3 is very impressive, but it is also kind of delicate, brittle, and it often feels like a hack. This has nothing to do with how resilient and flexible human brains are.
Volume matters. But movement also. We cannot escape movement and change just through sheer volume. The world is like a storm that never stops.
We are the static tree that gets larger and larger but keeps breaking over and over because it lacks the capacity to move with the storm.
Thinking is movement. Movement through reference frames. Movement across thousands of predictions and models unified through consensus mechanisms.
The way forward is through movement.
Afterword: In his book ""The Master Algorithm"" Pedro Domingos writes about different paradigms connected with deep learning: symbolists, connectionists, evolutionaries, bayesians and analogizers. It's clear that the path towards AGI could come through many different routes and combinations of approaches. In regards to Jeff and his team's work and theory, I am following, as professor Kenneth Stanley would say, a gradient of interestingness (and the magnitude of this gradient in regards to Jeff's work is pretty strong). It feels to me that Jeff's theory and work (alongside all his talented team) could hold inside very interesting and useful stepping stones that could bring us closer to AGI (or at a minimum their research could point us towards those stepping stones). So yes, we could get to AGI in many different ways, but so far the only intelligent system we know that is resilient and flexible enough is the one on top of our shoulders. So it does make a lot of sense that exploring in depth the latest research coming from neuroscience may point us towards useful stepping stones on the way to AGI.
And if you want to go deep into the mysteries of the entities that inspire our deep learning A.I systems and make your thoughts possible, check the related article below.
towardsdatascience.com
",24
https://medium.com/@karim-ouda/tutorial-document-classification-using-weka-aa98d5edb6fa?source=tag_archive---------9-----------------------,Tutorial: Document Classification using WEKA,Introduction,Karim Ouda,8,"This tutorial is an extension for ""Tutorial Exercises for the Weka Explorer"" chapter 17.5 in I Witten et al. 2011. Data Mining (3rd edition) [1] going deeper into Document Classification using WEKA
Upon completion of this tutorial you will learn the following1. How to approach a document classification problem using WEKA2. What are the options available in WEKA to prepare your dataset for Machine Learning classification algorithms3. Which algorithms works best for this problem4. How to evaluate the results5. Some tips
We will be using the ReutersCorn dataset which is already part of WEKA examples
It is assumed that you have WEKA installed on Linux and you have basic familiarity with the tool and machine learning
Document Classification is also a Data Mining problem and fortunately we can make use of the CRISP-DM (Cross Industry Standard Process for Data Mining) process, which according to Wikipedia is "" a data mining process model that describes commonly used approaches that data mining experts use to tackle problems"", the process is illustrated in the diagram below
Illustration 1: CRISP-DM Process
We will be covering all steps above except ""Deployment"" which is not relevant to this tutorial
Finally it is worth noting that this process is iterative, so in real life you will need to restart the process again based on the results of your evaluation/deployment
What we are doing in this tutorial is trying to find a Model that that can classify documents into ""Corn"" (Corn news) and Non-Corn ( not mainly about corn ) with best possible accuracy
Let's start the real work, in the following steps we will inspect the dataset to decide which steps are needed to prepare the data for classification
1. Go to ""/usr/share/doc/weka/examples/"" on Linux or similar path on Windows2. You will find the following files
ReutersCorn-train.arff.gzReutersCorn-test.arff.gz
The first one is the training dataset and the second will be used to test the model
3. Decompress both files using the following command
gunzip ReutersCorn-train.arff.gzgunzip ReutersCorn-test.arff.gz
4. Open the training file and inspect it
Illustration 2: ARFF file
As you can see the file is in WEKA ARFF format which is simple, a header section which includes description of attributes and the class label, so for our case we have 1 attribute which is ""Text"" of type String and ""class-att"" of type boolean (0 or 1) zero for non-corn and one for corn
The rest of the file is the data in the following format
Text, class labelText, class label
As you can see in the illustration below, there are many issues in the text file, such as
1. CASPITAL/small letters2. Special characters
Also there are some cases of redundancies and synonyms for the same word ( corn and maize ) and variations ( corning and cornfeed )
Illustration 3: Data cleaning examples
We will be fixing all these issues in the next stage, also you need to do the same for the test file (ReutersCorn-test.arff)
1. Open WEKA, Click on Explorer2. In the Preprocess tab, click Open file and select the training file3. From the Attributes section, click on ""class-att""
Illustration 4: WEKA input file initial inspection
Shown in the illustration above is the number of instances in the file, the number of attributes for each instances, the distribution of corn and non-corn instances which is 45 (2.9%) corn and 1509 (97.1%) non-corn news
4. You can also edit and view the file from WEKA, click Edit as shown below
Illustration 5: WEKA Editor
In this stage we will prepare the data for classification using information acquired from the observation phase, we will do the following1. Cleaning2. Text Tokenization & Transformation3. Attribute selection
Both cleaning and tokenization can be done using the StringToWordVector (STWV) filter in WEKA
What is Tokenization: To make the provided document classifiable using Machine Learning we need to do Feature extraction that is converting the normal text to a set of features that can then be used by the ML Algorithm to discriminate between corn and non-corn
In our case this will be done by STWV by assuming each word (String To Word) in the document is a feature and the number of occurrences in each instance is the feature value
1. Click on Choose button below Filter2. Choose weka->filters->unsupervised->attribute->StringToWordVector
Illustration 6: WEKA Filter
3. Click on the ""StringToWordVector"" word to open the options
Illustration 7: StringToWordVector Settings
4. Also click on WordTokenizer and fill the options as specified in the following table
Illustration 8: WordTokenizer
Setting: IDFTransform/TFTransform
Value: True
Details: Instead of calculating normal word frequency, use TFIDF calculation [3]
Setting: LowerCaseTokens
Value: True
Details: Convert all words to lowercase
Setting: OutputWordCounts
Value: True
Details: Instead of using word occurrence ( 0 and one ) as feature value, use word frequency or TFIDF
Setting: Tokenizer
Value: .,;:'""()?!/ -_><&#
Details: Split words by these characters
Setting: WordsToKeep
Value: Any appropriate number
Details: How many words to keep after tokenization, this will limit the number of attributes you will have
5. TIP: click Save to save the settings that you have just selected since we will need it in later stages6. Click OK7. Click Apply in the Filter section  far right
Illustration 9: Attributes list after feature extraction
Now you can see in the illustration above, the STWV produced more than 500 attributes (Features) where many of them are irrelevant, so how do we know which ones are significant ?
There are many way to spot the best attributes, I will list them here for your information, but for the sake of this tutorial the provided attributes will be left as is
1. From the pre-process tab you can click on each attribute and see if it makes a good split for the data
Illustration 10: Attribute split
2. You can use the Associator tab  the Apriori"" algorithm- to discover relations between important attributes ( you will need to use Filtered Associator  Discretize filter to be able to use it on this dataset )
Illustration 11: Apriori
3. One of the best options is to use the ""Select Attributes"" tab, choose InfoGainAttributeEval + Ranker search method, this will give you a list of the most significant attributes
Illustration 12: Select Attributes
4. You can also Visualize using the tab the attribute and try to find key splitter attributes
Illustration 13: Visualization
5. Finally you can run any Tree Algorithm and visualize the tree structure to see the main attributes used for splitting ( after running J48 go to the last item in the results list and right click  visualize tree )
Illustration 14: Visualized Decision Tree
Now that we have our input dataset preprocessed it is time to start the classification process
Note: before continuing, just go to the preprocess tab and open the training file again or click undo, we need to keep the file in its initial format since we will use the Filtered classifier to do the transformation automatically for both training and testing files
First we need a Baseline to compare performance against
1. Go to Classify tab, choose Filtered Classifier, then choose ZeroR (from rules) and StringToWordVector, don't forget to use the same setting that we saved earlier
Illustration 15: Filtered Classifier
2. Go to Test options, choose the ReutersCorn-test.arff testset
Illustration 16: Adding the test set
3. Click Start
Our current baseline shows 96% accuracy but 1 False Positive rate which is very bad, what the ZeroR actually does is always classifying the document as the majority class which is in our case is the non-corn class, this is why we are getting such result
Illustration 17: ZeroR Results
Now we will now do the classification, we will use JRIP algorithms for this tutorial, you can do your own tests on other algorithms specially Rules and Trees categories since they take attribute correlation into account and provide human readable model that you can communicate
1. Click on Filtered Classifier2. Choose JRIP algorithm from rules
Illustration 18: JRIP Classifier
3. Click OK, then Start the algorithm
The results shows higher accuracy than the baseline, also note that we got 1 TP (True positive) Rate for corn, which means all corn documents has been classified as corn
Illustration 19: Final Results
The good thing about this category of algorithms is the human readable model, note the JRIP rules above, which indicate that documents containing TFIDF values for the word ""corn"" greater than 2.62 will probably be about corn
99+% accuracy doesn't make sense, it is probably due to the fact that the dataset contains 97% of one class and 3 percent for the other, this is why we got high accuracy for ZeroR
To handle this issue, we need to focus on the TPR/ROC AREA instead of total accuracy, we can also down sample the dataset to re-balance it
1. I Witten et al. 2011. Data Mining (3rd edition). Elsevier. Ch.17 Tutorial Exercises for the Weka Explorer: https://moodle.umons.ac.be/pluginfile.php/43703/mod_resource/content/2/WekaTutorial.pdf2. Cross Industry Standard Process for Data Mining http://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining3. TFIDF (Wikipedia) http://en.wikipedia.org/wiki/Tf%E2%80%93idf
",25
https://towardsdatascience.com/understanding-random-forest-58381e0602d2?source=tag_archive---------0-----------------------,Understanding Random Forest,How the Algorithm Works and Why it Is So Effective,Tony Yiu,9,"A big part of machine learning is classification  we want to know what class (a.k.a. group) an observation belongs to. The ability to precisely classify observations is extremely valuable for various business applications like predicting whether a particular user will buy a product or forecasting whether a given loan will default or not.
Data science provides a plethora of classification algorithms such as logistic regression, support vector machine, naive Bayes classifier, and decision trees. But near the top of the classifier hierarchy is the random forest classifier (there is also the random forest regressor but that is a topic for another day).
In this post, we will examine how basic decision trees work, how individual decisions trees are combined to make a random forest, and ultimately discover why random forests are so good at what they do.
Let's quickly go over decision trees as they are the building blocks of the random forest model. Fortunately, they are pretty intuitive. I'd be willing to bet that most people have used a decision tree, knowingly or not, at some point in their lives.
It's probably much easier to understand how a decision tree works through an example.
Imagine that our dataset consists of the numbers at the top of the figure to the left. We have two 1s and five 0s (1s and 0s are our classes) and desire to separate the classes using their features. The features are color (red vs. blue) and whether the observation is underlined or not. So how can we do this?
Color seems like a pretty obvious feature to split by as all but one of the 0s are blue. So we can use the question, ""Is it red?"" to split our first node. You can think of a node in a tree as the point where the path splits into two  observations that meet the criteria go down the Yes branch and ones that don't go down the No branch.
The No branch (the blues) is all 0s now so we are done there, but our Yes branch can still be split further. Now we can use the second feature and ask, ""Is it underlined?"" to make a second split.
The two 1s that are underlined go down the Yes subbranch and the 0 that is not underlined goes down the right subbranch and we are all done. Our decision tree was able to use the two features to split up the data perfectly. Victory!
Obviously in real life our data will not be this clean but the logic that a decision tree employs remains the same. At each node, it will ask 
What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)?
Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model's prediction (see figure below).
The fundamental concept behind random forest is a simple but powerful one  the wisdom of crowds. In data science speak, the reason that the random forest model works so well is:
A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.
The low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don't constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So the prerequisites for random forest to perform well are:
The wonderful effects of having many uncorrelated models is such a critical concept that I want to show you an example to help it really sink in. Imagine that we are playing the following game:
Which would you pick? The expected value of each game is the same:
Expected Value Game 1 = (0.60*1 + 0.40*-1)*100 = 20
Expected Value Game 2= (0.60*10 + 0.40*-10)*10 = 20
Expected Value Game 3= 0.60*100 + 0.40*-100 = 20
What about the distributions? Let's visualize the results with a Monte Carlo simulation (we will run 10,000 simulations of each game type; for example, we will simulate 10,000 times the 100 plays of Game 1). Take a look at the chart on the left  now which game would you pick? Even though the expected values are the same, the outcome distributions are vastly different going from positive and narrow (blue) to binary (pink).
Game 1 (where we play 100 times) offers up the best chance of making some money  out of the 10,000 simulations that I ran, you make money in 97% of them! For Game 2 (where we play 10 times) you make money in 63% of the simulations, a drastic decline (and a drastic increase in your probability of losing money). And Game 3 that we only play once, you make money in 60% of the simulations, as expected.
So even though the games share the same expected value, their outcome distributions are completely different. The more we split up our $100 bet into different plays, the more confident we can be that we will make money. As mentioned previously, this works because each play is independent of the other ones.
Random forest is the same  each tree is like one play in our game earlier. We just saw how our chances of making money increased the more times we played. Similarly, with a random forest model, our chances of making correct predictions increase with the number of uncorrelated trees in our model.
If you would like to run the code for simulating the game yourself you can find it on my GitHub here.
So how does random forest ensure that the behavior of each individual tree is not too correlated with the behavior of any of the other trees in the model? It uses the following two methods:
Bagging (Bootstrap Aggregation)  Decisions trees are very sensitive to the data they are trained on  small changes to the training set can result in significantly different tree structures. Random forest takes advantage of this by allowing each individual tree to randomly sample from the dataset with replacement, resulting in different trees. This process is known as bagging.
Notice that with bagging we are not subsetting the training data into smaller chunks and training each tree on a different chunk. Rather, if we have a sample of size N, we are still feeding each tree a training set of size N (unless specified otherwise). But instead of the original training data, we take a random sample of size N with replacement. For example, if our training data was [1, 2, 3, 4, 5, 6] then we might give one of our trees the following list [1, 2, 2, 3, 6, 6]. Notice that both lists are of length six and that ""2"" and ""6"" are both repeated in the randomly selected training data we give to our tree (because we sample with replacement).
Feature Randomness  In a normal decision tree, when it is time to split a node, we consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node. In contrast, each tree in a random forest can pick only from a random subset of features. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.
Let's go through a visual example  in the picture above, the traditional decision tree (in blue) can select from all four features when deciding how to split the node. It decides to go with Feature 1 (black and underlined) as it splits the data into groups that are as separated as possible.
Now let's take a look at our random forest. We will just examine two of the forest's trees in this example. When we check out random forest Tree 1, we find that it it can only consider Features 2 and 3 (selected randomly) for its node splitting decision. We know from our traditional decision tree (in blue) that Feature 1 is the best feature for splitting, but Tree 1 cannot see Feature 1 so it is forced to go with Feature 2 (black and underlined). Tree 2, on the other hand, can only see Features 1 and 3 so it is able to pick Feature 1.
So in our random forest, we end up with trees that are not only trained on different sets of data (thanks to bagging) but also use different features to make decisions.
And that, my dear reader, creates uncorrelated trees that buffer and protect each other from their errors.
Random forests are a personal favorite of mine. Coming from the world of finance and investments, the holy grail was always to build a bunch of uncorrelated models, each with a positive expected return, and then put them together in a portfolio to earn massive alpha (alpha = market beating returns). Much easier said than done!
Random forest is the data science equivalent of that. Let's review one last time. What's a random forest classifier?
The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.
What do we need in order for our random forest to make accurate class predictions?
Thanks for reading. I hope you learned as much from reading this as I did from writing it. Cheers!
If you liked this article and my writing in general, please consider supporting my writing by signing up for Medium via my referral link here. Thanks!
",26
https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1?source=tag_archive---------8-----------------------,Probability concepts explained: Maximum likelihood estimation,Introducing the method of maximum likelihood for parameter estimation,Jonny Brooks-Bartlett,8,"In this post I'll explain what the maximum likelihood method for parameter estimation is and go through a simple example to demonstrate the method. Some of the content requires knowledge of fundamental probability concepts such as the definition of joint probability and independence of events. I've written a blog post with these prerequisites so feel free to read this if you think you need a refresher.
Often in machine learning we use a model to describe the process that results in the data that are observed. For example, we may use a random forest model to classify whether customers may cancel a subscription from a service (known as churn modelling) or we may use a linear model to predict the revenue that will be generated for a company depending on how much they may spend on advertising (this would be an example of linear regression). Each model contains its own set of parameters that ultimately defines what the model looks like.
For a linear model we can write this as y = mx + c. In this example x could represent the advertising spend and y might be the revenue generated. m and c are parameters for this model. Different values for these parameters will give different lines (see figure below).
So parameters define a blueprint for the model. It is only when specific values are chosen for the parameters that we get an instantiation for the model that describes a given phenomenon.
Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.
The above definition may still sound a little cryptic so let's go through an example to help understand this.
Let's suppose we have observed 10 data points from some process. For example, each data point could represent the length of time in seconds that it takes a student to answer a specific exam question. These 10 data points are shown in the figure below
We first have to decide which model we think best describes the process of generating the data. This part is very important. At the very least, we should have a good idea about which model to use. This usually comes from having some domain expertise but we wont discuss this here.
For these data we'll assume that the data generation process can be adequately described by a Gaussian (normal) distribution. Visual inspection of the figure above suggests that a Gaussian distribution is plausible because most of the 10 points are clustered in the middle with few points scattered to the left and the right. (Making this sort of decision on the fly with only 10 data points is ill-advised but given that I generated these data points we'll go with it).
Recall that the Gaussian distribution has 2 parameters. The mean, , and the standard deviation, . Different values of these parameters result in different curves (just like with the straight lines above). We want to know which curve was most likely responsible for creating the data points that we observed? (See figure below). Maximum likelihood estimation is a method that will find the values of  and  that result in the curve that best fits the data.
The true distribution from which the data were generated was f1 ~ N(10, 2.25), which is the blue curve in the figure above.
Now that we have an intuitive understanding of what maximum likelihood estimation is we can move on to learning how to calculate the parameter values. The values that we find are called the maximum likelihood estimates (MLE).
Again we'll demonstrate this with an example. Suppose we have three data points this time and we assume that they have been generated from a process that is adequately described by a Gaussian distribution. These points are 9, 9.5 and 11. How do we calculate the maximum likelihood estimates of the parameter values of the Gaussian distribution  and ?
What we want to calculate is the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we'll make our first assumption. The assumption is that each data point is generated independently of the others. This assumption makes the maths much easier. If the events (i.e. the process that generates the data) are independent, then the total probability of observing all of data is the product of observing each data point individually (i.e. the product of the marginal probabilities).
The probability density of observing a single data point x, that is generated from a Gaussian distribution is given by:
The semi colon used in the notation P(x; , ) is there to emphasise that the symbols that appear after it are parameters of the probability distribution. So it shouldn't be confused with a conditional probability (which is typically represented with a vertical line e.g. P(A| B)).
In our example the total (joint) probability density of observing the three data points is given by:
We just have to figure out the values of  and  that results in giving the maximum value of the above expression.
If you've covered calculus in your maths classes then you'll probably be aware that there is a technique that can help us find maxima (and minima) of functions. It's called differentiation. All we have to do is find the derivative of the function, set the derivative function to zero and then rearrange the equation to make the parameter of interest the subject of the equation. And voila, we'll have our MLE values for our parameters. I'll go through these steps now but I'll assume that the reader knows how to perform differentiation on common functions. If you would like a more detailed explanation then just let me know in the comments.
The above expression for the total probability is actually quite a pain to differentiate, so it is almost always simplified by taking the natural logarithm of the expression. This is absolutely fine because the natural logarithm is a monotonically increasing function. This means that if the value on the x-axis increases, the value on the y-axis also increases (see figure below). This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.
Taking logs of the original expression gives us:
This expression can be simplified again using the laws of logarithms to obtain:
This expression can be differentiated to find the maximum. In this example we'll find the MLE of the mean, . To do this we take the partial derivative of the function with respect to , giving
Finally, setting the left hand side of the equation to zero and then rearranging for  gives:
And there we have our maximum likelihood estimate for . We can do the same thing with  too but I'll leave that as an exercise for the keen reader.
No is the short answer. It's more likely that in a real world scenario the derivative of the log-likelihood function is still analytically intractable (i.e. it's way too hard/impossible to differentiate the function by hand). Therefore, iterative methods like Expectation-Maximization algorithms are used to find numerical solutions for the parameter estimates. The overall idea is still the same though.
Well this is just statisticians being pedantic (but for good reason). Most people tend to use probability and likelihood interchangeably but statisticians and probability theorists distinguish between the two. The reason for the confusion is best highlighted by looking at the equation.
These expressions are equal! So what does this mean? Let's first define P(data; , )? It means ""the probability density of observing the data with model parameters  and "". It's worth noting that we can generalise this to any number of parameters and any distribution.
On the other hand L(, ; data) means ""the likelihood of the parameters  and  taking certain values given that we've observed a bunch of data.""
The equation above says that the probability density of the data given the parameters is equal to the likelihood of the parameters given the data. But despite these two things being equal, the likelihood and the probability density are fundamentally asking different questions  one is asking about the data and the other is asking about the parameter values. This is why the method is called maximum likelihood and not maximum probability.
Least squares minimisation is another common method for estimating parameter values for a model in machine learning. It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the least squares method. For a more in-depth mathematical derivation check out these slides.
Intuitively we can interpret the connection between the two methods by understanding their objectives. For least squares parameter estimation we want to find the line that minimises the total squared distance between the data points and the regression line (see the figure below). In maximum likelihood estimation we want to maximise the total probability of the data. When a Gaussian distribution is assumed, the maximum probability is found when the data points get closer to the mean value. Since the Gaussian distribution is symmetric, this is equivalent to minimising the distance between the data points and the mean value.
If there is anything that is unclear or I've made some mistakes in the above feel free to leave a comment. In the next post I plan to cover Bayesian inference and how it can be used for parameter estimation.
Thank you for reading.
Data scientist at Deliveroo, public speaker, science communicator, mathematician and sports enthusiast.
",27
https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853?source=tag_archive---------5-----------------------,"Transformers Explained Visually (Part 3): Multi-head Attention, deep dive","A Gentle Guide to the inner workings of Self-Attention, Encoder-Decoder Attention, Attention Score and Masking, in Plain English.",Ketan Doshi,11,"This is the third article in my series on Transformers. We are covering its functionality in a top-down manner. In the previous articles, we learned what a Transformer is, its architecture, and how it works.
In this article, we will go a step further and dive deeper into Multi-head Attention, which is the brains of the Transformer.
Here's a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.
And if you're interested in NLP applications in general, I have some other articles you might like.
As we discussed in Part 2, Attention is used in the Transformer in three places:
Attention Input Parameters  Query, Key, and Value
The Attention layer takes its input in the form of three parameters, known as the Query, Key, and Value.
All three parameters are similar in structure, with each word in the sequence represented by a vector.
Encoder Self-Attention
The input sequence is fed to the Input Embedding and Position Encoding, which produces an encoded representation for each word in the input sequence that captures the meaning and position of each word. This is fed to all three parameters, Query, Key, and Value in the Self-Attention in the first Encoder which then also produces an encoded representation for each word in the input sequence, that now incorporates the attention scores for each word as well. As this passes through all the Encoders in the stack, each Self-Attention module also adds its own attention scores into each word's representation.
Decoder Self-Attention
Coming to the Decoder stack, the target sequence is fed to the Output Embedding and Position Encoding, which produces an encoded representation for each word in the target sequence that captures the meaning and position of each word. This is fed to all three parameters, Query, Key, and Value in the Self-Attention in the first Decoder which then also produces an encoded representation for each word in the target sequence, which now incorporates the attention scores for each word as well.
After passing through the Layer Norm, this is fed to the Query parameter in the Encoder-Decoder Attention in the first Decoder
Encoder-Decoder Attention
Along with that, the output of the final Encoder in the stack is passed to the Value and Key parameters in the Encoder-Decoder Attention.
The Encoder-Decoder Attention is therefore getting a representation of both the target sequence (from the Decoder Self-Attention) and a representation of the input sequence (from the Encoder stack). It, therefore, produces a representation with the attention scores for each target sequence word that captures the influence of the attention scores from the input sequence as well.
As this passes through all the Decoders in the stack, each Self-Attention and each Encoder-Decoder Attention also add their own attention scores into each word's representation.
In the Transformer, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word.
To understand exactly how the data is processed internally, let's walk through the working of the Attention module while we are training the Transformer to solve a translation problem. We'll use one sample of our training data which consists of an input sequence ('You are welcome' in English) and a target sequence ('De nada' in Spanish).
There are three hyperparameters that determine the data dimensions:
In addition, we also have the Batch size, giving us one dimension for the number of samples.
The Input Embedding and Position Encoding layers produce a matrix of shape (Number of Samples, Sequence Length, Embedding Size) which is fed to the Query, Key, and Value of the first Encoder in the stack.
To make it simple to visualize, we will drop the Batch dimension in our pictures and focus on the remaining dimensions.
There are three separate Linear layers for the Query, Key, and Value. Each Linear layer has its own weights. The input is passed through these Linear layers to produce the Q, K, and V matrices.
Now the data gets split across the multiple Attention heads so that each can process it independently.
However, the important thing to understand is that this is a logical split only. The Query, Key, and Value are not physically split into separate matrices, one for each Attention head. A single data matrix is used for the Query, Key, and Value, respectively, with logically separate sections of the matrix for each Attention head. Similarly, there are not separate Linear layers, one for each Attention head. All the Attention heads share the same Linear layer but simply operate on their 'own' logical section of the data matrix.
Linear layer weights are logically partitioned per head
This logical split is done by partitioning the input data as well as the Linear layer weights uniformly across the Attention heads. We can achieve this by choosing the Query Size as below:
Query Size = Embedding Size / Number of heads
In our example, that is why the Query Size = 6/2 = 3. Even though the layer weight (and input data) is a single matrix we can think of it as 'stacking together' the separate layer weights for each head.
The computations for all Heads can be therefore be achieved via a single matrix operation rather than requiring N separate operations. This makes the computations more efficient and keeps the model simple because fewer Linear layers are required, while still achieving the power of the independent Attention heads.
Reshaping the Q, K, and V matrices
The Q, K, and V matrices output by the Linear layers are reshaped to include an explicit Head dimension. Now each 'slice' corresponds to a matrix per head.
This matrix is reshaped again by swapping the Head and Sequence dimensions. Although the Batch dimension is not drawn, the dimensions of Q are now (Batch, Head, Sequence, Query size).
In the picture below, we can see the complete process of splitting our example Q matrix, after coming out of the Linear layer.
The final stage is for visualization only  although the Q matrix is a single matrix, we can think of it as a logically separate Q matrix per head.
We are ready to compute the Attention Score.
We now have the 3 matrices, Q, K, and V, split across the heads. These are used to compute the Attention Score.
We will show the computations for a single head using just the last two dimensions (Sequence and Query size) and skip the first two dimensions (Batch and Head). Essentially, we can imagine that the computations we're looking at are getting 'repeated' for each head and for each sample in the batch (although, obviously, they are happening as a single matrix operation, and not as a loop).
The first step is to do a matrix multiplication between Q and K.
A Mask value is now added to the result. In the Encoder Self-attention, the mask is used to mask out the Padding values so that they don't participate in the Attention Score.
Different masks are applied in the Decoder Self-attention and in the Decoder Encoder-Attention which we'll come to a little later in the flow.
The result is now scaled by dividing by the square root of the Query size, and then a Softmax is applied to it.
Another matrix multiplication is performed between the output of the Softmax and the V matrix.
The complete Attention Score calculation in the Encoder Self-attention is as below:
We now have separate Attention Scores for each head, which need to be combined together into a single score. This Merge operation is essentially the reverse of the Split operation.
It is done by simply reshaping the result matrix to eliminate the Head dimension. The steps are:
Since Embedding size =Head * Query size, the merged Score is (Batch, Sequence, Embedding size). In the picture below, we can see the complete process of merging for the example Score matrix.
Putting it all together, this is the end-to-end flow of the Multi-head Attention.
An Embedding vector captures the meaning of a word. In the case of Multi-head Attention, as we have seen, the Embedding vectors for the input (and target) sequence gets logically split across multiple heads. What is the significance of this?
This means that separate sections of the Embedding can learn different aspects of the meanings of each word, as it relates to other words in the sequence. This allows the Transformer to capture richer interpretations of the sequence.
This may not be a realistic example, but it might help to build intuition. For instance, one section might capture the 'gender-ness' (male, female, neuter) of a noun while another might capture the 'cardinality' (singular vs plural) of a noun. This might be important during translation because, in many languages, the verb that needs to be used depends on these factors.
The Decoder Self-Attention works just like the Encoder Self-Attention, except that it operates on each word of the target sequence.
Similarly, the Masking masks out the Padding words in the target sequence.
The Encoder-Decoder Attention takes its input from two sources. Therefore, unlike the Encoder Self-Attention, which computes the interaction between each input word with other input words, and Decoder Self-Attention which computes the interaction between each target word with other target words, the Encoder-Decoder Attention computes the interaction between each target word with each input word.
Therefore each cell in the resulting Attention Score corresponds to the interaction between one Q (ie. target sequence word) with all other K (ie. input sequence) words and all V (ie. input sequence) words.
Similarly, the Masking masks out the later words in the target output, as was explained in detail in the second article of the series.
Hopefully, this gives you a good sense of what the Attention modules in the Transformer do. When put together with the end-to-end flow of the Transformer as a whole that we went over in the second article, we have now covered the detailed operation of the entire Transformer architecture.
We now understand exactly what the Transformer does. But we haven't fully answered the question of why the Transformer's Attention performs the calculations that it does. Why does it use the notions of Query, Key, and Value, and why does it perform the matrix multiplications that we just saw?
We have a vague intuitive idea that it 'captures the relationship between each word with each other word', but what exactly does that mean? How exactly does that give the Transformer's Attention the capability to understand the nuances of each word in the sequence?
That is an interesting question and is the subject of the final article of this series. Once we learn that, we will truly understand the elegance of the Transformer architecture.
And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Let's keep learning!
",28
https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9?source=tag_archive---------1-----------------------,180 Data Science and Machine Learning Projects with Python,180 Data Science and Machine Learning Projects Solved and Explained with Python.,Aman Kharwal,4,"In this article, I will introduce you to more than 180 data science and machine learning projects solved and explained using the Python programming language.
I hope you liked this article on more than 180 data science and machine learning projects solved and explained by using the Python programming language. Feel free to ask your valuable questions in the comments section below.
From Hello, World to Building Robots
1.1K 
4
",29
https://betterprogramming.pub/5-online-courses-i-took-as-a-self-taught-data-scientist-36e0dc36f386?source=tag_archive---------3-----------------------,5 Online Courses I Took as a Self-Taught Data Scientist,"After taking over 36 courses, here are my five recommendations",Chris Zaire,7,"Let me just get this out of the way: I'm in no way an actual prodigy. I'm not creating AI that will change the world or creating equations that will solve the mysteries of the universe like our old pal Einstein.
But I'm in no way a beginner either. I've been learning and studying data science for over two years now and have gotten quite far. This is all while being totally self-taught as well.
My journey didn't start by going to school for data science. No, my journey started when I stumbled upon a course in this crazy thing called machine learning.
Needless to say, I loved the course and I've taken around 36 data science courses since then.
Let my experience be your knowledge as I tell you my top five courses in data science to go from beginner to ""prodigy.""
I can't stress this enough: If you haven't already taken a machine learning course, take this one.
This is the first online course I ever took, and it's the reason that 746,000 other students and I are in data science today.
The course was created by data scientist Kirill Eremenko. What's better than his soothing voice is the actual material.
Kirill teaches you all the knowledge you need to have a basic understanding of some of the most powerful and useful machine learning models. This includes models in regression, classification, clustering, reinforcement learning, natural language processing, and deep learning.
The mathematical background behind these models is taught with simple PowerPoint slides and put into action with line-by-line coding tutorials.
Trust me, take a course where they show you how to create models line by line. It is way less overwhelming and much easier to understand what's actually going on.
What makes this course stand out from the others I've taken is the amount of material. There is nearly 45 hours' worth of content and the course only costs about $15.
That's $0.33 per hour of material.
I've seen other machine learning courses with only half that material go for around $200.
This is by far my favorite course to start out in data science and learn one of the most important topics: machine learning.
click.linksynergy.com
It's no secret that to be a data scientist or acquire a data science role, you have to have some knowledge of SQL.
If you don't believe me, just look at any job posting for a data scientist and you will see SQL as a requirement.
The reason is simple: To perform data science, you need data in the form of a dataset. That dataset will not always be in the format you want. Therefore, languages like SQL were created to easily manipulate and query these datasets.
The moral of the story is it's important to learn SQL  and this is my favorite course to do just that.
The course is taught by Jose Portilla. He may not have the soothing voice that Kirill does, but he is still a very experienced and amazing instructor.
The structure of the course itself is why I love it so much. It's a nine-hour course that is packed to the brim with info. It doesn't waste time or constantly go over topics that have already been discussed.
This course also includes three assessments you do yourself. Assessments are great because they really show if you know the material or not. Having an assessment in a course is my favorite part and the reason why I have chosen one course over another.
click.linksynergy.com
Our old friend Kirill is back, and this time, he has a course dedicated to the most popular data science language: Python.
I don't know if you can tell, but I love this guy.
In all seriousness, though, this is the course you should take if you have no experience in programming and data science.
I took this course after his machine learning course, so I was kind of ahead, but if I had taken this course before that one, I would have understood the machine learning course's code way better.
This is a great beginner course because it teaches you all the fundamentals in Python (e.g. loops, lists, dictionaries, data frames) while also applying them to data science-based concepts.
The course also contains six assessments/exercises in order for you to apply what you learn from the different sections. Remember, not every course has these assessments. Make sure you take the ones that do so you can see if you truly are learning and not just watching.
The course is 11 hours long, but if you know the basics of coding, you can skip ahead and make it around nine hours long.
click.linksynergy.com
I'm biased with this course because it's not necessary, but in my case, it helped me a lot.
For those who may not know, pandas is one of the most powerful libraries in Python and focused on functions that can manipulate and change datasets.
The reason I think you should take this course is that, in reality, 80% of data science projects revolve around cleaning data. This means manipulating a dataset in a way to ensure that it's ready for the model chosen to be run on it.
As I mentioned in the last section, Python is the most popular data science language. Because of this, it's imperative you know pandas so you can use its functions to clean those datasets.
I took this course right before I was hired to be a sports analyst for Team Canada at the Olympics. Since the team and I were working with sports data, more often than not, we had very messy datasets.
This course paid off big-time for me because I was using pandas every day in Python, just trying to clean these datasets and get them ready for our models.
This course is the top-rated pandas course on Udemy. It's very easy to follow, as most of the videos demonstrate just one function at a time. This is great because if you ever forget about a specific function, it's very easy to find the video for it.
click.linksynergy.com
The last course you should take to be on your way in the data science field is one dedicated to data visualization.
Data science projects usually revolve around these four steps:
I've given you course recommendations for steps 2 and 3, but this is my recommendation to teach you step 4.
Tableau is one of the two best data visualization tools, with the other being PowerBI. Again, if you were to look at a job posting for a data scientist, a requirement would most likely be knowledge in Tableau or PowerBI.
I prefer Tableau, and it seems more and more companies are switching over as well.
This course was one of my favorites. It was easy to follow and the projects you do are just really cool. At the end of each section, I would look at the visualization I just made and be very impressed with myself.
It's a nine-hour course that teaches you how to go from very simple visualizations like bar charts to high-level ones like TreeMaps, dual-axis charts, and cluster graphs.
click.linksynergy.com
Data science has become one of the hottest fields in tech, but if you didn't take it in school, the only way to learn data science is by self-teaching.
I am self-taught and trust me, I know it can be hard to figure out where you should start and what courses to take.
The courses I mentioned established the core concepts of data science for me, and I'm positive they will do the same for you.
I hope my personal experience and recommendations help you on your way to going from beginner to data science prodigy.
Join my email list with 1k+ people to get The Complete Python for Data Science Cheat Sheet Booklet for free.
Advice for programmers.
1.2K 
11
Thanks to Anupam Chugh. 
1.2K claps
1.2K 
",30
https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------0-----------------------,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You...",Adam Geitgey,13,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in , , , Portugues, Tieng Viet,  or Italiano.
Giant update: I've written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic:
This technology is called face recognition. Facebook's algorithms are able to recognize your friends' faces after they have been tagged only a few times. It's pretty amazing technology  Facebook can recognize faces with 98% accuracy which is pretty much as good as humans can do!
Let's learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem  telling Will Ferrell (famous actor) apart from Chad Smith (famous rock musician)!
So far in Part 1, 2 and 3, we've used machine learning to solve isolated problems that have only one step  estimating the price of a house, generating new data based on existing data and telling if an image contains a certain object. All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result.
But face recognition is really a series of several related problems:
As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are too good at recognizing faces and end up seeing faces in everyday objects:
Computers are not capable of this kind of high-level generalization (at least not yet...), so we have to teach them how to do each step in this process separately.
We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms:
Let's tackle this problem one step at a time. For each step, we'll learn about a different machine learning algorithm. I'm not going to explain every single algorithm completely to keep this from turning into a book, but you'll learn the main ideas behind each one and you'll learn how you can build your own facial recognition system in Python using OpenFace and dlib.
The first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!
If you've used any camera in the last 10 years, you've probably seen face detection in action:
Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we'll use it for a different purpose  finding the areas of the image we want to pass on to the next step in our pipeline.
Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We're going to use a method invented in 2005 called Histogram of Oriented Gradients  or just HOG for short.
To find faces in an image, we'll start by making our image black and white because we don't need color data to find faces:
Then we'll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:
Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:
If you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:
This might seem like a random thing to do, but there's a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!
But saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.
To do this, we'll break up the image into small squares of 16x16 pixels each. In each square, we'll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc...). Then we'll replace that square in the image with the arrow directions that were the strongest.
The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:
To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:
Using this technique, we can now easily find faces in any image:
If you want to try this step out yourself using Python and dlib, here's code showing how to generate and view HOG representations of images.
Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:
To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.
To do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.
The basic idea is we will come up with 68 specific points (called landmarks) that exist on every face  the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:
Here's the result of locating the 68 face landmarks on our test image:
Now that we know were the eyes and mouth are, we'll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won't do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):
Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.
If you want to try this step out yourself using Python and dlib, here's the code for finding face landmarks and here's the code for transforming the image using those landmarks.
Now we are to the meat of the problem  actually telling faces apart. This is where things get really interesting!
The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?
There's actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can't possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.
What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you've ever watched a bad crime show like CSI, you know what I am talking about:
Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?
It turns out that the measurements that seem obvious to us humans (like eye color) don't really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.
The solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.
The training process works by looking at 3 face images at a time:
Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:
After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.
Machine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.
This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.
But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!
So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here's the measurements for our test image:
So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn't really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.
If you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.
This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.
You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We'll use a simple linear SVM classifier, but lots of classification algorithms could work.
All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!
So let's try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:
Then I ran the classifier on every frame of the famous youtube video of Will Ferrell and Chad Smith pretending to be each other on the Jimmy Fallon show:
It works! And look how well it works for faces in different poses  even sideways faces!
Let's review the steps we followed:
Now that you know how this all works, here's instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:
UPDATE 4/9/2017: You can still follow the steps below to use OpenFace. However, I've released a new Python-based face recognition library called face_recognition that is much easier to install and use. So I'd recommend trying out face_recognition first instead of continuing below!
I even put together a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed. You can download and run it on your computer very easily. Give the virtual machine a shot if you don't want to install all these libraries yourself!
Original OpenFace instructions:
If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter:
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I'd love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 5!
",31
https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa?source=tag_archive---------0-----------------------,9 Distance Measures in Data Science,The advantages and pitfalls of common distance measures,Maarten Grootendorst,10,"Many algorithms, whether supervised or unsupervised, make use of distance measures. These measures, such as euclidean distance or cosine similarity, can often be found in algorithms such as k-NN, UMAP, HDBSCAN, etc.
Understanding the field of distance measures is more important than you might realize. Take k-NN for example, a technique often used for supervised learning. As a default, it often uses euclidean distance. By itself, a great distance measure.
However, what if your data is highly dimensional? Would euclidean distance then still work? Or what if your data consists of geospatial information? Perhaps haversine distance would then be a better alternative!
Knowing when to use which distance measure can help you go from a poor classifier to an accurate model.
In this article, we will go through many distance measures and explore how and when they best can be used. Most importantly, I will be talking about their disadvantages so that you can recognize when to steer clear of certain measures.
NOTE: For most distance measures long elaborate papers could and have been written on their use-cases, advantages, and disadvantages. I will try to cover as much as possible but may fall short! Thus, consider this article a global overview of these measures.
We start with the most common distance measure, namely Euclidean distance. It is a distance measure that best can be explained as the length of a segment connecting two points.
The formula is rather straightforward as the distance is calculated from the cartesian coordinates of the points using the Pythagorean theorem.
Although it is a common distance measure, Euclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the features. Typically, one needs to normalize the data before using this distance measure.
Moreover, as the dimensionality increases of your data, the less useful Euclidean distance becomes. This has to do with the curse of dimensionality which relates to the notion that higher-dimensional space does not act as we would, intuitively, expect from 2- or 3-dimensional space. For a good summary, see this post.
Euclidean distance works great when you have low-dimensional data and the magnitude of the vectors is important to be measured. Methods like kNN and HDBSCAN show great results out of the box if Euclidean distance is used on low-dimensional data.
Although many other measures have been developed to account for the disadvantages of Euclidean distance, it is still one of the most used distance measures for good reasons. It is incredibly intuitive to use, simple to implement and shows great results in many use-cases.
Cosine similarity has often been used as a way to counteract Euclidean distance's problem with high dimensionality. The cosine similarity is simply the cosine of the angle between two vectors. It also has the same inner product of the vectors if they were normalized to both have length one.
Two vectors with exactly the same orientation have a cosine similarity of 1, whereas two vectors diametrically opposed to each other have a similarity of -1. Note that their magnitude is not of importance as this is a measure of orientation.
One main disadvantage of cosine similarity is that the magnitude of vectors is not taken into account, merely their direction. In practice, this means that the differences in values are not fully taken into account. If you take a recommender system, for example, then the cosine similarity does not take into account the difference in rating scale between different users.
We use cosine similarity often when we have high-dimensional data and when the magnitude of the vectors is not of importance. For text analyses, this measure is quite frequently used when the data is represented by word counts. For example, when a word occurs more frequently in one document over another this does not necessarily mean that one document is more related to that word. It could be the case that documents have uneven lengths and the magnitude of the count is of less importance. Then, we can best be using cosine similarity which disregards magnitude.
Hamming distance is the number of values that are different between two vectors. It is typically used to compare two binary strings of equal length. It can also be used for strings to compare how similar they are to each other by calculating the number of characters that are different from each other.
As you might expect, hamming distance is difficult to use when two vectors are not of equal length. You would want to compare same-length vectors with each other in order to understand which positions do not match.
Moreover, it does not take the actual value into account as long as they are different or equal. Therefore, it is not advised to use this distance measure when the magnitude is an important measure.
Typical use cases include error correction/detection when data is transmitted over computer networks. It can be used to determine the number of distorted bits in a binary word as a way to estimate error.
Moreover, you can also use Hamming distance to measure the distance between categorical variables.
The Manhattan distance, often called Taxicab distance or City Block distance, calculates the distance between real-valued vectors. Imagine vectors that describe objects on a uniform grid such as a chessboard. Manhattan distance then refers to the distance between two vectors if they could only move right angles. There is no diagonal movement involved in calculating the distance.
Although Manhattan distance seems to work okay for high-dimensional data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data.
Moreover, it is more likely to give a higher distance value than euclidean distance since it does not the shortest path possible. This does not necessarily give issues but is something you should take into account.
When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes. Take Euclidean distance, for example, would create a straight line between two vectors when in reality this might not actually be possible.
Chebyshev distance is defined as the greatest of difference between two vectors along any coordinate dimension. In other words, it is simply the maximum distance along one axis. Due to its nature, it is often referred to as Chessboard distance since the minimum number of moves needed by a king to go from one square to another is equal to Chebyshev distance.
Chebyshev is typically used in very specific use-cases, which makes it difficult to use as an all-purpose distance metric, like Euclidean distance or Cosine similarity. For that reason, it is suggested to only use it when you are absolutely sure it suits your use-case.
As mentioned before, Chebyshev distance can be used to extract the minimum number of moves needed by aking to go from one square to another. Moreover, it can be a useful measure in games that allow unrestricted 8-way movement.
In practice, Chebyshev distance is often used in warehouse logistics as it closely resembles the time an overhead crane takes to move an object.
Minkowski distance is a bit more intricate measure than most. It is a metric used in Normed vector space (n-dimensional real space), which means that it can be used in a space where distances can be represented as a vector that has a length.
This measure has three requirements:
The formula for the Minkowski distance is shown below:
Most interestingly about this distance measure is the use of parameter p. We can use this parameter to manipulate the distance metrics to closely resemble others.
Common values of p are:
Minkowski has the same disadvantages as the distance measures they represent, so a good understanding of metrics like Manhattan, Euclidean, and Chebyshev distance is extremely important.
Moreover, the parameter p can actually be troublesome to work with as finding the right value can be quite computationally inefficient depending on your use-case.
The upside to p is the possibility to iterate over it and find the distance measure that works best for your use case. It allows you a huge amount of flexibility over your distance metric, which can be a huge benefit if you are closely familiar with p and many distance measures.
The Jaccard index (or Intersection over Union) is a metric used to calculate the similarity and diversity of sample sets. It is the size of the intersection divided by the size of the union of the sample sets.
In practice, it is the total number of similar entities between sets divided by the total number of entities. For example, if two sets have 1 entity in common and there are 5 different entities in total, then the Jaccard index would be 1/5 = 0.2.
To calculate the Jaccard distance we simply subtract the Jaccard index from 1:
A major disadvantage of the Jaccard index is that it is highly influenced by the size of the data. Large datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar.
The Jaccard index is often used in applications where binary or binarized data are used. When you have a deep learning model predicting segments of an image, for instance, a car, the Jaccard index can then be used to calculate how accurate that predicted segment given true labels.
Similarly, it can be used in text similarity analysis to measure how much word choice overlap there is between documents. Thus, it can be used to compare sets of patterns.
Haversine distance is the distance between two points on a sphere given their longitudes and latitudes. It is very similar to Euclidean distance in that it calculates the shortest line between two points. The main difference is that no straight line is possible since the assumption here is that the two points are on a sphere.
One disadvantage of this distance measure is that it is assumed the points lie on a sphere. In practice, this is seldom the case as, for example, the earth is not perfectly round which could make calculation in certain cases difficult. Instead, it would be interesting to look towards Vincenty distance which assumes an ellipsoid instead.
As you might have expected, Haversine distance is often used in navigation. For example, you can use it to calculate the distance between two countries when flying between them. Note that it is much less suited if the distances by themselves are already not that large. The curvature will not have that large of an impact.
The Srensen-Dice index is very similar to Jaccard index in that it measures the similarity and diversity of sample sets. Although they are calculated similarly the Srensen-Dice index is a bit more intuitive because it can be seen as the percentage of overlap between two sets, which is a value between 0 and 1:
Like the Jaccard index, they both overstate the importance of sets with little to no ground truth positive sets. As a result, it could dominate the average score taken over multiple sets. It weights each item inversely proportionally to the size of the relevant set rather than treating them equally.
The use cases are similar, if not the same, as Jaccard index. You will find it typically used in either image segmentation tasks or text similarity analyses.
NOTE: There are many more distance measures than the 9 mentioned here. If you are looking for more interesting metrics, I would suggest you look into one of the following: Mahalanobis, Canberra, Braycurtis, and KL-divergence.
If you are, like me, passionate about AI, Data Science, or Psychology, please feel free to add me on LinkedIn or follow me on Twitter.
",32
https://medium.datadriveninvestor.com/the-sexiest-job-of-the-21st-century-isnt-sexy-anymore-fd5335a5d4d4?source=tag_archive---------1-----------------------,"The Sexiest Job of the 21st Century Isn't ""Sexy"" Anymore",Trends are changing now and data science is losing its charm,Nishi Kashyap,6,"Are you a data scientist?
If yes. Well, then you must be feeling pretty damn ""sexy"".
Sexy doing the hottest job of the century. Sexy being in the highest position, Sexy of being considered as a most demanded person. Sexy dealing with glamorous artificial intelligence, machine learning, big data, and all. Sexy earning around $13K to $120K per year. Sexy living your dream life while doing your so-called dream job.
Well, I hate to break it to you but you aren't... sexy anymore.
It was long back 8 years ago in 2012 when Harvard Business Review posted the article called ""data scientist"" as the sexiest job of the 21st Century.
But that's not the case anymore.
According to Financial Times's recent study, ""Data scientists are spending an average of 2 hours a week looking for a new job."" A study by Stack Overflow found that 13.2 % of data scientists were looking for a new job If you search for ""data scientist"" on LinkedIn, you might be surprised to see how many of these pros have ""seeking new position"" headlining their profile.
The trends are changing nowadays. The demand is no longer the same as before and data science is losing its charm.
But if being a data scientist is considered cool or sexy then  Why the hell are people leaving it?
Being the sexiest job does not mean everything about it is hot and sexy. Many aspects of Data Science are gruesome and sometimes scary.
Here are a few reasons data science (ever considered a sexy job) is losing its charm :
What is data science?
It's a simple question but answers are often very confusing.
Well, if you think about this, 'data science' is not something which we can refer to as funny but due to its confusing concepts, people have made n-number of inside jokes on data science:
Here's the truth...the industry doesn't work that way.
There are so many things to use to do a data science project or anything close to what we find in online science competitions.
""I would consider the data scientist to be a misunderstood job,"" said Kate Cascaden, technical recruiter in data science. ""Coding languages are sexy, data principles are interesting and intriguing, and they will absolutely affect the way technology is developed in the future. But it is also a market that is not fully understood by many organizations.""
Many people think of data science as an industry that only performs certain tasks but that is not true. A data scientist is a topic that combines many different tastes of work.
One of the most common issues in the data science field is the huge difference between expectation and reality. There's an ever-widening gap between what was expected by data scientists and what work they actually do within the industry.
""Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.""  Dan Ariely
This quote is so true. Many people are urging you to get into the science of data because it's all about solving complex problems with new learning algorithms that make a huge impact on a business. They think that being a data scientist is sexy and it will make them look cooler.
But that's not the case. It's not what it looks like.
The fact that people don't actually know exactly what data science is, leads them to make different expectations from the industry and when they enter the industry. Boom!! Boom!! And just like that, all their expectations are crushed.
And because people's expectation isn't equal to the reality that's the main reason why so many data scientists leave.
""There is a saying, 'A jack of all trades and a master of none.' When it comes to being a data scientist you need to be a bit like this, but perhaps a better saying would be, 'A jack of all trades and a master of some.'"" Brendan Tierney
We all love new challenges, don't we?
Many would disagree with the fact that the field of data science is ripe for these challenges given the rapid pace of development.
Considering technology is rapidly evolving, the necessity for a company to rent a knowledge scientist is additionally evolving. With the increasing demand for emerging technologies, businesses are trying to find data scientists to be skilled with new-age technologies instead of older programming languages like R, Ada, C, Haskell, etc.
Currently, companies are searching for newer skills like data visualization, machine learning, to call some, to form a more informed decision during this competitive landscape. Data scientists now need to know advanced mathematical tools and methods for calculating and integrating large data sets.
So, as the skills requirements are increasing, it's becoming very hard for people to get into this field, which, in turn, creates a shortage within the market.
There is a lot of scope in this field but the lack of people with actual skills needed is snatching the title of the 'most promising job' and 'sexiest job' and many people are walking away from it.
Many might argue with the fact that you can't fake 'sexy'.
Either you are or you aren't  you have it, or you don't.
And even if some organizations don't exactly know how to best tap the talent of their data scientists, but if those professionals are the ""real deal,"" that appeal can't be denied.
""Data scientist is still the sexiest job of the 21st Century,"" insisted Crawford. ""The professionals in this field are absolutely amazing and extremely intelligent. In what other fields can you be part detective, part innovator, and, depending on who you work for, part international spy.""
The role of a data scientist has changed dramatically which in turn affected the whole industry and its people. A lot of companies misrepresent 'data science' when they really are looking for is data analysts, data engineers, or business intelligence, analysts.
""Many companies need to stop looking for a unicorn and start building a data science team"", says the CEO of data applications firm Lattice.
The data scientist is a job that requires a lot of hard work, consistent efforts, persistence, and most importantly patience.
PATIENCE...and that's where the main problem arises.
Our generation lack patience skills, they need everything according to their expected timeline, they need results immediately without doing the actual hard work. It's like they want a pizza without putting any effort to order or without having the patience to wait for its delivery.
And that can't happen.
If they want a pizza then they gotta put an effort to make a phone call to order it and they gotta have to wait for it to be delivered.
It requires both: Hardwork+Patience.
There is so much to learn and no one is willing to do the hard work. It is very difficult for anyone to start with basics and excel in this field. This requires consistent efforts over time and people aren't patient.
""I would say 'yes,' but it depends on how and why you are using it,"" said data science recruiter, Frazer Spackman, at Huxley Associates. ""If you are working for a business where data science is integral to the business product, vision, and success, then this statement is true. But a lot of companies are misrepresenting 'data science' when really they are looking for data analysts, data engineers, or business intelligence analysts, in which case, the statement above doesn't apply.""
javascript.plainenglish.io
medium.com
medium.com
empowerment through data, knowledge, and expertise.
3.7K 
46
3.7K claps
3.7K 
",33
https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------1-----------------------,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You...",Adam Geitgey,16,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in , , , Portugues, Tieng Viet,  or Italiano.
Giant update: I've written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
Are you tired of reading endless news stories about deep learning and not really knowing what that means? Let's change that!
This time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we're going to explain the black magic that allows Google Photos to search your photos based on what is in the picture:
Just like Part 1 and Part 2, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone  which means that there's a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished!
(If you haven't already read part 1 and part 2, read them now!)
You might have seen this famous xkcd comic before.
The goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years.
In the last few years, we've finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one.
So let's do it  let's write a program that can recognize birds!
Before we learn how to recognize pictures of birds, let's learn how to recognize something much simpler  the handwritten number ""8"".
In Part 2, we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:
We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let's modify this same neural network to recognize handwritten text. But to make the job really simple, we'll only try to recognize one letter  the numeral ""8"".
Machine learning only works when you have data  preferably a lot of data. So we need lots and lots of handwritten ""8""s to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some ""8""s from the data set:
The neural network we made in Part 2 only took in a three numbers as the input (""3"" bedrooms, ""2000"" sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers?
The answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:
To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:
The handle 324 inputs, we'll just enlarge our neural network to have 324 input nodes:
Notice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an ""8"" and thee second output will predict the likelihood it isn't an ""8"". By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups.
Our neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone.
All that's left is to train the neural network with images of ""8""s and not-""8""s so it learns to tell them apart. When we feed in an ""8"", we'll tell it the probability the image is an ""8"" is 100% and the probability it's not an ""8"" is 0%. Vice versa for the counter-example images.
Here's some of our training data:
We can train this kind of neural network in a few minutes on a modern laptop. When it's done, we'll have a neural network that can recognize pictures of ""8""s with a pretty high accuracy. Welcome to the world of (late 1980's-era) image recognition!
It's really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic! ...right?
Well, of course it's not that simple.
First, the good news is that our ""8"" recognizer really does work well on simple images where the letter is right in the middle of the image:
But now the really bad news:
Our ""8"" recognizer totally fails to work when the letter isn't perfectly centered in the image. Just the slightest position change ruins everything:
This is because our network only learned the pattern of a perfectly-centered ""8"". It has absolutely no idea what an off-center ""8"" is. It knows exactly one pattern and one pattern only.
That's not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the ""8"" isn't perfectly centered.
We already created a really good program for finding an ""8"" centered in an image. What if we just scan all around the image for possible ""8""s in smaller sections, one section at a time, until we find one?
This approach called a sliding window. It's the brute force solution. It works well in some limited cases, but it's really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this!
When we trained our network, we only showed it ""8""s that were perfectly centered. What if we train it with more data, including ""8""s in all different positions and sizes all around the image?
We don't even need to collect new training data. We can just write a script to generate new images with the ""8""s in all kinds of different positions in the image:
Using this technique, we can easily create an endless supply of training data.
More data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns.
To make the network bigger, we just stack up layer upon layer of nodes:
We call this a ""deep neural network"" because it has more layers than a traditional neural network.
This idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly.
But even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn't going to get us all the way to a solution. We need to be smarter about how we process images into our neural network.
Think about it. It doesn't make sense to train a network to recognize an ""8"" at the top of a picture separately from training it to recognize an ""8"" at the bottom of a picture as if those were two totally different objects.
There should be some way to make the neural network smart enough to know that an ""8"" anywhere in the picture is the same thing without all that extra training. Luckily... there is!
As a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture:
As a human, you instantly recognize the hierarchy in this picture:
Most importantly, we recognize the idea of a child no matter what surface the child is on. We don't have to re-learn the idea of child for every possible surface it could appear on.
But right now, our neural network can't do this. It thinks that an ""8"" in a different part of the image is an entirely different thing. It doesn't understand that moving an object around in the picture doesn't make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks.
We need to give our neural network understanding of translation invariance  an ""8"" is an ""8"" no matter where in the picture it shows up.
We'll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images).
Instead of feeding entire images into our neural network as one grid of numbers, we're going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture.
Here's how it's going to work, step by step 
Similar to our sliding window search above, let's pass a sliding window over the entire original image and save each result as a separate, tiny picture tile:
By doing this, we turned our original image into 77 equally-sized tiny image tiles.
Earlier, we fed a single image into a neural network to see if it was an ""8"". We'll do the exact same thing here, but we'll do it for each individual image tile:
However, there's one big twist: We'll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we'll mark that tile as interesting.
We don't want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this:
In other words, we've started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting.
The result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big:
To reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn't at all!
We'll just look at each 2x2 square of the array and keep the biggest number:
The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we'll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits.
So far, we've reduced a giant image down into a fairly small array.
Guess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn't a match. To differentiate it from the convolution step, we call it a ""fully connected"" network.
So from start to finish, our whole five-step pipeline looks like this:
Our image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network.
When solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data.
The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize.
For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it's knowledge of sharp edges, the third step might recognize entire birds using it's knowledge of beaks, etc.
Here's what a more realistic deep convolutional network (like you would find in a research paper) looks like:
In this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories!
So how do you know which steps you need to combine to make your image classifier work?
Honestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error!
Now finally we know enough to write a program that can decide if a picture is a bird or not.
As always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we'll also add in the Caltech-UCSD Birds-200-2011 data set that has another 12,000 bird pics.
Here's a few of the birds from our combined data set:
And here's some of the 52,000 non-bird images:
This data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need millions of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data!
To build our classifier, we'll use TFLearn. TFlearn is a wrapper around Google's TensorFlow deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network.
Here's the code to define and train the network:
If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer.
As it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn't help, so I stopped it there.
Congrats! Our program can now recognize birds in images!
Now that we have a trained neural network, we can use it! Here's a simple script that takes in a single image file and predicts if it is a bird or not.
But to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time.
That seems pretty good, right? Well... it depends!
Our network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things.
For example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed ""not a bird"" every single time would be 95% accurate! But it would also be 100% useless.
We need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at how it failed, not just the percentage of the time that it failed.
Instead of thinking about our predictions as ""right"" and ""wrong"", let's break them down into four separate categories 
Using our validation set of 15,000 images, here's how many times our predictions fell into each category:
Why do we break our results down like this? Because not all mistakes are created equal.
Imagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we'd rather have false positives than false negatives. False negatives would be the worse possible case  that's when the program told someone they definitely didn't have cancer but they actually did.
Instead of just looking at overall accuracy, we calculate Precision and Recall metrics. Precision and Recall metrics give us a clearer picture of how well we did:
This tells us that 97% of the time we guessed ""Bird"", we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one!
Now that you know the basics of deep convolutional networks, you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don't even have to find your own images.
You also know enough now to start branching and learning about other areas of machine learning. Why not learn how to use algorithms to train computers how to play Atari games next?
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I'll only email you when I have something new and awesome to share. It's the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I'd love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 4, Part 5 and Part 6!
",34
https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53?source=tag_archive---------0-----------------------,A Comprehensive Guide to Convolutional Neural Networks  the ELI5 way,Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines...,Sumit Saha,7,"Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines. Researchers and enthusiasts alike, work on numerous aspects of the field to make amazing things happen. One of many such areas is the domain of Computer Vision.
The agenda for this field is to enable machines to view the world as humans do, perceive it in a similar manner and even use the knowledge for a multitude of tasks such as Image & Video recognition, Image Analysis & Classification, Media Recreation, Recommendation Systems, Natural Language Processing, etc. The advancements in Computer Vision with Deep Learning has been constructed and perfected with time, primarily over one particular algorithm  a Convolutional Neural Network.
A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.
The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.
An image is nothing but a matrix of pixel values, right? So why not just flatten the image (e.g. 3x3 image matrix into a 9x1 vector) and feed it to a Multi-Level Perceptron for classification purposes? Uh.. not really.
In cases of extremely basic binary images, the method might show an average precision score while performing prediction of classes but would have little to no accuracy when it comes to complex images having pixel dependencies throughout.
A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better.
In the figure, we have an RGB image which has been separated by its three color planes  Red, Green, and Blue. There are a number of such color spaces in which images exist  Grayscale, RGB, HSV, CMYK, etc.
You can imagine how computationally intensive things would get once the images reach dimensions, say 8K (76804320). The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets.
Image Dimensions = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg. RGB)
In the above demonstration, the green section resembles our 5x5x1 input image, I. The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel/Filter, K, represented in the color yellow. We have selected K as a 3x3x1 matrix.
The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering.
The filter moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.
In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between Kn and In stack ([K1, I1]; [K2, I2]; [K3, I3]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output.
The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.
There are two types of results to the operation  one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying Valid Padding in case of the former, or Same Padding in the case of the latter.
When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name  Same Padding.
On the other hand, if we perform the same operation without padding, we are presented with a matrix which has dimensions of the Kernel (3x3x1) itself  Valid Padding.
The following repository houses many such GIFs which would help you get a better understanding of how Padding and Stride Length work together to achieve results relevant to our needs.
github.com
Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.
There are two types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.
Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.
The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.
After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.
Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.
Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.
There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Some of them have been listed below:
GitHub Notebook  Recognising Hand Written Digits using MNIST Dataset with TensorFlow
github.com
",35
https://towardsdatascience.com/20-necessary-requirements-of-a-perfect-laptop-for-data-science-and-machine-learning-tasks-7d0c59c3cb63?source=tag_archive---------9-----------------------,20 Necessary Requirements of a Perfect Laptop for Data Science and Machine Learning Tasks,"If you're learning Data Science and Machine Learning, you definitely need a laptop. This is because you need to write and run your own code to get hands-on experience. When you also consider...",Rukshan Pramoditha,7,"If you're learning Data Science and Machine Learning, you definitely need a laptop. This is because you need to write and run your own code to get hands-on experience. When you also consider portability, the laptop is the best option instead of a desktop.
A traditional laptop may not be perfect for your data science and machine learning tasks. You need to consider laptop specifications carefully to choose the right laptop. If you're looking to buy a laptop for data science and machine learning tasks, this post is for you! Here, I'll discuss 20 necessary requirements of a perfect laptop data science and machine learning tasks.
Let's get started!
Always consider buying newer-generation processors. Intel 11th Gen processors and AMD 5th Gen (5000 Series) processors are now available. Intel 8th Gen, Intel 10th Gen and AMD 3rd Gen (3000 Series), AMD 4th Gen (4000 Series) processors are other options that you can consider. However, the processing power, new hardware compatibility, power efficiency and thermal management drastically increase with newer-generation processors.
This is one of the most critical requirements that you should consider. Most of the machine learning tasks require parallel computations. For example, when you train a Random Forest algorithm or performing hyperparameter tuning, you can specify a higher number of cores to be used by the algorithm when your processor has a higher number of cores. This will speed up the process significantly. Cores are the number of independent CPUs in a single chip. They are hardware. Threads are instructions that can be processed by a single CPU core. Always consider buying a laptop with a higher number of CPU cores and threads especially if your laptop doesn't come with a separate (discrete) graphic card (GPU). 4-cores  8-threads processor is the minimum requirement that I can recommend for you. If you can afford more money, you can go for 6-cores  12-threads or 8-cores  16-threads or higher.
The base frequency is the minimum speed of the processor. The higher the base frequency, the faster your processor. This is typically measured in Gigahertz (GHz).
Cache memory acts as a buffer between RAM and the CPU. It holds frequently-used data and instructions so that they are immediately available to the CPU when used again. The higher the cache memory, the faster your computer. Cache memory is typically measured in Megabytes (MB). An 8 MB of cache memory is recommended.
It is worth considering this if you're planning to upgrade the memory in the future. Recommend memory type is DDR-4, the size is 8 GB and the speed is 3200 MHz.
By considering the above requirements, I can recommend the following processors for you.
This is a list of my choice. You have the freedom to choose the right processor by considering both the above requirements and your budget.
This is considered by most people. But keep in mind that increasing the RAM size does not speed up your computer. Higher RAM sizes will allow you to multi-tasking. At least 8 GB RAM size is recommended. I do not recommend 4 GB RAM because the operating system already takes about 3 GB of the RAM and only 1 GB is available for other tasks. If you can afford and your laptop supports, upgrading to 12 GB or 16 GB is a perfect option. You will often want to install virtual operating systems on your laptop for big data analytics. Such virtual operating systems needs at least 4 GB of RAM. The current operating system tasks about 3 GB RAM. In this case, 8 GB of RAM will not be enough and 12 GB and 16 GB are perfect options.
The recommended speed is 2666 MHz. Do not go below this. Modern DDR-4 RAMs support 3200 MHz of bus speed. The higher the bus speed, the faster your computer.
This is the most critical requirement. Traditional laptops come with HDDs (Hard Disk Drives). HDDs are really really slow. If you buy an i7 laptop with a traditional HDD, your laptop will be really slow. It takes much time to boot up and open programs. HDDs have moving mechanical parts which delay the processing of information and reduce reliability and durability. Therefore, it is necessary to buy or upgrade your laptop with an SSD (Solid State Drive). SSDs are significantly powerful than HDDs. They have no moving parts and provide superior performance. NVMe SSD is an upgrade version of a normal SSD. They are 6x faster than a normal SSD. If your laptop motherboard supports for NVMe SSDs, you can consider upgrading. You can even replace your HDD with a normal SSD, but not with an NVMe SSD. For this, you should buy an SSD with a 2.5-inch form factor, not an M.2 form factor. NVMe SSDs do not support the 2.5-inch form factor.
If you consider buying a laptop with SSD, you may not be able to afford money to buy a 1 TB SSD because it is much expensive. The ideal size is 512 GB. Do not go below this.
NVIDIA and AMD are the two major brands of graphics cards. Tensorflow deep learning library uses the CUDA processor which compiles only on NVIDIA graphics cards. Therefore, I highly recommend you buy a laptop with an NVIDIA GPU if you're planning to do deep learning tasks. A GTX 1650 or higher GPU is recommended. Another advantage of having a separate graphics card is that an average GPU has more than 100 cores, but a standard CPU has 4 or 8 cores.
At least a 4 GB GPU is recommended.
This is something you should definitely consider for. Your eyes are really really worth. Data science and machine learning students spend hours of time in front of a laptop. Most electronic devices with displays emit harmful blue light and laptops do also. However, modern laptops have blue light filtering technology and flicker-free screens. Another great option is buying a monitor and connecting it with your existing laptop. Most modern monitors come with certified low-blue light, flicker-free screens.
A 15.6-inch or 17.3-inch display is highly recommended. Do not go below this. Having a 22-inch or 24-inch display is the perfect option. Therefore, consider buying a monitor and connecting it with your laptop.
A full-HD (1080p) or an HD (720p) display is recommended.
I can't tell you to buy a specific brand. It is totally up to you for deciding it. Here are some points to consider for.
Reliability refers to how often your laptop fails when operating. Unexpected shutdowns, blue screen errors and other hardware failures are the most common issues.
Durability refers to how long you can use your laptop. You can guess the durability of your laptop by looking at its warranty period. If you can use your laptop for at least 4 years, it will not waste your money.
Your chosen brand should provide product manuals, upgrade options and technical support.
Your chosen brand should provide easy upgrade options for adding RAM and secondary storage (e.g. SSDs)
A full-size keyboard with a number pad is highly recommended.
Windows 10 is recommended. It is user-friendly. If you want, other operating systems such as Linux can be easily run virtually within the Windows OS.
I hope you'll buy the right laptop next time!
My readers can sign up for a membership through the following link to get full access to every story I write and I will receive a portion of your membership fee.
Sign-up link: https://rukshanpramoditha.medium.com/membership
Thank you so much for your continuous support! See you in the next story. Happy learning to everyone!
Rukshan Pramoditha2021-06-05
",36
https://towardsdatascience.com/3-beginner-mistakes-ive-made-in-my-data-science-career-a177f1cf751b?source=tag_archive---------1-----------------------,3 Beginner Mistakes I've Made in My Data Science Career,Writing this feels like career suicide,Arunn Thevapalan,8,"""You have been talking a lot about your learnings and successes; why not spill some of your mistakes and regrets too?""
That was my sister after I had lost a bet yesterday.
Now she's forcing me into writing this article. I know what you're thinking  she has a fair point, but maybe I'm not comfortable (yet) with the world seeing my mistakes.
Writing this feels like career suicide.
Will you follow my work even after this? I don't know. Maybe that's why I don't see many experts talk about their early career mistakes. C'mon, surely they should have committed some too?
So guys, here I am, pressured to be honest about the mistakes I made when I started a career in data science. I'll dive deep into revealing causes for every mistake, its importance for beginner data scientists, and how we can eventually avoid them.
Let's see how this goes, shall we?
""So what are the characteristics of these clustered residents?"" my manager asked.
We had used the most advanced, recently released model to segment the residents of a smart city. The whole model was a black box, so we have no idea how it does the segmentation but gave the highest accurate clusters.
I thought for a minute; I couldn't come up with an answer. Our model had no interpretability.
I hadn't learned the lesson, though. At a later time, the client turned down our proof of concept for a potential project.
""This solution looks promising but let us get back to you. The investment of deploying this solution might be a bit too high.""
We had proposed a computer vision system to estimate the mass of fishes, using state-of-the-art object detection and depth estimation models. Still, we hadn't accounted for the expensive GPU-based computation that came along with that.
Whenever I have been presented with a problem to solve, my brain is used to thinking of neural networks and complex algorithms.
Computer vision? Convolutional Neural Networks. NLP? Transformers. Synthetic data generation? GANs. Tabular data? XGBoost. In a nutshell, I'd opt for the most complex algorithm out there because I believed, more the complexity, the better the solution.
To some extent, this idea is true, especially when you want to win Kaggle competitions. That's how these algorithms got popular in the first place.
Here's the twist: In the real world, a 2% improvement in accuracy need not be as significant as it is in hackathons and competitions. The interpretability of the solutions, the operational cost of the solutions matter much more.
I have only one piece of advice to give you, which has worked for me ever since I've understood how the real world and businesses work. This trick has made our clients the happiest and my life the easiest.
Are you ready for it?
Start simple.
You heard me. Start with simple machine learning algorithms. There's no point in complicating things upfront. Start with simpler solutions, which are more interpretable and are cost-efficient. There's no harm in experimenting with linear or logistic regression.
Christoph Molnar has written a gem of a book on how to use interpretable machine learning techniques. Always a good idea to keep you educated on these topics.
If the performance is satisfactory, you're good to go. If not, level up to a slightly complex one while accounting for the trade-off in interpretability and operational costs. This way, everyone's expectations are bound to meet 100% of the time. Win-win.
It's time I come clean; I was focused only on the cool stuff  building models and making predictions. For years into my career, I hadn't properly mastered how to visualize or tell a story through data.
I was dependant on our Business Intelligence team to build every dashboard and communicate the insights to the decision-makers. Knowing to use libraries such as matplotlib doesn't count  I'm talking about what's beyond that.
Data Visualizations make the data more natural for the human minds to comprehend and helps makes everybody's life much easier. Here are few instances I've seen data visualization make all the difference:
With time, my crucial realization was that every phase of a data science lifecycle serves different but integral purposes and needs to be treated with equal importance. Ignoring a phase for too long will bite you in the back sometime later.
First things first, understand the importance of every phase in the data science lifecycle. You can refer to this end-to-end project guide to get a holistic picture of different phases in data science. Once you understand the different phases and skills relevant to them, start building each skill as soon as possible.
Talking specifically about data viz, my simple action plan was:
There's much more to learn beyond plotting a graph using matplolib or seaborn; most of us stop our learning curve there.
This course from Coursera helped me understand the basics of data story-telling, essential design principles, building dashboards, and more. The most common tools in this space are PowerBI and Tableau, and the course uses the latter. This is the data viz course I wish I enrolled in much earlier than later.
Then you pick any dataset and build a dashboard to tell a story uncovering insights from the data. At this point, remember, you don't need to be perfect; you're only practicing your skills. Only you get to see what you've built, so feel free to experiment.
Finally, volunteer to help with data visualization elements of your next project and see how it goes. You can master the advanced techniques as you go; there's no hurry.
""What's Software Engineering got to do with Data Science anyway?""
During my undergraduate studies, I did a Software Engineering internship. I used to help the team in software development, write technical documentation and perform tests to assess the quality of development.
Interns get to work on the entire stack to gain exposure and a taste of what it's like to work as a software engineer. I disliked the experience so much that I knew software engineering wasn't meant for me.
Luckily, I found my passion in data science and left behind software engineering for good. I learned all the basic machine learning, volunteered for projects, built a portfolio, and eventually broke into the industry.
Whenever something on the lines of software engineering pops up in the process, I ignore them. ""What's Software Engineering got to do with Data Science anyway? There'll be software engineers in the team,"" I thought.
The need to write unit tests, clean codes, version control, building web applications, docker containers, object-oriented classes kept popping up constantly.
In hindsight, I ignored a crucial component of the popular data science Venn diagram because of a single bad experience I had during my internship.
Eventually, I had to give in and embrace software engineering. When I did, it wasn't bad at all. I realized we don't need to master everything, but only some instrumental skills. They call it software engineering for data scientists!
If you're a beginner learning data science, please come with a blank slate and an open mind, hungry for knowledge. There's a lot to learn, and you need some software engineering skills regardless of your background.
There's no need to panic; here are some resources I used which will be helpful to you:
I have friends who even came from Physics, Economics, or even Mechatronics background and learned these basics and are doing fine. So if they can do it, you surely can too.
Just don't be ignorant like me. That's all.
Yes, I was initially pressured into writing this, but I'm so glad I put it out.
If you've been reading so far, you might wonder, this guy didn't want to write this in the first place but then shared everything in-depth. True true true! I never intended to. As thoughts started pouring in, I kept writing because it felt like the right thing to do.
You deserve to see the complete picture  we all deserve it.
People assume everything's great and experts don't make any mistakes. I know you think so because I thought so too. But that's not true.
Most focus on sharing their successes and how they did it, hoping to inspire you to become better. There's nothing wrong with that idea. I've been doing it too.
But isn't giving a complete picture of reality the right thing to do?
I couldn't have become a Senior Data Scientist overnight, nor can anybody. I learned from the experts, and their courses, acquired the skills, applied them, and eventually got better at it.
In that process, I made mistakes; you will make some too. We all make mistakes, and that's okay. If you don't learn anything from your mistakes, that's not okay.
Learn from your mistakes and progress towards your goals. Everyone can do this. You definitely can, trust me, trust you.
As a note of disclosure, this article may have some affiliate links to share the best resources I've used at no extra cost to you. Thanks for your support!
For more helpful insights on breaking into data science, honest experiences, and learnings, consider joining my private list of email friends.
",37
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73?source=tag_archive---------1-----------------------,Understanding Variational Autoencoders (VAEs),"Building, step by step, the reasoning that leads to VAEs.",Joseph Rocca,23,"This post was co-written with Baptiste Rocca.
In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data, well-designed networks architectures and smart training techniques, deep generative models have shown an incredible ability to produce highly realistic pieces of content of various kind, such as images, texts and sounds. Among these deep generative models, two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).
In a previous post, published in January of this year, we discussed in depth Generative Adversarial Networks (GANs) and showed, in particular, how adversarial training can oppose two networks, a generator and a discriminator, to push both of them to improve iteration after iteration. We introduce now, in this post, the other major kind of deep generative models: Variational Autoencoders (VAEs). In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term ""variational"" comes from the close relation there is between the regularisation and the variational inference method in statistics.
If the last two sentences summarise pretty well the notion of VAEs, they can also raise a lot of questions. What is an autoencoder? What is the latent space and why regularising it? How to generate new data from VAEs? What is the link between VAEs and variational inference? In order to describe VAEs as well as possible, we will try to answer all this questions (and many others!) and to provide the reader with as much insights as we can (ranging from basic intuitions to more advanced mathematical details). Thus, the purpose of this post is not only to discuss the fundamental notions Variational Autoencoders rely on but also to build step by step and starting from the very beginning the reasoning that leads to these notions.
Without further ado, let's (re)discover VAEs together!
In the first section, we will review some important notions about dimensionality reduction and autoencoder that will be useful for the understanding of VAEs. Then, in the second section, we will show why autoencoders cannot be used to generate new data and will introduce Variational Autoencoders that are regularised versions of autoencoders making the generative process possible. Finally in the last section we will give a more mathematical presentation of VAEs, based on variational inference.
Note. In the last section we have tried to make the mathematical derivation as complete and clear as possible to bridge the gap between intuitions and equations. However, the readers that doesn't want to dive into the mathematical details of VAEs can skip this section without hurting the understanding of the main concepts. Notice also that in this post we will make the following abuse of notation: for a random variable z, we will denote p(z) the distribution (or the density, depending on the context) of this random variable.
In this first section we will start by discussing some notions related to dimensionality reduction. In particular, we will review briefly principal component analysis (PCA) and autoencoders, showing how both ideas are related to each others.
In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data (data visualisation, data storage, heavy computation...). Although there exists many different methods of dimensionality reduction, we can set a global framework that is matched by most (if not any!) of these methods.
First, let's call encoder the process that produce the ""new features"" representation from the ""old features"" representation (by selection or by extraction) and decoder the reverse process. Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding.
The main purpose of a dimensionality reduction method is to find the best encoder/decoder pair among a given family. In other words, for a given set of possible encoders and decoders, we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. If we denote respectively E and D the families of encoders and decoders we are considering, then the dimensionality reduction problem can be written
where
defines the reconstruction error measure between the input data x and the encoded-decoded data d(e(x)). Notice finally that in the following we will denote N the number of data, n_d the dimension of the initial (decoded) space and n_e the dimension of the reduced (encoded) space.
One of the first methods that come in mind when speaking about dimensionality reduction is principal component analysis (PCA). In order to show how it fits the framework we just described and make the link towards autoencoders, let's give a very high overview of how PCA works, letting most of the details aside (notice that we plan to write a full post on the subject).
The idea of PCA is to build n_e new independent features that are linear combinations of the n_d old features and so that the projections of the data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance). In other words, PCA is looking for the best linear subspace of the initial space (described by an orthogonal basis of new features) such that the error of approximating the data by their projections on this subspace is as small as possible.
Translated in our global framework, we are looking for an encoder in the family E of the n_e by n_d matrices (linear transformation) whose rows are orthonormal (features independence) and for the associated decoder among the family D of n_d by n_e matrices. It can be shown that the unitary eigenvectors corresponding to the n_e greatest eigenvalues (in norm) of the covariance features matrix are orthogonal (or can be chosen to be so) and define the best subspace of dimension n_e to project data on with minimal error of approximation. Thus, these n_e eigenvectors can be chosen as our new features and, so, the problem of dimension reduction can then be expressed as an eigenvalue/eigenvector problem. Moreover, it can also be shown that, in such case, the decoder matrix is the transposed of the encoder matrix.
Let's now discuss autoencoders and see how we can use neural networks for dimensionality reduction. The general idea of autoencoders is pretty simple and consists in setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks.
Thus, intuitively, the overall autoencoder architecture (encoder+decoder) creates a bottleneck for data that ensures only the main structured part of the information can go through and be reconstructed. Looking at our general framework, the family E of considered encoders is defined by the encoder network architecture, the family D of considered decoders is defined by the decoder network architecture and the search of encoder and decoder that minimise the reconstruction error is done by gradient descent over the parameters of these networks.
Let's first suppose that both our encoder and decoder architectures have only one layer without non-linearity (linear autoencoder). Such encoder and decoder are then simple linear transformations that can be expressed as matrices. In such situation, we can see a clear link with PCA in the sense that, just like PCA does, we are looking for the best linear subspace to project data on with as few information loss as possible when doing so. Encoding and decoding matrices obtained with PCA define naturally one of the solutions we would be satisfied to reach by gradient descent, but we should outline that this is not the only one. Indeed, several basis can be chosen to describe the same optimal subspace and, so, several encoder/decoder pairs can give the optimal reconstruction error. Moreover, for linear autoencoders and contrarily to PCA, the new features we end up do not have to be independent (no orthogonality constraints in the neural networks).
Now, let's assume that both the encoder and the decoder are deep and non-linear. In such case, the more complex the architecture is, the more the autoencoder can proceed to a high dimensionality reduction while keeping reconstruction loss low. Intuitively, if our encoder and our decoder have enough degrees of freedom, we can reduce any initial dimensionality to 1. Indeed, an encoder with ""infinite power"" could theoretically takes our N initial data points and encodes them as 1, 2, 3, ... up to N (or more generally, as N integer on the real axis) and the associated decoder could make the reverse transformation, with no loss during the process.
Here, we should however keep two things in mind. First, an important dimensionality reduction with no reconstruction loss often comes with a price: the lack of interpretable and exploitable structures in the latent space (lack of regularity). Second, most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations. For these two reasons, the dimension of the latent space and the ""depth"" of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction.
Up to now, we have discussed dimensionality reduction problem and introduce autoencoders that are encoder-decoder architectures that can be trained by gradient descent. Let's now make the link with the content generation problem, see the limitations of autoencoders in their current form for this problem and introduce Variational Autoencoders.
At this point, a natural question that comes in mind is ""what is the link between autoencoders and content generation?"". Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well ""organized"" by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network.
However, as we discussed in the previous section, the regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described.
To illustrate this point, let's consider the example we gave previously in which we described an encoder and a decoder powerful enough to put any N initial training data onto the real axis (each data point being encoded as a real value) and decode them without any reconstruction loss. In such case, the high degree of freedom of the autoencoder that makes possible to encode and decode with no information loss (despite the low dimensionality of the latent space) leads to a severe overfitting implying that some points of the latent space will give meaningless content once decoded. If this one dimensional example has been voluntarily chosen to be quite extreme, we can notice that the problem of the autoencoders latent space regularity is much more general than that and deserve a special attention.
When thinking about it for a minute, this lack of structure among the encoded data into the latent space is pretty normal. Indeed, nothing in the task the autoencoder is trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised. Thus, if we are not careful about the definition of the architecture, it is natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it can... unless we explicitly regularise it!
So, in order to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. Thus, as we briefly mentioned in the introduction of this post, a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.
Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space. The model is then trained as follows:
In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. We will see in the next subsection that we ensure this way both a local and global regularisation of the latent space (local because of the variance control and global because of the mean control).
Thus, the loss function that is minimised when training a VAE is composed of a ""reconstruction term"" (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a ""regularisation term"" (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. That regularisation term is expressed as the Kulback-Leibler divergence between the returned distribution and a standard Gaussian and will be further justified in the next section. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions.
The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: continuity (two close points in the latent space should not give two completely different contents once decoded) and completeness (for a chosen distribution, a point sampled from the latent space should give ""meaningful"" content once decoded).
The only fact that VAEs encode inputs as distributions instead of simple points is not sufficient to ensure continuity and completeness. Without a well defined regularisation term, the model can learn, in order to minimise its reconstruction error, to ""ignore"" the fact that distributions are returned and behave almost like classic autoencoders (leading to overfitting). To do so, the encoder can either return distributions with tiny variances (that would tend to be punctual distributions) or return distributions with very different means (that would then be really far apart from each other in the latent space). In both cases, distributions are used the wrong way (cancelling the expected benefit) and continuity and/or completeness are not satisfied.
So, in order to avoid these effects we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder. In practice, this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced). This way, we require the covariance matrices to be close to the identity, preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far apart from each others.
With this regularisation term, we prevent the model to encode data far apart in the latent space and encourage as much as possible returned distributions to ""overlap"", satisfying this way the expected continuity and completeness conditions. Naturally, as for any regularisation term, this comes at the price of a higher reconstruction error on the training data. The tradeoff between the reconstruction error and the KL divergence can however be adjusted and we will see in the next section how the expression of the balance naturally emerge from our formal derivation.
To conclude this subsection, we can observe that continuity and completeness obtained with regularisation tend to create a ""gradient"" over the information encoded in the latent space. For example, a point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases.
Note. As a side note, we can mention that the second potential problem we have mentioned (the network put distributions far from each others) is in fact almost equivalent to the first one (the network tends to return punctual distribution) up to a change of scale: in both case variances of distributions become small relatively to distance between their means.
In the previous section we gave the following intuitive overview: VAEs are autoencoders that encode inputs as distributions instead of points and whose latent space ""organisation"" is regularised by constraining distributions returned by the encoder to be close to a standard Gaussian. In this section we will give a more mathematical view of VAEs that will allow us to justify the regularisation term more rigorously. To do so, we will set a clear probabilistic framework and will use, in particular, variational inference technique.
Let's begin by defining a probabilistic graphical model to describe our data. We denote by x the variable that represents our data and assume that x is generated from a latent variable z (the encoded representation) that is not directly observed. Thus, for each data point, the following two steps generative process is assumed:
With such a probabilistic model in mind, we can redefine our notions of encoder and decoder. Indeed, contrarily to a simple autoencoder that consider deterministic encoder and decoder, we are going to consider now probabilistic versions of these two objects. The ""probabilistic decoder"" is naturally defined by p(x|z), that describes the distribution of the decoded variable given the encoded one, whereas the ""probabilistic encoder"" is defined by p(z|x), that describes the distribution of the encoded variable given the decoded one.
At this point, we can already notice that the regularisation of the latent space that we lacked in simple autoencoders naturally appears here in the definition of the data generation process: encoded representations z in the latent space are indeed assumed to follow the prior distribution p(z). Otherwise, we can also remind the the well-known Bayes theorem that makes the link between the prior p(z), the likelihood p(x|z), and the posterior p(z|x)
Let's now make the assumption that p(z) is a standard Gaussian distribution and that p(x|z) is a Gaussian distribution whose mean is defined by a deterministic function f of the variable of z and whose covariance matrix has the form of a positive constant c that multiplies the identity matrix I. The function f is assumed to belong to a family of functions denoted F that is left unspecified for the moment and that will be chosen later. Thus, we have
Let's consider, for now, that f is well defined and fixed. In theory, as we know p(z) and p(x|z), we can use the Bayes theorem to compute p(z|x): this is a classical Bayesian inference problem. However, as we discussed in our previous article, this kind of computation is often intractable (because of the integral at the denominator) and require the use of approximation techniques such as variational inference.
Note. Here we can mention that p(z) and p(x|z) are both Gaussian distribution. So, if we had E(x|z) = f(z) = z, it would imply that p(z|x) should also follow a Gaussian distribution and, in theory, we could ""only"" try to express the mean and the covariance matrix of p(z|x) with respect to the means and the covariance matrices of p(z) and p(x|z). However, in practice this condition is not met and we need to use of an approximation technique like variational inference that makes the approach pretty general and more robust to some changes in the hypothesis of the model.
In statistics, variational inference (VI) is a technique to approximate complex distributions. The idea is to set a parametrised family of distribution (for example the family of Gaussians, whose parameters are the mean and the covariance) and to look for the best approximation of our target distribution among this family. The best element in the family is one that minimise a given approximation error measurement (most of the time the Kullback-Leibler divergence between approximation and target) and is found by gradient descent over the parameters that describe the family. For more details, we refer to our post on variational inference and references therein.
Here we are going to approximate p(z|x) by a Gaussian distribution q_x(z) whose mean and covariance are defined by two functions, g and h, of the parameter x. These two functions are supposed to belong, respectively, to the families of functions G and H that will be specified later but that are supposed to be parametrised. Thus we can denote
So, we have defined this way a family of candidates for variational inference and need now to find the best approximation among this family by optimising the functions g and h (in fact, their parameters) to minimise the Kullback-Leibler divergence between the approximation and the target p(z|x). In other words, we are looking for the optimal g* and h* such that
In the second last equation, we can observe the tradeoff there exists  when approximating the posterior p(z|x)  between maximising the likelihood of the ""observations"" (maximisation of the expected log-likelihood, for the first term) and staying close to the prior distribution (minimisation of the KL divergence between q_x(z) and p(z), for the second term). This tradeoff is natural for Bayesian inference problem and express the balance that needs to be found between the confidence we have in the data and the confidence we have in the prior.
Up to know, we have assumed the function f known and fixed and we have showed that, under such assumptions, we can approximate the posterior p(z|x) using variational inference technique. However, in practice this function f, that defines the decoder, is not known and also need to be chosen. To do so, let's remind that our initial goal is to find a performant encoding-decoding scheme whose latent space is regular enough to be used for generative purpose. If the regularity is mostly ruled by the prior distribution assumed over the latent space, the performance of the overall encoding-decoding scheme highly depends on the choice of the function f. Indeed, as p(z|x) can be approximate (by variational inference) from p(z) and p(x|z) and as p(z) is a simple standard Gaussian, the only two levers we have at our disposal in our model to make optimisations are the parameter c (that defines the variance of the likelihood) and the function f (that defines the mean of the likelihood).
So, let's consider that, as we discussed earlier, we can get for any function f in F (each defining a different probabilistic decoder p(x|z)) the best approximation of p(z|x), denoted q*_x(z). Despite its probabilistic nature, we are looking for an encoding-decoding scheme as efficient as possible and, then, we want to choose the function f that maximises the expected log-likelihood of x given z when z is sampled from q*_x(z). In other words, for a given input x, we want to maximise the probability to have x = x when we sample z from the distribution q*_x(z) and then sample x from the distribution p(x|z). Thus, we are looking for the optimal f* such that
where q*_x(z) depends on the function f and is obtained as described before. Gathering all the pieces together, we are looking for optimal f*, g* and h* such that
We can identify in this objective function the elements introduced in the intuitive description of VAEs given in the previous section: the reconstruction error between x and f(z) and the regularisation term given by the KL divergence between q_x(z) and p(z) (which is a standard Gaussian). We can also notice the constant c that rules the balance between the two previous terms. The higher c is the more we assume a high variance around f(z) for the probabilistic decoder in our model and, so, the more we favour the regularisation term over the reconstruction term (and the opposite stands if c is low).
Up to know, we have set a probabilistic model that depends on three functions, f, g and h, and express, using variational inference, the optimisation problem to solve in order to get f*, g* and h* that give the optimal encoding-decoding scheme with this model. As we can't easily optimise over the entire space of functions, we constrain the optimisation domain and decide to express f, g and h as neural networks. Thus, F, G and H correspond respectively to the families of functions defined by the networks architectures and the optimisation is done over the parameters of these networks.
In practice, g and h are not defined by two completely independent networks but share a part of their architecture and their weights so that we have
As it defines the covariance matrix of q_x(z), h(x) is supposed to be a square matrix. However, in order to simplify the computation and reduce the number of parameters, we make the additional assumption that our approximation of p(z|x), q_x(z), is a multidimensional Gaussian distribution with diagonal covariance matrix (variables independence assumption). With this assumption, h(x) is simply the vector of the diagonal elements of the covariance matrix and has then the same size as g(x). However, we reduce this way the family of distributions we consider for variational inference and, so, the approximation of p(z|x) obtained can be less accurate.
Contrarily to the encoder part that models p(z|x) and for which we considered a Gaussian with both mean and covariance that are functions of x (g and h), our model assumes for p(x|z) a Gaussian with fixed covariance. The function f of the variable z defining the mean of that Gaussian is modelled by a neural network and can be represented as follows
The overall architecture is then obtained by concatenating the encoder and the decoder parts. However we still need to be very careful about the way we sample from the distribution returned by the encoder during the training. The sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called reparametrisation trick, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture and consists in using the fact that if z is a random variable following a Gaussian distribution with mean g(x) and with covariance H(x)=h(x).h^t(x) then it can be expressed as
Finally, the objective function of the variational autoencoder architecture obtained this way is given by the last equation of the previous subsection in which the theoretical expectancy is replaced by a more or less accurate Monte-Carlo approximation that consists, most of the time, into a single draw. So, considering this approximation and denoting C = 1/(2c), we recover the loss function derived intuitively in the previous section, composed of a reconstruction term, a regularisation term and a constant to define the relative weights of these two terms.
The main takeways of this article are:
To conclude, we can outline that, during the last years, GANs have benefited from much more scientific contributions than VAEs. Among other reasons, the higher interest that has been shown by the community for GANs can be partly explained by the higher degree of complexity in VAEs theoretical basis (probabilistic model and variational inference) compared to the simplicity of the adversarial training concept that rules GANs. With this post we hope that we managed to share valuable intuitions as well as strong theoretical foundations to make VAEs more accessible to newcomers, as we did for GANs earlier this year. However, now that we have discussed in depth both of them, one question remains... are you more GANs or VAEs?
Thanks for reading!
Other articles written with Baptiste Rocca:
towardsdatascience.com
towardsdatascience.com
",38
https://towardsdatascience.com/new-m1-who-dis-677e085baffd?source=tag_archive---------2-----------------------,Setting Up a New M1 MacBook for Data Science,Python/Pandas guy sets up shop on Apple Silicone,Robert Boscacci,6,"In April 2021 I was granted the privilege of unboxing a brand new 2020 M1 Apple MacBook Air.
If you're hoping to run Python libraries natively (i.e. without Rosetta 2 translation) on your new M1 MacBook, save yourself a lot of hassle: Use miniforge. It's like miniconda, but with Apple M1 supported by default.
I would have considered an Ubuntu setup, but I'm a sucker for some commercialized mac/win-only apps like Adobe Lightroom. I considered switching to Windows (and making use of WSL2): That way, I could still have my stupid GUI apps plus a Linuxy dev environment, then splurge on hardware. But there's just a special something about the industrial design of MacBooks! I like how effortless it is to throw a MacBook air in a backpack, to bring it with you. I have grown roots on macOS.
I was aware of some catches with Apple Silicon: The new arm64 processor architecture has posed a challenge for data science library compatibility. Basically, it's too newa problem I'm not accustomed to having. Still, the new Apple chips are fast, and the developer community is working to bridge the gap.
It's also not the most powerful laptop you could get for the price. That said, UX is probably more important to me than raw computing power on my local everyday machine, in a world where the heavy lifting of production ML is increasingly performed on cloud servers. I've got the basics of AWS and I'm taking classes on Docker, Kubernetes, Terraform, etc. I can still shell into my old x86 Hackintosh desktop running plaidML in a pinch.
I had been shopping for used 2016-2018 MacBooks on eBay (carefully trying to avoid butterfly keyboards) to replace my 2012 (which no longer supports the latest macOS), but then my employer intervened with an offer to buy me a new Apple machine outright, so I took them up on it. Thank you, employer! I have faith that full compatibility with all the popular data science libraries will come with time, and this computer will run faster and last longer than a used one.
So now I've got a shiny new MacBook. Here's the order in which I settled into my new hardware, for lack of fancy administrative setup scripts:
My python 3.8.6 build failed, I think because it's not compatible with the new chip. Now what?
This is a roadblock I was expecting to hit. I know not everyone has ported their Python data science libraries to work natively on the Apple M1 chip quite yet. I am hoping to avoid relying upon the Rosetta translator for much, so I will persist in trying to build some things from source or find workarounds.
This has been my rationale and process in setting up shop on a new M1 MacBook Air.
I tried using pyenv to manage python venvs in a mostly vanilla way at first, but that didn't work out super well.
I ended up just installing MiniForge (which is like MiniConda) via brew, which works fine for managing venvs. MiniForge's ""emphasis on supporting various CPU architectures"" comes in handy while we bridge the gap between Apple Silicon and open source data science (and other) libraries.
",39
https://towardsdatascience.com/are-the-new-m1-macbooks-any-good-for-data-science-lets-find-out-e61a01e8cad1?source=tag_archive---------8-----------------------,Are The New M1 Macbooks Any Good for Data Science? Let's Find Out,"Comparison with Intel Macbook Pro in Python, Numpy, Pandas, and Scikit-Learn",Dario Radecic,6,"Dario Radecic
The new Intel-free Macbooks have been around for some time now. Naturally, I couldn't resist and decided to buy one. What follows is a comparison between the 2019 Intel-based MBP and the new one in programming and data science tasks.
If I had to describe the new M1 chip in a single word, I would be this one  amazing. Continue reading for a more detailed description.
Data science aside, this thing is revolutionary. It runs several times faster than my 2019 MBP while remaining completely silent. I've run multiple CPU exhaustive tasks, and the fans haven't kicked in even once. And, of course, the battery life. It's incredible  14 hours of medium to heavy use without a problem.
But let's focus on the benchmarks. There are five in total:
If you're reading this article, I'm assuming you're considering if the new Macbooks are worth it for data science. They aren't ""deep learning workstations"" for sure, but they don't cost that much, to begin with.
All comparisons throughout the article are made between two Macbook Pros:
Not all libraries are compatible yet on the new M1 chip. I had no problem configuring Numpy and TensorFlow, but Pandas and Scikit-Learn can't run natively yet  at least I haven't found working versions.
The only working solution was to install these two through Anaconda. It still runs through a Rosseta 2 emulator, so it's a bit slower than native.
The test you'll see aren't ""scientific"" in any way, shape or form. They only compare runtimes in a different set of programming and data science tasks between the mentioned machines.
Let's start with the basic CPU and GPU benchmarks first. Geekbench 5 was used for the tests, and you can see the results below:
The results speak for themselves. M1 chip demolished Intel chip in my 2019 Mac. This benchmark only measures overall machine performance and isn't 100% relevant for data science benchmarks you'll see later.
Still, things look promising.
Here's a list of tasks performed in this benchmark:
The test was made only with built-in Python libraries, so Numpy wasn't allowed. You can see the Numpy benchmark in the next section.
Here's the code snippet for the test:
And here are the results:
As you can see, running Python on M1 Mac through Anaconda (and Rosseta 2 emulator) decreased the runtime by 196 seconds. It's best to run Python natively, as this further reduces the runtime by 43 seconds.
To conclude  Python is approximately three times faster when run natively on a new M1 chip, at least per this benchmark.
Here's a list of tasks performed in this benchmark:
The original benchmark script was taken from Markus Beuckelmann on Github, and modified slightly, so both start and end time is captured. Here's how the script looks like:
Here are the results:
Results obtained with Numpy are a bit strange, to say at least. It looks like Numpy runs faster on my 2019 Intel Mac for some reason. Maybe it's due to some optimizations, but I can't say for sure. If you know why, please don't hesitate to share in the comment section.
Next, let's compare the Pandas performance.
Pandas benchmark is quite similar to the Python one. Identical operations were performed, but the results were combined into a single data frame.
Here's a list of tasks:
Here's the code snippet for the test:
And here are the results:
As you can see, there's no measurement for ""native"" Pandas, as I haven't managed to install it. Still, Pandas on the M1 chip finished this benchmark two times faster.
As with Pandas, I haven't managed to install Scikit-Learn natively. You'll only see comparisons between Intel MBP and M1 MBP running through the Rosseta 2 emulator.
Here's a list of tasks performed in the benchmark:
It's a more or less standard model training procedure, disregarding testing out multiple algorithms, data preparation, and feature engineering.
Here's the code snippet for the test:
And here are the results:
The results convey the same information seen with Pandas  2019 Intel i5 processor takes two times longer to finish the same task.
The comparisons with the Intel-based 2019 Mac might be irrelevant to you. That's great  you have the benchmark scripts so you can run the tests on your machine. Let me know if you do so  I'm eager to find out about your configuration and how it compares.
The new M1 chips are amazing, and the best is yet to come. This is only the first generation, after all. Macbooks aren't machine learning workstations, but you're still getting a good bang for the buck.
Deep learning benchmarks with TensorFlow are coming out next week, so stay tuned.
Connect on LinkedIn.
Originally published at https://www.betterdatascience.com on January 23, 2021.
",40
https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9?source=tag_archive---------4-----------------------,Simple and Multiple Linear Regression in Python,Quick introduction to linear regression in Python,Adi Bronshtein,11,"Quick introduction to linear regression in Python
Hi everyone! After briefly introducing the ""Pandas"" library as well as the NumPy library, I wanted to provide a quick introduction to building models in Python, and what better place to start than one of the very basic models, linear regression? This will be the first post about machine learning and I plan to write about more complex models in the future. Stay tuned! But for right now, let's focus on linear regression.
In this blog post, I want to focus on the concept of linear regression and mainly on the implementation of it in Python. Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables  a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too:
As you can see, a linear relationship can be positive (independent variable goes up, dependent variable goes up) or negative (independent variable goes up, dependent variable goes down). Like I said, I will focus on the implementation of regression models in Python, so I don't want to delve too much into the math under the regression hood, but I will write a little bit about it. If you'd like a blog post about that, please don't hesitate to write me in the responses!
A relationship between variables Y and X is represented by this equation:
In this equation, Y is the dependent variable  or the variable we are trying to predict or estimate; X is the independent variable  the variable we are using to make predictions; m is the slope of the regression line  it represent the effect X has on Y. In other words, if X increases by 1 unit, Y will increase by exactly m units. (""Full disclosure"": this is true only if we know that X and Y have a linear relationship. In almost all linear regression cases, this will not be true!) b is a constant, also known as the Y-intercept. If X equals 0, Y would be equal to b (Caveat: see full disclosure from earlier!). This is not necessarily applicable in real life  we won't always know the exact relationship between X and Y or have an exact linear relationship.
These caveats lead us to a Simple Linear Regression (SLR). In a SLR model, we build a model based on data  the slope and Y-intercept derive from the data; furthermore, we don't need the relationship between X and Y to be exactly linear. SLR models also include the errors in the data (also known as residuals). I won't go too much into it now, maybe in a later post, but residuals are basically the differences between the true value of Y and the predicted/estimated value of Y. It is important to note that in a linear regression, we are trying to predict a continuous variable. In a regression model, we are trying to minimize these errors by finding the ""line of best fit""  the regression line from the errors would be minimal. We are trying to minimize the length of the black lines (or more accurately, the distance of the blue dots) from the red line  as close to zero as possible. It is related to (or equivalent to) minimizing the mean squared error (MSE) or the sum of squares of error (SSE), also called the ""residual sum of squares."" (RSS) but this might be beyond the scope of this blog post :-)
In most cases, we will have more than one independent variable  we'll have multiple variables; it can be as little as two independent variables and up to hundreds (or theoretically even thousands) of variables. in those cases we will use a Multiple Linear Regression model (MLR). The regression equation is pretty much the same as the simple regression equation, just with more variables:
This concludes the math portion of this post :) Ready to get to implementing it in Python?
There are two main ways to perform linear regression in Python  with Statsmodels and scikit-learn. It is also possible to use the Scipy library, but I feel this is not as common as the two other libraries I've mentioned. Let's look into doing linear regression in both of them:
Statsmodels is ""a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration."" (from the documentation)
As in with Pandas and NumPy, the easiest way to get or install Statsmodels is through the Anaconda package. If, for some reason you are interested in installing in another way, check out this link. After installing it, you will need to import it every time you want to use it:
Let's see how to actually use Statsmodels for linear regression. I'll use an example from the data science class I took at General Assembly DC:
First, we import a dataset from sklearn (the other library I've mentioned):
This is a dataset of the Boston house prices (link to the description). Because it is a dataset designated for testing and learning machine learning tools, it comes with a description of the dataset, and we can see it by using the command print data.DESCR (this is only true for sklearn datasets, not every dataset! Would have been cool though...). I'm adding the beginning of the description, for better understanding of the variables:
Running data.feature_names and data.target would print the column names of the independent variables and the dependent variable, respectively. Meaning, Scikit-learn has already set the house value/price data as a target variable and 13 other variables are set as predictors. Let's see how to run a linear regression on this dataset.
First, we should load the data as a pandas data frame for easier analysis and set the median home value as our target variable:
What we've done here is to take the dataset and load it as a pandas data frame; after that, we're setting the predictors (as df)  the independent variables that are pre-set in the dataset. We're also setting the target  the dependent variable, or the variable we're trying to predict/estimate.
Next we'll want to fit a linear regression model. We need to choose variables that we think we'll be good predictors for the dependent variable  that can be done by checking the correlation(s) between variables, by plotting the data and searching visually for relationship, by conducting preliminary research on what variables are good predictors of y etc. For this first example, let's take RM  the average number of rooms and LSTAT  percentage of lower status of the population. It's important to note that Statsmodels does not add a constant by default. Let's see it first without a constant in our regression model:
The output:
Interpreting the Table This is a very long table, isn't it? First we have what's the dependent variable and the model and the method. OLS stands for Ordinary Least Squares and the method ""Least Squares"" means that we're trying to fit a regression line that would minimize the square of distance from the regression line (see the previous section of this post). Date and Time are pretty self-explanatory :) So as number of observations. Df of residuals and models relates to the degrees of freedom  ""the number of values in the final calculation of a statistic that are free to vary.""
The coefficient of 3.6534 means that as the RM variable increases by 1, the predicted value of MDEV increases by 3.6534. A few other important values are the R-squared  the percentage of variance our model explains; the standard error (is the standard deviation of the sampling distribution of a statistic, most commonly of the mean); the t scores and p-values, for hypothesis test  the RM has statistically significant p-value; there is a 95% confidence intervals for the RM (meaning we predict at a 95% percent confidence that the value of RM is between 3.548 to 3.759).
If we do want to add a constant to our model  we have to set it by using the command X = sm.add_constant(X) where X is the name of your data frame containing your input (independent) variables.
The output:
Interpreting the Table  With the constant term the coefficients are different. Without a constant we are forcing our model to go through the origin, but now we have a y-intercept at -34.67. We also changed the slope of the RM predictor from 3.634 to 9.1021.
Now let's try fitting a regression model with more than one variable  we'll be using RM and LSTAT I've mentioned before. Model fitting is the same:
And the output:
Interpreting the Output  We can see here that this model has a much higher R-squared value  0.948, meaning that this model explains 94.8% of the variance in our dependent variable. Whenever we add variables to a regression model, R2 will be higher, but this is a pretty high R2. We can see that both RM and LSTAT are statistically significant in predicting (or estimating) the median house value; not surprisingly , we see that as RM increases by 1, MEDV will increase by 4.9069 and when LSTAT increases by 1, MEDV will decrease by -0.6557. As you may remember, LSTAT is the percentage of lower status of the population, and unfortunately we can expect that it will lower the median value of houses. With this same logic, the more rooms in a house, usually the higher its value will be.
This was the example of both single and multiple linear regression in Statsmodels. We could have used as little or as many variables we wanted in our regression model(s)  up to all the 13! Next, I will demonstrate how to run linear regression models in SKLearn.
SKLearn is pretty much the golden standard when it comes to machine learning in Python. It has many learning algorithms, for regression, classification, clustering and dimensionality reduction. Check out my post on the KNN algorithm for a map of the different algorithms and more links to SKLearn. In order to use linear regression, we need to import it:
Let's use the same dataset we used before, the Boston housing prices. The process would be the same in the beginning  importing the datasets from SKLearn and loading in the Boston dataset:
Next, we'll load the data to Pandas (same as before):
So now, as before, we have the data frame that contains the independent variables (marked as ""df"") and the data frame with the dependent variable (marked as ""target""). Let's fit a regression model using SKLearn. First we'll define our X and y  this time I'll use all the variables in the data frame to predict the housing price:
And then I'll fit a model:
The lm.fit() function fits a linear model. We want to use the model to make predictions (that's what we're here for!), so we'll use lm.predict():
The print function would print the first 5 predictions for y (I didn't print the entire list to ""save room"". Removing [0:5] would print the entire list):
Remember, lm.predict() predicts the y (dependent variable) using the linear model we fitted. You must have noticed that when we run a linear regression with SKLearn, we don't get a pretty table (okay, it's not that pretty... but it's pretty useful) like in Statsmodels. What we can do is use built-in functions to return the score, the coefficients and the estimated intercepts. Let's see how it works:
Would give this output:
This is the R2 score of our model. As you probably remember, this the percentage of explained variance of the predictions. If you're interested, read more here. Next, let's check out the coefficients for the predictors:
will give this output:
and the intercept:
that will give this output:
These are all (estimated/predicted) parts of the multiple regression equation I've mentioned earlier. Check out the documentation to read more about coef_ and intercept_.
So, this is has a been a quick (but rather long!) introduction on how to conduct linear regression in Python. In practice, you would not use the entire dataset, but you will split your data into a training data to train your model on, and a test data  to, you guessed it, test your model/predictions on. If you would like to read about it, please check out my next blog post. In the meanwhile, I hope you enjoyed this post and that I'll ""see"" you on the next one.
Thank you for reading!
",41
https://towardsdatascience.com/i-tripled-my-income-with-data-science-heres-how-2c32ceb782cf?source=tag_archive---------4-----------------------,I tripled my income with data science. Here's how.,"Over a year ago, I lost my job to the Covid-19 pandemic. It turned out to be a blessing in disguise.",Natassha Selvaraj,5,"Just over a year go, I was working part time as a private tutor while studying.
I was earning only slightly more than minimum wage during this time, just enough to cover expenses like food and petrol.
I lost my job during the pandemic, and was told that I could only go back to teaching once the nationwide lockdown was lifted.
After this happened, I suddenly realized I had a lot of free time. I didn't have to attend classes at university any longer. I also no longer had a job.
I took this opportunity to teach myself data science.
In just over a year, I managed to triple my salary by creating multiple streams of income with my data science knowledge.
I managed to land a data science internship which then turned into a full time job offer.
There are two advantages of taking a full time data science job:
Although this isn't where a huge portion of my income comes from, its consistent, which I like.
I learn something new everyday. I work on my communication skills when talking to people in different teams, build and scale machine learning models to large datasets, and come up with different techniques to improve customer journey.
The best part about this job is that unlike freelancing, I don't get to choose the projects I work on.
This means that even if I don't know how to do something, I have no choice but to learn within a day or two or get it done.
The task almost always gets done, and I walk away having learnt something new.
Also, as an introvert, I used to find it difficult to communicate with people and present during team meetings.
I didn't really have a choice once taking on a role as a data scientist, because a lot of my work involves presenting model insights and gathering business requirements from the client.
Due to this, my communication skills have improved dramatically over the past year.
During my free time (usually at night or during weekends), I write for data science.
I started out with writing articles around projects I built, in order to strengthen my portfolio.
I enjoyed creating, and my articles were a way of sharing my journey with others. I didn't know anyone else at that time to study with, and nobody I knew shared the same interest as me in data science.
I wrote so I could document my progress, and connect with a community of like-minded people who shared the same goals as me.
Over time, I realized that my articles added value for people who were on the same journey as I was.
As I wrote more, what had initially started out as a hobby began to generate revenue.
I was able to make passive income by simply writing about my experiences and posting them online.
I am now a top writer in technology and artificial intelligence on Medium, which is more than anything I could've imagined.
Once I started learning data science on my own, I would share links to the courses I was taking on Medium and LinkedIn.
However, it is only recently that I discovered affiliate marketing.
With affiliate marketing, you can share courses that you enjoyed with other people. If someone else clicks on the course link you shared, a small percentage of their course fee will go to you.
I am yet to earn a lot from affiliate marketing, mainly because I am very selective about the courses I promote.
I have taken almost all the courses I promote, and have done extensive research to compile the rest.
So this is a very small income stream, but I'm still adding it to this list.
Last year, I was looking for ways to make money online.
I surveyed sites like Fiverr and Upwork to see if I'd be able to take up any gigs, since I really needed a new source of income.
However, I felt like these platforms were overly competitive, and I didn't really fit into any of the categories on these sites.
After learning data science, I realized that there was a pretty big market for freelance data scientists.
There are many companies out there that don't require an entire data science team, and hire people on a contract basis to build and deploy models for them.
I'm currently working on a one-off machine learning project for a client, and I'm learning a lot along the way.
I also get freelance offers from publishers and technical sites to write data science articles for them.
All my clients have reached out to me after reading my articles or my LinkedIn posts, which is why its a great idea to write data science articles on Medium and build a social media following.
I've also recently started offering consultation sessions for people who are trying to learn data science or break into the industry, which is another source of my income. Again, I get most of my clients from my LinkedIn profile and Medium articles.
Losing my tutoring job due to the Covid-19 pandemic was one of the best things that happened to me.
I realized that I was stuck for a very long time with little idea of what to do in the future, and losing my job gave me some free time to introspect, learn, and decide what I really wanted to do.
Of course, I was lucky enough to have the opportunity to survive without a job for so long and actually learn the skills necessary to become a data scientist.
Upskilling and learning how to code was one of the best decisions I've ever made.
That's all for this article, thanks for reading!
An investment in knowledge pays the best interest  Benjamin Franklin
",42
https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c?source=tag_archive---------8-----------------------,Keyword Extraction process in Python with Natural Language Processing(NLP),"We will discuss spaCy, YAKE, rake-nltk and Gensim for Keyword Extraction Process.",Manmohan Singh,4,"We will discuss spaCy, YAKE, rake-nltk and Gensim for Keyword Extraction Process.
When you wake up in the morning, the first thing you do is open a phone and check messages. Your mind has trained to ignore the WhatsApp messages of those people and groups that you don't like. You decide the importance of the message by only checking the keywords of people and group name.
Your mind will extract the keywords from WhatsApp group name or contact name and train to like it or ignore it. It also depends on many other factors. The same behavior can be visible while reading articles, watching tv or Netflix series, etc.
Machine learning can mimic the same behavior. It is known as keyword extraction in Natural Language Processing (NLP). So, reading articles or news will depend on extracted keywords such as data science, machine learning, artificial intelligence, etc.
The keyword extraction process not only separates the articles but also helps in saving time on social media platforms. You can take the decision to read the post and comments based on their keywords.
You can check whether your article belongs to a current trend or not. Or your article will trend or not. Just search the extracted keywords on google trends. It is one of the factors, not the only factor.
Every article, post, comment has its own important word that makes them useful or useless. The keyword extraction process identifies those words and categorizes the text data.
In this article, we will go through the python libraries that help in the keyword extraction process.
Those libraries are:
Let's start.
SpaCy is all in one python library for NLP tasks. But, we are interested in the keyword extraction functionality of spaCy.
We will start with installing the spaCy library, then download a model en_core_sci_lg. After that, pass the article text into the NLP pipeline. It will return the extracted keywords.
Each model has its own functionality. If an article consists of medical terms, then use the en_core_sci_lg model. Otherwise, you can use the en_core_web_sm model.
Find the related code below.
Observations.
Use the YAKE python library to control the keyword extraction process.
Yet Another Keyword Extractor (Yake) library selects the most important keywords using the text statistical features method from the article. With the help of YAKE, you can control the extracted keyword word count and other features.
Find the related code below.
Observations.
Example -
For deduplication_threshold = 0.1
Output will be [ 'python and cython', 'software', 'ines', 'library is published'].
For deduplication_threshold = 0.9
Output will be [ 'python and cython', 'programming languages python', ' natural language processing', 'advanced natural language', 'languages python', 'language processing', 'ines montani', 'cython', 'advanced natural', 'honnibal and ines', 'software company explosion', 'natural language', 'programming languages', 'matthew honnibal', 'python', 'open-source software library','company explosion', 'spacy', 'processing', 'written'].
4. A numOfKeywords variable will determine the count of keywords extracted. If numOfKeywords = 20, then the total keyword extracted will be less than and equal to 20.
Other keyword extractor methods that you can test on your data.
You can form a powerful keyword extraction method by combining the Rapid Automatic Keyword Extraction (RAKE) algorithm with the NLTK toolkit. It is known as rake-nltk. It is a modified version of this algorithm. You can know more about rake-nltk here.Install the rake-nltk library using pip install rake-nltk.
Find the related code below.
Observations.
keyword_extracted = rake_nltk_var.get_ranked_phrases()[:5]
2. Rake-nltk performance is comparable to spacy.
Gensim is primarily developed for topic modeling. Over time, Gensim added other NLP tasks such as summarization, finding text similarity, etc. Here we will demonstrate the use of Genism for keyword extraction tasks.
Install genism using pip install gensim command.
Find the relevant code below.
Observations:
The keyword extraction process helps us in identifying the important words. It also effective in topic modeling tasks. You can know a lot about your text data by only a few keywords. These keywords will help you to determine whether you want to read an article or not.
In this article, I have explained 4 python libraries (spaCy, YAKE, rake-nltk, Gensim) that fetch the keywords from the article or text data. You can also search for other python libraries for a similar task.
Hopefully, this article will help you with your NLP tasks.
",43
https://medium.datadriveninvestor.com/machine-learning-models-for-market-beating-trading-strategies-c773ba46db66?source=tag_archive---------8-----------------------,Machine learning models for 100% better returns in Algo-trading,How to think about training and utilizing ML models for algorithmic trading.,Abhay Pawar,9,"Predicting stock markets has been an endeavor a lot of people have chased. I spent about 6 months building an end-to-end ML system for algorithmic trading. I've been running the production system to place paper orders for the last 5 months and generated returns of 23% as compared to S&P-500's 10.7%. Returns and risk metrics for paper trading and backtesting shown below.
Any ML-driven algorithmic trading system needs following components:
There's a lot to talk about! In this first post, we will focus on how I thought about training the ML models. Following is the list of topics covered in this post:
It's important to keep in mind that our aim is not to build the most predictive model, but to build one based on which a trading strategy will generate maximum returns. It would be naive to build a model without knowing what we want to do with it. We need to have some broad idea about the trading strategy to decide what the model should do. Let's come up with the simplest trading strategy:
Since we want to book profits when the stock goes up by x%, it would make sense to predict if the stock will indeed go up by x% in n days. We can use these predictions to buy stocks. We still don't know what x, y, and n should be. Maybe we can decide that by thinking a little bit about what data we want to or can use.
I decided to use only daily open, high, low, close price (OHLC) data to start with because it is easy to deal with. It's also much easier to build a system that takes trading decisions daily rather than, say, every hour. There's a lot of other data like company fundamentals, media sentiment, futures data that is easily available. I decided not to use them in first iteration to keep the production system simple.
Big caveat! A stock price time-series is a non-stationary process. The characteristics of price movement don't stay the same with time. And hence, predicting future prices by looking at historical patterns shouldn't be theoretically possible.
But, we aren't trying to predict the future price. The model will recommend certain stocks to buy every day and our aim really is to make sure that these recommendations outperform the market in the next n days (market here is all the stocks which we are considering for trading). That seems much more doable than predicting price.
So, now we know we are only using historic OHLC data for prediction. Long term movement of the stock price is dependent on how the company performs. It's unlikely that historic data can predict the price over a long period (say 1 year). It could be useful for predicting short term movements though (say over 10 days?). Day traders have used various technical indicators for such short term trading (also known as swing trading).
Also, stocks generally do go through ups and downs on a weekly/monthly basis like the Boeing stock shown below. If we can predict these accurately, that itself would be huge win.
Having looked at several price charts, predicting over n=10 days seemed like something possible to me. Also, once we have a backtesting framework, we can try several values of n and pick the one with the highest returns for our trading strategy. So, picking the wrong value is not a huge concern as long as we are in the ballpark correct range.
Thinking about x, we don't want too many or too little stocks going up by more than x% in n=10 days as those are +ve samples for the model. I decided to start out with x = 5% which gave about 13% +ve training samples (% of stocks going up by x=5% in 10 days).
Target: Did the price go up by more than 5% in the next 10 days?
Each day for each stock will be one row in the training data. Target would be if that stock's price went up by 5% in the next 10 days. For creating features, we can use maybe up to 3 months of past OHLC data from that day.
I stuck to the last 5 years of data from the top 100 companies by market cap in S&P500 for generating training data. We can aggregate training data from all the 100 stocks and train a single model. In this cominbed dataset there would be (100 stocks) x (5 years) x (250 trading days per year) = 125k rows.
Training a single model would provide a heavily regularized model as compared to building multiple models (one for each stock). This is a property we want because we want to act on a good signal which is true for any stock to reduce the risk. Having a model for each stock might also lead to overfitting because very little data is available per company (~250 data points per year per stock).
For the first iteration, I decided to stick to only hand-made features. The reason for staying away from RL or DL archs like LSTMs was interpretability. There's money involved and hence, we want a trading system that we understand. A well-understood system is an easy-to-fix system when things go wrong.
For features, we can create several technical indicators. They largely capture trends in the price and a lot of them are easily interpretable. I personally think the features shouldn't depend on the absolute price of the stock, but only the trend.
Following is an incomplete list of features I created using close price:
Several more that I didn't create, but could be useful: features from overall market price, same features from open, close, high, low prices.
We can use all types of ML algos to train a model now. I used XGBoost because it works well, is easy to use, and is easily interpretable. The model performed decently on train/val/test sets
And here are few of the top features:
At this point, we can't really say if this performance is good because we have no idea how much returns this model will generate. We need trading strategies utilizing the model and a backtesting framework to test their returns which we'll explore in later posts. It's still encouraging to see all recommended trades had on average +ve returns. We can come up with various such return related metrics to get some initial insights.
Understanding features is an essential task. We should ideally drop features that don't make sense or seem too noisy. To understand the relationship between each feature and target, we can look at pdp-like plots created from the train/validation data or a trained model.
I used featexp to generate target vs. feature plots from train data. These plots have target mean (% of stocks in train data that went up by 5% in 10 days) on Y-axis wrt the binned feature value (X axis). Bins are created such that they all have same # of samples.
For avg past 30 day return feature, stocks with very high (>8.3%) and very low (<-5%) value are much more likely to be +ve sample (went up by >5%). About 24% stocks in first bin and 17% in last bin.
Similar trend is seen in validation data except for the last bin where target mean is not that high. Train and validation data are from different time periods.
For RSI, seen in the next figure, the trend is exactly as expected. Stocks with low values of RSI are more likely to go up by 5%. A stock with low RSI is called 'underbought', signifying that it should be bought because prices will go higher.
I've included plots for two more features: MACD(30,10) and ratio of avg 2d price with current price. Trends for both signify that stocks that have recently gone down or up a lot are more likely to go up in next 10 days.
RSI is probably the only feature that has a linear relationship with target. Rest all have a non-linear relationship with higher target mean for extreme values.
Finally, it's very important to look at price time-series and see where the model gives high probability of going up. This should help in understanding how the model is acting. Let's look at two companies: TJX and AAPL.
After looking at several such time-series and understanding the features, I can summarize the following about the model:
It's basically picking stocks which have dropped recently or have a consistent upward trend. Of course with a lot more sophistication and fine tuning based on historic trends.
The results definitely look encouraging! But, the path to having a real trading system is still far away .
In the next post, we will talk about how to come up with the trading strategies for the model and backtest them. Lot of things to figure out: how to minimize risk, how to allocate capital and what metrics to even look at. This is an incredibly fun exercise and I'll keep you posted about the next post through twitter!
If you have any thoughts or questions, please feel free to reach out on twitter or linkedin.
twitter.com
Edit: Here's the second post on designing trading strategies using this model.
medium.datadriveninvestor.com
If you liked this, you might like my other post on feature exploration:
towardsdatascience.com
empowerment through data, knowledge, and expertise.
1.2K 
10
1.2K claps
1.2K 
",44
https://codeburst.io/how-to-install-ubuntu-desktop-with-a-graphical-user-interface-in-wsl2-95911ee2997f?source=tag_archive---------2-----------------------,How to Install Ubuntu Desktop With a Graphical User Interface in WSL2,The expanded tutorial with explanations and screenshots,David Littlefield,20,"""Attention: This article has been rewritten to simplify the process and include updates to the process. See updated article here.""
SourceForge is a web-based service that's used to download open-source software and host open-source software repositories for free. It provides a centralized version control system to manage open-source projects. It also provides an easy way for end-users to download and install the software.
Visual C++ X Server (VcXsrv) is a display server that's used in X Windows System to run Linux-based applications on the Windows operating system. It can display the graphical user interface of Linux-based applications and desktop environments that are running on a remote Linux-based computer.
The Allow Access option instructs the Windows Firewall to allow secure and insecure traffic on the specified networks for a specific program. It lets the program send data to and from the computer using various ports. This can be necessary for the program to work but it makes the computer less secure.
PowerShell is a command-line shell and object-oriented scripting language that's used to automate administrative tasks and configure system settings It can be used to automate practically anything in the operating system. It also replaced Command Prompt as the default system shell for Windows 10.
The Execution Policy is a feature in PowerShell that's meant to prevent the accidental execution of malicious scripts. It only allows the user to execute commands by default but it can be changed to load configuration files and run scripts. It can also be changed for the current session, user, or machine.
Windows Subsystem for Linux 2 (WSL2) is a Windows 10 feature that allows users run Linux on Windows without using dual-boot or a virtual machine. It has full access to both filesystems, GPU support, and network application support. It also provides access to thousands of Linux command-line tools.
Ubuntu Desktop is a desktop operating system that's known for being fast, secure, and free. It installs a metapackage that contains all the packages that make up the desktop environment which uses GNOME 3 by default. This provides a graphical user interface for the Ubuntu Linux distribution.
The Command Substitution $() is an operation that executes the command inside its parenthesis and stores the output in a variable to use later. It runs the command in a subshell, replaces that command with the output, and deletes any newlines. It can also be nested in other command substitutions.
The Make Directory (mkdir) command is used to create new directories. It can specify one or more relative or absolute paths with the name of the new directories to be created. It can also be used with the ""Parents"" flag to create parent directories as needed without overwriting a path that already exists.
The Change Directory (cd) command is used to change the current working directory to the specified directory. It can navigate to absolute and relative paths that start from the root and current working directory, respectively. It can also navigate to paths stored in variables and environment variables.
Apt-key is a program that uses key pairs to prevent users from downloading packages that contain malicious data. It stores the private key on the server that stores the packages and distributes the public key to users. It also uses the public key to verify the private key before packages can be downloaded.
The Source List Directory is a directory that contains separate source list files for individual repositories. It stores information that's used to download and update packages from a repository. It also supplements the main source list file that stores information about existing repositories on the computer.
The Update command is used to ensure the list of available packages is up to date. It downloads a package list from the repositories on the system which contains information about new and upgradable packages. It only updates information about the packages and doesn't actually upgrade the packages.
Root User is an account in Linux-based operating systems that's used for system administration. It can access all commands and files, modify the system in any way, and manage access and permissions for other users. It can also make the entire system inoperable in the event of a careless user error such as accidentally damaging or deleting an important system file.
APT Transport HTTPS is a program that's used in Linux-based operating systems to access metadata and packages in repositories that uses HTTPS Protocol. It enables package managers to download packages with end-to-end encryption. It also only gets used by APT based on whether the URLs were stored in the source list and source list directory as HTTPS or HTTP.
The GNU Privacy Guard (GPG) File is a public key that's used in Linux-based operating systems to verify the signature in a message to prove the identity of its sender. It contains a very large string of numbers and letters which are used to encrypt a message that can only be decrypted by the person who possesses the associated private key. It gets generated by the GNU Privacy Guard program as part of the key-pair system where the public key is shared with everyone but the private key is protected and kept secret by the owner.
Change Mode (Chmod) is a command that's used in Linux-based operating systems to change the access permissions of files and directories. It specifies the desired permission settings and the file or directory to be modified. It can also specify the permission settings using octal notation, which uses the numbers 0 through 7, or symbolic notation, which uses a range of letters.
The Source List Directory is a directory that contains separate source list files for individual repositories. It stores information that's used to download and update packages from a repository. It also supplements the main source list file that stores information about existing repositories on the computer.
The Update command is used to ensure the list of available packages is up to date. It downloads a package list from the repositories on the system which contains information about new and upgradable packages. It only updates information about the packages and doesn't actually upgrade the packages.
The Exit command is used in WSL2 to close the current shell and return to the calling program. It maintains the session of WSL2 in the background which allows users to resume working where they left off when they reopen WSL2. It can also be used to log out of the root account and return to WSL2.
Genie is a program that's used in WSL2 to run services that require systemd. It creates a container, runs systemd as process id 1 inside the container, and enters it. This is necessary because WSL2 doesn't support systemd but many programs rely on it to initialize, manage, and track services and daemons.
The Sudoers Directory is a directory that contains individual sudoers files that are loaded with the main sudoers file. It stores information that's used to manage users that are given sudo permissions. It also must use the same syntax as the main sudoers file so it's advised to edit these files with visudo.
The Desktop Environment Script is used to create a Bash script that starts the Ubuntu desktop. It mostly sets the environment variables that are needed for the GNOME 3 desktop environment to work as expected. It also sets the display variable which appends a custom display number to the hostname.
Wget is a program that's used to retrieve files from the internet using HTTP, HTTPS, and FTP protocols. It can perform recursive downloads, convert links for offline viewing of HTML, and support proxies. It can also perform multiple downloads, resume downloads, and download in the background.
The Unzip program is used to extract files from a zip file. It extracts the files to a new directory in the current directory by default but it can extract them to a specific directory by including the ""d"" option. It can also unzip multiple archive files by listing them sequentially or by using the wildcard character.
Ubuntu Design is a company website that provides guidelines to help create professional materials, websites, and programs that use the Ubuntu brand. It includes the official logos, fonts, icons, and color palette. It also includes suggested practices for web, design, product photography, and screenshots.
ImageMagick is a program that's used to display, create, convert, modify, and edit images in over 200 file formats. It can process large batches of images and perform precise modifications to a single image. It can also manipulate images from the command-line or a graphical user interface.
The Convert command is used by ImageMagick to convert images between file formats. It can perform a broad range of operations on images such as resize, blur, flip, crop, join, rotate, scale, and transform. It can also use 237 options to blend image-processing operations and create complex results.
The Exit command is used in WSL2 to close the current window without terminating the process. It reopens the shell that was previously running and maintains the current session of WSL2 in the background. This lets users continue working right where they left off when they reopen WSL2.
The Shut Down command is used to terminate the WSL2 distribution that's currently running in the background. It can be used to restart WSL2 after installing an update which allows the changes to take effect. This is needed because closing the window doesn't actually shut down the distribution.
The VcXsrv Script is used to create a PowerShell script that restarts VcXsrv. It mostly searches for VcXsrv in the running processes and stops the processes that use a particular display number. It also starts a new VcXsrv process that reuses the display number and displays Linux programs in a large window.
The Ubuntu Desktop Script is used to create a Visual Basic script that runs the PowerShell and Bash scripts. It mostly restarts VcXsrv by closing all the running processes and starting a new process. It also launches the Ubuntu Desktop by configuring and starting the GNOME 3 desktop environment.
The Shortcut Icon is used to execute the Visual Basic script that restarts the VcXsrv process and starts the desktop environment. It gets created using a method from the WshShell object in Windows Script Host which specifies the path to the shortcut icon, shortcut icon image, and Visual Basic script.
The Change Directory (cd) command is used to change the current working directory to the specified directory. It can navigate to absolute and relative paths that start from the root and current working directory, respectively. It can also navigate to paths stored in variables and environment variables.
File Explorer is a program that's used to provide a graphical user interface to open files and programs. It can navigate through storage drives and display the contents of the directories and subdirectories. It can also open a specific directory by specifying a relative or absolute path from the command-line.
GNOME Terminal is the default terminal emulator that's used on Ubuntu desktop environment. It can run commands, work with files, interact with other computers, and perform administrative tasks and configurations. It also features multiple tabs, user profiles, and custom startup commands.
The screen lock feature in GNOME 3 doesn't work properly. It locks the user out of the system which requires restarting Ubuntu Desktop to unlock the screen. It also occurs whether the system automatically locks the screen or the user manually does it but it can be avoided by disabling the screen lock.
Snap Store is a program that's used to provide a graphical user interface to find, install, and manage applications on Ubuntu. It can find featured and popular snaps using descriptions, ratings, reviews, and screenshots. It can also find snaps through browsing categories and searching for keywords.
""Hopefully, this article helped you get the , remember to subscribe to get more content ""
This article is part of a mini-series that helps readers set up everything they need to start learning about artificial intelligence, machine learning, deep learning, and or data science. It includes articles that contain instructions with copy and paste code and screenshots to help readers get the outcome as soon as possible. It also includes articles that contain instructions with explanations and screenshots to help readers learn about what's happening.
Centralized Version Control System (CVCS) is a version control system that has a client-server relationship where a repository is located on one server that provides access to many clients. It stores all the files and historical data on the server. It can also control user access but it can't be accessed offline.[Return]
The X Windows System (X11) is the standard toolkit and protocol that's used for building the graphical user interface on Unix-based operating systems. It handles the appearance of the windows, frames, buttons, toolbars, and title bars. It also receives user input from the mouse, keyboard, and touchscreen.[Return]
Windows Firewall is an application that's used to protect computers against attacks from the local network and the internet. It filters traffic based on IP address and port information. It permits all outbound traffic but it restricts inbound traffic to the sources that are explicitly stated in the firewall rules.[Return]
The Shell is an interpreter that presents the command-line interface to users and allows them to interact with the kernel. It lets them control the system using commands entered from a keyboard. It also translates the commands from the programming language into the machine language for the kernel.[Return]
The Interpreter is a program that reads through instructions that are written in human-readable programming languages and executes the instructions from top to bottom. It translates each instruction to a machine language the hardware can understand, executes it, and proceeds to the next instruction.[Return]
The Command-Line Interface (CLI) is a program that accepts text input from the user to run commands on the operating system. It lets them configure the system, install software, and access features that aren't available in the graphical user interface. It also gets referred to as the terminal or console.[Return]
The Kernel is the program at the heart of the operating system that controls everything in the computer. It facilitates the memory management, process management, disk management, and task management. It also facilitates communication between the programs and hardware in machine language.[Return]
The Desktop Environment is a collection of programs on top of an operating system that makes up the graphical user interface. It includes components like the windows manager, panels, menus, system tray, icons, and widgets. It also determines what the system looks like and how to interact with it.[Return]
GNOME 3 is a very popular desktop environment that has a simple, easy to use, and reliable user experience. It provides an unique user interface that's designed to focus on tasks and remove distractions. It also features a clean icon-less desktop, powerful search, tasks overview, and desktop extensions.[Return]
Ubuntu is an open source operating system that's built on top of Debian that includes thousands of supporting programs. It has become one of the most popular Linux distributions that's known for being easy to use, reliable, and free. It can also be used on desktops, servers, and internet of things devices.[Return]
The wslvar command is used in WSL2 to access the Windows environment variables from WSL2 . It specifies the environment variable but it doesn't require the variable name to be enclosed in percentage signs. It can also be combined with command substitution to store the output in a variable.[Return]
The Variable is the container that's used to store different types of values. It can assign or update a value by placing an equals sign between the specified variable name and value without a space around it. It can also reference the stored value by placing a dollar sign in front of the existing variable name.[Return]
The Environment Variable is a variable that's automatically created and maintained by the computer. It helps the system know where to install files, find programs, and check for user and system settings. It can also be used by graphical and command-line programs from anywhere on the computer.[Return]
The Repository is a storage location for binary packages that are located on remote servers. It needs to be present in the source list for the computer to install or update its containing packages. This helps provide a high level of security while making it easy to install programs in Linux distributions.[Return]
The Binary Package is an archive file that contains the files and directories needed to make its containing program work properly. It gets stored in the repository that contains all the programs for a specific Linux distribution. It also requires the Linux package manager to access, extract, and install it.[Return]
GNU Privacy Guard (GnuPG) is a program that's used in Linux-based operating systems to provide secure communication between parties. It provides cryptographic privacy and authentication using a combination of symmetric-key and public-key cryptography. It also follows the OpenPGP protocol which defines all the necessary components involved in sending encrypted message signatures, private keys, and public key certificates.[Return]
The Sudoers File is a text file that the sudo command uses to control which users can run what commands. It can allow users to run a specific command using elevated privileges without a password. It can also break the sudo program due to improper syntax so it's advised to edit this file with visudo.[Return]
The Super User Do (sudo) command is used to provide root level privileges to specific users. It enables them to enter their password to run commands that would otherwise be prohibited. It also only works for the main Ubuntu user by default but it can be used by other users by editing the sudoers file.[Return]
The Visudo command is used to open a modified version of the nano text editor that can edit the sudoers file without accidentally breaking the sudo program. It locks the file so that only one person can make changes at once. It also parses the file before saving to ensure there aren't any syntax errors.[Return]
The Display variable is an environment variable that's used to determine what screen to use when connecting to the display server. It contains the display name which includes a hostname, display number, and screen number. It also needs to be set because the program won't work without it.[Return]
The Screen Number is the optional number that's used to refer to a specific screen from a collection of monitors that share a mouse and keyboard. It should be a number that starts at zero and increments once per screen. It also only really applies to computers that actually have multiple screens.[Return]
The Display Number is the number that refers to the collection of monitors that share a mouse and keyboard. It should be a number that's followed by a period character that starts at zero and increments once per collection. It also must be included in the display name or the program won't execute.[Return]
The Hostname is the internet address that the program uses to connect to the computer that's running the display server. It should be an IP address, domain name, or abbreviated name that's followed by a colon character. It also defaults to the localhost when an internet address isn't provided.[Return]
Visual Basic Script (VBScript) is a scripting language that's based on Visual Basic. It was created for developing web pages but it has become a popular language for writing batch files on Windows. It can also be executed using Windows Scripting Host to interact with the Windows operating system.[Return]
The WshShell object is used in Visual Basic Script to allow users to interact with the Windows operating system. It has access to environment variables, system folders, and the registry. It can also create shortcuts, display pop-up windows, and run programs with command-line arguments and keystrokes.[Return]
Screen Lock is a security feature that prevents other people from accessing the computer. It locks the screen after a period of inactivity which requires entering the password to unlock the screen. It can also be manually locked by the user which also requires entering the password to unlock the screen.[Return]
Snap is a self-contained package that contains all the dependencies that are needed to work perfectly and securely on any Linux distribution. It bundles the program, supporting libraries, and metadata into a compressed package which can be updated and rolled back to a previous version automatically.[Return]
Bursts of code to power through your day.
66 
6
66 claps
66 
",45
https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607?source=tag_archive---------6-----------------------,Understanding Contrastive Learning,Learn how to learn without labels.,Ekin Tiu,8,"Contrastive learning is a machine learning technique used to learn the general features of a dataset without labels by teaching the model which data points are similar or different.
Let's begin with a simplistic example. Imagine that you are a newborn baby that is trying to make sense of the world. At home, let's assume you have two cats and one dog.
Even though no one tells you that they are 'cats' and 'dogs', you may still realize that the two cats look similar compared to the dog.
By merely recognizing the similarities and differences between our furry friends, our brains can learn the high-level features of the objects in our world.
For instance, we may subconsciously recognize that the two cats' have pointy ears, whereas the dog has droopy ears. Or we may contrast (hint-hint) the protruding nose of the dog to the flat face of the cats.
In essence, contrastive learning allows our machine learning model to do the same thing. It looks at which pairs of data points are ""similar"" and ""different"" in order to learn higher-level features about the data, before even having a task such as classification or segmentation.
Why is this so powerful?
It's because we can train the model to learn a lot about our data without any annotations or labels, hence the term, SELF-supervised learning.
In most real-world scenarios, we don't have labels for each image. Take medical imaging, for instance. To create labels, professionals have to spend countless hours looking at images to manually classify, segment, etc.
With contrastive learning, one can significantly improve model performance even when only a fraction of the dataset is labeled.
Now that we understand what contrastive learning is, and why it's useful, let's see how contrastive learning works.
In this article, I focus on SimCLRv2, one of the recent state-of-the-art contrastive learning approaches proposed by the Google Brain Team. For other contrastive learning methods such as Facebook's MoCo, I recommend reviewing the following article.
towardsdatascience.com
Fortunately, SimCLRv2 is very intuitive.
The entire process can be described concisely in three basic steps:
Over time, the model will learn that two images of cats should have similar representations and that the representation of a cat should be different than that of a dog.
This implies that the model is able to distinguish between different types of images without even knowing what the images are!
We can dissect this contrastive learning approach even further by breaking it down into three primary steps: data augmentation, encoding, and loss minimization.
We perform any combination of the following augmentations randomly: crop, resize, color distortion, grayscale. We do this twice per image in our batch, to create a positive pair of two augmented images.
We then use our Big-CNN neural network, which we can think of as simply a function, h = f(x), where 'x' is one of our augmented images, to encode both of our images as vector representations.
The output of the CNN is then inputted to a set of Dense Layers called the projection head, z = g(h) to transform the data into another space. This extra step is empirically shown to improve performance [2].
If you are unfamiliar with latent space and vector representations, I highly recommend reading my article that intuitively explains this concept before continuing.
towardsdatascience.com
By compressing our images into a latent space representation, the model is able to learn the high-level features of the images.
In fact, as we continue to train the model to maximize the vector similarity between similar images, we can imagine that the model is learning clusters of similar data points in the latent space.
For instance, cat representations will be closer together, but further apart from dog representations, since this is what we are training the model to learn.
Now that we have two vectors, z, we need a way to quantify the similarity between them.
Since we are comparing two vectors, a natural choice is cosine similarity, which is based on the angle between the two vectors in space.
It is logical that when vectors are closer (smaller angle between them) together in space, they are more similar. Thus, if we take the cosine(angle between the two vectors) as a metric, we will get a high similarity when the angle is close to 0, and a low similarity otherwise, which is exactly what we want.
We also need a loss function that we can minimize. One option is NT-Xent (Normalized Temperature-Scaled Cross-Entropy Loss).
We first compute the probability that the two augmented images are similar.
Notice that the denominator is the sum of e^similarity(all pairs, including negative pairs). Negative pairs are obtained by creating pairs between our augmented images, and all of the other images in our batch.
Lastly, we wrap this value around in a -log() so that minimizing this loss function corresponds to maximizing the probability that the two augmented images are similar.
For more details about the nuances of SimCLR, I recommend checking out the following article.
amitness.com
It's useful to note that since the original publication of the SimCLR framework, the authors have made the following major improvements to the pipeline [2]:
When we have very few labels, or if it's hard to obtain labels for a specific task (i.e. clinical annotation), we want to be able to use both the labeled data and the unlabeled data to optimize the performance and learning capacity of our model. This is the definition of semi-supervised learning.
A methodology that is gaining traction in literature is the unsupervised pre-train, supervised fine-tune, knowledge distillation paradigm [2].
In this paradigm, the self-supervised contrastive learning approach is a crucial 'pre-processing' step, that allows the Big CNN model (i.e. ResNet-152) to first learn general features from unlabeled data before trying to classify the images using limited labeled data.
The Google Brain team demonstrated that this semi-supervised learning approach is very label-efficient and that larger models can lead to greater improvements, especially for low label fractions.
It's interesting to note that similar self-supervised methods have already been used extensively in the Natural Language Processing realm.
For instance, Word2Vec, an algorithm to convert text to embedded vectors, uses a similar self-supervised approach. In this case, we want the words that are closer to each other in a sentence, to have more similar vector representations.
Thus, we create our 'positive pairs' by creating pairs between words within a window. We use a technique called negative sampling to create our negative pairs.
This post contains an intuitive and detailed explanation of the Word2Vec algorithm.
jalammar.github.io
Just like SimCLRv2, Word2Vec allows 'similar' words to have more similar vector representations in the latent space, and we can use these learned representations for more specific, downstream tasks such as text classification.
I hope this article provided a clear intuition about what contrastive learning is, how contrastive learning works, and when you can apply contrastive learning for your own projects. Self-supervised learning is truly amazing!
[1] Islam et al. Deep Learning-based Early Detection and Grading of Diabetic Retinopathy Using Retinal Fundus Images, 2018
[2] Chen et al. Big Self-Supervised Models are Strong Semi-Supervised Learners, 2020
CS @ Stanford University | Stanford ML Group
",46
https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47?source=tag_archive---------8-----------------------,Understanding Semantic Segmentation with UNET,A Salt Identification Case Study,Harshall Lamba,15,"Table of Contents:
Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. (Wikipedia)
Deep Learning has enabled the field of Computer Vision to advance rapidly in the last few years. In this post I would like to discuss about one specific task in Computer Vision called as Semantic Segmentation. Even though researchers have come up with numerous ways to solve this problem, I will talk about a particular architecture namely UNET, which use a Fully Convolutional Network Model for the task.
We will use UNET to build a first-cut solution to the TGS Salt Identification challenge hosted by Kaggle.
Along with this, my purpose of writing the blog is to also provide some intuitive insights on the commonly used operations and terms in Convolutional Networks for Image understanding. Some of these include Convolution, Max Pooling, Receptive field, Up-sampling, Transposed Convolution, Skip Connections, etc.
I will assume that the reader is already familiar with the basic concepts of Machine Learning and Convolutional Networks. Also you must have some working knowledge of ConvNets with Python and Keras library.
There are various levels of granularity in which the computers can gain an understanding of images. For each of these levels there is a problem defined in the Computer Vision domain. Starting from a coarse grained down to a more fine grained understanding, let's describe these problems below:
The most fundamental building block in Computer Vision is the Image classification problem where given an image, we expect the computer to output a discrete label, which is the main object in the image. In image classification we assume that there is only one (and not multiple) object in the image.
In localization along with the discrete label, we also expect the compute to localize where exactly the object is present in the image. This localization is typically implemented using a bounding box which can be identified by some numerical parameters with respect to the image boundary. Even in this case, the assumption is to have only one object per image.
Object Detection extends localization to the next level where now the image is not constrained to have only one object, but can contain multiple objects. The task is to classify and localize all the objects in the image. Here again the localization is done using the concept of bounding box.
The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. Because we're predicting for every pixel in the image, this task is commonly referred to as dense prediction.
Note that unlike the previous tasks, the expected output in semantic segmentation are not just labels and bounding box parameters. The output itself is a high resolution image (typically of the same size as input image) in which each pixel is classified to a particular class. Thus it is a pixel level image classification.
Instance segmentation is one step ahead of semantic segmentation wherein along with pixel level classification, we expect the computer to classify each instance of a class separately. For example in the image above there are 3 people, technically 3 instances of the class ""Person"". All the 3 are classified separately (in a different color). But semantic segmentation does not differentiate between the instances of a particular class.
If you are still confused between the differences of object detection, semantic segmentation and instance segmentation, below image will help to clarify the point:
In this post we will learn to solve the Semantic Segmentation problem using Fully Convolutional Network (FCN) called UNET.
If you are wondering, whether semantic segmentation is even useful or not, your query is reasonable. However, it turns out that a lot of complex tasks in Vision require this fine grained understanding of images. For example:
Autonomous driving is a complex robotics tasks that requires perception, planning and execution within constantly evolving environments. This task also needs to be performed with utmost precision, since safety is of paramount importance. Semantic Segmentation provides information about free space on the roads, as well as to detect lane markings and traffic signs.
Machines can augment analysis performed by radiologists, greatly reducing the time required to run diagnostic tests.
Semantic Segmentation problems can also be considered classification problems, where each pixel is classified as one from a range of object classes. Thus, there is a use case for land usage mapping for satellite imagery. Land cover information is important for various applications, such as monitoring areas of deforestation and urbanization.
To recognize the type of land cover (e.g., areas of urban, agriculture, water, etc.) for each pixel on a satellite image, land cover classification can be regarded as a multi-class semantic segmentation task. Road and building detection is also an important research topic for traffic management, city planning, and road monitoring.
There are few large-scale publicly available datasets (Eg : SpaceNet), and data labeling is always a bottleneck for segmentation tasks.
Precision farming robots can reduce the amount of herbicides that need to be sprayed out in the fields and semantic segmentation of crops and weeds assist them in real time to trigger weeding actions. Such advanced image vision techniques for agriculture can reduce manual monitoring of agriculture.
We will also consider a practical real world case study to understand the importance of semantic segmentation. The problem statement and the datasets are described in the below sections.
In any Machine Learning task, it is always suggested to spend a decent amount of time in aptly understanding the business problem that we aim to solve. This not only helps to apply the technical tools efficiently but also motivates the developer to use his/her skills in solving a real world problem.
TGS is one of the leading Geo-science and Data companies which uses seismic images and 3D renderings to understand which areas beneath the Earth's surface which contain large amounts of oil and gas.
Interestingly, the surfaces which contain oil and gas, also contain huge deposits of salt. So with the help of seismic technology, they try to predict which areas in the surface of the Earth contain huge amount of salts.
Unfortunately, professional seismic imaging requires expert human vision to exactly identify salt bodies. This leads to highly subjective and variable renderings. Moreover it could cause huge loss for the oil and gas company drillers if the human prediction is incorrect.
Thus TGS hosted a Kaggle Competition, to employ machine vision to solve this task with better efficiency and accuracy.
To read more about the challenge, click here.
To read more about seismic technology, click here.
Download the data files from here.
For simplicity we will only use train.zip file which contains both the images and their corresponding masks.
In the images directory, there are 4000 seismic images which are used by human experts to predict whether there could be salt deposits in that region or not.
In the masks directory, there are 4000 gray scale images which are the actual ground truth values of the corresponding images which denote whether the seismic image contains salt deposits and if so where. These will be used for building a supervised learning model.
Let's visualize the given data to get a better understanding:
The image on left is the seismic image. The black boundary is drawn just for the sake of understanding denoting which part contains salt and which does not. (Of course this boundary is not a part of the original image)
The image on the right is called as the mask which is the ground truth label. This is what our model must predict for the given seismic image. The white region denotes salt deposits and the black region denotes no salt.
Let's look at a few more images:
Notice that if the mask is entirely black, this means there are no salt deposits in the given seismic image.
Clearly from the above few images it can be inferred that its not easy for human experts to make accurate mask predictions for the seismic images.
Before we dive into the UNET model, it is very important to understand the different operations that are typically used in a Convolutional Network. Please make a note of the terminologies used.
There are two inputs to a convolutional operation
i) A 3D volume (input image) of size (nin x nin x channels)
ii) A set of 'k' filters (also called as kernels or feature extractors) each one of size (f x f x channels), where f is typically 3 or 5.
The output of a convolutional operation is also a 3D volume (also called as output image or feature map) of size (nout x nout x k).
The relationship between nin and nout is as follows:
Convolution operation can be visualized as follows:
In the above GIF, we have an input volume of size 7x7x3. Two filters each of size 3x3x3. Padding =0 and Strides = 2. Hence the output volume is 3x3x2. If you are not comfortable with this arithmetic then you need to first revise the concepts of Convolutional Networks before you continue further.
One important term used frequently is called as the Receptive filed. This is nothing but the region in the input volume that a particular feature extractor (filter) is looking at. In the above GIF, the 3x3 blue region in the input volume that the filter covers at any given instance is the receptive field. This is also sometimes called as the context.
To put in very simple terms, receptive field (context) is the area of the input image that the filter covers at any given point of time.
In simple words, the function of pooling is to reduce the size of the feature map so that we have fewer parameters in the network.
For example:
Basically from every 2x2 block of the input feature map, we select the maximum pixel value and thus obtain a pooled feature map. Note that the size of the filter and strides are two important hyper-parameters in the max pooling operation.
The idea is to retain only the important features (max valued pixels) from each region and throw away the information which is not important. By important, I mean that information which best describes the context of the image.
A very important point to note here is that both convolution operation and specially the pooling operation reduce the size of the image. This is called as down sampling. In the above example, the size of the image before pooling is 4x4 and after pooling is 2x2. In fact down sampling basically means converting a high resolution image to a low resolution image.
Thus before pooling, the information which was present in a 4x4 image, after pooling, (almost) the same information is now present in a 2x2 image.
Now when we apply the convolution operation again, the filters in the next layer will be able to see larger context, i.e. as we go deeper into the network, the size of the image reduces however the receptive field increases.
For example, below is the LeNet 5 architecture:
Notice that in a typical convolutional network, the height and width of the image gradually reduces (down sampling, because of pooling) which helps the filters in the deeper layers to focus on a larger receptive field (context). However the number of channels/depth (number of filters used) gradually increase which helps to extract more complex features from the image.
Intuitively we can make the following conclusion of the pooling operation. By down sampling, the model better understands ""WHAT"" is present in the image, but it loses the information of ""WHERE"" it is present.
As stated previously, the output of semantic segmentation is not just a class label or some bounding box parameters. In-fact the output is a complete high resolution image in which all the pixels are classified.
Thus if we use a regular convolutional network with pooling layers and dense layers, we will lose the ""WHERE"" information and only retain the ""WHAT"" information which is not what we want. In case of segmentation we need both ""WHAT"" as well as ""WHERE"" information.
Hence there is a need to up sample the image, i.e. convert a low resolution image to a high resolution image to recover the ""WHERE"" information.
In the literature, there are many techniques to up sample an image. Some of them are bi-linear interpolation, cubic interpolation, nearest neighbor interpolation, unpooling, transposed convolution, etc. However in most state of the art networks, transposed convolution is the preferred choice for up sampling an image.
Transposed convolution (sometimes also called as deconvolution or fractionally strided convolution) is a technique to perform up sampling of an image with learnable parameters.
I will not describe how transpose convolution works because Naoki Shibuya has already done a brilliant job in his blog Up sampling with Transposed Convolution. I strongly recommend you to go through this blog (multiple times if required) to understand the process of Transposed Convolution.
However, on a high level, transposed convolution is exactly the opposite process of a normal convolution i.e., the input volume is a low resolution image and the output volume is a high resolution image.
In the blog it is nicely explained how a normal convolution can be expressed as a matrix multiplication of input image and filter to produce the output image. By just taking the transpose of the filter matrix, we can reverse the convolution process, hence the name transposed convolution.
After reading this section, you must be comfortable with following concepts:
If you are confused with any of the terms or concepts explained in this section, feel free to read it again till you get comfortable.
The UNET was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. The architecture contains two paths. First path is the contraction path (also called as the encoder) which is used to capture the context in the image. The encoder is just a traditional stack of convolutional and max pooling layers. The second path is the symmetric expanding path (also called as the decoder) which is used to enable precise localization using transposed convolutions. Thus it is an end-to-end fully convolutional network (FCN), i.e. it only contains Convolutional layers and does not contain any Dense layer because of which it can accept image of any size.
In the original paper, the UNET is described as follows:
If you did not understand, its okay. I will try to describe this architecture much more intuitively. Note that in the original paper, the size of the input image is 572x572x3, however, we will use input image of size 128x128x3. Hence the size at various locations will differ from that in the original paper but the core components remain the same.
Below is the detailed explanation of the architecture:
Below is the Keras code to define the above model:
Model is compiled with Adam optimizer and we use binary cross entropy loss function since there are only two classes (salt and no salt).
We use Keras callbacks to implement:
We use a batch size of 32.
Note that there could be a lot of scope to tune these hyper parameters and further improve the model performance.
The model is trained on P4000 GPU and takes less than 20 mins to train.
Note that for each pixel we get a value between 0 to 1.0 represents no salt and 1 represents salt.We take 0.5 as the threshold to decide whether to classify a pixel as 0 or 1.
However deciding threshold is tricky and can be treated as another hyper parameter.
Let's look at some results on both training set and validation set:
Results on training set are relatively better than those on validation set which implies the model suffers from overfitting. One obvious reason could be the small number images used to train the model.
Thank you for interest in the blog. Please leave comments, feedback and suggestions if you feel any.
Full code on my GitHub repo here.
",47
https://towardsdatascience.com/transformers-141e32e69591?source=tag_archive---------7-----------------------,How Transformers Work,Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in...,Giuliano Giacaglia,14,"If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:
www.holloway.com
Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar  their program to defeat a top professional Starcraft player.
Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..
For models to perform sequence transduction, it is necessary to have some sort of memory. For example let's say that we are translating the following sentence to another language (French):
""The Transformers"" are a Japanese [[hardcore punk]] band. The band was formed in 1968, during the height of Japanese music history""
In this example, the word ""the band"" in the second sentence refers to the band ""The Transformers"" introduced in the first sentence. When you read about the band in the second sentence, you know that it is referencing to the ""The Transformers"" band. That may be important for translation. There are many examples, where words in some sentences refer to words in previous sentences.
For translating sentences like that, a model needs to figure out these sort of dependencies and connections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been used to deal with this problem because of their properties. Let's go over these two architectures and their drawbacks.
Recurrent Neural Networks have loops in them, allowing information to persist.
In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. A loop allows information to be passed from one step to the next.
The loops can be thought in a different way. A Recurrent Neural Network can be thought of as multiple copies of the same network, A, each network passing a message to a successor. Consider what happens if we unroll the loop:
This chain-like nature shows that recurrent neural networks are clearly related to sequences and lists. In that way, if we want to translate some text, we can set each input as the word in that text. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information.
The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.
Consider a language model that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence ""the clouds in the sky"", we don't need further context. It's pretty obvious that the next word is going to be sky.
In this case where the difference between the relevant information and the place that is needed is small, RNNs can learn to use past information and figure out what is the next word for this sentence.
But there are cases where we need more context. For example, let's say that you are trying to predict the last word of the text: ""I grew up in France... I speak fluent ..."". Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text.
RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.
In theory, RNNs could learn this long-term dependencies. In practice, they don't seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem.
When arranging one's calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.
RNNs don't do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.
LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important.
Internally, a LSTM looks like the following:
Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output. I won't go into detail on the mechanics of each cell. If you want to understand how each cell works, I recommend Christopher's blog post:
colah.github.io
With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating.
The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don't do too well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.
That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it's hard to parallelize the work for processing sentences, since you are have to process word by word. Not only that but there is no model of long and short range dependencies. To summarize, LSTMs and RNNs present 3 problems:
To solve some of these problems, researchers created a technique for paying attention to specific words.
When translating a sentence, I pay special attention to the word I'm presently translating. When I'm transcribing an audio recording, I listen carefully to the segment I'm actively writing down. And if you ask me to describe the room I'm sitting in, I'll glance around at the objects I'm describing as I do so.
Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.
To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens.
The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention.
For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage.
The step in green in charge of creating the hidden states from the input. Instead of passing only one hidden state to the decoders as we did before using attention, we pass all the hidden states generated by every ""word"" of the sentence to the decoding stage. Each hidden state is used in the decoding stage, to figure out where the network should pay attention to.
For example, when translating the sentence ""Je suis etudiant"" to English, requires that the decoding step looks at different words when translating it.
Or for example, when you translate the sentence ""L'accord sur la zone economique europeenne a ete signe en aout 1992."" from French to English, and how much attention it is paid to each input.
But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text.
Convolutional Neural Networks help solve these problems. With them we can
Some of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are Convolutional Neural Networks.
The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. Not only that, but the ""distance"" between the output word and any input for a CNN is in the order of log(N)  that is the size of the height of the tree generated from the output to the input (you can see it on the GIF above. That is much better than the distance of the output of a RNN and an input, which is on the order of N.
The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That's why Transformers were created, they are a combination of both CNNs with attention.
To solve the problem of parallelization, Transformers try to solve the problem by using Convolutional Neural Networks together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another.
Let's take a look at how Transformer works. Transformer is a model that uses attention to boost the speed. More specifically, it uses self-attention.
Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders.
Each encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network.
The encoder's inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.
Note: This section comes from Jay Allamar blog post
Let's start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.
Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.
The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512.
In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that's directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.
Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.
Next, we'll switch up the example to a shorter sentence and we'll look at what happens in each sub-layer of the encoder.
Let's first look at how to calculate self-attention using vectors, then proceed to look at how it's actually implemented  using matrices.
The first step in calculating self-attention is to create three vectors from each of the encoder's input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.
Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don't HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.
Multiplying x1 by the WQ weight matrix produces q1, the ""query"" vector associated with that word. We end up creating a ""query"", a ""key"", and a ""value"" projection of each word in the input sentence.
What are the ""query"", ""key"", and ""value"" vectors?
They're abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you'll know pretty much all you need to know about the role each of these vectors plays.
The second step in calculating self-attention is to calculate a score. Say we're calculating the self-attention for the first word in this example, ""Thinking"". We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.
The score is calculated by taking the dot product of the query vector with the key vector of the respective word we're scoring. So if we're processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.
The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper  64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they're all positive and add up to 1.
This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it's useful to attend to another word that is relevant to the current word.
The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).
The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).
That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let's look at that now that we've seen the intuition of the calculation on the word level.
Transformers basically work like that. There are a few other details that make them work better. For example, instead of only paying attention to each other in one dimension, Transformers use the concept of Multihead attention.
The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating ""kicked"" in the sentence ""I kicked the ball"", you may ask ""Who kicked"". Depending on the answer, the translation of the word to another language can change. Or ask other questions, like ""Did what?"", etc...
Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation.
I gave an overview of how Transformers work and why this is the technique used for sequence transduction. If you want to understand in depth how the model works and all its nuances, I recommend the following posts, articles and videos that I used as a base for summarizing the technique
",48
https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------8-----------------------,Yes you should understand backprop,"When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit...",Andrej Karpathy,7,"When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:
""Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?""
This is seemingly a perfectly sensible appeal - if you're never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of ""it's worth knowing what's under the hood as an intellectual curiosity"", or perhaps ""you might want to improve on the core algorithm later"", but there is a much stronger and practical argument, which I wanted to devote a whole post to:
> The problem with Backpropagation is that it is a leaky abstraction.
In other words, it is easy to fall into the trap of abstracting away the learning process  believing that you can simply stack arbitrary layers together and backprop will ""magically make them work"" on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.
We're starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can ""saturate"" and entirely stop learning  your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):
If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (""vanish""), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.
Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you're using basic SGD, this would make the lower layers of a network train much slower than the higher ones.
TLDR: if you're using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn't cause them to be fully saturated. See a longer explanation in this CS231n lecture video.
Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:
If you stare at this for a while you'll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn't ""fire""), then its weights will get zero gradient. This can lead to what is called the ""dead ReLU"" problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron's weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It's like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.
TLDR: If you understand backpropagation and your network has ReLUs, you're always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.
Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I'll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):
This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you'll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.
What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b...)? This sequence either goes to zero if |b| < 1, or explodes to infinity when |b|>1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.
TLDR: If you understand backpropagation and you're using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.
Lets look at one more  the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector  turns out this trivial operation is not supported in TF). Anyway, I searched ""dqn tensorflow"", clicked the first link, and found the core code. Here is an excerpt:
If you're familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s',a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.
The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.
The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:
It's a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can't meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.
I submitted an issue on the DQN repo and this was promptly fixed.
Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because ""TensorFlow automagically makes my networks learn"", you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.
The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.
That's it for now! I hope you'll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I'm aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)
",49
https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60?source=tag_archive---------1-----------------------,PCA using Python (scikit-learn),My last tutorial went over Logistic Regression using Python. One of the things learned was that you can speed up the fitting of a machine learning algorithm by changing the optimization algorithm. A...,Michael Galarnyk,8,"My last tutorial went over Logistic Regression using Python. One of the things learned was that you can speed up the fitting of a machine learning algorithm by changing the optimization algorithm. A more common way of speeding up a machine learning algorithm is by using Principal Component Analysis (PCA). If your learning algorithm is too slow because the input dimension is too high, then using PCA to speed it up can be a reasonable choice. This is probably the most common application of PCA. Another common application of PCA is for data visualization.
To understand the value of using PCA for data visualization, the first part of this tutorial post goes over a basic visualization of the IRIS dataset after applying PCA. The second part uses PCA to speed up a machine learning algorithm (logistic regression) on the MNIST dataset.
With that, let's get started! If you get lost, I recommend opening the video below in a separate tab.
The code used in this tutorial is available below
PCA for Data Visualization
PCA to Speed-up Machine Learning Algorithms
For a lot of machine learning applications it helps to be able to visualize your data. Visualizing 2 or 3 dimensional data is not that challenging. However, even the Iris dataset used in this part of the tutorial is 4 dimensional. You can use PCA to reduce that 4 dimensional data into 2 or 3 dimensions so that you can plot and hopefully understand the data better.
The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below will load the iris dataset.
PCA is effected by scale so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the dataset's features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data.
The original data has 4 columns (sepal length, sepal width, petal length, and petal width). In this section, the code projects the original data which is 4 dimensional into 2 dimensions. I should note that after dimensionality reduction, there usually isn't a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation.
Concatenating DataFrame along axis = 1. finalDf is the final DataFrame before plotting the data.
This section is just plotting 2 dimensional data. Notice on the graph below that the classes seem well separated from each other.
The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert 4 dimensional space to 2 dimensional space, you lose some of the variance (information) when you do this. By using the attribute explained_variance_ratio_, you can see that the first principal component contains 72.77% of the variance and the second principal component contains 23.03% of the variance. Together, the two components contain 95.80% of the information.
While there are other ways to speed up machine learning algorithms, one less commonly known way is to use PCA. For this section, we aren't using the IRIS dataset as the dataset only has 150 rows and only 4 feature columns. The MNIST database of handwritten digits is more suitable as it has 784 feature columns (784 dimensions), a training set of 60,000 examples, and a test set of 10,000 examples.
You can also add a data_home parameter to fetch_mldata to change where you download the data.
The images that you downloaded are contained in mnist.data and has a shape of (70000, 784) meaning there are 70,000 images with 784 dimensions (784 features).
The labels (the integers 0-9) are contained in mnist.target. The features are 784 dimensional (28 x 28 images) and the labels are simply numbers from 0-9.
Typically the train test split is 80% training and 20% test. In this case, I chose 6/7th of the data to be training and 1/7th of the data to be in the test set.
The text in this paragraph is almost an exact copy of what was written earlier. PCA is effected by scale so you need to scale the features in the data before applying PCA. You can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the dataset's features. Note you fit on the training set and transform on the training and test set. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data.
Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.
Fit PCA on training set. Note: you are fitting PCA on the training set only.
Note: You can find out how many components PCA choose after fitting the model using pca.n_components_ . In this case, 95% of the variance amounts to 330 principal components.
Step 1: Import the model you want to use
In sklearn, all machine learning models are implemented as Python classes
Step 2: Make an instance of the Model.
Step 3: Training the model on the data, storing the information learned from the data
Model is learning the relationship between digits and labels
Step 4: Predict the labels of new data (new images)
Uses the information the model learned during the model training process
The code below predicts for one observation
The code below predicts for multiple observations at once
Measuring Model Performance
While accuracy is not always the best metric for machine learning algorithms (precision, recall, F1 Score, ROC Curve, etc would be better), it is used here for simplicity.
The whole point of this section of the tutorial was to show that you can use PCA to speed up the fitting of machine learning algorithms. The table below shows how long it took to fit logistic regression on my MacBook after using PCA (retaining different amounts of variance each time).
The earlier parts of the tutorial have demonstrated using PCA to compress high dimensional data to lower dimensional data. I wanted to briefly mention that PCA can also take the compressed representation of the data (lower dimensional data) back to an approximation of the original high dimensional data. If you are interested in the code that produces the image below, check out my github.
This is a post that I could have written on for a lot longer as PCA has many different uses. I hope this post helps you with whatever you are working on. My next machine learning tutorial goes over How to Speed up Scikit-Learn Model Training. If you any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter. If you want to learn about other algorithms, please consider taking my Machine Learning with Scikit-Learn LinkedIn Learning course.
",50
https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74?source=tag_archive---------4-----------------------,Hyperparameter Tuning the Random Forest in Python,Improving the Random Forest Part Two,Will Koehrsen,12,"Improving the Random Forest Part Two
So we've built a random forest model to solve our machine learning problem (perhaps by following this end-to-end guide) but we're not too impressed by the results. What are our options? As we saw in the first part of this series, our first step should be to gather more data and perform feature engineering. Gathering more data and feature engineering usually has the greatest payoff in terms of time invested versus improved performance, but when we have exhausted all data sources, it's time to move on to model hyperparameter tuning. This post will focus on optimizing the random forest model in Python using Scikit-Learn tools. Although this article builds on part one, it fully stands on its own, and we will cover many widely-applicable machine learning concepts.
I have included Python code in this article where it is most instructive. Full code and data to follow along can be found on the project Github page.
The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as we might turn the knobs of an AM radio to get a clear signal (or your parents might have!). While model parameters are learned during training  such as the slope and intercept in a linear regression  hyperparameters must be set by the data scientist before training. In the case of a random forest, hyperparameters include the number of decision trees in the forest and the number of features considered by each tree when splitting a node. (The parameters of a random forest are the variables and thresholds used to split each node learned during training). Scikit-Learn implements a set of sensible default hyperparameters for all models, but these are not guaranteed to be optimal for a problem. The best hyperparameters are usually impossible to determine ahead of time, and tuning a model is where machine learning turns from a science into trial-and-error based engineering.
Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model. However, evaluating each model only on the training set can lead to one of the most fundamental problems in machine learning: overfitting.
If we optimize the model for the training data, then our model will score very well on the training set, but will not be able to generalize to new data, such as in a test set. When a model performs highly on the training set but poorly on the test set, this is known as overfitting, or essentially creating a model that knows the training set very well but cannot be applied to new problems. It's like a student who has memorized the simple problems in the textbook but has no idea how to apply concepts in the messy real world.
An overfit model may look impressive on the training set, but will be useless in a real application. Therefore, the standard procedure for hyperparameter optimization accounts for overfitting through cross validation.
The technique of cross validation (CV) is best explained by example using the most common method, K-Fold CV. When we approach a machine learning problem, we make sure to split our data into a training and a testing set. In K-Fold CV, we further split our training set into K number of subsets, called folds. We then iteratively fit the model K times, each time training the data on K-1 of the folds and evaluating on the Kth fold (called the validation data). As an example, consider fitting a model with K = 5. The first iteration we train on the first four folds and evaluate on the fifth. The second time we train on the first, second, third, and fifth fold and evaluate on the fourth. We repeat this procedure 3 more times, each time evaluating on a different fold. At the very end of training, we average the performance on each of the folds to come up with final validation metrics for the model.
For hyperparameter tuning, we perform many iterations of the entire K-Fold CV process, each time using different model settings. We then compare all of the models, select the best one, train it on the full training set, and then evaluate on the testing set. This sounds like an awfully tedious process! Each time we want to assess a different set of hyperparameters, we have to split our training data into K fold and train and evaluate K times. If we have 10 sets of hyperparameters and are using 5-Fold CV, that represents 50 training loops. Fortunately, as with most problems in machine learning, someone has solved our problem and model tuning with K-Fold CV can be automatically implemented in Scikit-Learn.
Usually, we only have a vague idea of the best hyperparameters and thus the best approach to narrow our search is to evaluate a wide range of values for each hyperparameter. Using Scikit-Learn's RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values.
As a brief recap before we get into model tuning, we are dealing with a supervised regression machine learning problem. We are trying to predict the temperature tomorrow in our city (Seattle, WA) using past historical weather data. We have 4.5 years of training data, 1.5 years of test data, and are using 6 different features (variables) to make our predictions. (To see the full code for data preparation, see the notebook).
Let's examine the features quickly.
In previous posts, we checked the data to check for anomalies and we know our data is clean. Therefore, we can skip the data cleaning and jump straight into hyperparameter tuning.
To look at the available hyperparameters, we can create a random forest and examine the default values.
Wow, that is quite an overwhelming list! How do we know where to start? A good place is the documentation on the random forest in Scikit-Learn. This tells us the most important settings are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features). We could go read the research papers on the random forest and try to theorize the best hyperparameters, but a more efficient use of our time is just to try out a wide range of values and see what works! We will try adjusting the following set of hyperparameters:
To use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting:
On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.
Now, we instantiate the random search and fit it like any Scikit-Learn model:
The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.
We can view the best parameters from fitting the random search:
From these results, we should be able to narrow the range of values for each hyperparameter.
To determine if random search yielded a better model, we compare the base model with the best random search model.
We achieved an unspectacular improvement in accuracy of 0.4%. Depending on the application though, this could be a significant benefit. We can further improve our results by using grid search to focus on the most promising hyperparameters ranges found in the random search.
Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:
This will try out 1 * 4 * 2 * 3 * 3 * 4 = 288 combinations of settings. We can fit the model, display the best hyperparameters, and evaluate performance:
It seems we have about maxed out performance, but we can give it one more try with a grid further refined from our previous results. The code is the same as before just with a different grid so I only present the results:
A small decrease in performance indicates we have reached diminishing returns for hyperparameter tuning. We could continue, but the returns would be minimal at best.
We can make some quick comparisons between the different approaches used to improve performance showing the returns on each. The following table shows the final results from all the improvements we made (including those from the first part):
Model is the (very unimaginative) names for the models, accuracy is the percentage accuracy, error is the average absolute error in degrees, n_features is the number of features in the dataset, n_trees is the number of decision trees in the forest, and time is the training and predicting time in seconds.
The models are as follows:
Overall, gathering more data and feature selection reduced the error by 17.69% while hyperparameter further reduced the error by 6.73%.
In terms of programmer-hours, gathering data took about 6 hours while hyperparameter tuning took about 3 hours. As with any pursuit in life, there is a point at which pursuing further optimization is not worth the effort and knowing when to stop can be just as important as being able to keep going (sorry for getting all philosophical). Moreover, in any data problem, there is what is called the Bayes error rate, which is the absolute minimum possible error in a problem. Bayes error, also called reproducible error, is a combination of latent variables, the factors affecting a problem which we cannot measure, and inherent noise in any physical process. Creating a perfect model is therefore not possible. Nonetheless, in this example, we were able to significantly improve our model with hyperparameter tuning and we covered numerous machine learning topics which are broadly applicable.
To further analyze the process of hyperparameter optimization, we can change one setting at a time and see the effect on the model performance (essentially conducting a controlled experiment). For example, we can create a grid with a range of number of trees, perform grid search CV, and then plot the results. Plotting the training and testing error and the training time will allow us to inspect how changing one hyperparameter impacts the model.
First we can look at the effect of changing the number of trees in the forest. (see notebook for training and plotting code)
As the number of trees increases, our error decreases up to a point. There is not much benefit in accuracy to increasing the number of trees beyond 20 (our final model had 100) and the training time rises consistently.
We can also examine curves for the number of features to split a node:
As we increase the number of features retained, the model accuracy increases as expected. The training time also increases although not significantly.
Together with the quantitative stats, these visuals can give us a good idea of the trade-offs we make with different combinations of hyperparameters. Although there is usually no way to know ahead of time what settings will work the best, this example has demonstrated the simple tools in Python that allow us to optimize our machine learning model.
As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu
",51
https://towardsdatascience.com/top-3-reasons-why-i-sold-my-m1-macbook-pro-as-a-data-scientist-abad1226f52a?source=tag_archive---------4-----------------------,Top 3 Reasons Why I Sold My M1 Macbook Pro as a Data Scientist,M1 Mac will get you 90% there. But is 90% enough?,Dario Radecic,4,"Dario Radecic
M1 chip is fantastic. Imagine a blazingly fast processor, all-day battery life, and no thermal issues. Sounds great, at least on paper. Still, it had to go.
In case you want a single sentence summary  some data science libraries are either impossible or near to impossible to run natively, connecting two external displays is a nightmare, and finally, eGPUs aren't supported.
That's precisely how the article is structure, so feel free to navigate to a section that interests you the most:
I'm not a big fan of Anaconda Python distribution. Overall, it's a great idea, but I prefer a clean installation of Python 3 and dependency management on the fly. Still, Anaconda seemed like a go-to way on the M1 chip.
The default Python 3 on M1 was 3.9.x, which you'll first have to downgrade to 3.8.x to make some libraries work. Not a big deal, but an extra step for sure. Even after the downgrade, the only consistent thing I saw when installing libraries natively was a bunch of red lines in the Terminal.
Want to install TensorFlow? Great, but please install a specific version of Numpy and five other packages beforehand. Needless to say, but these versions get overridden when installing other packages if you don't specify some extra parameters (or if you don't install them in a virtual environment). Kind of a hassle if you want TensorFlow always available.
Anaconda worked fine, but there wasn't an official release for the M1 chip at the time of testing. This means the entire distribution runs through an emulator called Rosetta 2, which does a terrific job. Still, it's not native support.
For my daily job, I need to communicate with cloud databases a lot, mainly Oracle. The only sane way to do so with Python is through Oracle Instant Client, which isn't ported to the new chip.
This was just a short list of things that didn't work or didn't work as expected. I'm sure any other tech professional can add issues to the list.
Feel free to skip this section if you're using a single external display.
13"" isn't enough for comfortable 8+ hours work sessions. Sure, work from home means work from bed on some days, but you'll need that extra screen real estate more often than not.
I'm using two Dell U2419H monitors. One of them is in a normal horizontal position, while the other is pivoted vertically, as you can see from the following image:
Say what you want, but writing code on a vertical monitor is not something you can easily let go of. Take a look at the following image and you'll immediately get the gist:
In a nutshell  vertical space is a massive productivity booster for anything involving code. Having a single monitor and pivoting it isn't the best option. Having two relatively cheap and color-accurate monitors is a way to go for me.
Unfortunately, the M1 chip in Macbook Pro and Macbook Air supports only a single monitor. There are some ways around it, like purchasing a docking station with DisplayLink, but the recommended ones weren't available on Amazon the last time I checked.
I'm completely fine with sacrificing a dedicated Nvidia GPU to get an ultraportable and sleek-looking laptop. Still, having an option to connect a GPU via thunderbolt was always an option with Intel-based Macs. Not a cheap one, but it was there.
The M1 chip changed that in a bad way. It doesn't support eGPU at all, and there's nothing you can do about it. This means you can forget occasional gaming sessions. I'm aware no one buys Macs for gaming, but having the option can't hurt.
The M1 chip does come with a neural engine, and it should help a bit for basic deep learning tasks if you're into that. But it's still a laptop, so don't expect some crazy performance. You'll have to switch to Collab or cloud GPUs for that.
This last point shouldn't be a deal-breaker if you're into deep learning since mobile GPUs can only get you so far.
To conclude  M1 Macs will get you 90% there, but they are not the best option if you need something ultra-specific. It's easy to go along with the hype, but once it passes the frustration kicks in. At least that was the case for me.
As many would say  never buy the first generation of Apple products. I agree.
What are your experiences with the M1 chip? I'm eager to hear both pros and cons for any IT profession.
Originally published at https://www.betterdatascience.com on March 26, 2021.
",52
https://medium.com/analytics-vidhya/train-a-custom-yolov4-object-detector-using-google-colab-61a659d4868?source=tag_archive---------3-----------------------,TRAIN A CUSTOM YOLOv4 OBJECT DETECTOR (Using Google Colab),(TUTORIAL FOR BEGINNERS),Techzizou,13,"(NOTE: For this YOLOv4 Tutorial, we will be cloning the Darknet git repository in a folder on our google drive)
NOTE: If you get disconnected or lose your session for some reason, you have to run steps 2, 5, and 6 again to mount the drive, edit makefile and build darknet every single time, otherwise the darknet executable will not work.
Create a folder named yolov4 in your google drive. Next, create another folder named training inside the yolov4 folder. This is where we will save our trained weights (This path is mentioned in the obj.data file which we will upload later)
Run the following command to create a symbolic link so that now the path /content/gdrive/My\ Drive/ is equal to /mydrive
Navigate to /mydrive/yolov4 folder
Clone the Darknet git repository in the yolov4 folder on your drive.
You can also view the folder from Colab since the drive has already been mounted. See pic below.
I have uploaded my custom files for mask detection on my GitHub. I am working with 2 classes i.e. ""with_mask"" and ""without_mask"".
Input image example (Image1.jpg)
You can use any software for labeling like the labelImg tool.
I use an open-source labeling tool called OpenLabeling with a very simple UI.
Click on the link below to know more about the labeling process and other software for it:
NOTE: Garbage In = Garbage Out. Choosing and labeling images is the most important part. Try to find good-quality images. The quality of the data goes a long way towards determining the quality of the result.
The output YOLO format label file looks as shown below.
Put all the input image "".jpg"" files and their corresponding YOLO format labeled "".txt"" files in a folder named obj.
Create its zip file obj.zip and upload it to the yolov4 folder on your drive.
Download the yolov4-custom.cfg file from darknet/cfg directory, make changes to it, and upload it to the yolov4/data folder on your drive.
You can also download the custom config file from AlexeyAB's Github.
Make the following changes in the custom config file:
So if classes=1 then it should be filters=18. If classes=2 then write filters=21.
You can tweak other parameter values too like the learning rate, angle, saturation, exposure, and hue once you've understood how the basics of the training process work. For beginners, the above changes will suffice.
The obj.data file has :
Has objects' names  each in a new line. Make sure the classes are in the same order as in the class_list.txt file used while labeling the images so the index id of every class is the same as mentioned in the labeled YOLO text files.
(To divide all image files into 2 parts. 90% for train and 10% for test)
This process.py script creates the files train.txt & test.txt where the train.txt file has paths to 90% of the images and test.txt has paths to 10% of the images.
You can download the process.py script from my GitHub.
**IMPORTANT: The ""process.py"" script has only the "".jpg"" format written in it, so other formats such as "".png"","".jpeg"", or even "".JPG""(in capitals) won't be recognized. If you are using any other format, make changes in the process.py script accordingly.
Now that we have uploaded all the files, our yolov4 folder on our drive should look like this:
(Also set CUDNN, CUDNN_HALF, and LIBSO to 1)
The current working directory is /mydrive/yolov4/darknet
Clean the data and cfg folders except for the labels folder inside the data folder which is required for writing label names on the detection boxes.
So just remove all other files from the data folder and completely clean the cfg folder as we already have our custom config file in the yolov4 folder on our drive.
This step is optional.
7(a) Unzip the obj.zip dataset and its contents so that they are now in /darknet/data/ folder
7(b) Copy your yolov4-custom.cfg file so that it is now in /darknet/cfg/ folder
7(c) Copy the obj.names and obj.data files so that they are now in /darknet/data/ folder
7(d) Copy the process.py file into the current darknet directory
The current working directory is /mydrive/yolov4/darknet
List the contents of the data folder to check if the train.txt and test.txt files have been created.
The above process.py script creates the two files train.txt and test.txt where train.txt has paths to 90% of the images and test.txt has paths to 10% of the images. The process.py script file I am using has path data/obj written in it since the current working directory is /mydrive/yolov4/darknet. We unzipped the images into the data/obj folder in Step 7(a). The train.txt and test.txt files look like as shown below.
Here we use transfer learning. Instead of training a model from scratch, we use pre-trained YOLOv4 weights which have been trained up to 137 convolutional layers. Run the following command to download the YOLOv4 pre-trained weights file.
For best results, you should stop the training when the average loss is less than 0.05 if possible or at least constantly below 0.3, else train the model until the average loss does not show any significant change for a while.
The map parameter here gives us the Mean Average Precision. The higher the mAP the better it is for object detection.
You can visit the official AlexeyAB Github page which gives a detailed explanation on when to stop training. Click on the link below to jump to that section.
github.com
NOTE: If you get disconnected or lose your session for some reason, you have to run steps 2, 5, and 6 again to mount the drive, edit makefile and build darknet every single time, otherwise the darknet executable will not work.
If you get disconnected or lose your session, you don't have to start training your model from scratch again. You can restart training from where you left off. Use the weights that were saved last. The weights are saved every 100 iterations as yolov4-custom_last.weights in the yolov4/training folder on your drive. (The path we gave as backup in ""obj.data"" file).
So to restart training run Steps 2, 5, 6, and then run the following command:
Press (Ctrl + Shift + i) . Go to console. Paste the following code and press Enter.
You can check the performance of all the trained weights by looking at the chart.png file. However, the chart.png file only shows results if the training does not get interrupted i.e. if you do not get disconnected or lose your session. If you restart training from a saved point, this will not work.
If this does not work, there are other methods to check your performance. One of them is by checking the mAP of the trained weights.
You can check mAP for all the weights saved every 1000 iterations for eg:- yolov4-custom_4000.weights, yolov4-custom_5000.weights, yolov4-custom_6000.weights, and so on. This way you can find out which weights file gives you the best result. The higher the mAP the better it is.
Run the following command to check the mAP for a particular saved weights file where xxxx is the iteration number for it.(eg:- 4000,5000,6000,...)
You can do it either manually or by simply running the code below
Upload an image to your google drive to test.
Run your custom detector on an image with this command. (The thresh flag sets the minimum accuracy required for object detection)
For running detector on images captured by a webcam run the following code. This is the code snippet provided by Google Colab for camera capture except for the last two lines which run the detector on the saved image as we did in the previous command above.
Upload a video to your google drive to test.
Run your custom detector on a video with this command. (The thresh flag sets the minimum accuracy required for object detection)
First, import dependencies, define helper functions, load your custom YOLOv4 files, and then run the detector on a webcam.
Run the code below. (NOTE: Adjust the custom files in line 22 to your files)
NOTE: The dataset I have collected for mask detection contains mostly close-up images. For more long-shot images you can search online. There are many sites where you can download labeled and unlabeled datasets. I have given a few links at the bottom under Dataset Sources. I have also given a few links for mask datasets. Some of them have more than 10,000 images.
Though we can make certain tweaks and changes to our training config file or add more images to the dataset for every type of object class through augmentation, we have to be careful so that it does not cause overfitting, which affects the accuracy of the model.
For beginners, you can start simply by using the config file I have uploaded on my GitHub. I have also uploaded my mask images dataset along with the YOLO format labeled text files, which although might not be the best but will give you a good start on how to train your own custom detector model using YOLO. You can find a labeled dataset of better quality or an unlabeled dataset and label it yourself later.
I have uploaded my custom mask dataset and all the other files needed for training a custom YOLOv4 detector on my GitHub link below.
github.com
www.kaggle.com
YOLOv4 custom training Tutorial
www.buymeacoffee.com/techzizou
www.patreon.com/techzizou
You can download datasets for many objects from the sites mentioned below. These sites also contain images of many classes of objects along with their annotations/labels in multiple formats such as the YOLO_DARKNET text files and the PASCAL_VOC XML files.
I have used these 3 datasets for my labeled dataset:
More Mask Datasets
Analytics Vidhya is a community of Analytics and Data...
132 
4
",53
https://towardsdatascience.com/6-machine-learning-certificates-to-pursue-in-2021-2070e024ae9d?source=tag_archive---------7-----------------------,6 Machine Learning Certificates to Pursue in 2021,Having one of these in your resume can make a lot of difference.,Sara A. Metwalli,6,"Data science is one of the most versatile fields ever around; even its name is not very explanatory of what the field actually involves. Perhaps that's one reason people find this field quite challenging and difficult to get into and even more difficult to show professionalism.
It is well known within the data science community that to be a ""good"" data scientist is all about how strong of a portfolio you build, how diverse your projects are, and how well they show your ability to solve any problem creatively and efficiently.
Although being a data scientist  or have a specialty in any of its branches  doesn't require a university degree, having some certificate that proves your profession in some aspects of the field can transform your portfolio and take your career on step further.
This doesn't only apply to the field in general, but also to all its branches. Perhaps the most famous branch of the field is machine learning. Machine learning is everywhere in our lives now, in our computers, phones, even kitchen appliances.
towardsdatascience.com
But just like data science, the term machine learning is an umbrella for som many algorithms and techniques, so how can you show your future employer that when you say ""I am a machine learning expert,"" you actually mean it?
Some companies and universities designed tests and courses that if you pass, you know how to navigate yourself in the sea of machine learning, and hence, you're ca[able to solve any problem anyone can put in front of you. This article will cover the top 6 machine learning certificates that you can pursue this year and elevate your portfolio and chances to land your dream job.
The first certificate on this list is offered by MIT. The professional certificate program in machine learning and artificial intelligence is a short program offered to people with previous machine learning knowledge and newcomers, giving them the ability to gain the latest knowledge in the field.
This certificate is not cheap  $325 to register for it  because it's not just a test, rather a full set of courses and materials. This short program's core focuses on using machine learning algorithms and techniques in big data and text processing. But, if you want, you can extend the scope of the certificate  for extra fees  to cover more precise usage of machine learning, such as machine learning in the medical field, or computer version, or efficient deep learning, etc.
Each of the courses in this short program has a set number of days that you need to complete them, and the entire program courses need to be completed within 36 months of registering for the program.
towardsdatascience.com
The machine learning course and certificate offered by Stanford University is perhaps the better option for those who want to get into machine learning and earn a certificate at the same time. You can either audit the course for free or pay $79 to obtain a certificate upon completing the course.
This course is one of the most famous and wholesome machine learning courses you could come by; it is taught by professor Andrew Ng one of Coursera's founders and an instructor with more than 10 million happy students. The machine learning course alone was taken/still by almost 4 million students. The course also offers subtitles in 10 languages for students whose English is not their first or preferred language.
During this course  11 weeks  you will learn everything from the absolute beginning, covering maths and statistics to machine learning algorithms' fundamentals and their application in computer vision, medicine, audio manipulation, and database mining.
Another certificate you can earn by taking a course on Coursera is the machine learning professional certificate offered by one of the computing industry legends, IBM. Like the Stanford University course, you can audit this course for free or earn a $39/ month certificate.
This professional certificate program includes 6 courses covering all the knowledge you need to understand both the theoretical aspects of machine learning algorithms and their practical uses. Although you may make more of the course if one has some programming knowledge, you can still take this course even if you don't know much programming.
This course will also teach you how to use Jupyter notebooks and IBM Watson to build and develop your own projects to add to your portfolio once you have completed the course.
towardsdatascience.com
The final course-based certificate in this list is the machine learning certificate offered by Harvard University on edX. This course is a part of a bigger, broader data science certificate offered by Harvard University. You can either audit this course or earn a certificate upon completion for $99.
This course will cover the basics of machine learning, the basic algorithms, and techniques, how and when to use cross-validation, how to build a recommendation system, and some of the commonly used, most popular, and new machine learning algorithms.
This course is designed to be completed within 8 weeks. However, it is also self-paced, which means you can take however amount of time you need to finish this course and earn your certificate.
So far, all the certificates we covered required a specific course or a set of courses to be completed to earn the certificate. The professional machine learning engineer certificate by Google is different. This is just a certificate, meaning you need only to take a test to obtain this certificate.
For $200, you can take this Google test to measure your familiarity and ability to frame machine learning problems, design solutions, process data, and develop machine learning models. Not just that, you also need to show that you can automate efficient machine learning pipelines and optimize your solutions.
Although this certificate is not accompanied by a course that you must take to earn the certificate, Google still offers materials that you can use to prepare for the test as well as webinars given by Google experts to help you pass that test and make the most of the provided materials.
towardsdatascience.com
A more specific certificate is offered by Amazon, in particular the AWS system. The AWS certified machine learning is a certificate designed to measure one's ability to design, develop and deploy machine learning models using the AWS cloud. This certificate can be obtained with a fee of $400.
Similar to the Google machine learning engineer certificate, the AWS certified machine learning specialty doesn't require a specific course completion to obtain. This certificate is aimed more towards people who are very familiar with machine learning algorithms and techniques rather than complete beginners.
The test is available in three languages, English, Korean and Chinese. Amazon also offers some materials and practice tests that you can use to prepare for the test and pass it from the first try.
Showing profession in any field is not easy; it gets even harder when the field you're trying to prove profession in is a diverse broad field with so many branches and techniques. Machine learning is one of the famous tech fields out there that covers more than just programming; it requires math, problem-solving skills, and even communication skills.
So, how can you prove your capabilities to your employer? Although having diverse, strong projects in your portfolio speaks loud of how capable you are, having a certificate from a top university or industrial company can be the aspect of your portfolio that leads you to land the job.
towardsdatascience.com
In this article, I presented you with 6 certificates designed and developed to test one's ability to tackle and find solutions to the most complex machine learning problems. They prove that you know how to use different algorithms to solve different problems and determine the most appropriate algorithm for any given problem.
Going the extra mile to study and get these degrees will defiantly pay off the next time you apply for a job and even during the job interview. After all, good efforts always pay off if they are combined with patience and perseverance.
",54
https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89?source=tag_archive---------0-----------------------,"What to do with ""small"" data?",By Ahmed El Deeb,Ahmed El Deeb,7,"By Ahmed El Deeb
Many technology companies now have teams of smart data-scientists, versed in big-data infrastructure tools and machine learning algorithms, but every now and then, a data set with very few data points turns up and none of these algorithms seem to be working properly anymore. What the hell is happening? What can you do about it?
Most data science, relevance, and machine learning activities in technology companies have been focused around ""Big Data"" and scenarios with huge data sets. Sets where the rows represent documents, users, files, queries, songs, images, etc. Things that are in the thousands, hundreds of thousands, millions or even billions. The infrastructure, tools, and algorithms to deal with these kinds of data sets have been evolving very quickly and improving continuously during the last decade or so. And most data scientists and machine learning practitioners have gained experience is such situations, have grown accustomed to the appropriate algorithms, and gained good intuitions about the usual trade-offs (bias-variance, flexibility-stability, hand-crafted features vs. feature learning, etc.). But small data sets still arise in the wild every now and then, and often, they are trickier to handle, require a different set of algorithms and a different set of skills. Small data sets arise is several situations:
Problems of small-data are numerous, but mainly revolve around high variance:
1- Hire a statistician
I'm not kidding! Statisticians are the original data scientists. The field of statistics was developed when data was much harder to come by, and as such was very aware of small-sample problems. Statistical tests, parametric models, bootstrapping, and other useful mathematical tools are the domain of classical statistics, not modern machine learning. Lacking a good general-purpose statistician, get a marine-biologist, a zoologist, a psychologist, or anyone who was trained in a domain that deals with small sample experiments. The closer to your domain the better. If you don't want to hire a statistician full time on your team, make it a temporary consultation. But hiring a classically trained statistician could be a very good investment.
2- Stick to simple models
More precisely: stick to a limited set of hypotheses. One way to look at predictive modeling is as a search problem. From an initial set of possible models, which is the most appropriate model to fit our data? In a way, each data point we use for fitting down-votes all models that make it unlikely, or up-vote models that agree with it. When you have heaps of data, you can afford to explore huge sets of models/hypotheses effectively and end up with one that is suitable. When you don't have so many data points to begin with, you need to start from a fairly small set of possible hypotheses (e.g. the set of all linear models with 3 non-zero weights, the set of decision trees with depth <= 4, the set of histograms with 10 equally-spaced bins). This means that you rule out complex hypotheses like those that deal with non-linearity or feature interactions. This also means that you can't afford to fit models with too many degrees of freedom (too many weights or parameters). Whenever appropriate, use strong assumptions (e.g. no negative weights, no interaction between features, specific distributions, etc.) to restrict the space of possible hypotheses.
3- Pool data when possible
Are you building a personalized spam filter? Try building it on top of a universal model trained for all users. Are you modeling GDP for a specific country? Try fitting your models on GDP for all countries for which you can get data, maybe using importance sampling to emphasize the country you're interested in. Are you trying to predict the eruptions of a specific volcano? ... you get the idea.
4- Limit Experimentation
Don't over-use your validation set. If you try too many different techniques, and use a hold-out set to compare between them, be aware of the statistical power of the results you are getting, and be aware that the performance you are getting on this set is not a good estimator for out of sample performance.
5- Do clean up your data
With small data sets, noise and outliers are especially troublesome. Cleaning up your data could be crucial here to get sensible models. Alternatively you can restrict your modeling to techniques especially designed to be robust to outliers. (e.g. Quantile Regression)
6- Do perform feature selection
I am not a big fan of explicit feature selection. I typically go for regularization and model averaging (next two points) to avoid over-fitting. But if the data is truly limiting, sometimes explicit feature selection is essential. Wherever possible, use domain expertise to do feature selection or elimination, as brute force approaches (e.g. all subsets or greedy forward selection) are as likely to cause over-fitting as including all features.
7- Do use Regularization
Regularization is an almost-magical solution that constraints model fitting and reduces the effective degrees of freedom without reducing the actual number of parameters in the model. L1 regularization produces models with fewer non-zero parameters, effectively performing implicit feature selection, which could be desirable for explainability of performance in production, while L2 regularization produces models with more conservative (closer to zero) parameters and is effectively similar to having strong zero-centered priors for the parameters (in the Bayesian world). L2 is usually better for prediction accuracy than L1.
8- Do use Model Averaging
Model averaging has similar effects to regularization is that it reduces variance and enhances generalization, but it is a generic technique that can be used with any type of models or even with heterogeneous sets of models. The downside here is that you end up with huge collections of models, which could be slow to evaluate or awkward to deploy to a production system. Two very reasonable forms of model averaging are Bagging and Bayesian model averaging.
9- Try Bayesian Modeling and Model Averaging
Again, not a favorite technique of mine, but Bayesian inference may be well suited for dealing with smaller data sets, especially if you can use domain expertise to construct sensible priors.
10- Prefer Confidence Intervals to Point Estimates
It is usually a good idea to get an estimate of confidence in your prediction in addition to producing the prediction itself. For regression analysis this usually takes the form of predicting a range of values that is calibrated to cover the true value 95% of the time or in the case of classification it could be just a matter of producing class probabilities. This becomes more crucial with small data sets as it becomes more likely that certain regions in your feature space are less represented than others. Model averaging as referred to in the previous two points allows us to do that pretty easily in a generic way for regression, classification and density estimation. It is also useful to do that when evaluating your models. Producing confidence intervals on the metrics you are using to compare model performance is likely to save you from jumping to many wrong conclusions.
This could be a somewhat long list of things to do or try, but they all revolve around three main themes: constrained modeling, smoothing and quantification of uncertainty.
Most figures used in this post were taken from the book ""Pattern Recognition and Machine Learning"" by Christopher Bishop.
Rants about machine learning and its future
1.2K 
7
",55
https://towardsdatascience.com/time-series-forecasting-with-pycaret-regression-module-237b703a0c63?source=tag_archive---------7-----------------------,Time Series Forecasting with PyCaret Regression Module,A step-by-step tutorial for time-series forecasting using PyCaret,Moez Ali,7,"PyCaret is an open-source, low-code machine learning library and end-to-end model management tool built-in Python for automating machine learning workflows. It is incredibly popular for its ease of use, simplicity, and ability to build and deploy end-to-end ML prototypes quickly and efficiently.
PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few lines only. This makes the experiment cycle exponentially fast and efficient.
PyCaret is simple and easy to use. All the operations performed in PyCaret are sequentially stored in a Pipeline that is fully automated for deployment. Whether it's imputing missing values, one-hot-encoding, transforming categorical data, feature engineering, or even hyperparameter tuning, PyCaret automates all of it. To learn more about PyCaret, watch this 1-minute video.
This tutorial assumes that you have some prior knowledge and experience with PyCaret. If you haven't used it before, no problem  you can get a quick headstart through these tutorials:
Installing PyCaret is very easy and takes only a few minutes. We strongly recommend using a virtual environment to avoid potential conflicts with other libraries.
PyCaret's default installation is a slim version of pycaret which only installs hard dependencies that are listed here.
When you install the full version of pycaret, all the optional dependencies as listed here are also installed.
PyCaret Regression Module is a supervised machine learning module used for estimating the relationships between a dependent variable (often called the 'outcome variable', or 'target') and one or more independent variables (often called 'features', or 'predictors').
The objective of regression is to predict continuous values such as sales amount, quantity, temperature, number of customers, etc. All modules in PyCaret provide many pre-processing features to prepare the data for modeling through the setup function. It has over 25 ready-to-use algorithms and several plots to analyze the performance of trained models.
Time series forecasting can broadly be categorized into the following categories:
This tutorial is focused on the second category i.e. Machine Learning.
PyCaret's Regression module default settings are not ideal for time series data because it involves few data preparatory steps that are not valid for ordered data (data with a sequence such as time series data).
For example, the split of the dataset into train and test set is done randomly with shuffling. This wouldn't make sense for time series data as you don't want the recent dates to be included in the training set whereas historical dates are part of the test set.
Time-series data also requires a different kind of cross-validation since it needs to respect the order of dates. PyCaret regression module by default uses k-fold random cross-validation when evaluating models. The default cross-validation setting is not suitable for time-series data.
The following section in this tutorial will demonstrate how you can change default settings in PyCaret Regression Module easily to make it work for time series data.
For the purpose of this tutorial, I have used the US airline passengers dataset. You can download the dataset from Kaggle.
Since algorithms cannot directly deal with dates, let's extract some simple features from dates such as month and year, and drop the original date column.
I have manually split the dataset before initializing the setup . An alternate would be to pass the entire dataset to PyCaret and let it handle the split, in which case you will have to pass data_split_shuffle = False in the setup function to avoid shuffling the dataset before the split.
Now it's time to initialize the setup function, where we will explicitly pass the training data, test data, and cross-validation strategy using the fold_strategy parameter.
The best model based on cross-validated MAE is Least Angle Regression (MAE: 22.3). Let's check the score on the test set.
MAE on the test set is 12% higher than the cross-validated MAE. Not so good, but we will work with it. Let's plot the actual and predicted lines to visualize the fit.
The grey backdrop towards the end is the test period (i.e. 1960). Now let's finalize the model i.e. train the best model i.e. Least Angle Regression on the entire dataset (this time, including the test set).
Now that we have trained our model on the entire dataset (1949 to 1960), let's predict five years out in the future through 1964. To use our final model to generate future predictions, we first need to create a dataset consisting of the Month, Year, Series column on the future dates.
Now, let's use the future_df to score and generate predictions.
Let's plot it.
Wasn't that easy?
There is no limit to what you can achieve using this lightweight workflow automation library in Python. If you find this useful, please do not forget to give us  on our GitHub repository.
To hear more about PyCaret follow us on LinkedIn and Youtube.
Join us on our slack channel. Invite link here.
Build your own AutoML in Power BI using PyCaret 2.0Deploy Machine Learning Pipeline on Azure using DockerDeploy Machine Learning Pipeline on Google Kubernetes EngineDeploy Machine Learning Pipeline on AWS FargateBuild and deploy your first machine learning web appDeploy PyCaret and Streamlit app using AWS Fargate serverlessBuild and deploy machine learning web app using PyCaret and StreamlitDeploy Machine Learning App built using Streamlit and PyCaret on GKE
DocumentationBlogGitHubStackOverflowInstall PyCaretNotebook TutorialsContribute in PyCaret
Click on the links below to see the documentation and working examples.
ClassificationRegressionClusteringAnomaly DetectionNatural Language ProcessingAssociation Rule Mining
",56
https://towardsdatascience.com/15-habits-i-stole-from-highly-effective-data-scientists-441b1d46c572?source=tag_archive---------5-----------------------,15 Habits I Learned from Highly Effective Data Scientists,I'm using these habits in 2021 to become a more effective future data scientist.,Madison Hunter,12,"When it comes to breaking into the field of data science, you need to use every trick in the book to give yourself that one advantage that pushes you over the finish line.
So why not try to emulate the habits of the best in the business?
This article isn't a ""get rich quick"" method to becoming an efficient data scientist. Instead, it shows the habits that have helped the best data scientists get to where they are.
It's often said that a data scientist's worth is determined by the impact they can have on an organization. That impact begins with becoming an efficient and effective data scientist through the development of good habits.
How many current data science technologies arose only in the last ten or so years? Pretty much most of them.
By entering into the realm of data science with the motivation that you're going to take a good crack at it, you've relegated yourself to a lifetime of constant learning. Don't worry, it's not as bleak as it sounds.
However, what should be kept in the back of your mind at all times is that to remain relevant in the workforce, you need to stay up to date with technology. So, if you've been doing data analysis with MATLAB your whole career, try learning to code in Python. If you've been creating your visualizations with Matplotlib, try using Plotly for something fresh.
How to implement this habit: Take an hour every week (or as much time as you can spare), and experiment with new technologies. Figure out which technologies are relevant by reading blog posts, and pick a couple you would like to add to your stack. Then, create some personal projects to learn how to use the new technologies to the best of their abilities.
I always seem to be blessed with getting to read and deal with code that has terrible documentation and no supporting comments to help me understand what the heck is going on.
Part of me used to chalk it up to the whistful meanderings of programmers, until one day, I realized that it's just the sign of a bad programmer.
All good programmers I've dealt with are those who provide clear, concise documentation to support their work and litter their programs with helpful comments to describe what certain lines of code are doing. This is especially pertinent for data scientists who are using complex algorithms and machine learning models to solve problems.
How to implement this habit: Take some time to either read good code documentation or articles on how to write good code documentation. To practice, write documentation for old personal projects, or take some time to revamp the documentation of your current projects. Since a good portion of the data science world runs on Python, check out this really well-written article on how to document Python code:
realpython.com
The stereotype that developers are pasty-skinned social outcasts who lock themselves into solitude to write code destined for world domination is an outdated generalization that doesn't reflect the modern complexities of the tech industry as a whole.
""Nobody is an island.""  the favorite quote of many data scientists
The intricacies of data science have made it such that a large support network of professionals both within and outside the data science community is necessary to solve the variety of problems that made data scientists necessary.
However, the importance of community doesn't just stop at the professional level. With the data science field expanding, it's necessary to help pave the way for future analysts and engineers so they too can make an impact and further support other data scientists.
With the ""sexiness"" of the data science field diminishing, the only way to make necessary changes will be to start a community-wide movement that inspires the industry to change for the better.
How to implement this habit: Become a mentor, write informative blog posts, join data science forums and help answer questions, start a Youtube channel to share your experiences, enter Kaggle competitions and hackathons, or create courses to help future data scientists learn the skills they need to break into the industry.
Refactoring is the process of cleaning up your code without changing its original function. While refactoring is a process born from necessity in software development situations, refactoring can be a useful habit for data scientists.
My mantra when refactoring is ""less is more"".
I find that when I initially write code to solve data science problems, I usually throw good coding practices out of the door in favor of writing code that works when I need it to. In other words, a lot of spaghetti code happens. Then, after I get my solution to work, I'll go back and clean up my code.
How to implement this habit: Take a look at old code and ask if the same code could be written more efficiently. If so, take some time to educate yourself on best coding practices and look for ways where you can shorten, optimize, and clarify your code. Check out this great article that outlines best practices for code refactoring:
www.altexsoft.com
There are so many productivity-enhancing extensions for IDEs out there that, surprisingly, some people haven't chosen to optimize their workflows yet.
This habit is so unique to everyone that it really comes to down determining which tools, workspaces, and workflows make you the most effective and efficient data scientist you could be.
How to implement this habit: Once a year (or more often if that works better for you), take stock of your overall effectiveness and efficiency, and determine where you could improve. Perhaps this means working on your machine learning algorithms first thing in the morning, or sitting on an exercise ball instead of a chair, or adding a new extension to your IDE that will lint your code for you. Experiment with different workspaces, tools, and workflows until you enter your optimal form.
From what I've seen, data science is 75% understanding business problems and 25% writing models to figure out how to solve them.
Coding, algorithms, and mathematics are the easy part. Understanding how to implement them so they can solve a specific business problem, not so much. By taking more time to understand the business problem and the objectives you're trying to solve, the rest of the process will be much smoother.
To understand the problems facing the industry you're working in, you need to do a little investigation to gather some context with which to support your knowledge of the problems you're trying to solve. For instance, you need to understand what makes the customers of a particular business tick, or the specific goals an engineering firm is trying to reach.
How to implement this habit: Take some time to research the specific company you're working for and the industry that they're in. Write a cheat sheet that you can refer to, containing the major goals of the company, and the issues it may face within its specific industry. Don't forget to include algorithms that you may want to use to solve business problems or ideas for machine learning models that could be useful in the future. Add to this cheat sheet whenever you discover something useful and soon you'll have a treasure trove of industry-related tidbits.
No, not in life. In your code and your workflow.
It's often argued that the best data scientists use the least amount of code, the least amount of data, and the simplest algorithms to get the job done.
Though by minimalist I don't immediately want you to assume scarcity. Often when someone discusses the importance of minimalism in code that leads people to try to develop outrageous solutions that use only a few lines of code. Stop that. Yes, it's impressive, but is that really the best use of your time?
Instead, once you get comfortable with data science concepts, begin to look for ways that you can optimize your code to make it simple, clean, and short. Use simple algorithms to get the job done, and don't forget to write re-usable functions to remove redundancies.
How to implement this habit: As you progress as a data scientist, begin to push yourself to write more efficient solutions, write less code, and use simpler algorithms and models to get the job done. Learn how to shorten your code without reducing its effectiveness, and leave plenty of comments to explain how contracted versions of code works.
I'll be the first to admit that I severely neglect functions when I'm writing data analysis code for the first time. Spaghetti code fills my IDE as I struggle to reason my way through different analyses. If you looked at my code you would probably deem it too far gone and volunteer to take it out behind the barn to put it out of its misery.
Once I've managed to cobble together a half-decent result, I'll then go back to try to fix the equivalent of a bad accident. By packaging my code into functions, I quickly remove unnecessary complexities and redundancies. If that's the only thing I do to my code, then I will already have simplified it to a point that I can revisit the solution and understand how I got to that point.
How to implement this habit: Don't forget the importance of functions when writing code. It's often said that the best developers are lazy developers because they figure out how to create solutions that don't require much work. After you've written a solution, go back and bundle redundant or complex code into functions to help organize and simplify your code.
Test-driven development (TDD) is a software development principle that focuses on writing code with incremental improvements that are constantly tested. TDD runs on a ""Red, Green, Refactor"" system that encourages developers to build a test suite, write implementation code, and then optimize the codebase.
TDD can be implemented successfully by data scientists to produce analytics pipelines, develop a proof of concept, work with data subsets, and ensure that functioning code isn't broken during the development process.
How to implement this habit: Study up on test-driven development, and determine whether or not this technique can add something to your workflow. TDD isn't the perfect answer for every problem, but it can be useful if implemented thoughtfully. Check out this article that gives a great description of TDD and offers an example of how to implement it into data science projects:
towardsdatascience.com
Ever make a pull request and have your computer blow up with error messages and issues coming out of the wazoo? I have. It sucks.
During those moments when you feel like introducing whoever made such a large commit to your fist, take a breath, and remember that this person obviously didn't take the time to implement good habits growing up.
What's the golden rule of team-based software development? Make small, frequent commits.
How to implement this habit: Get into the practice of frequently committing your code changes and just as regularly making pull requests to get the latest code. Every change you or another person makes could break the whole project, so it's important to make small changes that are easy to revert and likely only affect one part or layer of the project.
Depending on who you ask, the industry either has too many data scientists or too few.
Regardless of whether the industry is becoming saturated or arid, you will be competing with tons of highly qualified, and often over-qualified, candidates for a single job. This means that in the lead-up to applying for jobs, you need to have already developed the habit of self-improvement. Today, everyone is obsessed with upskilling, and for good reason. This trend should be no exception to data scientists.
How to implement this habit: Make a skill inventory and see how you stack up to the requirements employers include in job postings. Are you a Pythonista who can efficiently use relevant libraries such as Keras, NumPy, Pandas, PyTorch, TensorFlow, Matplotlib, Seaborn, and Plotly? Can you write a memo detailing your latest findings and how they can improve the efficiency of your company by 25%? Are you comfortable with working as part of a team to complete a project? Identify any shortcomings and find some good online courses or resources to bolster your skills.
In 7 Habits of Highly Effective People, Stephen Covey discusses the principle of ""beginning with the end in mind"".
To effectively relate this to data science projects, you need to ask yourself in the planning phase of a project what the desired outcome of the project is. This will help shape the path of the project and will give you a roadmap of outcomes that need to be met to reach the final goal. Not only that but determining the outcome of the project will give you an idea of the feasibility and sustainability of the project as a whole.
How to implement this habit: Begin each project with a planning session that lays out exactly what you hope to achieve at the end of the development period. Determine which problem you will be attempting to solve, or which piece of evidence you are trying to gather. Then, you can begin to answer feasibility and sustainability questions that will shape the milestones and outcomes of your project. From there, you can start writing code and machine learning models with a clear plan in place to guide you to the end of your project.
After attempting unsuccessfully to prepare a freshman lecture on why spin-V2 particles obey Fermi-Dirac statistics, Richard Feynman famously said ""I couldn't reduce it to the freshman level. That means we really don't understand it."" Known as ""The Great Explainer"", Feynman left a legacy that data scientists can only hope to emulate.
Data science, the art of using data to tell a compelling story, is only successful if the storyteller understands the story they are trying to tell. In other words, it's your task to understand so that you can be understood. Developing this habit early on of understanding what you're trying to accomplish, such that you can share it with someone else to a fair level of comprehension, will make you the most effective data scientist in the room.
How to implement this habit: Use The Feynman Technique to develop a deep level of understanding of the concepts you're trying to discover and the problems you're trying to solve. This method aligns itself well with the data science process of analyzing data and then explaining the results to generally non-data science stakeholders. In short, you refine your explanation of the topic to such a point that you can explain it in simple, non-jargon terms that can be understood by anyone.
In a field dominated by Masters and Ph.D. holders, research papers are often used to share industry news and insight.
Research papers are useful ways to see how others are solving problems, widen our perspectives, and keep up to date with the latest trends.
How to implement this habit: Pick one or two research papers to read every week that are relevant to your current work or to technologies that you're interested in pursuing or studying. Try to set aside time for this literature review every week to make this a priority. Become familiar with the Three-Pass Approach to reading research papers, which helps you gather pertinent information quickly. To really solidify your understanding of the papers, try to implement something that you learned from your reading into a personal project, or share what you learned with work colleagues.
The world of data science is changing rapidly, from the technologies used to the goals being attained. Don't be that data scientist who is stuck in their ways, unwilling to change.
Not only does being open to change force you to continue improving as a professional, but it also keeps you relevant in a quickly changing industry that will spit you out the moment you fall behind.
How to implement this habit: Whenever a new technology or practice makes the news, take a test-drive and see what that new technology or practice brings to the table. Even if you just read the documentation, you can keep yourself up-to-date on the changing trends of the industry. Furthermore, you can bring a perspective on the technology to your company and help them navigate technological changes and advances. Being that person in the office with your ear to the ground can help you stay ahead of the curve, and can also help you guide your team and company to better, more efficient solutions.
Developing good habits, at any stage in your data science career, allows you to fulfill your potential of becoming an effective member of the team who makes a large impact on whatever problem they're trying to solve.
There's no better time than right now to take the time to set yourself up for future success.
",57
https://towardsdatascience.com/over-100-data-scientist-interview-questions-and-answers-c5a66186769a?source=tag_archive---------1-----------------------,OVER 100 Data Scientist Interview Questions and Answers!,"Interview Questions from Amazon, Google, Facebook, Microsoft, and more!",Terence Shin,45,"I know this is long...
Really long. But don't be intimidated by the length  I have broken this down into four sections (machine learning, stats, SQL, miscellaneous) so that you can go through this bit by bit.
Think of this as a workbook or a crash course filled with hundreds of data science interview questions that you can use to hone your knowledge and to identify gaps that you can then fill afterwards.
I hope you find this helpful and wish you the best of luck in your data science endeavors!
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
There are many steps that can be taken when data wrangling and data cleaning. Some of the most common steps are listed below:
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
There are a number of ways to handle unbalanced binary classification (assuming that you want to identify the minority class):
While boxplots and histograms are visualizations used to show the distribution of the data, they communicate information differently.
Histograms are bar charts that show the frequency of a numerical variable's values and are used to approximate the probability distribution of the given variable. It allows you to quickly understand the shape of the distribution, the variation, and potential outliers.
Boxplots communicate different aspects of the distribution of data. While you can't see the shape of the distribution through a box plot, you can gather other information like the quartiles, the range, and outliers. Boxplots are especially useful when you want to compare multiple charts at the same time because they take up less space than histograms.
Both L1 and L2 regularization are methods used to reduce the overfitting of training data. Least Squares minimizes the sum of the squared residuals, which can result in low bias but high variance.
L2 Regularization, also called ridge regression, minimizes the sum of the squared residuals plus lambda times the slope squared. This additional term is called the Ridge Regression Penalty. This increases the bias of the model, making the fit worse on the training data, but also decreases the variance.
If you take the ridge regression penalty and replace it with the absolute value of the slope, then you get Lasso regression or L1 regularization.
L2 is less robust but has a stable solution and always one solution. L1 is more robust but has an unstable solution and can possibly have multiple solutions.
StatQuest has an amazing video on Lasso and Ridge regression here.
A neural network is a multi-layered model inspired by the human brain. Like the neurons in our brain, the circles above represent a node. The blue circles represent the input layer, the black circles represent the hidden layers, and the green circles represent the output layer. Each node in the hidden layers represents a function that the inputs go through, ultimately leading to an output in the green circles. The formal term for these functions is called the sigmoid activation function.
If you want a step by step example of creating a neural network, check out Victor Zhou's article here.
If you're a visual/audio learner, 3Blue1Brown has an amazing series on neural networks and deep learning on YouTube here.
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
Cross-validation is essentially a technique used to assess how well a model performs on a new independent dataset. The simplest example of cross-validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and the testing data to test the model.
There isn't a one-size-fits-all metric. The metric(s) chosen to evaluate a machine learning model depends on various factors:
There are a number of metrics that can be used, including adjusted r-squared, MAE, MSE, accuracy, recall, precision, f1 score, and the list goes on.
Recall attempts to answer ""What proportion of actual positives was identified correctly?""
Precision attempts to answer ""What proportion of positive identifications was actually correct?""
A false positive is an incorrect identification of the presence of a condition when it's absent.
A false negative is an incorrect identification of the absence of a condition when it's actually present.
An example of when false negatives are more important than false positives is when screening for cancer. It's much worse to say that someone doesn't have cancer when they do, instead of saying that someone does and later realizing that they don't.
This is a subjective argument, but false positives can be worse than false negatives from a psychological point of view. For example, a false positive for winning the lottery could be a worse outcome than a false negative because people normally don't expect to win the lottery anyways.
Supervised learning involves learning a function that maps an input to an output based on example input-output pairs [1].
For example, if I had a dataset with two variables, age (input) and height (output), I could implement a supervised learning model to predict the height of a person based on their age.
Unlike supervised learning, unsupervised learning is used to draw inferences and find patterns from input data without references to labeled outcomes. A common use of unsupervised learning is grouping customers by purchasing behavior to find target markets.
Check out my article 'All Machine Learning Models Explained in Six Minutes' if you'd like to learn more about this!
There are two main ways that you can do this:
A) Adjusted R-squared.
R Squared is a measurement that tells you to what extent the proportion of variance in the dependent variable is explained by the variance in the independent variables. In simpler terms, while the coefficients estimate trends, R-squared represents the scatter around the line of best fit.
However, every additional independent variable added to a model always increases the R-squared value  therefore, a model with several independent variables may seem to be a better fit even if it isn't. This is where adjusted R2 comes in. The adjusted R2 compensates for each additional independent variable and only increases if each given variable improves the model above what is possible by probability. This is important since we are creating a multiple regression model.
B) Cross-Validation
A method common to most people is cross-validation, splitting the data into two sets: training and testing data. See the answer to the first question for more on this.
NLP stands for Natural Language Processing. It is a branch of artificial intelligence that gives machines the ability to read and understand human languages.
There are a couple of reasons why a random forest is a better choice of model than a support vector machine:
Dimensionality reduction is the process of reducing the number of features in a dataset. This is important mainly in the case when you want to reduce variance in your model (overfitting).
Wikipedia states four advantages of dimensionality reduction (see here):
In its simplest sense, PCA involves project higher dimensional data (eg. 3 dimensions) to a smaller space (eg. 2 dimensions). This results in a lower dimension of data, (2 dimensions instead of 3 dimensions) while keeping all original variables in the model.
PCA is commonly used for compression purposes, to reduce required memory and to speed up the algorithm, as well as for visualization purposes, making it easier to summarize data.
One major drawback of Naive Bayes is that it holds a strong assumption in that the features are assumed to be uncorrelated with one another, which typically is never the case.
One way to improve such an algorithm that uses Naive Bayes is by decorrelating the features so that the assumption holds true.
There are a couple of drawbacks of a linear model:
Another way of asking this question is ""Is a random forest a better model than a decision tree?"" And the answer is yes because a random forest is an ensemble method that takes many weak decision trees to make a strong learner. Random forests are more accurate, more robust, and less prone to overfitting.
Mean Squared Error (MSE) gives a relatively high weight to large errors  therefore, MSE tends to put too much emphasis on large deviations. A more robust alternative is MAE (mean absolute deviation).
The assumptions are as follows:
Extreme violations of these assumptions will make the results redundant. Small violations of these assumptions will result in a greater bias or variance of the estimate.
Multicollinearity exists when an independent variable is highly correlated with another independent variable in a multiple regression equation. This can be problematic because it undermines the statistical significance of an independent variable.
You could use the Variance Inflation Factors (VIF) to determine if there is any multicollinearity between independent variables  a standard benchmark is that if the VIF is greater than 5 then multicollinearity exists.
There are a couple of metrics that you can use:
R-squared/Adjusted R-squared: Relative measure of fit. This was explained in a previous answer
F1 Score: Evaluates the null hypothesis that all regression coefficients are equal to zero vs the alternative hypothesis that at least one doesn't equal zero
RMSE: Absolute measure of fit.
Decision trees are a popular model, used in operations research, strategic planning, and machine learning. Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy.
Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. By relying on a ""majority wins"" model, it reduces the risk of error from an individual tree.
For example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all 4 decision trees, the predicted value would be 1. This is the power of random forests.
Random forests offer several other benefits including strong performance, can model non-linear boundaries, no cross-validation needed, and gives feature importance.
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
A kernel is a way of computing the dot product of two vectors xx and yy in some (possibly very high dimensional) feature space, which is why kernel functions are sometimes called ""generalized dot product"" [2]
The kernel trick is a method of using a linear classifier to solve a non-linear problem by transforming linearly inseparable data to linearly separable ones in a higher dimension.
When the number of features is greater than the number of observations, then performing dimensionality reduction will generally improve the SVM.
Overfitting is an error where the model 'fits' the data too well, resulting in a model with high variance and low bias. As a consequence, an overfit model will inaccurately predict new data points even though it has a high accuracy on the training data.
Boosting is an ensemble method to improve a model by reducing its bias and variance, ultimately converting weak learners to strong learners. The general idea is to train a weak learner and sequentially iterate and improve the model by learning from the previous learner. You can learn more about it here.
Be sure to subscribe to never miss another article on data science guides, tricks and tips, life lessons, and more!
We need to make some assumptions about this question before we can answer it. Let's assume that there are two possible places to purchase a particular item on Amazon and the probability of finding it at location A is 0.6 and B is 0.8. The probability of finding the item on Amazon can be explained as so:
We can reword the above as P(A) = 0.6 and P(B) = 0.8. Furthermore, let's assume that these are independent events, meaning that the probability of one event is not impacted by the other. We can then use the formula...
P(A or B) = P(A) + P(B)  P(A and B)P(A or B) = 0.6 + 0.8  (0.6*0.8)P(A or B) = 0.92
Check out the Amazon data scientist interview guide here.
This can be answered using the Bayes Theorem. The extended equation for the Bayes Theorem is the following:
Assume that the probability of picking the unfair coin is denoted as P(A) and the probability of flipping 10 heads in a row is denoted as P(B). Then P(B|A) is equal to 1, P(BA) is equal to 0.510, and P(A) is equal to 0.99.
If you fill in the equation, then P(A|B) = 0.9118 or 91.18%.
A convex function is one where a line drawn between any two points on the graph lies on or above the graph. It has one minimum.
A non-convex function is one where a line drawn between any two points on the graph may intersect other points on the graph. It characterized as ""wavy"".
When a cost function is non-convex, it means that there's a likelihood that the function may find local minima instead of the global minimum, which is typically undesired in machine learning models from an optimization perspective.
For this, I'm going to look at the eight rules of probability laid out here and the four different counting methods (see more here).
Eight rules of probability
Counting Methods
Factorial Formula: n! = n x (n -1) x (n  2) x ... x 2 x 1Use when the number of items is equal to the number of places available.Eg. Find the total number of ways 5 people can sit in 5 empty seats.= 5 x 4 x 3 x 2 x 1 = 120
Fundamental Counting Principle (multiplication)This method should be used when repetitions are allowed and the number of ways to fill an open place is not affected by previous fills.Eg. There are 3 types of breakfasts, 4 types of lunches, and 5 types of desserts. The total number of combinations is = 5 x 4 x 3 = 60
Permutations: P(n,r)= n! / (nr)!This method is used when replacements are not allowed and order of item ranking matters.Eg. A code has 4 digits in a particular order and the digits range from 0 to 9. How many permutations are there if one digit can only be used once?P(n,r) = 10!/(10-4)! = (10x9x8x7x6x5x4x3x2x1)/(6x5x4x3x2x1) = 5040
Combinations Formula: C(n,r)=(n!)/[(nr)!r!]This is used when replacements are not allowed and the order in which items are ranked does not mater.Eg. To win the lottery, you must select the 5 correct numbers in any order from 1 to 52. What is the number of possible combinations?C(n,r) = 52! / (52-5)!5! = 2,598,960
Brilliant provides a great definition of Markov chains (here):
""A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed.""
The actual math behind Markov chains requires knowledge on linear algebra and matrices, so I'll leave some links below in case you want to explore this topic further on your own.
See more here or here.
The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color. Let's walk through each step.
Let's say the first card you draw from each deck is a red Ace.
This means that in the deck with 12 reds and 12 blacks, there's now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to 11/(11+12) or 11/23.
In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to 23/(23+24) or 23/47.
Since 23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards.
Edit: Thank you guys for commenting and pointing out that it should be -$35!
This isn't a trick question. The answer is simply to perform a hypothesis test:
Learn more about hypothesis testing here.
Since a coin flip is a binary outcome, you can make an unfair coin fair by flipping it twice. If you flip it twice, there are two outcomes that you can bet on: heads followed by tails or tails followed by heads.
P(heads) * P(tails) = P(tails) * P(heads)
This makes sense since each coin toss is an independent event. This means that if you get heads  heads or tails  tails, you would need to reflip the coin.
You can tell that this question is related to Bayesian theory because of the last statement which essentially follows the structure, ""What is the probability A is true given B is true?"" Therefore we need to know the probability of it raining in London on a given day. Let's assume it's 25%.
P(A) = probability of it raining = 25%P(B) = probability of all 3 friends say that it's rainingP(A|B) probability that it's raining given they're telling that it is rainingP(B|A) probability that all 3 friends say that it's raining given it's raining = (2/3)3 = 8/27
Step 1: Solve for P(B)P(A|B) = P(B|A) * P(A) / P(B), can be rewritten asP(B) = P(B|A) * P(A) + P(B|not A) * P(not A)P(B) = (2/3)3 * 0.25 + (1/3)3 * 0.75 = 0.25*8/27 + 0.75*1/27
Step 2: Solve for P(A|B)P(A|B) = 0.25 * (8/27) / ( 0.25*8/27 + 0.75*1/27)P(A|B) = 8 / (8 + 3) = 8/11
Therefore, if all three friends say that it's raining, then there's an 8/11 chance that it's actually raining.
Since these events are not independent, we can use the rule:P(A and B) = P(A) * P(B|A) ,which is also equal toP(not A and not B) = P(not A) * P(not B | not A)
For example:
P(not 4 and not yellow) = P(not 4) * P(not yellow | not 4)P(not 4 and not yellow) = (36/39) * (27/36)P(not 4 and not yellow) = 0.692
Therefore, the probability that the cards picked are not the same number and the same color is 69.2%.
You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis. Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true. Last, you would set the level of the significance (alpha) and if the p-value is less than the alpha, you would reject the null  in other words, the result is statistically significant.
A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.
3 practical examples include the power law, the Pareto principle (more commonly known as the 80-20 rule), and product sales (i.e. best selling products vs others).
It's important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.
Statistics How To provides the best definition of CLT, which is:
""The central limit theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution."" [1]
The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.
'Statistical power' refers to the power of a binary hypothesis, which is the probability that the test rejects the null hypothesis given that the alternative hypothesis is true. [2]
Selection bias is the phenomenon of selecting individuals, groups or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population.
Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group.
Types of selection bias include:
Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you adding bias in the sense that you're assuming that the data is not as spread out as it might actually be.
Observational data comes from observational studies which are when you observe certain variables and try to determine if there is any correlation.
Experimental data comes from experimental studies which are when you control certain variables and hold them constant to determine if there is any causality.
An example of experimental design is the following: split a group up into two. The control group lives their lives normally. The test group is told to drink a glass of wine every night for 30 days. Then research can be conducted to see how wine affects sleep.
Mean imputation is the practice of replacing null values in a data set with the mean of the data.
Mean imputation is generally bad practice because it doesn't take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score that he actually should.
Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance.
An outlier is a data point that differs significantly from other observations.
Depending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If the outlier is caused by a measurement error, it's important to remove them from the dataset. There are a couple of ways to identify outliers:
Z-score/standard deviations: if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if it's equal to +/- 3, then it's an outlier.Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score.
Interquartile Range (IQR): IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q1-1.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.
Other methods include DBScan clustering, Isolation Forests, and Robust Random Cut Forests.
An inlier is a data observation that lies within the rest of the dataset and is unusual or an error. Since it lies in the dataset, it is typically harder to identify than an outlier and requires external data to identify them. Should you identify any inliers, you can simply remove them from the dataset to address them.
There are several ways to handle missing data:
The best method is to delete rows with missing data as it ensures that no bias or variance is added or removed, and ultimately results in a robust and accurate model. However, this is only recommended if there's a lot of data to start with and the percentage of missing values is low.
First I would conduct EDA  Exploratory Data Analysis to clean, explore, and understand my data. See my article on EDA here. As part of my EDA, I could compose a histogram of the duration of calls to see the underlying distribution.
My guess is that the duration of calls would follow a lognormal distribution (see below). The reason that I believe it's positively skewed is because the lower end is limited to 0 since a call can't be negative seconds. However, on the upper end, it's likely for there to be a small proportion of calls that are extremely long relatively.
You could use a QQ plot to confirm whether the duration of calls follows a lognormal distribution or not. See here to learn more about QQ plots.
Administrative datasets are typically datasets used by governments or other organizations for non-statistical reasons.
Administrative datasets are usually larger and more cost-efficient than experimental studies. They are also regularly updated assuming that the organization associated with the administrative dataset is active and functioning. At the same time, administrative datasets may not capture all of the data that one may want and may not be in the desired format either. It is also prone to quality issues and missing entries.
There are a number of potential reasons for a spike in photo uploads:
The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause.
Be sure to subscribe to never miss another article on data science guides, tricks and tips, life lessons, and more!
Root cause analysis: a method of problem-solving used for identifying the root cause(s) of a problem [5]
Correlation measures the relationship between two variables, range from -1 to 1. Causation is when a first event appears to have caused a second event. Causation essentially looks at direct relationships while correlation can look at both direct and indirect relationships.
Example: a higher crime rate is associated with higher sales in ice cream in Canada, aka they are positively correlated. However, this doesn't mean that one causes another. Instead, it's because both occur more when it's warmer outside.
You can test for causation using hypothesis testing or A/B testing.
When there are a number of outliers that positively or negatively skew the data.
There are 4 combinations of rolling a 4 (1+3, 3+1, 2+2):P(rolling a 4) = 3/36 = 1/12
There are combinations of rolling an 8 (2+6, 6+2, 3+5, 5+3, 4+4):P(rolling an 8) = 5/36
The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.
Eg. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times.
You can use the margin of error (ME) formula to determine the desired sample size.
Potential biases include the following:
There are many things that you can do to control and minimize bias. Two common things include randomization, where participants are assigned by chance, and random sampling, sampling in which each member has an equal probability of being chosen.
A confounding variable, or a confounder, is a variable that influences both the dependent variable and the independent variable, causing a spurious association, a mathematical relationship in which two or more variables are associated but not causally related.
A/B testing is a form of hypothesis testing and two-sample hypothesis testing to compare two versions, the control and variant, of a single variable. It is commonly used to improve and optimize user experience and marketing.
Check out my article, A Simple Guide to A/B Testing for Data Science.
You can use hypothesis testing to prove that males are taller on average than females.
The null hypothesis would state that males and females are the same height on average, while the alternative hypothesis would state that the average height of males is greater than the average height of females.
Then you would collect a random sample of heights of males and females and use a t-test to determine if you reject the null or not.
Since we looking at the number of events (# of infections) occurring within a given timeframe, this is a Poisson distribution question.
Null (H0): 1 infection per person-daysAlternative (H1): >1 infection per person-days
k (actual) = 10 infectionslambda (theoretical) = (1/100)*1787p = 0.032372 or 3.2372% calculated using .poisson() in excel or ppois in R
Since p-value < alpha (assuming 5% level of significance), we reject the null and conclude that the hospital is below the standard.
Use the General Binomial Probability formula to answer this question:
p = 0.8n = 5k = 3,4,5
P(3 or more heads) = P(3 heads) + P(4 heads) + P(5 heads) = 0.94 or 94%
Using Excel...p =1-norm.dist(1200, 1020, 50, true)p= 0.000159
x = 3mean = 2.5*4 = 10
using Excel...
p = poisson.dist(3,10,true)p = 0.010336
Precision = Positive Predictive Value = PVPV = (0.001*0.997)/[(0.001*0.997)+((1-0.001)*(1-0.985))]PV = 0.0624 or 6.24%
See more about this equation here.
p-hat = 60/100 = 0.6z* = 1.96n = 100This gives us a confidence interval of [50.4,69.6]. Therefore, given a confidence interval of 95%, if you are okay with the worst scenario of tying then you can relax. Otherwise, you cannot relax until you got 61 out of 100 to claim yes.
Therefore the confidence interval = 100 +/- 19.6 = [964.8, 1435.2]
Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy.
Using the General Addition Rule in probability:P(mother or father) = P(mother) + P(father)  P(mother and father)P(mother) = P(mother or father) + P(mother and father)  P(father)P(mother) = 0.17 + 0.06-0.12P(mother) = 0.11
Since 70 is one standard deviation below the mean, take the area of the Gaussian distribution to the left of one standard deviation.
= 2.3 + 13.6 = 15.9%
Given a confidence level of 95% and degrees of freedom equal to 8, the t-score = 2.306
Confidence interval = 1100 +/- 2.306*(30/3)Confidence interval = [1076.94, 1123.06]
Upper bound = mean + t-score*(standard deviation/sqrt(sample size))0 = -2 + 2.306*(s/3)2 = 2.306 * s / 3s = 2.601903Therefore the standard deviation would have to be at least approximately 2.60 for the upper bound of the 95% T confidence interval to touch 0.
See here for full tutorial on finding the Confidence Interval for Two Independent Samples.
Confidence Interval = mean +/- t-score * standard error (see above)
mean = new mean  old mean = 3-5 = -2
t-score = 2.101 given df=18 (20-2) and confidence interval of 95%
standard error = sqrt((0.62*9+0.682*9)/(10+10-2)) * sqrt(1/10+1/10)standard error = 0.352
confidence interval = [-2.75, -1.25]
Assuming we subtract in this order (New System  Old System):
confidence interval formula for two independent samples
mean = new mean  old mean = 4-6 = -2
z-score = 1.96 confidence interval of 95%
st. error = sqrt((0.52*99+22*99)/(100+100-2)) * sqrt(1/100+1/100)standard error = 0.205061lower bound = -2-1.96*0.205061 = -2.40192upper bound = -2+1.96*0.205061 = -1.59808
confidence interval = [-2.40192, -1.59808]
Write a SQL query to get the second highest salary from the Employee table. For example, given the Employee table below, the query should return 200 as the second highest salary. If there is no second highest salary, then the query should return null.
This query says to choose the MAX salary that isn't equal to the MAX salary, which is equivalent to saying to choose the second-highest salary!
Here are three SQL concepts to review before your next interview!
Write a SQL query to find all duplicate emails in a table named Person.
First, a subquery is created to show the count of the frequency of each email. Then the subquery is filtered WHERE the count is greater than 1.
Given a Weather table, write a SQL query to find all dates' Ids with higher temperature compared to its previous (yesterday's) dates.
In plain English, the query is saying, Select the Ids where the temperature on a given day is greater than the temperature yesterday.
The Employee table holds all employees. Every employee has an Id, a salary, and there is also a column for the department Id.
The Department table holds all departments of the company.
Write a SQL query to find employees who have the highest salary in each of the departments. For the above tables, your SQL query should return the following rows (order of rows does not matter).
Mary is a teacher in a middle school and she has a table seat storing students' names and their corresponding seat ids. The column id is a continuous increment. Mary wants to change seats for the adjacent students.
Can you write a SQL query to output the result for Mary?
For the sample input, the output is:
Note:If the number of students is odd, there is no need to change the last one's seat.
Two weighings would be required (see part A and B above):
I'm not 100% sure about the answer to this question but will give my best shot!
Let's take the instance where there's an increase in the prime membership fee  there are two parties involved, the buyers and the sellers.
For the buyers, the impact of an increase in a prime membership fee ultimately depends on the price elasticity of demand for the buyers. If the price elasticity is high, then a given increase in price will result in a large drop in demand and vice versa. Buyers that continue to purchase a membership fee are likely Amazon's most loyal and active customers  they are also likely to place a higher emphasis on products with prime.
Sellers will take a hit, as there is now a higher cost of purchasing Amazon's basket of products. That being said, some products will take a harder hit while others may not be impacted. It is likely that premium products that Amazon's most loyal customers purchase would not be affected as much, like electronics.
There are a number of possible variables that can cause such a discrepancy that I would check to see:
Check out more Facebook data science interview questions here
Generally, you would want to probe the interviewer for more information but let's assume that this is the only information that he/she is willing to give.
Focusing on likes per user, there are two reasons why this would have gone up. The first reason is that the engagement of users has generally increased on average over time  this makes sense because as time passes, active users are more likely to be loyal users as using the platform becomes a habitual practice. The other reason why likes per user would increase is that the denominator, the total number of users, is decreasing. Assuming that users that stop using the platform are inactive users, aka users with little engagement and fewer likes than average, this would increase the average number of likes per user.
The explanation above can also be applied to minutes spent on the platform. Active users are becoming more engaged over time, while users with little usage are becoming inactive. Overall the increase in engagement outweighs the users with little engagement.
To take it a step further, it's possible that the 'users with little engagement' are bots that Facebook has been able to detect. But over time, Facebook has been able to develop algorithms to spot and remove bots. If were a significant number of bots before, this can potentially be the root cause of this phenomenon.
The total number of likes in a given year is a function of the total number of users and the average number of likes per user (which I'll refer to as engagement).
Some potential reasons for an increase in the total number of users are the following: users acquired due to international expansion and younger age groups signing up for Facebook as they get older.
Some potential reasons for an increase in engagement are an increase in usage of the app from users that are becoming more and more loyal, new features and functionality, and an improved user experience.
The metrics that determine a product's success are dependent on the business model and what the business is trying to achieve through the product. The book Lean analytics lays out a great framework that one can use to determine what metrics to use in a given scenario:
You can perform an A/B test by splitting the users into two groups: a control group with the normal number of ads and a test group with double the number of ads. Then you would choose the metric to define what a ""good idea"" is. For example, we can say that the null hypothesis is that doubling the number of ads will reduce the time spent on Facebook and the alternative hypothesis is that doubling the number of ads won't have any impact on the time spent on Facebook. However, you can choose a different metric like the number of active users or the churn rate. Then you would conduct the test and determine the statistical significance of the test to reject or not reject the null.
Lift: lift is a measure of the performance of a targeting model measured against a random choice targeting model; in other words, lift tells you how much better your model is at predicting things than if you had no model.
KPI: stands for Key Performance Indicator, which is a measurable metric used to determine how well a company is achieving its business objectives. Eg. error rate.
Robustness: generally robustness refers to a system's ability to handle variability and remain effective.
Model fitting: refers to how well a model fits a set of observations.
Design of experiments: also known as DOE, it is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. [4] In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables).
80/20 rule: also known as the Pareto principle; states that 80% of the effects come from 20% of the causes. Eg. 80% of sales come from 20% of customers.
Quality assurance: an activity or set of activities focused on maintaining a desired level of quality by minimizing mistakes and defects.
Six sigma: a specific type of quality assurance methodology composed of a set of techniques and tools for process improvement. A six sigma process is one in which 99.99966% of all outcomes are free of defects.
[1] Central Limit Theorem, Definition and Examples in Easy Steps, Statistics How To
[2] Power, Statistics, Wikipedia
[3] Anthropic principle, Wikipedia
[4] Design of experiments, Wikipedia
[5] Root cause analysis, Wikipedia
If you enjoyed this be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
",58
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6?source=tag_archive---------8-----------------------,Activation Functions in Neural Networks,"Sigmoid, tanh, Softmax, ReLU, Leaky ReLU EXPLAINED !!!",SAGAR SHARMA,5,"It's just a thing function that you use to get the output of node. It is also known as Transfer Function.
It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).
The Activation Functions can be basically divided into 2 types-
FYI: The Cheat sheet is given below.
As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range.
Equation : f(x) = x
Range : (-infinity to infinity)
It doesn't help with the complexity or various parameters of usual data that is fed to the neural networks.
The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this
It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.
The main terminologies needed to understand for nonlinear functions are:
Derivative or Differential: Change in y-axis w.r.t. change in x-axis.It is also known as slope.
Monotonic function: A function which is either entirely non-increasing or non-decreasing.
The Nonlinear Activation Functions are mainly divided on the basis of their range or curves-
The Sigmoid Function curve looks like a S-shape.
The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.
The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.
The function is monotonic but function's derivative is not.
The logistic sigmoid function can cause a neural network to get stuck at the training time.
The softmax function is a more generalized logistic activation function which is used for multiclass classification.
tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).
The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.
The function is differentiable.
The function is monotonic while its derivative is not monotonic.
The tanh function is mainly used classification between two classes.
Both tanh and logistic sigmoid activation functions are used in feed-forward nets.
The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.
As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.
Range: [ 0 to infinity)
The function and its derivative both are monotonic.
But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.
It is an attempt to solve the dying ReLU problem
Can you see the Leak? 
The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.
When a is not 0.01 then it is called Randomized ReLU.
Therefore the range of the Leaky ReLU is (-infinity to infinity).
Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.
When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.
Happy to be helpful. Support me.
So, follow me on Medium, LinkedIn to see similar posts.
Any comments or if you have any questions, write them in the comment.
Clap it! Share it! Follow Me!
theffork.com
hackernoon.com
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
medium.com
",59
https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c?source=tag_archive---------6-----------------------,A One-Stop Shop for Principal Component Analysis,"At the beginning of the textbook I used for my graduate stat theory class, the authors (George Casella and Roger Berger) explained in the...",Matt Brems,15,"At the beginning of the textbook I used for my graduate stat theory class, the authors (George Casella and Roger Berger) explained in the preface why they chose to write a textbook:
""When someone discovers that you are writing a textbook, one or both of two questions will be asked. The first is ""Why are you writing a book?"" and the second is ""How is your book different from what's out there?"" The first question is fairly easy to answer. You are writing a book because you are not entirely satisfied with the available texts.""
I apply the authors' logic here. Principal component analysis (PCA) is an important technique to understand in the fields of statistics and data science... but when putting a lesson together for my General Assembly students, I found that the resources online were too technical, didn't fully address our needs, and/or provided conflicting information. It's safe to say that I'm not ""entirely satisfied with the available texts"" here.
As a result, I wanted to put together the ""What,"" ""When,"" ""How,"" and ""Why"" of PCA as well as links to some of the resources that can help to further explain this topic. Specifically, I want to present the rationale for this method, the math under the hood, some best practices, and potential drawbacks to the method.
While I want to make PCA as accessible as possible, the algorithm we'll cover is pretty technical. Being familiar with some or all of the following will make this article and PCA as a method easier to understand: matrix operations/linear algebra (matrix multiplication, matrix transposition, matrix inverses, matrix decomposition, eigenvectors/eigenvalues) and statistics/machine learning (standardization, variance, covariance, independence, linear regression, feature selection). I've embedded links to illustrations of these topics throughout the article, but hopefully these will serve as a reminder rather than required reading to get through the article.
Let's say that you want to predict what the gross domestic product (GDP) of the United States will be for 2017. You have lots of information available: the U.S. GDP for the first quarter of 2017, the U.S. GDP for the entirety of 2016, 2015, and so on. You have any publicly-available economic indicator, like the unemployment rate, inflation rate, and so on. You have U.S. Census data from 2010 estimating how many Americans work in each industry and American Community Survey data updating those estimates in between each census. You know how many members of the House and Senate belong to each political party. You could gather stock price data, the number of IPOs occurring in a year, and how many CEOs seem to be mounting a bid for public office. Despite being an overwhelming number of variables to consider, this just scratches the surface.
TL;DR  you have a lot of variables to consider.
If you've worked with a lot of variables before, you know this can present problems. Do you understand the relationships between each variable? Do you have so many variables that you are in danger of overfitting your model to your data or that you might be violating assumptions of whichever modeling tactic you're using?
You might ask the question, ""How do I take all of the variables I've collected and focus on only a few of them?"" In technical terms, you want to ""reduce the dimension of your feature space."" By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model. (Note: This doesn't immediately mean that overfitting, etc. are no longer concerns  but we're moving in the right direction!)
Somewhat unsurprisingly, reducing the dimension of the feature space is called ""dimensionality reduction."" There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:
Feature elimination is what it sounds like: we reduce the feature space by eliminating features. In the GDP example above, instead of considering every single variable, we might drop all variables except the three we think will best predict what the U.S.'s gross domestic product will look like. Advantages of feature elimination methods include simplicity and maintaining interpretability of your variables.
As a disadvantage, though, you gain no information from those variables you've dropped. If we only use last year's GDP, the proportion of the population in manufacturing jobs per the most recent American Community Survey numbers, and unemployment rate to predict this year's GDP, we're missing out on whatever the dropped variables could contribute to our model. By eliminating features, we've also entirely eliminated any benefits those dropped variables would bring.
Feature extraction, however, doesn't run into this problem. Say we have ten independent variables. In feature extraction, we create ten ""new"" independent variables, where each ""new"" independent variable is a combination of each of the ten ""old"" independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.
You might say, ""Where does the dimensionality reduction come into play?"" Well, we keep as many of the new independent variables as we want, but we drop the ""least important ones."" Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important. But  and here's the kicker  because these new independent variables are combinations of our old ones, we're still keeping the most valuable parts of our old variables, even when we drop one or more of these ""new"" variables!
Principal component analysis is a technique for feature extraction  so it combines our input variables in a specific way, then we can drop the ""least important"" variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the ""new"" variables after PCA are all independent of one another. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another. If we decide to fit a linear regression model with these ""new"" variables (see ""principal component regression"" below), this assumption will necessarily be satisfied.
If you answered ""yes"" to all three questions, then PCA is a good method to use. If you answered ""no"" to question 3, you should not use PCA.
The section after this discusses why PCA works, but providing a brief summary before jumping into the algorithm may be helpful for context:
Here, I walk through an algorithm for conducting PCA. I try to avoid being too technical, but it's impossible to ignore the details here, so my goal is to walk through things as explicitly as possible. A deeper intuition of why the algorithm works is presented in the next section.
Before starting, you should have tabular data organized with n rows and likely p+1 columns, where one column corresponds to your dependent variable (usually denoted Y) and p columns where each corresponds to an independent variable (the matrix of which is usually denoted X).
Note two things in this graphic:
Because our principal components are orthogonal to one another, they are statistically linearly independent of one another... which is why our columns of Z* are linearly independent of one another!
8. Finally, we need to determine how many features to keep versus how many to drop. There are three common methods to determine this, discussed below and followed by an explicit example:
Because each eigenvalue is roughly the importance of its corresponding eigenvector, the proportion of variance explained is the sum of the eigenvalues of the features you kept divided by the sum of the eigenvalues of all features.
Consider this scree plot for genetic data. (Source: here.) The red line indicates the proportion of variance explained by each feature, which is calculated by taking that principal component's eigenvalue divided by the sum of all eigenvalues. The proportion of variance explained by including only principal component 1 is 1/(1 + 2 + ... + p), which is about 23%. The proportion of variance explained by including only principal component 2 is 2/(1 + 2 + ... + p), or about 19%.
The proportion of variance explained by including both principal components 1 and 2 is (1 + 2)/(1 + 2 + ... + p), which is about 42%. This is where the yellow line comes in; the yellow line indicates the cumulative proportion of variance explained if you included all principal components up to that point. For example, the yellow dot above PC2 indicates that including principal components 1 and 2 will explain about 42% of the total variance in the model.
Now let's go through some examples:
(Note: Some scree plots will have the size of eigenvectors on the Y axis rather than the proportion of variance. This leads to equivalent results, but requires the user to manually calculate the proportion of variance. An example of this can be seen here.)
Once we've dropped the transformed variables we want to drop, we're done! That's PCA.
While PCA is a very technical method relying on in-depth linear algebra algorithms, it's a relatively intuitive method when you think about it.
Thus, PCA is a method that brings together:
PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant.
Yes, more than I can address here in a reasonable amount of space. The one I've most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn't drop. (This is where the independence of the columns of Z* comes in; by regressing Y on Z*, we know that the required independence of independent variables will necessarily be satisfied. However, we will need to still check our other assumptions.)
The other commonly-seen variant I've seen is kernel PCA.
I hope you found this article helpful! Check out some of the resources below for more in-depth discussions of PCA. Let me know what you think, especially if there are suggestions for improvement.
I've been told that a Chinese translation of this article has been made available here. (Thanks, Jakukyo Friel!)
I want to offer many thanks to my friends Ritika Bhasker, Joseph Nelson, and Corey Smith for their suggestions and edits. You should check Ritika and Joseph out on Medium  their posts are far more entertaining than mine. (Corey is too focused on not getting his Ph.D. research scooped to have a Medium presence.)
I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.
Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above. He correctly pointed out that Z* = ZP*, not ZTP*. Thanks also to Chienlung Cheung for noticing another typo in Step 8 above and noted that I had conflated ""eigenvector"" with ""eigenvalue"" in one line.
This is a list of resources I used to compile this PCA article as well as other resources I've generally found helpful to understand PCA. If you know of any resources that would be a good inclusion to this list, please leave a comment and I'll add them.
",60
https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1?source=tag_archive---------6-----------------------,5 Things You Should Know About Covariance,"When dealing with problems on statistics and machine learning, one of the most frequently encountered terms is covariance. While most of...",Sergen Cansiz,7,"When dealing with problems on statistics and machine learning, one of the most frequently encountered terms is covariance. While most of us know that variance represents the variation of values in a single variable, we may not be sure what covariance stands for. Besides, knowing covariance can provide way more information on solving multivariate problems. Most of the methods for preprocessing or predictive analysis depend on the covariance. Multivariate outlier detection, dimensionality reduction, and regression can be given as examples.
In this article, I am going to explain five things that you should know about covariance. Instead of explaining it from the definition in Wikipedia, we will try to understand it from its formula. After reading this article, you will be able to answer the following questions.
It would be better to go over the variance to understand the covariance. The variance explains how the values vary in a variable. It depends on how the values far from each other. Take a look at Formula 1 to understand how variance gets calculated.
In the formula, each value in the variable subtracts from the mean of that variable. After the differences are squared, it gets divided by the number of values (N) in that variable. Okay, what happens when the variance is low or high. You can see Figure 1 to understand what happens if the variance value is low or high.
Now, it is time to have a look at the covariance formula. It is as simple as the variance formula. Unlike the variance, covariance is calculated between two different variables. Its purpose is to find the value that indicates how these two variables vary together. In the covariance formula, the values of both variables are multiplied by taking the difference from the mean. You can see Formula 2 to understand it clearly.
The only difference between variance and covariance is using the values and means of two variables instead of one. Now, let's take a look at the second thing that you should know.
Note: As you can see from Formula 1 and Formula 2, there are two different formulas as population known and unknown. When we work on sample data, we don't know the population mean, we know only the sample mean. That's why we should use the formula with N-1. When we have the all population of the subject, we can you the with N.
The second thing that you should know is the covariance matrix. Because covariance can only be calculated between two variables, covariance matrices stand for representing covariance values of each pair of variables in multivariate data. Also, the covariance between the same variables equals variance, so, the diagonal shows the variance of each variable. Suppose there are two variables as x and y in our data set. The covariance matrix should look like Formula 3.
It is a symmetric matrix that shows covariances of each pair of variables. These values in the covariance matrix show the distribution magnitude and direction of multivariate data in multidimensional space. By controlling these values we can have information about how data spread among two dimensions.
The third thing that you should know about covariance is their positive, negative, and zero states. We can go over the formula to understand it. When Xi-Xmean and Yi-Ymean are both negative or positive at the same time, multiplication returns a positive value. If the sum of these values is positive, covariance gets found as positive. It means variable X and variable Y variate in the same direction. In other words, if a value in variable X is higher, it is expected to be high in the corresponding value in variable Y too. In short, there is a positive relationship between them. If there is a negative covariance, this is interpreted right as the opposite. That is, there is a negative relationship between the two variables.
The covariance can only be zero when the sum of products of Xi-Xmean and Yi-Ymeanis is zero. However, the products of Xi-Xmean and Yi-Ymean can be near-zero when one or both are zero. In such a scenario, there aren't any relations between variables. To understand it clearly, you can see the flowing Figure 2.
As another possible scenario, we can have a distribution something like in Figure 3. It happens while the covariance is near zero and the variance of variables are different.
Unlike correlation, covariance values do not have a limit between -1 and 1. Therefore, it may be wrong to conclude that there might be a high relationship between variables when the covariance is high. The size of covariance values depends on the difference between values in variables. For instance, if the values are between 1000 and 2000 in the variable, it possible to have high covariance. However, if the values are between 1 and 2 in both variables, it is possible to have a low covariance. Therefore, we can't say the relationship in the first example is stronger than the second. The covariance stands for only the variation and relation direction between two variables. You can understand it from Figure 4.
Although the covariance in the first figure is very large, the relationship can be higher or the same in the second figure. (The values in Figure 4 are given as examples, they aren't from any data set and aren't true values)
What do eigenvalues and eigenvectors tell us? These are the essential part of the covariance matrix. The methods that require a covariance matrix to find the magnitude and direction of the data points use eigenvalues and eigenvectors. For example, the eigenvalues represent the magnitude of the spread in the direction of the principal components in PCA. In Figure 5, the first and second plots show the distribution of points when the covariance is near zero. When the covariance is zero, eigenvalues will be directly equal to the variance values. The third and fourth plots represent the distribution of points when the covariance is different from zero. Unlike the first two, eigenvalues and eigenvectors should be calculated for these two.
As can be seen from Figure 5, the eigenvalues represent the magnitude of the spread for both variables x and y. The eigenvectors show the direction. It is possible to find the angle of propagation from the arccosine of the value v[0,0] when the covariance is positive. If the covariance is negative, the cosine of the valuev[0,0]gives the spread direction.
How do you find eigenvalues and eigenvectors from the covariance matrix? You can find both eigenvectors and eigenvalues using NumPY in Python. First thing you should do is to find covariance matrix using method numpy.cov(). After you found the covariance matrix you can use the method numpy.linalg.eig(M) to find eigenvectors and eigenvalues.
You can read my other article to find out how eigenvalues are used in principal component analysis.
sergencansiz.medium.com
Covariance is one of the most used measurements in data science. Knowing covariance with its details provides many opportunities to understand multivariate data. Therefore, I wanted to share with you the five things that you should know about covariance. Please feel free to leave a comment if you have any questions or recommendations.
",61
https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?source=tag_archive---------2-----------------------,How to build your own Neural Network from scratch in Python,A beginner's guide to understanding the inner workings of Deep Learning,James Loy,7,"Update: When I wrote this article a year ago, I did not expect it to be this popular. Since then, this article has been viewed more than 450,000 times, with more than 30,000 claps. It has also made it to the front page of Google, and it is among the first few search results for 'Neural Network'. Many of you have reached out to me, and I am deeply humbled by the impact of this article on your learning journey.
This article also caught the eye of the editors at Packt Publishing. Shortly after this article was published, I was offered to be the sole author of the book Neural Network Projects with Python. Today, I am happy to share with you that my book has been published!
The book is a continuation of this article, and it covers end-to-end implementation of neural network projects in areas such as face recognition, sentiment analysis, noise removal etc. Every chapter features a unique neural network architecture, including Convolutional Neural Networks, Long Short-Term Memory Nets and Siamese Neural Networks. If you're looking to create a strong machine learning portfolio with deep learning projects, do consider getting the book!
You can get the book from Amazon: Neural Network Projects with Python
Motivation: As part of my personal journey to gain a better understanding of Deep Learning, I've decided to build a Neural Network from scratch without a deep learning library like TensorFlow. I believe that understanding the inner workings of a Neural Network is important to any aspiring Data Scientist.
This article contains what I've learned, and hopefully it'll be useful for you as well!
Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output.
Neural Networks consist of the following components
The diagram below shows the architecture of a 2-layer Neural Network (note that the input layer is typically excluded when counting the number of layers in a Neural Network)
Creating a Neural Network class in Python is easy.
Training the Neural Network
The output y of a simple 2-layer Neural Network is:
You might notice that in the equation above, the weights W and the biases b are the only variables that affects the output y.
Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network.
Each iteration of the training process consists of the following steps:
The sequential graph below illustrates the process.
As we've seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is:
Let's add a feedforward function in our python code to do exactly that. Note that for simplicity, we have assumed the biases to be 0.
However, we still need a way to evaluate the ""goodness"" of our predictions (i.e. how far off are our predictions)? The Loss Function allows us to do exactly that.
There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we'll use a simple sum-of-sqaures error as our loss function.
That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference.
Our goal in training is to find the best set of weights and biases that minimizes the loss function.
Now that we've measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases.
In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases.
Recall from calculus that the derivative of a function is simply the slope of the function.
If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as gradient descent.
However, we can't directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.
Phew! That was ugly but it allows us to get what we needed  the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly.
Now that we have that, let's add the backpropagation function into our python code.
For a deeper understanding of the application of calculus and the chain rule in backpropagation, I strongly recommend this tutorial by 3Blue1Brown.
Now that we have our complete python code for doing feedforward and backpropagation, let's apply our Neural Network on an example and see how well it does.
Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn't exactly trivial for us to work out the weights just by inspection alone.
Let's train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss monotonically decreasing towards a minimum. This is consistent with the gradient descent algorithm that we've discussed earlier.
Let's look at the final prediction (output) from the Neural Network after 1500 iterations.
We did it! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values.
Note that there's a slight difference between the predictions and the actual values. This is desirable, as it prevents overfitting and allows the Neural Network to generalize better to unseen data.
Fortunately for us, our journey isn't over. There's still much to learn about Neural Networks and Deep Learning. For example:
I'll be writing more on these topics soon, so do follow me on Medium and keep and eye out for them!
I've certainly learnt a lot writing my own Neural Network from scratch.
Although Deep Learning libraries such as TensorFlow and Keras makes it easy to build deep nets without fully understanding the inner workings of a Neural Network, I find that it's beneficial for aspiring data scientist to gain a deeper understanding of Neural Networks.
This exercise has been a great investment of my time, and I hope that it'll be useful for you as well!
",62
https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8?source=tag_archive---------9-----------------------,The best explanation of Convolutional Neural Networks on the Internet!,"CNNs have wide applications in image and video recognition, recommender systems and natural language processing. In this article, the...",Harsh Pokharna,5,"CNNs have wide applications in image and video recognition, recommender systems and natural language processing. In this article, the example that I will take is related to Computer Vision. However, the basic concept remains the same and can be applied to any other use-case!
For a quick recap of Neural Networks, here's a very clearly explained article series.
CNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output. The whole network has a loss function and all the tips and tricks that we developed for neural networks still apply on CNNs. Pretty straightforward, right?
So, how are Convolutional Neural Networks different than Neural Networks?
What do we mean by this?
Unlike neural networks, where the input is a vector, here the input is a multi-channeled image (3 channeled in this case).
There are other differences that we will talk about in a while.
Before we go any deeper, let us first understand what convolution means.
We take the 5*5*3 filter and slide it over the complete image and along the way take the dot product between the filter and chunks of the input image.
For every dot product taken, the result is a scalar.
So, what happens when we convolve the complete image with the filter?
I leave it upon you to figure out how the '28' comes. (Hint: There are 28*28 unique positions where the filter can be put on the image)
The convolution layer is the main building block of a convolutional neural network.
The convolution layer comprises of a set of independent filters (6 in the example shown). Each filter is independently convolved with the image and we end up with 6 feature maps of shape 28*28*1.
Suppose we have a number of convolution layers in sequence. What happens then?
All these filters are initialized randomly and become our parameters which will be learned by the network subsequently.
I will show you an example of a trained network.
Take a look at the filters in the very first layer (these are our 5*5*3 filters). Through back propagation, they have tuned themselves to become blobs of coloured pieces and edges. As we go deeper to other convolution layers, the filters are doing dot products to the input of the previous convolution layers. So, they are taking the smaller coloured pieces or edges and making larger pieces out of them.
Take a look at image 4 and imagine the 28*28*1 grid as a grid of 28*28 neurons. For a particular feature map (the output received on convolving the image with a particular filter is called a feature map), each neuron is connected only to a small chunk of the input image and all the neurons have the same connection weights. So again coming back to the differences between CNN and a neural network.
Parameter sharing is sharing of weights by all neurons in a particular feature map.
Local connectivity is the concept of each neural connected only to a subset of the input image (unlike a neural network where all the neurons are fully connected)
This helps to reduce the number of parameters in the whole system and makes the computation more efficient.
I will not be talking about the concept of zero padding here as the idea is to keep it simple. Interested people can read about it separately!
A pooling layer is another building block of a CNN.
Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently.
The most common approach used in pooling is max pooling.
We have already discussed about convolution layers (denoted by CONV) and pooling layers (denoted by POOL).
RELU is just a non linearity which is applied similar to neural networks.
The FC is the fully connected layer of neurons at the end of CNN. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks and work in a similar way.
I hope you understand the architecture of a CNN now. There are many variations to this architecture but as I mentioned before, the basic concept remains the same. In case you have any doubts/feedback, please comment.
You can follow me to read more TechnologyMadeEasy articles!
And if you want your friends to read this too, click share!
References: http://cs231n.github.io/convolutional-networks/#overview
Blockchains | Ethereum | Computer Vision | Machine Learning
11.1K 
26
",63
https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8?source=tag_archive---------8-----------------------,Lambda Functions with Practical Examples in Python,"How, when to use, and when not to use Lambda functions",Susan Maina,6,"When I first came across lambda functions in python, I was very much intimidated and thought they were for advanced Pythonistas. Beginner python tutorials applaud the language for its readable syntax, but lambdas sure didn't seem user-friendly.
However, once I understood the general syntax and examined some simple use cases, using them was less scary.
Simply put, a lambda function is just like any normal python function, except that it has no name when defining it, and it is contained in one line of code.
A lambda function evaluates an expression for a given argument. You give the function a value (argument) and then provide the operation (expression). The keyword lambda must come first. A full colon (:) separates the argument and the expression.
In the example code below, x is the argument and x+x is the expression.
Before we get into practical applications, let's mention some technicalities on what the python community thinks is good and bad with lambda functions.
Pros
Cons
At the end of this article, we'll look at commonly used code examples where Lambda functions are discouraged even though they seem legitimate.
But first, let's look at situations when to use lambda functions. Note that we use lambda functions a lot with python classes that take in a function as an argument, for example, map() and filter(). These are also called Higher-order functions.
This is when you execute a lambda function on a single value.
In the code above, the function was created and then immediately executed. This is an example of an immediately invoked function expression or IIFE.
Filter(). This is a Python inbuilt library that returns only those values that fit certain criteria. The syntax is filter(function, iterable). The iterable can be any sequence such as a list, set, or series object (more below).
The example below filters a list for even numbers. Note that the filter function returns a 'Filter object' and you need to encapsulate it with a list to return the values.
Map(). This is another inbuilt python library with the syntax map(function, iterable).
This returns a modified list where every value in the original list has been changed based on a function. The example below cubes every number in the list.
A Series object is a column in a data frame, or put another way, a sequence of values with corresponding indices. Lambda functions can be used to manipulate values inside a Pandas dataframe.
Let's create a dummy dataframe about members of a family.
Lambda with Apply() function by Pandas. This function applies an operation to every element of the column.
To get the current age of each member, we subtract their birth year from the current year. In the lambda function below, x refers to a value in the birthyear column, and the expression is 2021(current year) minus the value.
Lambda with Python's Filter() function. This takes 2 arguments; one is a lambda function with a condition expression, two an iterable which for us is a series object. It returns a list of values that satisfy the condition.
Lambda with Map() function by Pandas. Map works very much like apply() in that it modifies values of a column based on the expression.
We can also perform conditional operations that return different values based on certain criteria.
The code below returns 'Male' if the Status value is father or son, and returns 'Female' otherwise. Note that apply and map are interchangeable in this context.
I mostly use Lambda functions on specific columns (series object) rather than the entire data frame, unless I want to modify the entire data frame with one expression.
For example rounding all values to 1 decimal place, in which case all the columns have to be float or int datatypes because round() can't work on strings.
In the example below, we use apply on a dataframe and select the columns to modify in the Lambda function. Note that we must use axis=1 here so that the expression is applied column-wise.
2. Passing functions inside Lambda functions. Using functions like abs which only take one number- argument is unnecessary with Lambda because you can directly pass the function into map() or apply().
Ideally, functions inside lambda functions should take two or more arguments. Examples are pow(number,power) and round(number,ndigit). You can experiment with various in-built python functions to see which ones need Lambda functions in this context. I've done so in this notebook.
3. Using Lambda functions when multiple lines of code are more readable. An example is when you are using if-else statements inside the lambda function. I used the example below earlier in this article.
The same results can be achieved with the code below. I prefer this way because you can have endless conditions and the code is simple enough to follow. More on vectorized conditions here.
Many programmers who don't like Lambdas usually argue that you can replace them with the more understandable list comprehensions, built-in functions, and standard libraries. Generator expressions (similar to list comprehensions) are also handy alternatives to the map() and filter() functions.
Whether or not you decide to embrace Lambda functions in your code, you need to understand what they are and how they are used because you will inevitably come across them in other peoples' code.
",64
https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775?source=tag_archive---------2-----------------------,The Complete Guide to Time Series Analysis and Forecasting,"Understand moving average, exponential smoothing, stationarity, autocorrelation, SARIMA, and apply these techniques in two projects.",Marco Peixeiro,13,"Whether we wish to predict the trend in financial markets or electricity consumption, time is an important factor that must now be considered in our models. For example, it would be interesting to forecast at what hour during the day is there going to be a peak consumption in electricity, such as to adjust the price or the production of electricity.
Enter time series. A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable and the goal is usually to make a forecast for the future.
However, there are other aspects that come into play when dealing with time series.
Is it stationary?
Is there a seasonality?
Is the target variable autocorrelated?
In this post, I will introduce different characteristics of time series and how we can model them to obtain accurate (as much as possible) forecasts.
Go beyond the basics and apply advanced models, such as SARIMAX, VARMAX, CNN, LSTM, ResNet, autoregressive LSTM with the Applied Time Series Analysis in Python course!
Informally, autocorrelation is the similarity between observations as a function of the time lag between them.
Above is an example of an autocorrelation plot. Looking closely, you realize that the first value and the 24th value have a high autocorrelation. Similarly, the 12th and 36th observations are highly correlated. This means that we will find a very similar value at every 24 unit of time.
Notice how the plot looks like sinusoidal function. This is a hint for seasonality, and you can find its value by finding the period in the plot above, which would give 24h.
Seasonality refers to periodic fluctuations. For example, electricity consumption is high during the day and low during night, or online sales increase during Christmas before slowing down again.
As you can see above, there is a clear daily seasonality. Every day, you see a peak towards the evening, and the lowest points are the beginning and the end of each day.
Remember that seasonality can also be derived from an autocorrelation plot if it has a sinusoidal shape. Simply look at the period, and it gives the length of the season.
Stationarity is an important characteristic of time series. A time series is said to be stationary if its statistical properties do not change over time. In other words, it has constant mean and variance, and covariance is independent of time.
Looking again at the same plot, we see that the process above is stationary. The mean and variance do not vary over time.
Often, stock prices are not a stationary process, since we might see a growing trend, or its volatility might increase over time (meaning that variance is changing).
Ideally, we want to have a stationary time series for modelling. Of course, not all of them are stationary, but we can make different transformations to make them stationary.
You may have noticed in the title of the plot above Dickey-Fuller. This is the statistical test that we run to determine if a time series is stationary or not.
Without going into the technicalities of the Dickey-Fuller test, it test the null hypothesis that a unit root is present.
If it is, then p > 0, and the process is not stationary.
Otherwise, p = 0, the null hypothesis is rejected, and the process is considered to be stationary.
As an example, the process below is not stationary. Notice how the mean is not constant through time.
There are many ways to model a time series in order to make predictions. Here, I will present:
The moving average model is probably the most naive approach to time series modelling. This model simply states that the next observation is the mean of all past observations.
Although simple, this model might be surprisingly good and it represents a good starting point.
Otherwise, the moving average can be used to identify interesting trends in the data. We can define a window to apply the moving average model to smooth the time series, and highlight different trends.
In the plot above, we applied the moving average model to a 24h window. The green line smoothed the time series, and we can see that there are 2 peaks in a 24h period.
Of course, the longer the window, the smoother the trend will be. Below is an example of moving average on a smaller window.
Exponential smoothing uses a similar logic to moving average, but this time, a different decreasing weight is assigned to each observations. In other words, less importance is given to observations as we move further from the present.
Mathematically, exponential smoothing is expressed as:
Here, alpha is a smoothing factor that takes values between 0 and 1. It determines how fast the weight decreases for previous observations.
From the plot above, the dark blue line represents the exponential smoothing of the time series using a smoothing factor of 0.3, while the orange line uses a smoothing factor of 0.05.
As you can see, the smaller the smoothing factor, the smoother the time series will be. This makes sense, because as the smoothing factor approaches 0, we approach the moving average model.
Double exponential smoothing is used when there is a trend in the time series. In that case, we use this technique, which is simply a recursive use of exponential smoothing twice.
Mathematically:
Here, beta is the trend smoothing factor, and it takes values between 0 and 1.
Below, you can see how different values of alpha and beta affect the shape of the time series.
This method extends double exponential smoothing, by adding a seasonal smoothing factor. Of course, this is useful if you notice seasonality in your time series.
Mathematically, triple exponential smoothing is expressed as:
Where gamma is the seasonal smoothing factor and L is the length of the season.
SARIMA is actually the combination of simpler models to make a complex model that can model time series exhibiting non-stationary properties and seasonality.
At first, we have the autoregression model AR(p). This is basically a regression of the time series onto itself. Here, we assume that the current value depends on its previous values with some lag. It takes a parameter p which represents the maximum lag. To find it, we look at the partial autocorrelation plot and identify the lag after which most lags are not significant.
In the example below, p would be 4.
Then, we add the moving average model MA(q). This takes a parameter q which represents the biggest lag after which other lags are not significant on the autocorrelation plot.
Below, q would be 4.
After, we add the order of integration I(d). The parameter d represents the number of differences required to make the series stationary.
Finally, we add the final component: seasonality S(P, D, Q, s), where s is simply the season's length. Furthermore, this component requires the parameters P and Q which are the same as p and q, but for the seasonal component. Finally, D is the order of seasonal integration representing the number of differences required to remove seasonality from the series.
Combining all, we get the SARIMA(p, d, q)(P, D, Q, s) model.
The main takeaway is: before modelling with SARIMA, we must apply transformations to our time series to remove seasonality and any non-stationary behaviors.
That was a lot of theory to wrap our head around! Let's apply the techniques discussed above in our first project.
We will try to predict the stock price of a specific company. Now, predicting the stock price is virtually impossible. However, it remains a fun exercise and it will be a good way to practice what we have learned.
We will use the historical stock price of the New Germany Fund (GF) to try to predict the closing price in the next five trading days.
You can grab the dataset and notebook here.
As always, I highly recommend you code along! Start your notebook, and let's go!
First, we import some libraries that will be helpful throughout our analysis. Also, we define the mean average percentage error (MAPE), as this will be our error metric.
Then, we import our dataset and we previous the first ten entries, and you should get:
As you can see, we have a few entries concerning a different stock than the New Germany Fund (GF). Also, we have an entry concerning intraday information, but we only want end of day (EOD) information.
First, we remove unwanted entries.
Then, we remove unwanted columns, as we solely want to focus on the stock's closing price.
If you preview the dataset, you should see:
Awesome! We are ready for exploratory data analysis!
We plot the closing price over the entire time period of our dataset.
You should get:
Clearly, you see that this is not a stationary process, and it is hard to tell if there is some kind of seasonality.
Let's use the moving average model to smooth our time series. For that, we will use a helper function that will run the moving average model on a specified time window and it will plot the result smoothed curve:
Using a time window of 5 days, we get:
As you can see, we can hardly see a trend, because it is too close to actual curve. Let's see the result of smoothing by the previous month, and previous quarter.
Trends are easier to spot now. Notice how the 30-day and 90-day trend show a downward curve at the end. This might mean that the stock is likely to go down in the following days.
Now, let's use exponential smoothing to see if it can pick up a better trend.
Here, we use 0.05 and 0.3 as values for the smoothing factor. Feel free to try other values and see what the result is.
As you can see, an alpha value of 0.05 smoothed the curve while picking up most of the upward and downward trends.
Now, let's use double exponential smoothing.
And you get:
Again, experiment with different alpha and beta combinations to get better looking curves.
As outlined previously, we must turn our series into a stationary process in order to model it. Therefore, let's apply the Dickey-Fuller test to see if it is a stationary process:
You should see:
By the Dickey-Fuller test, the time series is unsurprisingly non-stationary. Also, looking at the autocorrelation plot, we see that it is very high, and it seems that there is no clear seasonality.
Therefore, to get rid of the high autocorrelation and to make the process stationary, let's take the first difference (line 23 in the code block). We simply subtract the time series from itself with a lag of one day, and we get:
Awesome! Our series is now stationary and we can start modelling!
Now, for SARIMA, we first define a few parameters and a range of values for other parameters to generate a list of all possible combinations of p, q, d, P, Q, D, s.
Now, in the code cell above, we have 625 different combinations! We will try each combination and train SARIMA with each so to find the best performing model. This might take while depending on your computer's processing power.
Once this is done, we print out a summary of the best model, and you should see:
Awesome! We finally predict the closing price of the next five trading days and evaluate the MAPE of the model.
In this case, we have a MAPE of 0.79%, which is very good!
Now, to compare our prediction with actual data, we take financial data from Yahoo Finance and create a dataframe.
Then, we make a plot to see how far we were from the actual closing prices:
It seems that we are a bit off in our predictions. In fact, the predicted price is essentially flat, meaning that our model is probably not performing well.
Again, this is not due to our procedure, but to the fact that predicting stock prices is essentially impossible.
From the first project, we learned the entire procedure of making a time series stationary before using SARIMA to model. It is a long and tedious process, with a lot of manual tweaking.
Now, let's introduce Facebook's Prophet. It is a forecasting tool available in both Python and R. This tool allows both experts and non-experts to produce high quality forecasts with minimal efforts.
Let's see how we can use it in this second project!
The title says it all: we will use Prophet to help us predict air quality!
The full notebook and dataset can be found here.
Let's make some predictions!
As always, we start by importing some useful libraries. We then print out the first five rows:
As you can see, the dataset contains information about the concentrations of different gases. They were recorded at every hour for each day. You can find a description of all features here.
If you explore the dataset a bit more, you will notice that there are many instances of the value -200. Of course, it does not make sense to have a negative concentration, so we will need to clean the data before modelling.
Therefore, we need to clean the data.
Here, we start off by parsing our date column to turn into ""dates"".
Then, we turn all the measurements into floats.
After, we aggregate the data by day, by taking the average of each measurement.
At this point, we still have some NaN that we need to get rid of. Therefore, we remove the columns that have more than 8 NaN. That way, we can then remove rows containing NaN values without losing too much data.
Finally, we aggregate the data by week, because it will give a smoother trend to analyze.
We can plot the trends of each chemical. Here, we show that of NOx.
Oxides of nitrogen are very harmful, as they react to form smog and acid rain, as well as being responsible for the formation of fine particles and ground level ozone. These have adverse health effects, so the concentration of NOx is a key feature of air quality.
We will solely focus on modelling the NOx concentration. Therefore, we remove all other irrelevant columns.
Then, we import Prophet.
Prophet requires the date column to be named ds and the feature column to be named y, so we make the appropriate changes.
At this point, our data looks like this:
Then, we define a training set. For that we will hold out the last 30 entries for prediction and validation.
Afterwards, we simply initialize Prophet, fit the model to the data, and make predictions!
You should see the following:
Here, yhat represents the prediction, while yhat_lower and yhat_upper represent the lower and upper bound of the prediction respectively.
Prophet allows you to easily plot the forecast and we get:
As you can see, Prophet simply used a straight downward line to predict the concentration of NOx in the future.
Then, we check if the time series has any interesting features, such as seasonality:
Here, Prophet only identified a downward trend with no seasonality.
Evaluating the model's performance by calculating its mean absolute percentage error (MAPE) and mean absolute error (MAE), we see that the MAPE is 13.86% and the MAE is 109.32, which is not that bad! Remember that we did not fine tune the model at all.
Finally, we just plot the forecast with its upper and lower bounds:
Congratulations on making it to the end! This was a very long, but informative article. You learned how to robustly analyze and model time series and applied your knowledge in two different projects.
I hope you found this article useful, and I hope you will refer back to it.
To learn the latest techniques for time series analysis, checkout this course:
Cheers!
Reference: Many thanks to this article for the amazing introduction to time series analysis!
",65
https://towardsdatascience.com/8-ways-to-filter-pandas-dataframes-d34ba585c1b8?source=tag_archive---------6-----------------------,8 Ways to Filter Pandas Dataframes,A practical guide for efficient data analysis,Soner Yldrm,5,"Soner Yldrm
Pandas is a popular data analysis and manipulation library for Python. The core data structure of Pandas is dataframe which stores data in tabular form with labelled rows and columns.
A common operation in data analysis is to filter values based on a condition or multiple conditions. Pandas provides a variety of ways to filter data points (i.e. rows). In this article, we will cover 8 different ways to filter a dataframe.
We start by importing the libraries.
Let's create a sample dataframe for the examples.
We can use the logical operators on column values to filter rows.
We have selected the rows in which the value in ""val"" column is greater than 0.5.
The logical operators also works on strings.
Only the names that come after 'Jane' in alphabetical order are selected.
Pandas allows for combining multiple logical operators. For instance, we can apply conditions on both val and val2 columns as below.
The ""&"" signs stands for ""and"" , the ""|"" stands for ""or"".
The isin method is another way of applying multiple condition for filtering. For instance, we can filter the names that exist in a given list.
Pandas is a highly efficient library on textual data as well. The functions and methods under the str accessor provide flexible ways to filter rows based on strings.
For instance, we can select the names that start with the letter ""J"".
The contains function under the str accessor returns the values that contain a given set of characters.
We can pass a longer set of characters to the contains function depending on the strings in the data.
The tilde operator is used for ""not"" logic in filtering. If we add the tilde operator before the filter expression, the rows that do not fit the condition are returned.
We get the names that do not start with the letter ""J"".
The query function offers a little more flexibility at writing the conditions for filtering. We can pass the conditions as a string.
For instance, the following code returns the rows that belong to the B category and have a value higher than 0.5 in the val column.
In some cases, we do not have a specific range for filtering but just need the largest or smallest values. The nlargest and nsmallest functions allow for selecting rows that have the largest or smallest values in a column, respectively.
We specify the number of largest or smallest values to be selected and the name of the column.
The loc and iloc methods are used to select rows or columns based on index or label.
Thus, they can be used for filtering. However, we can only select a particular part of the dataframe without specifying a condition.
If the dataframe has integer index, the indices and labels of the rows are the same. Thus, both loc and iloc accomplished the same thing on the rows.
Let's update the index of the dataframe to demonstrate the difference between loc and iloc better.
We cannot pass integers to the loc method now because the labels of indices are letters.
We have covered 8 different ways of filtering rows in a dataframe. All of them are useful and come in handy for particular cases.
Pandas is a powerful library for both data analysis and manipulation. It provides numerous functions and methods to handle data in tabular form. As with any other tool, the best way to learn Pandas is through practicing.
Thank you for reading. Please let me know if you have any feedback.
",66
https://medium.com/@googlewalkout/standing-with-dr-timnit-gebru-isupporttimnit-believeblackwomen-6dadc300d382?source=tag_archive---------9-----------------------,Standing with Dr. Timnit Gebru  #ISupportTimnit #BelieveBlackWomen,"We, the undersigned, stand in solidarity with Dr. Timnit Gebru, who was terminated from her position as Staff Research Scientist and...",Google Walkout For Real Change,149,"We, the undersigned, stand in solidarity with Dr. Timnit Gebru, who was terminated from her position as Staff Research Scientist and Co-Lead of Ethical Artificial Intelligence (AI) team at Google, following unprecedented research censorship. We call on Google Research to strengthen its commitment to research integrity and to unequivocally commit to supporting research that honors the commitments made in Google's AI Principles.
Until December 2, 2020, Dr. Gebru was one of very few Black women Research Scientists at the company, which boasts a dismal 1.6% Black women employees overall. Her research accomplishments are extensive, and have profoundly impacted academic scholarship and public policy. Dr. Gebru is a pathbreaking scientist doing some of the most important work to ensure just and accountable AI and to create a welcoming and diverse AI research field.
Instead of being embraced by Google as an exceptionally talented and prolific contributor, Dr. Gebru has faced defensiveness, racism, gaslighting, research censorship, and now a retaliatory firing. In an email to Dr. Gebru's team on the evening of December 2, 2020, Google executives claimed that she had chosen to resign. This is false. In their direct correspondence with Dr. Gebru, these executives informed her that her termination was immediate, and pointed to an email she sent to a Google Brain diversity and inclusion mailing list as pretext.
The contents of this email are important. In it, Dr. Gebru pushed back against Google's censorship of her (and her colleagues') research, which focused on examining the environmental and ethical implications of large-scale AI language models (LLMs), which are used in many Google products. Dr. Gebru and her colleagues worked for months on a paper that was under review at an academic conference. In late November, five weeks after the piece had been internally reviewed and approved for publication through standard processes, Google leadership made the decision to censor it, without warning or cause. Dr. Gebru asked them to explain this decision and to take accountability for it, and for their lackluster stand on discriminatory and harassing workplace conditions. The termination is an act of retaliation against Dr. Gebru, and it heralds danger for people working for ethical and just AI  especially Black people and People of Color  across Google.
Research integrity can no longer be taken for granted in Google's corporate research environment, and Dr. Gebru's firing has overthrown a working understanding of what kind of research Google will permit. This is also a moment of reckoning beyond Google. As we know, Dr. Gebru is one of the few people exerting pressure from the inside against the unethical and undemocratic incursion of powerful and biased technologies into our daily lives. This is a public service, and its importance cannot be overstated. Google's retaliation against Dr. Gebru, and its move to silence this work, concerns us all.
We have the following demands of Google Research leadership:
To add your name to the list of academic, civil society, and industry supporters (Googlers, see go/stand-with-timnit):
Signed,
2695 Googlers and 4302 academic, industry, and civil society supporters
Googlers include:
@zigdon, Senior SREA. Assaf, Software EngineerAaron Homer, SWEAaron Sy, Software EngineerAaron Wood, Senior Software EngineerAaron Zakem, Software EngineerAashni, Account StrategistAbby Beck, DesignerAbby HorowitzAbby Lyons, Software EngineerAbi LaBounty, Site Reliability EngineerAdam Klein, Staff Software EngineerAdam Olshansky, Application EngineerAdam Paszke, Research ScientistAdam PearceAdele McDonald, Software EngineerAdesola Sanusi, Product ManagerAdina KatzAditi, Software EngineerAdji Bousso Dieng, Research ScientistAdrian Ludwin, Senior Software EngineerAdriana R , Global MobilityAdrien Kunysz, Systems EngineerAekta Shah, UXR JEDI (Justice, Equity, Diversity and Inclusion)Aileen, StrategistAlaa Salama, Data Centre Sustainability Programme ManagerAlan McAvinney, Software EngineerAlan McLean, Staff Interaction DesignerAlan Morales, Site Reliability EngineerAlana Conner, Staff UX ResearcherAlberta Devor, Software EngineerAlberto B, Staff Site Reliability EngineerAlec GlassfordAlec Story, Senior Software Engineeraleksandra culver, Senior Site Reliability EngineerAlex Berliner, Software EngineerAlex Chiu, Software EngineerAlex Haig, Software EngineerAlex Hanna, Senior Research ScientistAlex Iriza, Software EngineerAlex Liu, Software EngineerAlex Nicksay, Staff Software EngineerAlex Peterson, SWEAlex Saiontz, Product ManagerAlex Zani, Software EngineerAlexander D'Amour, Senior Research ScientistAlexander Gorban, Senior Research ScientistAlexandra Camargo, Software EngineerAlexandra Muhler, Trust & SafetyAlexandru Tudor, Software EngineerAlexios MantzarlisAli Razavi, Research Engineer, DeepMindAlice, Software EngineerAlice Crawford, Site Reliability EngineerAlice Lemieux, Software EngineerAlice Moloney, UX Content StrategistAlicia Chang, Technical WriterAlina S, Software EngineerAlison C.Allie Miller, UX ResearcherAllison KemmerlingAllison Raaum, Research OperationsAlyssa Haroldsen, Software EngineerAlyssa McDevitt, Software EngineerAlyssa Vessey, Software EngineerAmanda Pype, User Experience ResearcherAmber Ogata, Software EngineerAmber Zhang, Software EngineerAmelia Archer, Software EngineerAmelia Brunner, Software EngineerAmina Howard, Business AnalystAmy Skerry-Ryan, Senior Software EngineerAmy Unruh, SWEAna Rocha, News Product Experience StrategistAnass Koudiss, UX Programme ManagerAndan Lauber, Technical Program ManagerAndrea Dunlap, Makani ArchivistAndreas C. Schou, Staff Privacy Engineer, Machine LearningAndreas Haas, SWEAndreas S., Software EngineerAndrew Blatner, Firmware Engineer, Verily Life SciencesAndrew Brook, Engineering DirectorAndrew Fitz Gibbon, Developer AdvocateAndrew Gainer-Dewar, Ph.D., Software EngineerAndrew Gorcester, Developer Programs EngineerAndrew Matsuoka, Senior Software EngineerAndrew Sherman, Software EngineerAndrew SmartAndrew Wesson, Software EngineerAndrew Zaldivar, Ph.D., Senior Developer Relations Engineer, Ethical AI, Google ResearchAndy Blank, Software EngineerAndy Schneider, Software EngineerAndy Walner, Product StrategyAngela Chen, Software EngineerAngela Pablo, UX Designer, Equity EngineeringAngelica Inguanzo, Software EngineerAngelica Pando, Software EngineerAngjoo Kanazawa, Research Scientist; Assistant Professor, EECS UC BerkeleyAnh Chi Pham, UX DesignerAniran Chandravongsri, Software EngineerAnn Fernandez, Google Children's Center EducatorAnna Greenwood, Program ManagerAnna Her, UX ResearcherAnna Nachesa, Software EngineerAnna WysenAnnamaria Andolino, Ads Policy LeadAnnie Sullivan, Staff Software EngineerAnsonZAnthony Hinton, Test EngineerAnthony RuhierAnthony Tordillos, Senior Software EngineerAntonia Mora, Program ManagerAnuraag KAnz Whitney, Software EngineerApurva Panse, Product ManagerAradhana Sinha, Software EngineerArash Afkanpour, Software EngineerAriana Bray, Developer Programs EngineerAriel Koren, Product Marketing Manager, Google for EducationArkanath Pathak, Software EngineerAsfandyar Qureshi, Staff Software EngineerAshe Robinson, Software EngineerAshley Alese Edwards, US Partnerships Manager, News LabAshley Rascoe, ABPAshley Ray-Harris, Project LeadAtish Agarwala, AI ResidentAubrie Lee, Brand Manager (Naming)Austin Griffin, Product SpecialistAustin Myers, Software EngineerAutumn Stroupe, Senior Interaction DesignerAvi Romanoff, Product ManagerAyan Daniels, Design LeadAyla Ounce, Site Reliability EngineerBaq Haidri, Software EngineerBarrijean WalshBart Bal, EngineerBart van Merrienboer, Research ScientistBaruch Tabanpour, Software EngineerBecca Milman, SWEBecca Roelofs, Research scientistBecky Norling-Ruggles, SWEBen Calabrese, Senior Software EngineerBen Gwin, Analyst, TVCBen HutchinsonBen Kellogg, Software EngineerBen Lickly, Software EngineerBen Love, Staff SREBen Packer, Senior Software EngineerBen Poole, Research ScientistBene Webster, UX Program ManagerBenjamin H.Benjamin Wolf, Senior Software Engineer/Tech LeadBerk Ustun, Visiting ResearcherBGP, Data ScientistBharath S, Software EngineerBlake Jensen, Site Reliability EngineerBlithe Rocher, Engineering ManagerBobby Dygert, Senior Software EngineerBonnie Zhou, Software EngineerBosun AdeotiBrady Rennell, Channels SpecialistBrandon Jewett-Hall, Software EngineerBrandon Parrott, Program ManagerBrandon Tory Thorpe, Senior Software Engineer, Google AIBrendan Luu, Interaction DesignerBrendan Shillingford, Research ScientistBrian Ellis, Software EngineerBrian Ichter, Research ScientistBrian M, Software EngineerBrian Nachbar, Software EngineerBrianna GroenhoutBritny Herzog, Program ManagerBruce Dawson, SWEBryan SeyboldBurak Emir, Senior Staff Software EngineerC, SWEC TianC.O. Lee Boyce Jr., Aerospace EngineerCait Phillips, Software DeveloperCaitlin Donhowe, Software EngineerCaitlyn Britt, Software EngineerCarl Atupem, Software EngineerCaroline Hermans, UXECaroline Ho, Software EngineerCaroline Liu, Software EngineerCassandra Cruz, SWECassandra Fox, SRECatherina Xu, Product ManagerCathy Mao, Program ManagerCecilia Rabess, Data ScientistCesar Francisco Ibarra, SWE-SREChanning Kimble-Brown, Software EngineerCharles Tam, Software EngineerChelsea Price-Gallinat, Inclusion Lead, Google Children's CenterCherie Meyer, Software EngineerCheryl Marriott, Sr. UX ResearcherChewy Shaw, Site Reliability EngineerChieu Nguyen, Computational LinguistChloe Snyder, Software EngineerChris Barrick, Cloud AI Solutions SREChris Demeke, Product ManagerChris Hsu, Senior Site Reliability Engineer, Cloud AIChris Koch, Software EngineerChris Latimer, SWEChris Neffshade, Data ScientistChris Palmer, Software Security EngineerChris Silverberg, Software EngineerChris Sinco, UX DesignerChris Torres, Product Technology ManagerChris WilsonChrissie Brodigan, Sr. User Experience ResearcherChristian Biesinger, Senior Software EngineerChristian Brunschen, Software EngineerChristiane Silva Pinto, APMM, SMB Ads MarketingChristie Brandt, senior software engineerChristina Holland, Software EngineerChristine Htoon, Program ManagerChristopher Anderson, Senior Software EngineerChristopher Chestnut, SWEChristopher Schmidt, Senior Software EngineerChristopher Suter, Senior Software EngineerChristy Cui, Software EngineerChuol Deng, Technical Program ManagerClaire HansfordClaudia McKenzieClay Murphy, Technical WriterClayton Robbins, Diversity Equity Inclusiveness Program ManagerClement Beauseigneur, Software EngineerColden Cullen, Software EngineerCole Murphy, Software engineerColeen Elliott, Technical Product ExpertColin Bottles, AnalystColin Phipps, Site Reliability EngineerColin Raffel, Staff Research ScientistColin Whitmarsh, Software EngineerColine Devin, Research ScientistColm Buckley, Engineering DirectorConan Dooley, Privacy EngineerConner Kasten, Software EngineerConnor Regan, Senior Analyst, YouTube PolicyConrad Parker, Software EngineerCortney Cassidy, Visual Designer and design ethics advocateCraig Chasseur, Senior Software EngineerCraig Citro,Craig LabenzCrystal Gomes, Technical WriterCurtis Belmonte, Software EngineerCyril Diagne, Artist in residenceDamien Desfontaines, Senior Software EngineerDamien Engels, Senior Software EngineerDan Afergan, Software engineer (Ph.D.)Dan Ellis, Research ScientistDan Finnie, Software EngineerDan Garrette, Research ScientistDan Nanas, Program ManagerDan Park, Senior UX DesignerDana Jansens, Staff Software EngineerDania Durnas, Software EngineerDaniel Dressler, Software EngineerDaniel Gillick, Research ScientistDaniel Johnson, AI ResidentDaniel Lepage, Software EngineerDaniel Lucio, Community Impact Manager, Google FiberDaniel Manesh, Software EngineerDaniel Margo, SWEDaniel Margulis, Software EngineerDaniel Mintz, Product ManagerDaniel Papasian, Staff Software EngineerDaniel Ringwalt, Software EngineerDaniel Sherizen, Automation EngineerDaniel Simmons-Marengo, Software EngineerDaniel T. Speckhard, AI ResidentDaniel Toth, Trust & SafetyDaniela Gonzalez, Engineering ResidentDaniele Midi, Senior Software EngineerDanielle Johnson, AnalystDanieta Morgan, Senior Program ManagerDanny Timpone, Technical Solutions ManagerDaphne Jacobsen, Staffing Opperations AssociateDarren Davis, Senior Software EngineerDaryl Ducharme, Software EngineerDave Fayram, SRMDave Watson (Google Health), Staff Software EngineerDavi Barbosa, Staff Software EngineerDavid Chou, Software EngineerDavid Ha, Research ScientistDavid Kao, Senior Software EngineerDavid M. Chess, Software EngineerDavid McLeishDavid Newgas, TPMDavid Smydra, Head, News Content StrategyDavid Soergel, Software EngineerDavid W. Baker, Director of EngineeringDavid Westbrook, Technical WriterDaylan Kelting, Software EngineerDean Schaffer, Product ManagerDeepak Nathani, Pre-Doctoral ResearcherDem Gerolemou, Visual DesignerDerek Faulkner, Senior Software EngineerDevon Hollowood, Ph.D., Software EngineerDi Dang, UXDiana Scholl, creative strategistDiana Wilson , Associate product marketing managerDiego ToledoDionna Glaze, SWE IIIDirk Weissenborn, Research ScientistDirtbag Copyleft, Corporate GruntDivya Pillai, Software EngineerDominique Wimmer, Program Manager, Equity EngineeringDonald Martin, Jr., Social Impact Technology StrategistDonald S. Black, Senior Software EngineerDongoh Park, Ph.D, Product Policy AdvisorDoug Rinckes, Software EngineerDouglas Dollars, Staff Program ManagerDr Gabrielle Anderson, Senior SRE, Dr Gabrielle Anderson, Senior SREDr Josh DeprezDr. Alon Altman, Senior SWE-SREDr. Christof Leng, Site Reliability Engineering ManagerDr. Christoph Best, Computational ScientistDr. Henning Meyer, Senior Research Software EngineerDr. Jadrian Miles, Software EngineerDr. Kimberly Wilber, Software EngineerDr. Mark Mascaro, Senior SREDr. Nicholas Kong, Senior Software EngineerDr. Pierre Fite-Georgel, Engineer Manager IIDrew Mochak, Accessibility QADrew Perttula, SWEDushyant Rao, Research ScientistDustin Tran, Research ScientistDylan Baker, Software Engineer, Ethical AIDylan Hatch, Software EngineerDylan Vener, Software EngineerE. Lily Yu, Technical WriterE. McKean, Program ManagerEamon Gaffney, Software EngineerEdina Bakos, Public Policy ManagerEgo Obi , Snr Program Manager, CorpEngEl Mahdi El Mhamdi, Research Scientist and Ass. ProfessorElfe KuestersEli Brandt, Senior Software EngineerElias M, Software EngineerElijah Soria, Senior Software EngineerElisa Meyer, Specialist EngineerEliza Velasquez, Software EngineerElizabeth Martyn, Analytical LinguistEllen Jiang, Software EngineerElliotte Rusty Harold, Software EngineerElyse Guilfoyle, Program ManagerEmilio Garcia, Responsible Innovation Program ManagerEmily Campbell , Intern Program ManagerEmily Chang, AnalystEmily Conn, Product ManagerEmily Denton, Senior Research ScientistEmily Li, Software EngineerEmily Maier, SREEmily Masten, Software EngineerEmily Rapp, Product ManagerEmily SmithEmily Yeager, Intern Staffing PartnerEmma Beede, UX Researcher, Google HealthEmma Freeman, Software EngineerEmma Jackson, Senior Program Manager, AccessibilityEmmanuel Orsini, Software EngineerErez BibiEric Barbera, News PartnershipsEric Ehizokhale, Product ManagerEric Eslinger, Software EngineerEric Lewis, Software EngineerEric Loreaux, ML SWEEric Miller, Software EngineerEric MintzEric Rinehart, Software EngineerEric Spishak-Thomas, Senior Software EngineerEric Willisson, Software EngineerEric Z, Software EngineerErty Seidohl, Software EngineerEryn TaylorEsther O., Program ManagerEthan Craigo, Software EngineerEva Schlinger, Research Software EngineerEvan Brown, Senior Software EngineerEvan Murphy, Software EngineerEvan Nichols, Software EngineerEyas Sharaiha, Senior Software EngineerFab Turcios, Program ManagerFabienne Brookman-Amissah, Ads Policy, Strategy & Operations ManagerFabrice de Gans-Riberi, Software EngineerFaith GT, Tech WriterFaizan Muhammad, Engineering ResidentFarheen Malik, UX DesignerFarooq C, Data StrategistFede Carnevale, Research ScientistFego Ahia, Software EngineerFernando DiazFiona Lee, 14-year GooglerFlavio, Software EngineerFlorbela Lei, Software EngineerFlorian Koenigsberger, Product Marketing ManagerFlorian Rathgeber, Site Reliability EngineerFrancesco Visin, Research ScientistFrancis Carr, Software EngineerFred BertschGabe Benjamin, Senior Software EngineerGabe Krabbe, Senior SREGabrella Ehioghiren, CSMGabriel Dulac-Arnold, ResearcherGabriel Jimenez, Product Marketing ManagerGabriel Kerneis, PhD, Senior Software EngineerGabriel Portal, Software EngineerGabriel Schubiner, SWEGabriela BeloGalen Corey, Software EngineerGalen Panger, Senior UX ResearcherGarrett Maron, Data AnalystGary Kacmarcik, SWEGaurav Jain, Research EngineerGaurav Singh, Software EngineerGauri Iyer, UX EngineerGautam Hathi, Software EngineerGeethanjali Eswaran, SREGeorge Foster, Research ScientistGeremy Heitz, Senior Staff Software EngineerGhenesis MendezGheorghe Comanici, Research Scientist, DeepMindGianni Gambetti, Software EngineerGill Ward, UXR lead, Wellbeing Lab and FitGlenn ConneryGloria Liou, Product ManagerGloria OdoemelamGrace Lightner, Software EngineerGrace Roller, Software EngineerGrafton Daniels, Software EngineerGraham Rogers, Software EngineerGrant Rodgers, Software EngineerGreg Edelston, Software EngineerGreg Wayne, Research ScientistGregory ClarkGriffin Boyce, Privacy EngineerGuido Trotter, Software Engineering Managergwx, SREHakeem Angulu, Software EngineerHanna Kollo, Software EngineerHannah E. Deen, Test EngineerHannah Pascal, Software EngineerHannah Pho, Software EngineerHansa Srinivasan, Software Engineerharrison clarke, SWEHarrison LHarrison ShanklinHarsh Modi, Software EngineerHayes NeumanHazel Troost, Software EngineerHeather Shaffer, Sr Program ManagerHeidi, Software EngineerHema Manickavasagam, Product Managerhilary nicole, systems analyst, responsible innovationHolly Meadows-Smith, Account ExecutiveHoney P. Rosenbloom, DEI AnalystHugo Larochelle, Research ScientistI'm already listed on the public postIan Gowen, Senior Software EngineerIan McKellar, Senior Software EngineerIgor Mordatch, Research ScientistIke McCreery, Senior Site Reliability EngineerIkechukwu Uchendu, AI ResidentIlham Kurnia, Senior Software EngineerIndia Adams, PMMIris Qu, UX EngineerIsaac Caswell, Research ScientistIsaac Clerencia, Site Reliability EngineerIsaac Schwabacher, Site Reliability EngineerIzzie Zahorian, Experience ResearcherJ Colin Crowley, Software EngineerJ Pratt, Software EngineerJ. LuongJack Ley, Site Reliability EngineerJackie Fahmy, Software EngineerJackie Roessler, Global Product Lead, ShoppingJackson Tolins, PhD, UX ResearcherJaclyn Shea, User Experience ResearcherJacob Austin, AI ResidentJacob Eisenstein, Research ScientistJacob Hobbie, SWEJacob Kaufman-Osborn, Engineering ManagerJacob Reynolds, Site Reliability Engineer, Google CloudJacqueline Leykam, Senior Software EngineerJacqueline Rajuai, Program ManagerJacqueline WuJake Fried, Software EngineerJames Arps, Privacy EngineerJames BradburyJames Driscoll, Privacy & Data ProtectionJames Lin, Software EngineerJames Wexler, Staff Software EngineerJamie Kinney, Solutions Architect, Google CloudJamie Leach , Search Quality LeadJan Botha, Research ScientistJanak Ramakrishnan, Software EngineerJane Brown, Software EngineerJanny Zhang, Search Software EngineerJasmijn Bastings, Research EngineerJason Bice, WriterJason Gurevitch, SWEJasper Louie, Software EngineerJasper Uijlings, Research ScientistJay Conrod, Senior Software EngineerJayanth M, Technical Account Manager, Smart Homes and IoTJazbel Wang, SWEJc Narasimhan, Database EngineerJean-Marc Francois, Senior Software EngineerJed Hartman, Technical EditorJeff Kang, Software EngineerJeff Rodowicz, Software EngineerJeff Warshaw, User Experience ResearcherJeffrey Herman, Senior Software Engineer, YouTubeJeffrey Lyman, Software EngineerJeffrey Yasskin, Chrome Standards EngineerJen Carter, Product ManagerJen Harvey, Head of Brand and Storytelling, Accelerator ProgramsJennaJenna V, Software EngineerJenni Kilduff, Software EngineerJennifer Daniel, DesignerJennifer Pierre, User Experience ResearcherJennifer Thakar, Software EngineerJennifer Wang, Software EngineerJenny DeMarie, RecruiterJenny Hamer, AI Resident, Google ResearchJenny Ware, User Experience ResearcherJeremy Archer, Software EngineerJeremy Lee, SWEJeremy Salwen, Software EngineerJerilyn Franz, Site Reliability EngineerJerrod Howlett, Product Solutions Lead, Google Ad ManagerJerry Wu, SWEJess Holbrook, Co-founder, People + AI Research at GoogleJess Muskin-Pierret, SWEJessan Hutchison-Quillian, Senior Software EngineerJesse Engel, Staff Research ScientistJessica Ahoni, Program Manager, Area 120Jessica Hamrick, Research Scientist, DeepMindJessica Hyde, Trust & SafetyJessica KirchnerJessica Schrouff, Research Scientist  BrainJesus Castellano, Search Product SpecialistJimmy Hastings, Software EngineerJimmy Nugent, Software EngineerJiri HronJJ W.Joao Pedro Goncalves, Senior Staff Site Reliability EngineerJoel Wasserman, Software EngineerJoelle Skaf, Staff Software EngineerJoey Parrish, TLM, SWEJohan Ferret, Student ResearcherJohanna, SWEJohn Kaster, Senior Software EngineerJohn Panzer, SWEJon Chong, Strategic Partner ManagerJonas Jongejan, Creative TechnologistJonathan Endale, Technical Program ManagerJonathan Francis, Data Lead, Google Creative Technology TeamJonathan Huang, Research ScientistJonathan Schuster, Software EngineerJong Hyuk Choi, Ph.D, Solutions Architect, AI & Machine LearningJono Sadeghi, APMMJorg Barfurth, Software EngineerJoseph Rollins, Software EngineerJosh Belanich, Software EngineerJosh Hernandez, SWEJoshua Marxen, Software EngineerJoshua O'Madadhain, Technical Debt CollectorJosieKate Cooley, SWEJoy Zhong, Software EngineerJoyce Pan, Senior Software EngineerJozef Janovsky, Trust & SafetyJP Sugarbroad, Senior Software EngineerJulia BarrettJulia Elliott, Program ManagerJulia Haines, UX ResearcherJulia Kelly, AnalystJulia Kreutzer, Research ScientistJulia Li, Channel Partner ManagerJulia Tufts, Software EngineerJulian Gruber, Policy Escalation SpecialistJulian Modesto, Software EngineerJulie Xia, software engineerJunaedy Liem, Strategy & OpsJusti Sexton , EducatorJustice Cohen, DesignerJustin Cosentino, Research Software EngineerJustin Green, SWEJustin SybrandtJustine Owen, Technical WriterJZ, Staff UX ResearcherK. GordonKacey Saff, Software EngineerKai Ninomiya, Senior Software EngineerKaila Kilwein, Account ManagerKaitlin Huben, Staff Software EngineerKara Levy, Software Engineering ManagerKaran M, Product ManagerKaren Feng, Staff Software EngineerKarina Leal, Global Commodity ManagerKarl Dudfield, Software EngineerKarla Barrios Ramos, RecruiterKarol Hausman, Research ScientistKarthik Kribakaran, Software EngineerKat E., Market ResearcherKate Donahue, Software EngineerKate Kenneally, Software EngineerKate Meizner, User Experience ResearcherKatharina Lindenthal, Program ManagerKatherine Ann Heller, Research ScientistKatherine LeeKathi R Kitner, Staff Researcher/AnthropologistKayla Schlechtinger, Software EngineerKeithe Bennett, Technical Operations ManagerKelly Zeh, Program ManagerKelsey Robb, Account StrategistKelvin Lee, Software EngineerKen ChengKendall Marks, Staff Software EngineerKendra Williams, Program ManagerKeni Herman , Instructional DesignerKenneth Knowles, Staff Software EngineerKeren Gu, Research Engineer (DeepMind)Kerri ConnollyKevin Dickerson, Customer EngineerKevin G., Software EngineerKevin Most, Software EngineerKevin Sunga, AnalystKevin White, Program ManagerKevin Wilson, Software EngineerKhalida Abdulrahim, Responsibility LeadKieran Murphy, AI ResidentKim Stachenfeld, Research Scientist, DeepMindKinda Akash, UX Motion Design LeadKira Lauring, Software EngineerKirk Boyer, SWEKree Cole-McLaughlin, Software EngineerKris Kennaway, Staff Site Reliability EngineerKris Popendorf, PhD, SWEKristy Lyons, Brand Marketing ManagerKuba Piwowar, Digital Acceleration Lead, Technology Researcher (Google, SWPS University)Kwasi Mensah, Senior Software EngineerKyle Dhillon, Senior Software EngineerKylee Gilman, Software EngineerL0R4 Hurtig, Program ManagerLadan MohamedLara Harrow, software engineerLarry Adams, Senior Product ManagerLarry R., SWELaura O'BrienLaura White-Avian, SRE  SWELauren Allen, Program ManagerLauren Celenza, Senior DesignerLauren Fernandez, Manager, YouTubeLauren Sant'Anna, Administrative Business Partner, Google ResearchLauren Siegel, Engineering ResidentLauren Wilcox, Staff Researcher, Google / Associate Professor, Georgia TechLaurent Dinh, Research Scientist, Google BrainLea Coligado, Software EngineerLeah Cole, Developer Relations EngineerLeila Zwanziger, Software engineerLeon Scroggins III, Senior Software EngineerLeslie Ogoe, UX Program ManagerLexi Walker, Software EngineerLexie Judd, Product ManagerLiam Hopkins, Systems AdministratorLilian YoungLillian C., Product ManagerLindsay Hall, Staff Software EngineerLindsay Nuon , Security, Privacy, Mergers & AcquisitionsLindsey, EngineerLio Benz, Interaction DesignerLisa Bao, Software EngineerLisa Dawdy-Hesterberg, Quantitative UX ResearcherLisa Guinn, Google Cloud Technical TrainerLisa Hirsch, Technical WriterLisa LLisa Wang, Software EngineerLiza Burakova, Security EngineerLJ ErwinLogan, Software EngineerLoic Matthey, Staff Research ScientistLori K, Software EngineerLori Williams, Software EngineerLotanna Okoli, Software EngineerLouis DeScioli, UX EngineerLu Zeng, Site Reliability EngineerLucas Sanders, software engineerLuciano Martins, Machine Learning SpecialistLucy Fox, Software EngineerLucy Hughes, UX leadLucy Vasserman, Staff Software EngineerLuis Bruno, SRELuke Farrell, Associate Product ManagerLyla M. Fujiwara, Software EngineerLynn Jepsen, UX EngineerLynne Goerner, Software EngineerM. Paul Weeks, Software EngineerMaarten Bosma, Software EngineerMackenzie Clark, Software EngineerMaddie Gaither , RecruiterMaddie Stone, Security EngineerMadeleine Clare Elish, Senior Research ScientistMadeline Lein, SWEMadeline SherwoodMaggie Hodges, User Experience ResearcherMaggie Witzenburg, Software EngineerMaia Deutsch, Product Manager, Open SourceMalaika Handa, Software engineerMalley Oberle, Program ManagerMalorie Gilbert, Program ManagerManas Tungare, Tech Lead / ManagerMandy Waite, Developer Relations EngineerManfred Georg, Staff Software EngineerMangpo Phothilimthana, Senior Research ScientistManu Cornet, Software EngineerMarc Ettlinger, LinguistMarc Henson, Program ManagerMarco Tulio PiresMarcos Boyington, Staff Software EngineerMarcus Little, Marketing Manager, Google CloudMargaret Meyerhofer, Senior Software EngineerMaribeth, Research EngineerMarie Collins, Business AnalystMarija Ivica, SWEMario Tanev, Software EngineerMark Brody, SWE-SREMark Diaz, Research ScientistMark Dudley, Software EngineerMark Kahugu , Site Reliability EngineeringMark Nichols, Senior Software EngineerMark Young, DesignerMartin Gracia Jr., SWEMary, Senior Software EngineerMary Gardiner, Site Reliability EngineeringMatias Pelenur, Staff Software EngineerMatt Casey, Staff Software EngineerMatt Diaz, UX ResearcherMatt MagerMatt Rubin, Senior Software EngineerMatthew Albright, Staff Software EngineerMatthew Dempsky, Senior Software EngineerMatthew Hayes, Software EngineerMatthew Kroen, Software EngineerMatthew Lawlor, Staff Software EngineerMatthew Slane, Software Engineer, Engineering ProductivityMatthew Symonds, Software EngineerMatthew Watson, Software EngineerMatty Williams, Technical WriterMaurice Kenji Clarke, Senior UX Designer, GoogleMax Shenfield, Software EngineerMaya Lekova, Software EngineerMeana Kasi, Program ManagerMegan , UXRMegan Devlin, Program ManagerMehdi Mulani, Software EngineerMelanie DeJong, Technical WriterMelissa, Software engineerMelissa Galonsky, Melissa GalonskyMelissa HernandezMelissa Kohl, Software EngineerMelissa Morales , Partnerships Manager, GoogleMelissa Rodriguez, Security Engineering ManagerMercedes Jenkins, Global DEI StrategistMeredith Slota, Engineering ManagerMicah Lamdin, Senior Software EngineerMichael C., Quantitative UX ResearcherMichael Cerrato, Site Reliability EngineerMichael De Rosa, Manager, Payments SREMichael Fahey, Software EngineerMichael Fountaine, Technical WriterMichael Kreins, LegalMichael L, SWEMichael VanBemmel, Site Reliability EngineerMichael VartanMichael WiczerMichelle Ahn, Software EngineerMichelle Carney, Sr UX Researcher, Google AIMichelle Casbon, Senior Software EngineerMichelle Dimon, Research ScientistMichelle GMichelle Lee, Account ManagerMiguel Barreto, Software EngineerMike Dory, User Experience EngineerMike Humphrey, UX Designermike nonemacherMike Perkowitz, Software EngineerMikita Belahlazau, Software EngineerMiles Hinson, Software EngineerMio Milenkovic, SWEMiriam Melnick, Software EngineerMiriam Zimmerman, Software EngineerMisty Masvalo, Children's Center TeacherMitchell Gu, Hardware EngineerMJ Pham, Googler & Former U.S. diplomatMohammad Hamze, Associate PrincipalMohammad Mahdian, Senior Staff Research ScientistMushfeq KhanN Garg, Software EngineerNa'kia Channey, Program ManagerNaiara Rocha , Program ManagerNancy Chang, Senior Research ScientistNando de Freitas, Research ScientistNaomi Black, Senior Technical Program ManagerNatalie Collina, Software EngineerNatalie Harris, Research EngineerNatalie Schluter, Senior Research ScientistNatalie Sidhom, Software EngineerNatalie Weizenbaum, Senior Software EngineerNatarajan Krishnaswami, Senior Software EngineerNatasha Jaques, Research ScientistNathan Herring, Staff Software EngineerNathan Kiner, Technical Program Manager, Google ResearchNathan Perry, Software EngineerNavi DhaliwalNegar Rostamzadeh, Research ScientistNgozi Harrison, Partner Development, YouTubeNic Taylor, SWENicholas Kwon, Software EngineerNicholas Welna, Software EngineerNick Donovan, User Experience ResearcherNick Felt, Software EngineerNick G, Staff Software EngineerNicki Anselmo, Program ManagerNico Sallembien, I18n Software EngineerNicolas D, Software EngineerNicolas Le Roux, Research scientistNicolas MayorazNicolas Pena Moreno, Software EngineerNicolas Raoul, Technical Solutions ManagerNicole Limtiaco, Software EngineerNicole Power, Software engineerNicole Youssef, Legal Policy SpecialistNidhi Hebbar, PMNinhursag C., Software EngineerNitesh Donti, Senior Software EngineerNithum ThainNithya Sambasivan, HCI researcherNitya Priyahita, Policy Program ManagerNM Amadeo, Software EngineerNoah Broestl, Technical Program ManagerNoah Kareus, Senior UX DesignerNoah Rouleau, Product Manager: Ads Experimentation, Analysis, & DebuggingOlga PrilepovaOlivia Gann, Mechanical Design EngineerOlivia Redfield, Senior LinguistOmnia Saed, Sr. Staffing Operations AssociateOscar Ponce, Software EngineerOwen Lytle, Software EngineerOwen Williams, Senior Software EngineerOzzie Aery Fallick, Software EngineerP. Varet, Site Reliability EngineerP.Y. Laligand, Software EngineerPablo Samuel Castro, Staff Research Software EngineerPaige Pritchard, UX ResearcherParker Barnes, Product ManagerPascal Lamblin, Software DeveloperPat Verga, Research ScientistPatrick Conner, Senior Software EngineerPatrick Coskren, Senior Software EngineerPatrick Rabuzzi, Program ManagerPaul M. Willis, LinguistPaul MannPaul Mougin, Software engineerPaul Wankadia, Systems EngineerPaula Le, UX DesignerPaulo W, engPazia Bermudez-Silverman, Software EngineerPedro Gonnet, Senior Software Engineer (Research)Penporn Koanantakool, Senior Software EngineerPeta-Gay Clarke, GooglerPeter Battaglia, Senior Staff Research Scientist, DeepMindPeter Brooks, Software EngineerPeter Kasting, Senior Software EngineerPeter LonjersPeter Norvig, Research DirectorPeter Wubbels, Software EngineerPhil Gross, Senior Software EngineerPhilip Braunstein, Software EngineerPhillips Mitchell, Product SpecialistPia ChamberlainPierre St Juste, PhD, Senior Software EngineerPiotr Mirowski, Staff Research Scientist, DeepMindpizza person, Senior Software EngineerPoki Chui, Strategic Cloud EngineerPramook Khungurn, Software EngineerPrateek Kohli, AnalystPrecious UgwumbaPriya Gupta, Software EngineerQ. B., Software EngineerQuinn Kennedy, Software EngineerQuirin Niedernhuber, Senior Software EngineerR Diamond, Software EngineerR Quong, EngR. Alex Hofer, SWER. Fox, Site Reliability EngineerRachel ClaflinRachel Farber, Program ManagerRachel Harsley, Senior Software EngineerRachel Keaton, Software EngineerRachel Lightfoot, Marketing ManagerRachel MeyersRachel Saputro, Cloud ISV Growth & StrategyRahn Kim, Software EngineerRaksha Muthukumar, Software EngineerRalf Haring, Senior Technical Account ManagerRamana Kumar, Senior Research Scientist, DeepMindRandall Bosetti, Software EngineerRapha Gontijo Lopes, Research AssociateRaquel Small-Weikert, Legal SpecialistRasmi ElasmarRaymond Xu, Machine Learning Software EngineerRazi ShabanRebecca Ackermann, DesignerRebecca Kelly, SWERebecca Mason, Software EngineerReed Fernandez, Logistics LeadRegina NReilly Grant, Staff Software EngineerRene Hendrix, Program ManagerRhett Robinson, Staff Software Engineer, 14-year GooglerRianne van den Berg, Research scientistRichard Hustvedt, Site Reliability EngineerRichard Zhang, Research EngineerRigel Swavely, Software Engineer, Machine LearningRiley Wong, ML SWERJ Skerry-Ryan, Staff Software EngineerRK Popkin, Group Product ManagerRob RamkishunRob Ruenes, Senior Software Engineer, Google NewsRob Whitaker, Software EngineerRobel MengistuRobert Hunter Jr., Software EngineerRobert Nelson, Account StrategistRobert Sturrock, Financial AnalystRobert Sumi, Software EngineerRoderick Bovee, Senior Software EngineerRoland McGrathRomain, Software EngineerRony Patel, UX ResearcherRose Hogenson, SWERose Nguyen, Technical WriterRose V., Software engineerRosie Kerwin, Software EngineerRuby Chang, Google Cloud for EducationRupert Shuttleworth, Senior Software EngineerRussell WhiteRyan Anderson, Staff Site Reliability EngineerRyan C, Software EngineerRyan Garza, Global Product LeadRyan Lester, Software EngineerRyan Mather, Interaction DesignerRyan Metcalf, Corporate Operations EngineerRyan Ouellette, Software EngineerRyan Russell, Software EngineerS-R Hong, Software EngineerSage LaTorra, Engineering ManagerSalahodeen Abdul-Kafi, Product ManagerSalim Virji, Site Reliability EngineerSam Castle, Software EngineerSam Heft-Luthy, Product Manager, Privacy & Data ProtectionSam RichardSamir Datta, Product AnalystSandra, SWESanura N'Jaka, Software EngineerSara Latorre, SWE apprenticeSarah Bell, Senior Staff Software EngineerSarah de Haas, Research Program ManagerSarah Llewelyn, Software EngineerSarah Meiklejohn, Staff Research ScientistSarah Pratt, Site Reliability EngineerSaravanan Ganesh, Software EngineerSasha Ayvazov, Software EngineerSasha Blair-Goldensohn, Phd, Senior Software EngineerSawyer Thompson, Software EngineerScott French, Software Engineer, Site ReliabilityScott N, SWEScout O'Beirne, DEI Program ManagerSedem Tay-Agbozo, Product ManagerSelena Christian, Senior Enterprise Program ManagerSerena ChenSerena Wang, Software EngineerSergio Guadarrama, Staff Software EngineerSeth T, Software EngineerShane Hansen, Customer EngineerShane McDaniel, Senior Software Engineer, 13-year GooglerShanelle Roman, Product ManagerShant StepanianShanthanu Bhardwaj, Software EngineerSharon L, Software EngineerShashank Ramaprasad, Software EngineerShawn Tabai, Senior Software EngineerShayna Pepin, Program ManagerSheldon Sandbekkhaug, Software EngineerShelly Bensal, Research Engineer, DeepMindShiv ParikhShreenath Regunathan, ProductShreya Agrawal, Software EngineerShubha Raghvendra, Product ManagerShubha Rajan, Developer Programs EngineerSiamak Tazari, Staff Software EngineerSimeon Vincent, Developer AdvocateSimon Kornblith, Research ScientistSimone WSkip Allums, UX Design ManagerSonali R, Software EngineerSonya Alexandrova, Software EngineerSophia O, SWESpencer Small, Software EngineerSRStacey Balter, UX ResearcherStephan Hoyer, Senior Software EngineerStephane Henriot, Software EngineerStephanie Brito, Software EngineerStephanie DiBenedetto, Site Reliability EngineerStephanie Stroka, Software EngineerStephanie TStephen Hays, PhD RecruiterStephen Koo, SWESteve Hardt, Software Engineer, PrivacySteven Bills, Software EngineerSteven Croop, Customer Engineer, Google CloudSufy, Software EngineerSumudu Fernando, Senior Software EngineerSusan Ashlock, Software Engineering ManagerSusanna Zaraysky, Content StrategistSverre Rabbelier, Senior Software EngineerSybil Lewis, Policy SpecialistSydney Hodge, Software EngineerSylvia K. Isler, D.Sc., Senior Engineering ManagerT.O., TLTabitha Yong, Senior Designer at Google AITamina Pitt, Software EngineerTamsyn Waterhouse, Senior Software EngineerTaviaTaylor Killian, Graduate Student and Student Researcher; University of Toronto, Vector Institute and GoogleTed Klimenko, Policy SpecialistTed McCarthy, User ResearcherTed Stein, Software EngineerTeddy Katz, Software EngineerTess Eisenberger, Senior Software EngineerTH, Software EngineerThinh Ha, Strategic Cloud EngineerThom Nelson, Software EngineerThomas Brovelli, LinguistThomas Lapotre, SWE, YouTubeThomas O'Brien, Research ScientistThomas Wouters, Senior Software EngineerTiffany Deng, Program ManagerTiffany Sun, SWETim Aidley, Software EngineerTim Harley, Research EngineerTim Hesterberg, Senior Data ScientistTim Kuehn, Senior Software EngineerTim SchusterTim Wilde, Senior SRETina L. Zeng, Interaction DesignerTitouan Rigoudy, Software EngineerTodd Layton, Software EngineerTom Murray, Software EngineerTom Schaul, Senior Staff Research Scientist, DeepMindTom Wang, Software EngineerTony EffikTony Fader, Staff Software EngineerTony Shih, SWETony Young, Senior Software EngineerTravis Scholtens, EngineerTrina Sarkar, Software EngineerTristan, Software EngineerTurner Hayes, Frontend Software DeveloperTyler Bleuel, Associate Product Marketing ManagerTyler Liechty, Data EngineerTyler Morse, Staff Software EngineerValentina Nesci, Diversity, Equity and InclusionValerie Nguon, Software EngineerValerie O'Brien, Analytical LinguistVenkat Karun, Staff Software Engineer, Search Ranking, Google BrasilVerna Coleman, Sales Team LeadVeronica B, Technical Solutions ConsultantVicki Tardif, Staff OntologistVictorVictor Alvarez, SREVictoria, Software EngineerVictoria Krakovna, Senior Research Scientist, DeepMindViet-Tam Luu, Senior Software EngineerVincent D, Program ManagerVincent Dumoulin, Research ScientistVincent J. Hellendoorn, Visiting FacultyVinesh Kannan, Software EngineerVinodkumar Prabhakaran, Research ScientistViorica Patraucean, Research ScientistVladimir Yakunin, Software EngineerWalker Aumann, Staff SREWendly Saintil, Software EngineerWesley Raphael Jr., StrategistWill Hayworth, Site Reliability EngineerWill OurslerWilliam Chargin, Software EngineerWilliam Hopkins, Data EngineerWilliam W. Cohen, Principal Scientist, GoogleWillis H., Software EngineerWillis Zhang, Google Cloud Customer Engineerwilmer@, SREWren Middleton, Senior SWEX. Eyee, Outreach Lead  Responsible InnovationXavier Pujol, Software EngineerYara Ibrahim, Business Intelligence AnalystYash Sekhon, Software EngineerYefim, Software EngineerYinfu Chen, SWEYiu-Cho Leung, Software EngineerYousef Saed, Technical Program ManagerYuri, Software EngineerYuri Grinshteyn, Site Reliability EngineerYuri Iwahara, Associate Product Marketing ManagerYusef Shafi, Staff Software EngineerZ Edens, SREZach, SWEZach Howell, Software Engineer IIIZach LoafmanZach Parent, Software EngineerZachary Butler, Technical solutions consultantZachary Sam Zaiss, UX Research ManagerZachary Walz, Privacy SpecialistZahra Khan, Systems Engineer, WaymoZan Armstrong, User Experience EngineerZara Brownless, UX Researcher, Trust & SafetyZee Fryer, AI ResidentZelda Mariet, Research ScientistZoe Winkworth, Software EngineerZora Tung, Software Engineer
Academic, civil society, and industry supporters:
Meredith Whittaker, Faculty Director, AI Now Institute, Minderoo Research Professor, NYUMar Hicks, Ph.D., Associate Professor of History of Technology, Illinois Institute of TechnologyRam Shankar Siva Kumar, Berkman Klein Center for Internet and Society at Harvard UniversityMutale Nkonde, Fellow Digital Civil Society Lab, StanfordDr. Mona Sloane, Fellow, Institute for Public Knowledge, New York UniversityJack Poulson, Executive Director, Tech InquiryKeith O'Hara, Bard CollegeJevan Hutson, University of Washington School of Law '20Ben Tarnoff, Co-Founder, Logic MagazineIrene Knapp, Tech InquiryKate Henne, Professor and Director, School of Regulation and Global Governance, The Australian National UniversityAudrey Beard, AI / ML software engineer, and Co-Founding Member of the Coalition for Critical TechnologyLilly Irani, Associate Professor, UC San Diego and Program Co-Chair, ACM Fairness Accountability and Transparency Conference 2021Forough Poursabzi-Sangdeh, MicrosoftVictoria Vassileva, Arthur AIPaola Ricaurte Quijano, Berkman Klein Center for Internet and Society at Harvard UniversityLiz Fong-Jones, Principal Developer Advocate, honeycomb.io; former GooglerKendra Albert, Harvard Law SchoolLeif Hancox-Li, Responsible AI, CDML/VaLT, Capital OneJonathan Garcia, Senior Director of Programs & Inclusion at Hack the Hood.Rebekah Tromble, PhD, Director, Institute for Data, Democracy & Politics, George Washington UniversityWilliam Agnew, University of Washington SchoolRoya Pakzad, Founder, TaraazJulien Cornebise, Ph.D., Honorary Associate Professor, University College LondonSuresh Venkatasubramanian, Professor, School of Computing, University of UtahDr. Beth Semel, MIT Emanuel Moss, Researcher, AI on the Ground Initiative, Data & Society Research InstituteJanet D. Stemwedel, Ph.D., Professor of Philosophy, San Jose State UniversitySherri Rose, Associate Professor, Stanford UniversityElizabeth Chin, PhD Candidate, Stanford UniversityDr. Florian Krautli, University of ZurichBrian M. Bot, Principal Scientist, Sage BionetworksCrystal Lee, MIT, Berkman Klein Center for Internet and Society at Harvard UniversitySergio ""Checo"" Gonzales, Stanford University, PhD Student, Biomedical InformaticsSarah Pickman, Doctoral candidate, History of Science and Medicine, Yale UniversitySophie Waldman, XooglerLucy Low, University of WaterlooGabriella Coleman, Wolfe Chair in Scientific and Technological Literacy, McGill UniversitySamuel Adrian Massey III, Principal UX Designer, HERE TechnologiesDawna Bagherian, PhD Candidate, California Institute of TechnologyRebecca Rivers, Research Engineer, NYUHenry Farrell, SNF Agora Professor of International Affairs, Johns Hopkins SAISSarah Fox, Assistant Professor, School of Computer Science, Carnegie Mellon UniversityBenjamin VanderSloot, Assistant Professor of Computer Science, University of Detroit MercyJennifer Jacobs, Assistant Professor of Media Arts and Technology, UC Santa BarbaraDr. Ignatius Ezeani, Lancaster University, UKSusan Wehling, Professor of Spanish, Valdosta State UniversityAlyssa Smith, MIT 2017 alumNaj Austin, Founder/CEO of Ethel's club & Somewhere GoodAnna Lauren Hoffmann, Assistant Professor, University of WashingtonGautam Machiraju, Biomedical Informatics PhD Candidate at Stanford UniversityElizabeth Kaziunas, Ph.D., Ph.D., AI Now Institute, New York UniversityLauren Wolfe, Research Data Specialist, Fred Hutchinson Cancer Research CenterHannah Mieczkowski, PhD Candidate in Communication, Stanford UniversityAnima Anandkumar, Bren Professor, California Institute of TechnologyDr. Ben Wagner, Assistant Professor | TPM | TU Delft Faculty of Technology, Policy and ManagementRoban Hultman Kramer, Machine Learning Engineer, StripeColin Rhinesmith, Associate Professor and Director, Community Informatics Lab at Simmons UniversityJulienne LaChance, PhD Candidate in Mechanical and Aerospace Engineering, Princeton University, and Lead Instructor of Princeton AI4ALL Bhaskar Mitra, MicrosoftRoel Dobbe, Assistant Professor, Faculty of Technology, Policy and Management, Delft University of TechnologyMiles Brundage, OpenAICiamac Moallemi, William von Mueffling Professor of Business, Columbia UniversityDr. Sasha Costanza-Chock, Faculty Associate, Berkman-Klein Center for Internet & Society, Harvard UniversityLachlan Simpson, Systems Administrator, University of NSWZachary Terner, Research Associate, National Institute of Statistical Sciences (NISS)Joy Buolamwini, Founder, Algorithmic Justice LeagueMichael Veale, Ph.D., Lecturer in Digital Rights and Regulation, UCLDaniel Schwarz Piotr Mirowski, Staff Research Scientist, DeepMindJonathan Zong, Ph.D. Candidate, Massachusetts Institute of TechnologyJeremy Howard, Founding Researcher, fast.ai Distinguished Research Scientist, University of San FranciscoJill Dimond, PhD, Sassafras Tech CollectiveGabriel Grill, PhD Student, University of Michiganfederica bianco, University of DelawareSarah Myers West, Ph.D., Postdoctoral Researcher, AI Now InstituteAlicia DeVos, PhD Student, Human-Computer Interaction Institute, Carnegie Mellon UniversityJon Pincus, CTO, The Nexus TodayDr. Alexandra Chassanoff, Assistant Professor, North Carolina Central University Dr. Siddharth Garg, Institute Associate Professor of Electrical and Computer Engineering, New York UniversityNick Seaver, Assistant Professor, Tufts UniversityAngelina Wang, PhD Student, Princeton UniversityInioluwa Deborah Raji, Fellow, Mozilla FoundationAshok Khosla, President Khosla Foundation and Trust, Adjunct Professor, University of Alaska, Past Managing Director, Apple IndiaAna Brandusescu, Professor of Practice, Centre for Interdisciplinary Research on Montreal, McGill UniversitySeth Erickson, Dr. Britt Paris, Assistant Professor, Department of Library and Information Science, Rutgers UniversityLuke Stark, Assistant Professor, Faculty of Information and Media Studies, University of Western OntarioVaroon Mathur, Research Fellow, AI Now Institute and Engelberg Center on Innovation Law and Policy NYUArivuchelvan G, Software EngineerLiya Weldegebriel, Doctoral Candidate, Environmental Engineering, UC Berkeley Thomas Varsavsky, PhD Student, University College LondonJoe Futoma, Research Scientist, Apple Khimya Khetarpal, PhD Candidate, McGill University, Mila MontrealAndrew Guthrie Ferguson, Professor of Law, American University Washington College of LawJessica Hammer, Assistant Professor, HCII & ETC, Carnegie Mellon UniversityHelen Tilley, Associate Professor, History of Science and African Studies, Northwestern UniversityTina Eliassi-Rad, Professor, Northeastern UniversityTina Fetner, Professor of Sociology, McMaster UniversityAna Marasovic, postdoc, Allen Institute for AIArvind Satyanarayan, Assistant Professor, MIT CSAILDeepta Rajan, IBM ResearchDr. J. Nathan Matias, Citizens and Technology Lab, Cornell UniversityRumman Chowdhury, CEO, ParityNaomi Schiller, Associate Professor of Anthropology, Brooklyn College and CUUY Graduate CenterApril Sagan, PhD Candidate, Rensselaer Polytechnic InstituteNaomi Schiller, Associate Professor of Anthropology, Brooklyn College and CUUY Graduate CenterJulia Rhodes Davis, Senior Advisor, Algorithmic Justice LeagueDaniel Lowd, Associate Professor, Department of Computer and Information Science, University of Oregon Suchin Gururangan, PhD candidate, University of WashingtonEden Medina, Associate Professor, MIT Program in Science, Technology, and SocietyWonyoung So, PhD Student, Department of Urban Studies and Planning, MIT Nicholas Selby, Graduate Student, MITGillian Smith, Associate Professor, Computer Science, Worcester Polytechnic InstituteShireen, Founder of Stop Online Violence Against Women Inc.Lily Xu, PhD Candidate, Harvard UniversityShauna Gordon-McKeon, Tech InquiryYonatan Bisk, Assistant Professor, School of Computer Science, Carnegie Mellon UniversityEric Robsky Huntley, Ph.D., Lecturer in Urban Science and Planning, Massachusetts Institute of TechnologyMichael Miller Yoder, PhD Candidate, Language Technologies Institute, Carnegie Mellon UniversityJulian Posada, PhD Candidate, University of TorontoShireen, Founder, Digital Sisters/as & Stop Online Violence Against WomenElias Khalil, Assistant Professor, University of TorontoJessie J. Smith, PhD Student, University of Colorado BoulderErin LeDell, Chief Machine Learning Scientist at H2O.ai; Founder of Women in Machine Learning & Data Science (WiMLDS)Ross Teixeira, PhD Student in Computer Science, Center for Information Technology Policy, Princeton UniversityPiotr Sapiezynski, Associate Research Scientist, Northeastern UniversityM. R. Sauter, Assistant Professor, College of Information Studies, University of Maryland  College ParkRachel K. Walker, PhD, RN, FAAN, Associate Professor & PhD Program Director, College of Nursing, University of Massachusetts AmherstTodd Wolfson, Rutgers University & Media, Inequality & Change Center (MIC)Evan Selinger, Professor of Philosophy, Rochester Institute of TechnologyEthan Goan, PhD Candidate, Queensland University of TechnologyMatt Rafalow, Senior Researcher, YouTube ResearchSwabha Swayamdipta, Postdoctoral Investigator, AI2Amber Solomon, PhD Candidate in Human-Centered ComputingDelia Shelton, NIH NIEHS K99/R00 FellowRazvan Amironesei, PhD, Data Ethics Research Fellow, University of San FranciscoBrad Weslake, Associate Professor of Philosophy, NYU ShanghaiLauren Chambers, Staff Technologist, ACLU of MassachusettsDr. E'lana Jordan, Qualitative/Ethnographic ResearcherSamir Gadre, PhD Student, Columbia UniversityEmma Kaywin, Doctoral Student  Health Education, Health and Behavior Studies, Columbia Teachers CollegeShamika Goddard , Doctoral Student, Department of Information Science, University of Colorado, BoulderKareem Estefan, PhD candidate, Modern Culture and Media, Brown UniversityRaesetje Sefala, Research Intern, MilaRachel Thomas, PhD, Director, Center for Applied Data Ethics, University of San FranciscoVishal Bakshi, Adjunct Faculty, Engineering and Technology Department, City College of San FranciscoJoshua Loftus, New York UniversityThorsten Busch, University of St. Gallen, Trinity College Dublin & HEC MontrealNaomi Klein, Gloria Steinem Endowed Chair in Media, Culture and Feminist Studies, Rutgers UniversityNicole Hughes, Algorithmic Justice LeagueDr. Cynthia L. Bennett, Research Engineer at Apple, Inc. and Post doctoral Researcher at Carnegie Mellon UniversityJohn Fallot, Co-Founder, Prosocial Design NetworkJonah Dahlquist, Software EngineerAda Worcester, Site Reliability Engineer, former GooglerPaul Duke, Software Engineer; former GooglerConnor Gilroy, PhD Student in Sociology, University of WashingtonAnne Spencer Rosss, PhD Candidate, University of WashingtonJesse Thomason, Assistant Professor, University of Southern CaliforniaNikita Srivatsan, PhD Student, Carnegie Mellon UniversityHal Daume III, Professor, University of Maryland / Senior Principal Researcher, Microsoft ResearchLindsay Weinberg, Clinical Assistant Professor, Honors College, Purdue UniversitySara FitzGerald, Grad Student, University of South CarolinaBrian C. Keegan, Ph.D., Assistant Professor, Department of Information Science, University of Colorado BoulderVikas Gosain, MicrosoftSerife Wong, Founder, Icarus SalonJulie Setele, PhD, MLIS student, University of MissouriLori Williams, Software EngineerQuincy K. Brown, Ph.D, blackcomputeHER.orgAneesha Kommineni, MicrosoftAlexis Baria, data scientist, Direct SupplyKrzys Chwaa, Yale UniversityStephanie Jowett, Lecturer, Queensland University of TechnologySherri Rollins, City Councillor, Fort Rouge East Fort Garry Ward, City of Winnipeg Andrew Sellars, Director, BU/MIT Technology Law Clinic, Boston University School of LawChrista Hartsock, Engineering Manager Code for America, Co-founder Logic MagazineAndy Sellars, Boston University School of LawNancy Clements, RN, BSNLaurence Berland, terminated Google Senior SRE and organizerMichael Gasser, Emer. Assoc. Prof., School of Informatics, Computing, & Engineering, Indiana UniversityCharlie Snell, Undergraduate researcher at Berkeley AI ResearchLisa Dyer, Signing as selfRachel Szabo, PhD student, MITElizabeth M. Adams, Race & Tech Fellow, Center for Comparative Studies in Race and Ethnicity, StanfordJulia Copley, UX Content DesignerTed Pedersen, Professor, University of Minnesota, DuluthAdam M. Smith, Assistant Professor of Computational Media, UC Santa CruzMandy Henk, CEO, Tohatoha Aotearoa CommonsSara Kingsley, School of Computer Science, Carnegie Mellon UniversityPhilip Chodrow, Hedrick Visiting Assistant Adjunct Professor of Mathematics. University of California, Los AngelesSorelle Friedler, Associate Professor of Computer Science, Haverford College; Co-Founder, Conference on Fairness, Accountability, and Transparency; former GooglerSelam Gano, MITRonald Niezen, Professor, McGill UniversityJake Vasilakes, PhD Candidate, University of ManchesterManuel Sabin, PhD, Postdoc at COHUBICOL Project, Radboud UniversityEva Short, Software Engineer at GracenoteAnthony Barranco, Software Engineer, UbisoftWilliam Morris, Director of Data Science, Faraday IncAvanti Shrikumar, Postdoctoral Fellow, Stanford UniversityAnimesh Garg, Assistant Professor, University of TorontoDavid Paulius, Postdoctoral Researcher, Technical University of MunichNeal Patwari, Professor, Electrical and Systems Engineering and Computer Science and Engineering, Washington University in Saint LouisNikhil Dharmaraj, Undergraduate Student, Harvard College '23Nicole E. Weber, PhD Candidate, Rutgers University & AI Now InstituteMonika Viktorova, ConsultantChris Emezue, Research ScientistKyle McDonald, Founder IYOIYORodney Sampson, Executive Chairman and Chief Executive Officer, Opportunity Hub; Nonresident Senior Fellow, Brookings Institution; Keohane Distinguished Visiting Professor, University of North Carolina at Chapel Hill and Duke UniversityBianca Lepe, PhD Student, BE & CSAIL, MITYannik Kumar, Master's student, University of ChicagoIga Kozlowska, MicrosoftAbhishek Das, Research Scientist, Facebook AI ResearchJoshua Feldman, Data Scientist, BlueDot Inc.Michelle Carney, ML + UX Researcher, Berkeley Rodrigo Ochigame, PhD Candidate, Massachusetts Institute of TechnologyJonathan Dinu, Independent Researcher, Jonathan IndustriesMatthew Kay, Assistant Professor, Computer Science and Communication Studies, Northwestern UniversityDan Bouk, Associate Professor, Colgate UniversityKelly B. Wagman, MIT Comparative Media StudiesSebastian Ruf, Postdoctoral Researcher, Northeastern UniversityDr. Magdalena Olszanowski, Concordia UniversityThomas Krendl Gilbert, PhD candidate, UC BerkeleySarah Wylie, UI Engineer Aerica Shimizu Banks, Founder and Principal, ShisoEthan Baker, PhD Candidate, Massachusetts Institute of TechnologyKarina Halevy, HarvardErhardt Graeff, Assistant Professor of Social and Computer Science, Olin College of EngineeringVicente Ordonez, Assistant Professor of Computer Science, University of VirginiaJonah Ko, R&D Engineer, ME/EEHadas Kress-Gazit, Professor, MAE, Cornell UniversityDebashis Sinha, Sound artistSritej Attaluri, Sritej Attaluri, UC BerkeleyBjorn Lutjens, PhD Candidate, Massachusetts Institute of TechnologyMichael Dowden, CEO of Andromeda Galactic SolutionsGwynn Sturdevant, PhD, HarvardShelly Glennon, ex-Googler, founder A Bigger TableAndrew Fitzgerald, PhD Candidate, Stanford UniversityGrant R. Vousden-Dishington, Research Software Engineer at the Anti-Defamation LeagueSusan E. Cuffaro, Founding Member, Gig Workers' CollectiveKentrell Owens, PhD Student, Computer Science and Engineering, University of WashingtonRebecca Alemayehu Lauren Lee McCarthy, Associate Professor and Interim Associate Dean for EDI, UCLA School of Arts and ArchitectureSean McDonald, Co-founder Digital Public and FrontlineSMSAli Alkhatib, Research Fellow, Center for Applied Data Ethics, University of San FranciscoDevin Guillory, Ph.D Candidate UC BerkeleyJennifer Crump Hassan Hijazi, Scientist, Los Alamos National LaboratoryJessica Lee, Carnegie Mellon UniversityCatherine D'Ignazio, Assistant Professor of Urban Science & Planning, MITCraig Ewert Dr. Simone Browne, Associate Professor, University of Texas at AustinAlexander Voss, Lecturer in Software Engineering, School of Computer Science, University of St AndrewsNif Ward, Software Development EngineerVeena Calambur, Data Scientist, PfizerEmily Cunningham, User Experience Designer, Amazon Employees for Climate JusticeDorothy R. Santos, Ph.D. Student, University of California, Santa CruzChloe R. Autio, Lead, Data and Responsible AI Policy at Intel Corporation Miguel Alonso Jr, Florida International UniversityAmandalynne Paullada, Department of Linguistics, University of WashingtonMeareg Hailemariam, Lecturer, Dakar American University of Science and TechnologyMason Kortz, Harvard Cyberlaw ClinicSohini Upadhyay, PhD Student, Harvard University Jason Radford, Principal Research Scientist, Northeastern UniversityPhilip Butler, Seekr Project Founder, Assistant Professor, Iliff AI Institute, Iliff School of TheologyNaLette Brodnax, Assistant Professor of Data Science, McCourt School of Public Policy, Georgetown UniversityMichelle Bakels, Lead Developer and Instructor, Boca CodeJoanne Ma, Graduate Student, Berkeley School of InformationMeital Hoffman, MITAnoush Najarian, NeurIPS Meetup Chair, ICML Virtual ChairShannon McNair, Business Operations ManagerNathan Cooper, Ph.D. Computer Science Graduate Student, The College of William and MaryEvan ""Pete"" Walsh, Allen Institute for AIMomin M. Malik, Ph.D., Data ScientistAmy Shropshire, Faculty in Marketing at Columbus State Community CollegePatrick Durusau, Anne Kavalerchik, PhD Student, Indiana University, Sociology & InformaticsPedro Reynolds-Cuellar, Ph.D Student, MITJakita O. Thomas, Ph.D., blackcomputeHER.orgAtri Rudra, Professor, Department of Computer Science and Engineering,University at BuffaloBrian Tesch, Technical Recruiter, formerly AmazonNeilly H. Tan, PhD Student, University of WashingtonAnastasia Schaadhardt, PhD student, University of WashingtonFanta Traore, Co-founder of the Sadie Collective, dual degree student at YaleJeffrey Gleason, ML Engineer, Kungfu.aiDr. Lee Clement, Software Engineer, OxboticaGretchen Krueger, OpenAILucas Lima, Software EngineerRanaji Deb, ArtistClaire Stapleton, former GooglerFenwick McKelvey, Concordia UniversityVanessa Suarez, Data Engineer, CandidJacob Metcalf, PhD, Data & Society Research InstituteJenny Korn, Founding Coordinator of the Race+Tech+Media Working Group at the Berkman Klein Center for Internet and Society at Harvard UniversityJennifer Strickland, Principal, Jen Strickland DesignMitchell Wortsman, University of WashingtonKellie Owens, Researcher, Data & Society Research InstituteAnjalie Field, PhD Student, Carnegie Mellon UniversityVincent M. Southerland, Executive Director, Center on Race, Inequality, and the Law, New York University School of LawGabriel Grill, PhD Student, University of MichiganIsaac Johnson, Research ScientistRandi Williams, PhD Student, MIT Media LabEva Yezerets, Johns Hopkins University, PhD Student, Biomedical EngineeringDr. Chris Fairless, Postdoctoral Researcher ETH ZurichBaobao Zhang, Postdoctoral Fellow, Cornell UniversityElena Lucherini, Computer Science PhD candidate at Princeton UniversityAmeet Rahane, Research Technologist II, Feinberg School of Medicine, Northwestern UniversityDanyel Fisher, Principal Design Researcher, Honeycomb.ioEmma Bedor Hiland, PhD, School of Communication Studies at James Madison UniversityAlan Mackworth, Professor Emeritus of Computer Science, UBCMeron Feleke, Managing Partner, Impala CommunicationMia Shah-Dand, CEO  Lighthouse3, Founder  Women in AI EthicsTMJenifer Sunrise Winter, Professor, University of Hawaii at ManoaBharat Prakash, PhD Student, University of Maryland Baltimore CountyJulia Silge, Software Engineer, RStudio PBCCaroline Peralta-Neel, UX Designer, ITP-NYU 2020Yolanda A. Rankin, Ph.D.  Florida State University School of InformationRobert Soden, Assistant Professor, University of TorontoJeannette Bohg, Assistant Professor, Stanford UniversityLauren Klein, Associate Professor, Departments of English and Quantitative Theory & Methods, Emory University Tawana Petty, Fellow, Digital Civil Society Lab, StanfordGeoff Korb, Data ScientistSharif Amit Kamran, PhD student, University of Nevada, RenoChelsey Rhodes, PhD Student, University of TorontoIgor (Gary) Rubinov, PhD, co-founder Dovetail LabsRussell Y. Neches, Postdoctoral Scholar, Joint Genome Institute, Lawrence Berkeley National LaboratoryMayowa Oke, Princeton University, Department of NeuroscienceClara Sherley-Appel, UX Writer, AtlassianAngus Galloway, PhD StudentBridget Burns, MIT PhD Student, Department of Urban Studies and PlanningMichael Madaio, Microsoft ResearchJessie Daniels, PhD, Professor, Sociology, Hunter College; Professor, Sociology, Critical Psychology & Africana Studies, The Graduate Center, CUNYDr. Andrew Hamilton-Wright, Associate Professor, School of Computer Science, University of GuelphN. Lewis, Software EngineerHessie Jones, Women in AI Ethics Collective, #100Brilliant Women in AIShivanand Venkanna Sheshappanavar, PhD Student, University of DelawareZineb BelmKaddem, Moroccan human and digital rights activistAlan Mislove, Professor and Associate Dean, Northeastern UniversityNikhil Krishnaswamy, Assistant Professor of Computer Science, Colorado State UniversityRada Mihalcea, Professor, Computer Science and Engineering, University of MichiganJesse Kriss, NetflixKatherine Huang, Massachusetts Institute of TechnologyNari Johnson, Harvard CollegeNatalia Bilenko, Stitch Fix, Queer in AI, Resistance AIRoger McNamee, Author, Zucked: Waking Up to the Facebook Catastrophe, Co-Founder, Elevation PartnersBethany Edmunds, Director of Computer Science, Northeastern UniversityJacob Danovitch, McGill University, MilaDouglas H. King, Senior Research Programmer  The Wharton School of The University of PennsylvaniaProf. Michelle Greene, Bates CollegeMansour AlAnsari, AI/ML Team Fellow at the World Economic Forum's Center for the Fourth Industrial Revolution (C4IR)Roger Allan Ford, Professor of Law, University of New Hampshire; Affiliated Fellow, Yale Information Society ProjectJasmine McNealy, Associate Professor, University of Florida, College of Journalism and CommunicationsKristin Branson, Group Leader, HHMI Janelia Research CampusMichael Correll, Senior Research Staff, Tableau SoftwareHenry M. Clever, Ph.D. Candidate and Roboticist, Georgia Institute of TechnologyKaren Frost-Arnold, Associate Professor of Philosophy, Hobart & William Smith CollegesErin McElroy, Postdoctoral Researcher, AI Now Institute, New York UniversityMelissa Kwan, Harvard CS StudentLydia Daboussi, Postdoctoral Research Fellow, Salk Institute for Biological StudiesKira Goldner, Postdoctoral Fellow at Columbia; MD4SG Co-FounderCarolyn Ge, Harvard Women in CS Co-President, Harvard College '22Victoria Nwobodo, TechnologistJoana M. F. da Trindade, PhD Student, MIT CSAILBen Dodge, Princeton UniversityWill Urmston, Software EngineerSarah Mohamed, Software EngineerRuha Benjamin, Professor of African American Studies, Princeton UniversityAlex Lu, PhD Candidate, Department of Computer Science, University of TorontoHyunjin Seo, Faculty Associate, Berkman Klein Center for Internet & Society at Harvard UniversityLucy Suchman, Professor Emerita, Lancaster University, UKKhalid Kadir, PhD, Continuing Lecturer, UC BerkeleyYousif Hassan, Science and Technology Studies Program, York UniversityCamille Francois , GraphikaWilliam H. Hsu, Professor, Kansas State UniversityNicki Washington, Ph.D., Professor of the Practice of Computer Science, Duke UniversityHassan Kane, Mehari K. Tesfay, PhD Candidate, College of Engineering, University of Nebraska-LincolnTarcizio Silva, Tech + Society Fellow, Mozilla FoundationNicholas Proferes, Assistant Professor of Critical Data Studies, Arizona State UniversityDr Emma L Briant, Associate Researcher in Human Rights at Bard College Anne Jonas, PhD Candidate, University of California BerkeleyScott A. George, MIDS, Energy and Sustainability Data ScientistAdam Darby, Web DeveloperFlorian Golemo, Postdoctoral Fellow, Mila MontrealGrace Dewson, Systems Program Manager, AnitaB.orgGreg d'Eon, University of British ColumbiaChristine Geenng, University of WashingtonAndrew Clement, Professor Emeritus, Faculty of Information, University of TorontoIfeoma Ozoma, Former GooglerSamuel DiBella, Cypurr CollectiveShannon Vallor, Baillie Gifford Chair in the Ethics of Data and Artificial IntelligenceSamarth Sinha, University of TorontoErich Ludwig, Assembly Fellow (2019) @ Berkman Klein Center + MITDesmond Upton Patton, Associate Professor, Columbia University Nataliya Nedzhvetskaya, Doctoral Student, University of California, BerkeleyKade Crockford, Director, Technology for Liberty Program, ACLU of MassachusettsSam Hinds Director of Creative Strategy, Data & Society Sam Hinds, Director of Creative Strategy, Data & SocietyEmma DeSoto Graduate Student at MIT Emma DeSoto, Graduate Student at MITMaria Smith AI Researcher & Ph.D. Student, University of California, Berkeley Maria Smith, AI Researcher & Ph.D. Student, University of California, BerkeleyChristina Wilmot PhD student, UCLA; former Googler Christina Wilmot, PhD student, UCLA; former GooglerRichmond Wong Postdoctoral Fellow, UC Berkeley Richmond Wong, Postdoctoral Fellow, UC BerkeleyKaleem Rahman, Senior Product Manager, MicrosoftJulie Carpenter, PhD, Research Scientist, member of Women in AI Ethics and the Ethics + Emerging Sciences Group.Catherine Farman, President, CFPB Union NTEU 335Ariel Szekely, PhD Student, MIT CSAILCaris Moses, MIT, Graduate StudentSumanth Ratna, Student, Thomas Jefferson High School for Science and TechnologyAlex Bloemendal, PhD, Institute Scientist, Broad Institute of MIT and HarvardKristy Carpenter, PhD student, Stanford UniversityCathy O'Neil, CEO of ORCAATheodora Lau, Founder, Unconventional VenturesLoretta L.C. Brady, Ph.D., MAC, Co-Director, Center for Teaching Excellence, Director, Requity Labs, Saint Anselm College, Manchester, NHPeter Baldes, Associate ProfessorAaron Mendon-Plasek, Columbia UniversityDavid Murakami Wood, Former Canada Research Chair (Tier II) in Surveillance Studies, Associate Professor, Department of Sociology, Queen's University, Kingston, Ontario; Co-editor-in-Chief, Surveillance & SocietyMitra Kiciman, (Incoming) Software Engineer at GoogleSusanna Raj, Cognitive Science/AI Ethics ResearcherEmanuelle Burton, Lecturer in Ethics, Department of Computer Science, University of Illinois at ChicagoSamara Trilling, Aspen Tech Policy Hub fellowTegan Garland, XooglerAndrew A. Adams, Deputy Director, Centre for Business Information Ethics, Meiji UniversityDr. Kobi Leins, Senior Research Fellow in Digital Ethics | Centre for AI and Digital Ethics, Non-Resident Fellow of the United Nations Institute for Disarmament Research, School of Computing and Information Systems, Melbourne School of Engineering, The University of Melbournedanah boyd, Microsoft ResearchSheshera Mysore, Graduate Student, UMass AmherstJamie Haddock, Visiting CAM Assistant Professor, UCLAOdest Chadwicke Jenkins, Ph.D., aaphdcs listserv memberNate Beard, PhD Student, University of MarylandAndy Weinstein, Ethical Tech CEO, Bottler Distribution GroupSeyda Ipek, University of California, IrvineLionel Yelibi, Investment Research Analyst, Pension Reserves Investment Management Board (MassPRIM)Mohamed Abdelhack, Washington University in St. LouisAlex Nguyen, PhD Student, Neuroscience, Princeton UniversityLester Briggins, System AdministratorMorgan G. Ames, UC Berkeley School of InformationSeyda Ipek, University of California, IrvineLionel Yelibi, Investment Research Analyst, Pension Reserves Investment Management Board (MassPRIM)Alexander Wait Zaranek, PhD, Chief Innovation Officer, Curii CorporationDavid Rokeby, Director, BMO Lab for Creative Research in the Arts, Performance, Emerging Technologies and AI, University of TorontoRushi Shah, JD Student at Harvard Law School and PhD Student at Princeton University's Center for Information Technology PolicySudip Upadhyay, DevOps EngineerJana Thompson, Graduate student in UX Design, Maryland Institute College of ArtJoshua Cohen, University of California, Berkeley; Boston ReviewR. Luke DuBois, Associate Professor of Integrated Design & Media, Music Technology, and Interactive Telecommunications, New York UniversityTara Chklovski, CEO, TechnovationStephen Kenyon, Graduate Student, MBS, Rutgers UniversityCindy Hood, Associate Professor of Computer Science, Illinois Institute of TechnologyColin McMillen, PhD; former Staff Engineer at Google ResearchChris Lindgren, Assistant Professor of Technical Communication, Department of English, Virginia TechPriya P. Pillai, Software Engineer, Broad Institute of MIT and HarvardBen Green, University of MichiganAtul Butte, MD, PhD, Representing themselves, but working as Distinguished Professor, University of California, San Francisco, and Chief Data Scientist, University of California HealthSoren Spicknall, The Movement CooperativeJessica L. Feuston, Postdoctoral Associate, University of Colorado BoulderJonny Sun, PhD Candidate, MIT; Berkman Klein Center for Internet and Society at Harvard UniversityJade Abbott, Retro RabbitDr. John Flackett, Head of AiLabNatasha Jaques, Research ScientistEdward Burnell, Postdoctoral Researcher at MIT, formerly at Google [x]Maggie Wang, Harvard CollegeNate TeBlunthuis, PhD Candidate Department of Communication University of WashingtonMeghana Ravikumar, ML Engineer, IntelJed Brown, Assistant Professor of Computer Science, University of Colorado BoulderFlora Amwayi, Patent AttorneyMary Anne Smart, Graduate Student, UC San DiegoSinead Williamson, Assistant Professor, Department of Statistics and Data Science, University of Texas at AustinKathleen Tuite, PhD, University of Washington, UnaffiliatedEbitie Amughan, Talent ConsultantJosephine Hoy, PhD Student, University of WashingtonClaire Kim, Harvard Kennedy School & Harvard Business SchoolEric Wang, Director of Machine Intelligence, TurnitinChrystal Starbird, Ph.D., Postdoctoral Fellow, Yale UniversityMat Rawsthorne, CGMA ESRC PhD Service User Researcher & Text Analytics Lead, NIHR Nottingham Biomedical Research Centre  Mental Health & TechnologyMaria Sumner, Technical Program Manager, FacebookDanae Metaxa, PhD Candidate in Computer Science, Stanford UniversitySavannah Thais, Postdoctoral Research, Princeton Institute for Computational Science and EngineeringLaura Forlano, Ph.D., Associate Professor of Design, Institute of Design, Illinois Institute of TechnologyBen Jackson, Founder, For the WinAdithyan Sujithkumar, Undergraduate Researcher, UC BerkeleyAlice Xiang, Partnership on AIXiaowei R. Wang, UC Berkeley PhD student and Logic MagazineCynthia Matuszek, Assistant Professor, Computer Science and Electrical Engineering, UMBCJoseph Schafer, University of WashingtonSafiya Umoja Noble, Associate Professor and Co-Director, UCLA Center for Critical Internet InquiryDr. Charlton McIlwain, Professor of Media Culture, and Communication, New York University & Founder, Center for Critical Race & Digital StudiesS.Ramasamy, Professor, Addis Ababa Science and Technology University, EthiopiaJingying Yang, Product Design, Partnership on AIKelly Anneken, Content Designer, PinterestAdrian Bauer, Machine Learning Engineer, XooglerJames Ryan, Visiting Assistant Professor, Carleton CollegeRediet Abebe, Harvard Society of Fellows, UC BerkeleyEzinne Nwankwo, Duke UniversityTony Robert Cochran, Writer & Social Critic, Former Union Organizer, Former Communicators Director at Occupy Wall StreetChristo Wilson, Associate Professor, Khoury College of Computer Sciences, Northeastern UniversityAngela VandenBroek, PhD Candidate, Binghamton UniversityFiel Guhit, MFA Design and Technology, Parsons School of Design  The New SchoolAdam Summerville, Assistant Professor, Cal Poly PomonaSonja Solomun, Research Director, Centre for Media, Technology and Democracy, McGill UniversitySarah T. Roberts, Ph.D., Associate Professor, Co-Director, UCLA Center for Critical Internet Inquiry, University of CaliforniaIrene Tema, PhD student, Oregon State UniversityNnamdi Anthony Orduh, Post Graduate student, Applied Artificial IntelligenceRoberto Lopez Cervera, University of MinnesotaNathan Partlan, PhD Candidate, Northeastern UniversityCarlos Castellanos, MFA , PhD, Assistant Professor, School of Interactive Games & Media, Rochester Institute of TechnologyAngelica Parente, Stanford University AlumEric C. Larson, Associate Professor, Department of Computer Science, Lyle School of Engineering, Southern Methodist UniversityDavid F. Green Jr., Ph.D, Associate Professor of English and African American Language Subarna Tripathi, Research Scientist, Intel LabsConlon Novak, Master of Human-Computer Interaction Student at Carnegie Mellon UniversityBradley Leimer, Co-Founder, Unconventional VenturesTina M. Park, Ph.D., Research Fellow, Partnership on AIKwanele Gumbi, Gumbi Global  CEOAndra Keay, Director Silicon Valley Robotics, Founder Women in Robotics, Scholar CITRIS People and RobotsBistra Dilkina, co-Director of USC Center for AI in Society, Associate Professor, University of Southern CaliforniaRajesh Veeraraghavan, Georgetown UniversityRachel Hong, Harvard Women in CS Co-President, Harvard College '21Tonya M. Evans, Professor, Penn State Dickinson Law & Founder, Advantage Evans, LLCJustine Zhang, PhD Student in Information Science, Cornell UniversityJoseph Ko, PhD Student, University of Southern CaliforniaFernando A. Delgado, Cornell University Dr Daniel Angus, Associate Professor of Digital Communication, Digital Media Research Centre, Queensland University of TechnologyBekalu Temesgen, Software EngineerRonald Garcia, Associate Professor, University of British ColumbiaSarah Villeneuve, Program Lead, Partnership on AINausheen R. Shah, Assistant Professor, Dept. of Physics and Astronomy, Wayne State UniversityJohn William Templeton, author Silicon Ceiling 20: Equal Opportunity in High Technology; publisher, Journal of Black Innovation; co-founder, National Black Business MonthFrancois Pelletier, Data ScientistMadison Dunitz, CZI, Software EngineerNina Banks, Associate Professor of Economics, Bucknell UniversityTarleton Gillespie, Microsoft ResearchAlan Ding, computer science student, Princeton UniversityDean Jansen, Executive Director  Participatory Culture Foundation, Affiliate  Data & Society Research InstituteSmai Fullerton, Software EngineerMarlena Wisniak, Senior Advisor, ECNLPaula Ashton, UX WriterRaju RK Penmathsa, IT Solution Architect, Cummins, IncMichael Zimmer, Associate Professor, Marquette UniversityKandrea Wade, Doctoral Student, Information Science, University of Colorado BoulderHassan Asif, PhD student, iSchool, University of TorontoUshnish Sengupta, PhD Candidate, University of TorontoTan Zhi-Xuan, PhD Student, Massachusetts Institute of TechnologyAdam Johnston, Software Engineer, AffirmTibebe Asfaw , Software EngineerKate Crawford, Ph.D., Senior Principal Researcher, Microsoft Research; Co-founder, AI Now Institute, NYU; inaugural Visiting Chair of AI and Justice, Ecole Normale Superieure.Masood Kamandy, Media Arts & Technology PhD StudentPamela Mishkin, OpenAIKemi Bello, Communications Manager, Partnership on AIFlorian Richoux, Senior Researcher, AISTEthan Tsai, PhD Student, ELFIN UCLASloane Davidson, Founder and CEO, Hello NeighborAnne Sullivan, Assistant Professor, Digital Media, Georgia Institute of TechnologyCrystal M. Fleming, Professor of Sociology and Africana Studies, Associate Faculty, Department of Women's, Gender and Sexuality Studies, Stony Brook UniversityNatalie B. Milman, PhD, Professor of Educational TechnologyAmy Zhou, Harvard WECode Co-Chair, Harvard UniversityAmba Kak, Director of Global Policy & Programs, AI Now Institute NYUBen Gansky, NSF-NRT Citizen-centered Smart Cities Fellow, PhD student, School for the Future of Innovation in Society, Arizona State UniversityKrystal Maughan, University of VermontPnar Barlas, Research AssociateDamon McCoy, Associate Professor, NYU Tandon School of EngineeringZeerak Waseem, Ph.D. Candidate in Computer Science, University of SheffieldJon Wasserman, Associate Director, Program Management, Global Lead, Social Impact, frogColin Gray, Assistant Professor, Purdue UniversityBerend Alberts-de Gier, Rotterdam University of Applied SciencesManuel Portela PostDoc Researcher at the Web Science and Social Computing Group, Universitat Pompeu Fabra, SpainLaurent Barcelo, Chief Strategy & Industry 4.0 at VidensCarlos Castillo, Distinguished Research Professor, Universitat Pompeu FabraShady Elbassuoni, Associate Professor of Computer science, the American University of BeirutAlfredo Mendez, Product Strategist, Independent Advisor to C-level scaleups @pmbydesignSasha Luccioni, postdoctoral researcher, Universite de Montreal & MilaDr Ioanna Manolopoulou Associate Professor in Statistics, University College LondonDavid S. Lim, Graduate Student in Computer Science, Stanford UniversityTereza Iofciu, Head Coach Data Science, neuefischeStef Garasto, Lecturer in Data Science, University of GreenwichGina Neff, Professor of Technology & Society, Oxford Internet Institute, University of OxfordAndres Ferraro, PhD Student. Universitat Pompeu FabraLeah Brown, PhD student, Information Science, Western UniversityBrooke Jarrett MSPH, PhD Candidate, Johns Hopkins Bloomberg School of Public HealthAngela Riggs, QA ManagerAli Rigby, Research ScientistIago Bojczuk, Alum, MIT Comparative Media StudiesAbeba Stone, Software ArchitectGarnett Achieng, Research fellow at PollicyMarcos Santan, Fundacao Oswaldo Cruz (FIOCRUZ)Rocco Santoro, senior statistician, DaccudeKim Crayton, Founder of #causeascne and the Antiracist EconomistMerve Hickok, Founder, AIethicist.orgAnibal Monasterio Astobiza, PhD ILCLI-UPV/EHU; LI2FE (Laboratorio de Investigacion e Intervencion Filosofica y Etica), Visiting fellow in Global Health and Social Medicine, Center for Bioethics, Harvard Medical School, Harvard UniversityGreg Wilson, RStudio PBCLouis McCallum, Senior Lecturer, Creative Computing InstituteDr. Catherine Cronin, National Forum for the Enhancement of Teaching and Learning in Higher EducationJosh Cowls, PhD student, Oxford Internet Institute, University of OxfordShira Mitchell, StatisticianJean-Rassaire Fouefack, PhD Candidate, IMT-Atlantique/University of Cape TownVincent Warmerdam, Research Advocate, Rasa Technologies Inc.Felix Vannier, M.Sc student at Sorbonne University, FranceSam Foreman, Postdoc @ Argonne National LaboratoryMary Sanford, PhD student, University of Oxford, Oxford Internet InstituteKevin Lin, PhD student, UC BerkeleySeth Lazar, Professor of Philosophy, Project Leader, Humanising Machine Intelligence Australian National UniversityAbeba Birhane, University College Dublin, IrelandMatthew Phillipps, PhD Candidate, Computer Science, ANUMathieu Rousseau, Undergraduate Student, Physics. UCLouvain, BelgiumJulie Lee, Postdoctoral researcher, University College LondonPetros Terzis, PhD Student, University of WinchesterTewodros Taffese (Teddy), Student, PhD in Human Factors/HCI Rice UniversityAndy McMurry, PhD Chief Science Officer, Ciox RWDTodd Hartsburg, Registered NurseMia Zamora, Ph.D., Kean UniversityPhillip Brooker, Senior Lecturer in Sociology (University of Liverpool)Andreas Kirsch, DPhil candidate, OATML, University of OxfordDushyant Rao, Senior Research ScientistFreddie Kalaitzis, Senior Research Fellow, University of OxfordSamuel Lippl, Research Assistant, Columbia UniversityBill Cannon, MSc STS studentYannis Kalantidis, Researcher, NAVER LABS EuropeTessa Darbyshire, Scientific Editor, Patterns, Cell PressDivine Maloney, PhD Candidate Clemson University, Microsoft Ada Lovelace Fellow, Human Centered ComputingFanny Hidvegi, Europe Policy Manager, Access NowBen Rush, Consultant in Public Health MedicineHenning Bumann, Data ScientistEstefania Piedrahita, public art grants manager at Idartes, Bogota, Colombia.Dr Daniela Huppenkothen, Research Scientist, SRON Netherlands Institute for Space ResearchAnil Kulkarni, Senior Software Engineer, FacebookStanislav Nikolov, Research Engineer, DeepMindSalma Abdel Magid, PhD Student in CS, Harvard UniversitySarah Drinkwater, Omidyar NetworkLeon Derczynski, IT University of Copenhagen Lucy Li, PhD student, University of California Berkeley YONG Xin Hui, PhD student, University of PittsburghDustin Wright, PhD Student at University of CopenhagenLaura Carter, PhD candidate, University of EssexLe Nguyen Hoang, Computer Science Communicator and Researcher at EPFLPranav A, Sentinel AIGiulio Valentino Dalla Riva, Lecturer in Data Science, School of Mathematics and Statistics | Te Kura Pangarau, University of Canterbury | Te Whare Wananga o Waitaha, Christchurch, New Zealand | Otautahi, AotearoaAlice Oh, Associate Professor, KAIST Dr. Noni Symeonidou, Associate Professor of Entrepreneurship and Innovation, Warwick Business School, The University of WarwickMcKane Andrus, Research Associate, Partnership on AITanmayee Narendra, PhD Student, Universitat Tubingen Sarah Bouchat, Assistant Professor, Political Science, Northwestern UniversityMax Little, AI Researcher, OxfordArnav Arora,, MSc student, University of CopenhagenConor Daly, Deep Learning DeveloperAbdallah Bashir, Research AssistantJingyi Li, CS Doctoral Candidate, Stanford UniversityCharlotte MinskyLukas Daniel Klausner, Researcher, Institute of IT Security Research, St. Polten University of Applied SciencesReuben Binns, Associate Professor of Human-Centred Computing, University of OxfordAndrew Strait, Head of Research Partnerships, Ada Lovelace InstituteJane O SunAshwin Machanavajjhala, Associate Professor, Department of Computer Science, Duke UniversityMine Cetinkaya-Rundel, University of Edinburgh, Duke University, and RStudioMarietje Schaake, Stanford UniversityShobhit Hathi, Applied Scientist, MicrosoftKatie Love, scientific researcherTadiwos-Feyissa Mergiya Jess Morley, DPhil Candidate, Oxford Internet InstituteBirhanu Eshete, Assistant Professor of Computer Science, University of Michigan, DearbornOrion J Taylor, Lead Data Scientist, NYU Public Safety LabAshia Wilson, Assistant Professor, MIT LIDSIlias Chalkidis, PhD Candidate, Athens University of Economics and BusinessJasper Rijkeboer, Software EngineerJeanna Matthews, Professor, Department of Computer Science, Clarkson UniversityAnya Belz, Professor of Computer Science, University of Brighton, UKCasey Hong, MITLeslye Tinson, Lecturer of Psychology and African American Studies, San Jose State UniversityGeorge ChiesaXanda Schofield, Assistant Professor, Harvey Mudd CollegeDaniel S. Hain, Associate Professor, Ph.D., Innovation Economics and Data Science, Aalborg University Business SchoolSamee Ibraheem, PhD Student, UC BerkeleyKiante Brantley, University of Maryland, PhD StudentIsabelle Augenstein, Associate Professor, University of CopenhagenRicardo Baeza-Yates, Director of Data Science, Northeastern University at SV, ACM & IEEE FellowBeneal Brook Shamsu, student at Illinois Institute of TechnologyVeronika Cheplygina, Assistant Professor, Eindhoven University of TechnologyGunes Acar, KU LeuvenDr. Alexa Hagerty, University of Cambridge, Leverhulme Centre for the Future of IntelligenceNeha Nayak Kennard, Graduate Student, UMass AmherstPatrick Lam, Associate Professor of Electrical and Computer Engineering, University of WaterlooSusanne Oechsner, Researcher, University of ViennaBeneal Brook Shamsu,student at Illinois Institute of TechnologyDr. Stephen G. Odaibo, M.D., M.S.(Math), M.S.(Comp. Sci.), RETINA-AI Health, Inc., Founder and Chief Executive OfficerCorinne Cath, PhD Candidate at the Oxford Internet Institute and Alan Turing Institute for Data ScienceTadiwos-Feyissa MergiyaJason Webster, Data ScientistStanislav Kirdey, software engineerAngela Cardoso, citizen & consultantSamin Aref, Max Planck Institute for Demographic ResearchDr. Hendrik Heuer, Postdoc, University of Bremen, GermanyJihan Salsabila, Hardware EngineerElizabeth Patitsas, Assistant professor, School of Computer Science, McGill UniversityDiptodip Deb, Software Engineer, HHMI Janelia Research CampusBernease Herman, University of WashingtonLorenzo Porcaro, PhD Student, Universitat Pompeu FabraMichael R. Crusoe, Research Software Engineer, University of Tubingen, Faculty of Mathematics and Natural Sciences, Department of Computer ScienceNoopur Raval, PhD, Postdoctoral Researcher, AI Now Institute, New York UniversityRoberta Fischli, PhD student and research assistant, University of St. GallenMike Endale, CEO of MoxitOwen Blacker, co-founder & former trustee/director at Open Rights Group and at mySocietyJohn Winn, MicrosoftDr. Shannon McWeeney, Professor and Division Head, Associate Director, Computational Biomedicine, OHSU Knight Cancer InstituteKate RotondoBradley Gram-Hansen, University of OxfordMartine Ballinger, PhD student at EMBL, HeidelbergTamara Erickson, Crown CastlePiper Horscroft, Software EngineerNicolas Lomenie, Associate Professor, AI, University of ParisFaris Cuchi Gezahegn, Co-founder at House of Guramayle, Creator of Alen ShowAdil Salim, Postdoctoral fellow, KAUST, Saudi ArabiaWillie Boag, PhD Student, MIT CSAILJames Stomber, Oxford Internet Institute Boury Mbodj, Big Data Developer, McGill AlumniMehitabel Glenhaber, PhD Student, USC Annenberg School of CommunicationsDr. Hady Ba, Philosopher, Cheikh Anta Diop UniversityGemma Dawson, PwC (South Africa) | Data AnalystMcCoy Patino, Software Engineer, MicrosoftMedha Patki, Master's student, Harvard Kennedy SchoolChithrupa Ramesh, Ph.D., Machine Learning Engineering Consultant, Zuken Limited, Bristol, UK.Elaine Nsoesie, Boston UniversityDr. Wenlong Li, Postdoctoral Research Fellow, University of BirminghamLindsay Tan, Director, Design Ecology Lab, Auburn UniversityTimothy LaRock, PhD Candidate, Northeastern UniversityMark Nye, owner, Skylight Technology Consulting, LLCTheodore Walls, wisemindtrading.com Cassidy Puckett, Assistant Professor, Sociology, Emory UniversityAndre Mintz, Federal University of Minas Gerais, BrazilJacqueline Wernimont, Ph.D., Distinguished Chair in Digital Humanities & Social Engagement, Associate Prof of Women's, Gender, & Sexuality Studies, DartmouthMicaela Mantegna, Affiliate, Berkman Klein Center for Internet and Society at Harvard UniversityTei Laine, Ph.D, Independent researcherJennifer Ajderian, PhD ResearcherJonathan Gray, Lecturer in Critical Infrastructure Studies, Department of Digital Humanities, King's College LondonLauren Frey, ConsultantHiba Chougrad, Assistant Professor, University Sidi Mohamed Ben AllahThor Kell, Senior Software Engineer, SpotifyLinnet Taylor, Associate Professor, Tilburg Institute for Law, Technology and SocietyMelissa McCradden, The Hospital for Sick Children; University of Toronto Valerie Carey, Data ScientistEmily Berkowitz, Actionable Intelligence for Social Policy at the University of PennsylvaniaDr. Theodora Dryer, NYUA., Ph.D. Student, Stony Brook UniversityAnita Chikkatur, Associate Professor, Carleton CollegeAlondra Nelson, Institute for Advanced StudyScarlett Winter Kelsey, Research Data Analyst Associate, Institute for Sexual and Gender Minority Health and Wellbeing at Northwestern UniversityNanna Bonde Thylstrup, Associate Professor, Department of Management, Society and Communication Aimee Schwab-McCoy, Ph.D, Assistant Professor of Statistics, Program Director: Data Science, Creighton UniversityPablo Aragon, Ph.D., Research Scientist and Adjunct ProfessorTigist Mengistu, System Administrator, Robert Bosch GmbH , Germany.Rochelle Davis, Associate Professor of Cultural Anthropology, Georgetown UniversityJordan Harrod, PhD Student, Harvard-MIT Health Sciences and Technology ProgramDr Cory Doctorow (hc), visiting professor of Computer Science, Open University (UK); visiting professor of practice UNC Dept of Library and Information Science; research affiliate, MIT Media Lab; co-founder, UK Open Rights Group; special advisor, Electronic Frontier FoundationPanayiotis Smeros, PhD Student @ EPFLJonas Jongejan, Creative TechnologistMatt Harrington, MS in ML at Virginia TechJanet Haven, Executive Director, Data & SocietyAnand Sheombar, HU University of Applied Sciences UtrechtSarah Williams, MIT Associate Professor of Technology and Urban Planning, Director of Civic Data Design LabKatherine M. Kinnaird, Assistant Professor of Computer Science and Statistical & Data Sciences, Smith CollegePamela M. Jasper, PMP, CEO, JasperConsulting.AI, member BlackInAI, NeurIPS 2020 presenter, founder of FAIR -Framework for AI Risk TM Chithrupa Ramesh, Ph.D., Machine Learning Engineering Consultant, Zuken Limited, Bristol, UK. Sonia Roberts, a PhD candidate in robotics at the University of PennsylvaniaJennifer Grannen, Undergraduate Researcher, UC BerkeleyTicha Sethapakdi, PhD Student, MITNyasha Chimhandamba Christina J. Colclough, The Why Not LabParag Mital, CTO at HyperSurfaces.com and Adjunct Faculty at UCLAEmily Brooks, CUNY Kerryn Gammie, Data Scientist Virgil Hilts, Senior Escape Artist, Proton MailAlexandra To, Assistant Professor, Northeastern UniversityHarmanpreet Kaur, PhD Candidate in Computer Science and Information, University of MichiganBabafemi Badero, Product Designer at Cornell Ryan Harrison, Ph.D, Software EngineerTim Tesch, Private Consultant & MKE Tech AdvocateAkesha Horton, PhD, Director of Curriculum and Instruction, School of Informatics, Computing, and Engineering, Indiana UniversityVasundhara Gautam, Speech Recognition Engineer, DialpadVarun, Software EngineerMoa AlemayehuCrystal Houston, Co-founder, Citefull & Instructor, ""We Rise"" Black Entrepreneurship Accelerator Dr. Patrick Meier, Executive Director, WeRoboticsEphrem Tadesse, LecturerDeirdre K. Mulligan, Professor, School of Information, Faculty Director, Berkeley Center for Law and Technology, University of California, BerkeleyPriya Donti, PhD student, Carnegie Mellon UniversityKayla Huber, Ph.D. Student, University of Minnesota Twin CitiesChristina, HMS, MITTristan Bergh, Data Scientist, iOCO Dr. Jorge Ortiz, Assistant Professor, Rutgers UniversityJuan Pablo Zuluaga G?mez, Idiap Research InstituteMarcelino Pena, MA student, University of California  IrvineDr. Elizabeth Lane Lawley, Professor, Golisano College of Computing & Information Sciences, Rochester Institute of TechnologyDr Elizabeth Van Couvering, Karlstad University, SwedenBryan Naegele, Staff Engineer, SimplebetSteven Rick, PhD Candidate, UC San DiegoTesfagabir Meharizghi, Data Scientist, AWSReihaneh Rabbany, Canada CIFAR AI Chair, Mila & Assistant Professor, School of Computer Science, McGill University, CanadaDavid Bruno, Computer and Electrical Engineering UndergraduateOlalekan Joseph Akintande, PhD candidate & Advocate AI Fairness, University of IbadanTricia Wang, Tech Ethnographer, Data & SocietyMason A. Porter, Professor of Mathematics, UCLAKira Evans, Employee of the Chan Zuckerberg InitiativeAbel Quintero, UX Engineer, Ivy.ai, and QTBIPOC Committee Lead, Out in TechDr. Christian DiCanio, Assistant Professor, Department of Linguistics, University at BuffaloEdward McFowland III, Assistant Professor of Information and Decision Sciences, University of MinnesotaBrian Brubach, Assistant Professor, Wellesley CollegeChristina Dunbar-Hester, Associate Professor, Annenberg School for Communication, University of Southern CaliforniaCeleste Kidd, Assistant Professor of Psychology, UC BerkeleyJulie T. Do, University of WashingtonTeon L. Brooks, Ph.D., Senior Data Scientist, MozillaNikka Ghalili, LinkedIn Software EngineerSarah Colby, Former Software EngineerJS Tan, MIT, formerly MicrosoftSuhaas Bhat, Harvard CollegeArmisha Roberts, University of Florida Ph.D. StudentJeff Brown, Diversity and Inclusion Research Fellow, Partnership on AIJohn Fowler, PhD Student, University of WashingtonSophia Dong, Software EngineerCedric Lombion, Data and Innovation Lead at Open Knowledge FoundationDr Emma Byrne, freelance science and technology writerBharath Hariharan, PhD., Assistant Professor, Department of Computer Science, Cornell UniversityMeredith Broussard, Associate Professor, New York UniversityEdward Wright, Senior Software Engineer, Index ExchangeDr. Judy Goldsmith, Professor of Computer Science, University of KentuckyAnnika Marie Schoene, University of HullClement Allen, PhD, Professor, Florida A&M UniversityPeter Wijeratne, PhD, University College LondonAlice Moloney, UX Content StrategistSebastian Assaf, Staff Visual DesignerKrzysztof Gajos, Professor of Computer Science, Harvard UniversityKenneth Joseph, Assistant Professor, University at BuffaloKatie A. Siek, Ph.D., Professor and Informatics Department Chair, Indiana University, Luddy School of Informatics, Computing, and EngineeringMichael Dickard, PhD, UX Research Manager, VanguardJoel Zylberberg, Ph.D., CIFAR Associate Fellow of Learning in Machines and Brains, and Canada Research Chair at York UniversityYacine Jernite, Research Scientist at Hugging FaceAngele Christin, Assistant Professor, Department of Communication, Stanford UniversityBrianna Yang, Oxford Internet InstituteRenata Avila, Co-Founder and Senior Advisor, A+ AllianceKim Wilkens, Tech-Girls founder, Ed.D. candidate at the University of VirginiaHarlan Yu, UpturnAlex Chen, graduate student, Harvard UniveristyAlex Bigelow, Postdoctoral Researcher, University of ArizonaMichael McKenna, Senior Data Scientist, CVS HealthSaka Nuru, QuickBooksHannah Pullen-Blasnik, PhD Student, Sociology, Columbia UniversityPeter Rood, Software Engineer, YeildmoPeaks Krafft, Senior Research Fellow, Oxford Internet Institute, University of OxfordDr. Marzyeh Ghassemi, Assistant Professor, MIT, IMES & EECSTawanna Dillahunt, Associate Professor, University of Michigan Nicolas Perrin-Gilbert, Research Scientist, UPMCKaren Levy, Assistant Professor, Information Science, Cornell UniversityEllen Pao, CEO, Project IncludeDanielle Keats Citron, Austin B. Fletcher Professor of Law, Boston University School of Law, Vice President, Cyber Civil Rights InitiativeLukas Braun, DPhil Candidate, University of OxfordFrank Fu, Software EngineerJay D. Aronson, Professor of Science, Technology, & Society and Director, Center for Human Rights Science, Carnegie Mellon UniversityAmy A. Winecoff, Data Scientist, Center for Information Technology Policy, Princeton UniversityRobyn Caplan, Data & Society Research InstituteDivya Ramesh, PhD Candidate, University of MichiganTiffani J. Bright, PhD,FACMI Biomedical Informatician, IBM Watson HealthJason Griffey, Director of Strategic Initiatives, National Information Standards OrganizationDaniel Martin, Principal Software Engineer, CrowdStrikeDorothea Salo, Information School, University of Wisconsin-MadisonMatt Bernius, Principal Design Researcher, Code for AmericaJaren Haber, PhD, Postdoctoral Fellow at Georgetown University Alden Golab, MSCAPP (UChicago), Senior Data Engineer at the New York TimesJamelle Watson-Daniels, Director of Research, Data for Black LivesJackie Cohen, Senior Software EngineerJim Waldo, Gordon McKay Professor of the Practice of Computer Science, Harvard UniversityEric Gilbert, School of Information and CSE, University of MichiganJonnie Penn, Ph.D, Postdoc, University of CambridgeMichael L. Best, Professor of International Affairs and Interactive Computing, Georgia Institute of TechnologyAmy Li, Artist/Photographer Kate Armstrong, Legal Technology Project ManagerJeff Ward, Clinical Professor of Law, Duke LawMatthew Sun, PhD Student, Princeton UniversityMaya Indira Ganesh, Leuphana University, Luneburg, GermanyEthan Zuckerman, Associate Professor of Public Policy, Information and Communication, University of Massachusetts AmherstDaniel Nkemelu, PhD Student at Georgia Institute of TechnologySolon Barocas, Microsoft Research and Cornell UniversityEddie Kay, Senior Engineering Manager, Toast; former GooglerJennifer Neville, Samuel D. Conte Professor of Computer Science, Purdue UniversityDr. Brandeis Marshall, Faculty Associate, Berkman-Klein Center for Internet & Society, Harvard UniversityDr. Emma Beauxis-Aussalet, Assistant Professor of Ethical Computing, Vrije Universiteit AmsterdamJavier RandoJessica Vitak, Associate Professor, College of Information Studies, University of MarylandJana Fehr, PhD student in Machine LearningLinda Khumalo, MSc. Student, University of the Witwatersrand, Johannesburg, South AfricaGabriel Pereira, PhD Fellow, Aarhus University (Denmark)Deborah DeGeorge, librarianDustin Jamner, PhD student, MIT CSAILBrian Callaci, Postdoctoral Scholar | Data & SocietySalome Viljoen, Joint Research Fellow, Information Law Institute at NYU School of Law and Digital Life Initiative at Cornell Tech Sumegha Garg, Postdoctoral Fellow at Harvard Sidney A. Rothstein, Assistant Professor, Department of Political Science, Williams CollegeM. de Blanc, Graduate Student, NYUAndreattah Chuma  poet, IT Law Masters student, University of StrathclydeDr Karin Sim Smith, MT ScientistHanna Wallach, MicrosoftEran Toch, Tel Aviv UniversityHeather Yager, MIT LibrariesMark Chu-Carroll, Phd. Senior Software Engineer, SpotifyKiran Samuel, PhD student, Department of Sociology, Columbia UniversityMenzi Hlope, Pearson Institute of Higher EducationDr. Shiri Dori-Hacohen, CEO & Founder, AuCoDeSerena Booth, MIT PhD Candidate and XooglerMax Marion, Machine Learning Engineer at KUNGFU.AIDella Jenkins | Executive Director, Actionable Intelligence for Social Policy (AISP), University of Pennsylvania, School of Social Policy & PracticeChristopher L. Dancy, Assistant Professor, Dept of Computer Science, Bucknell UniversitySunaina Pamudurthy, Student, Harvard Kennedy SchoolMatt Whitlock, PhD Student, UMass AmherstBert Huang, Assistant Professor, Tufts UniversityBarbara Engelhardt, Associate Professor, Princeton UniversityBurcu Baykurt, Assistant Professor at UMass Amherst and Faculty Associate at the Berkman Klein Center for Internet and Society, Harvard UniversityLyel Resner, Adjunct Professor, Civic Technology, NYUViet Vu, Ryerson UniversityJordi Weinstock, Lecturer, Harvard Law School Sandra Pallier, designer at Microsoft & co-organiser of ClimateAction.techThomas Haugland Johansen, PhD Candidate, UiT The Arctic University of NorwayScarlet Galvan, Collection Strategist Librarian, Grand Valley State University LibrariesSergei Volodin, Swiss Federal Institute of Technology in Lausanne (EPFL), Master's studentClemence Hermann, Master's Student at Glion Institute of Higher Education Claudia Negri Ribalta, PhD student, Paris I Pantheon-Sorbonne Universite Alexandra Stiver, PhD, Computing and CommunicationsRebecca Fiebrink, Creative Computing Institute, University of the Arts LondonVincent Mosco, Professor Emeritus, Queen's University, CanadaBrad Wyble, Penn State UniversityRandall Reed Professor of Religious Studies, Appalachian State University, and Co-chair of the American Academy of Religion, Artificial Intelligence and Religion Research SeminarGavin Abercrombie, Research Associate, Heriot Watt UniversitySara Beery, PhD Candidate, Caltech Melanie WarrickNikhil Garg, postdoc, UC BerkeleyCamilla Longden, Research Software Engineer, Microsoft ResearchBlaine Wishart, Berkeley, CA, LateBindingJames Clark, Professor at McGill University, General Chair ICCV2021Dr. Kristine Gloria, Associate Director of Emerging Technologies, Aspen DigitalDr. Alex Ketchum, IGSF of McGill UniversityAnna Chung, Experience Designer, Publicis SapientDr. Beth Coleman, Associate Professor Institute of Communication, Culture, Information and Technology & Faculty of Information, University of Toronto; Director City as Platform lab; Google Artists and Machines Intelligence awardeePatrick Gildersleve, PhD Candidate, Oxford Internet Institute, University of OxfordNick Ruest, Associate Librarian, Digital Scholarship Infrastructure Department, York UniversitySudhir Chella Rajan, Professor, Humanities and Social Sciences, Indo-German Centre for Sustainability, Indian Institute of Technology MadrasArmin Namavari, PhD Student in Computer Science, Cornell UniversitySneha Visakha, Legal Researcher Jeff Lockhart, PhD Candidate, University of MichiganSeleeke Flingai, Ph.D., MPA, MIT 2011 alumAna Valdivia, Research Associate, King's College LondonFelix M. Simon, PhD student, Oxford Internet Institute, University of OxfordTejumade Afonja, AI Saturdays LagosRachel West, Product Manager at Turner Sports  WarnerMediaMakazi Mtingwa, Founder, FunderdustSofia O., AnalystPatrick Gildersleve, PhD Candidate, Oxford Internet Institute, University of OxfordBibi Reisdorf, D.Phil., Assistant Professor, UNC CharlotteThiago Serra, Assistant Professor, Bucknell UniversityBerhan Taye, Africa Policy Manager, Access NowMarie-Therese Png, Oxford Internet Institute Sajid Ali, PhD Candidate in Applied Physics, Northwestern UniversityJulie Hui, Assistant Professor, University of MichiganMichael Burke, RA, University of EdinburghLuigina Ciolfi, Professor, University College Cork Kat M. Gray, Graduate Teaching Assistant/PhD student, Virginia TechIngmar Weber, Research Director for Social Computing, Qatar Computing Research InstituteLiz B. Marquis, PhD Candidate, University of Michigan School of InformationAlyssa Browne, PhD candidate, Department of Sociology, Pre-doctoral Trainee, Carolina Population Center, University of North Carolina at Chapel HillDaniel J. Weitzner, 3Com Founders Principal Research Scientist, MIT Computer Science and Artificial Intelligence LabSusan M. Dray, Ph.D., CUXP, President, Dray & AssociatesTessema M. Mengistu, Assistant Professor, Computer Science Department, George Mason University Memo Akten, PhD, Artist, Asst Professor of Computational Arts, UC San DiegoArjen P. de Vries, Professor of Information Retrieval, Radboud University, NLLagnajit Pattanaik, Graduate Student, MITEmily M. Bender, Professor, Department of Linguistics, University of WashingtonColleen Dorsey, JD, Director, Organizational Ethics & Compliance, University of St. ThomasOliver Kennedy; Dept. of Computer Science & Engineering; University at BuffaloNatalie Araujo Melo, PhD Student, Northwestern UniversityKaren Gregory, Senior Lecturer, Sociology, University of EdinburghAndrew Saxe, Associate Professor, Department of Experimental Psychology, University of OxfordSean McIntyre, Director, Uncharted Software Inc.Seble Negatu, PhD Student, University of Pennsylvania Ninhursag C., Software EngineerCharles Radclyffe, CEO EthicsGrade, AI Governance & ESGJohn Urbanik, Senior Data Scientist, Recursion PharmaceuticalsSamantha Breslin, Assistant Professor, University of CopenhagenNikita Nangia, PhD student, New York UniversityMichelle Li, Principal Researcher, Uncharted SoftwareVinod Subramanian, PhD student, Queen Mary University of LondonMarc Aidinoff, Massachusetts Institute of TechnologyErin Simpson, Associate Director of Technology Policy, Center for American ProgressDr. Ananya Chakravarti, Associate Professor of History at Georgetown University/ RadicalxChangeWilliam Clyde Partin, Researcher, Data & Society Research Institute Amanuel Wondimu, Stress Engineer, '19 Wichita State University alumni Marc Farley, Founder Fair WorkforceDiana Floegel, Doctoral Candidate, Rutgers UniversityDaniel Shiffman, Associate Arts Professor, ITP/IMA, Tisch School of the Arts, New York UniversityRyan J. Gallagher, PhD Student, Northeastern UniversityRyan Amos, PhD Candidate in Computer Science, Center for Information Technology Policy, Princeton UniversityJessie Stickgold-Sarah, Writing Rhetoric and Professional Communication, MITKolawole Adebayo, Research Scientist, Datalive AnalyticsCarol Willing, Steering Council, ACM 2017 Software System Awardee  Project Jupyter, Fellow  Python Software FoundationPranava Madhyastha, Research Fellow , Department of Computing, Imperial College LondonBradley A. Clements, PhD Student, University of Toronto Faculty of Information (iSchool)Bonnie Stewart, University of WindsorCathryn Ploehn, MDes, Lecturer in Interaction Design, The University of Texas at AustinAnna Gifty Opoku-Agyeman, Co-founder of The Sadie CollectiveSam Brandt, Technical WriterJerome Hodges, PhD, Managing Director and Chief Research Officer, Jain Family InstituteMathieu Forcier, PhDSohrob Kazerounian, AI Research Lead, Vectra AIMelanie Walsh, Postdoctoral Associate, Cornell University Nina da Hora  Algorithms Fairness Research Computer Science PUCRioNicholas Weaver, ICSI & UC BerkeleyKiran Vaidhya Venkadesh, PhD Candidate, RadboudumcPamela Oliver, Professor Emerita of Sociology, University of Wisconsin  MadisonPedro Diogo Carvalho Monteiro, Master student at Federal University of Bahia (UFBA)/ BrazilApril Swoboda, MD, Medical Oncologist & Assistant Professor of MedicineAnand D. Sarwate, Associate Professor, Rutgers UniversitySachin Kumar, PhD Student, Carnegie Mellon UniversitySubho Majumdar, Senior Inventive Scientist, AT&T Labs ResearchMarius Miron, Universitat Pompeu Fabra, BarcelonaAdam Stasiw, Software Engineer at Flatiron HealthRosamunde Van Brakel, Research Professor Chair in Surveillance Studies, Vrije Universiteit BrusselKartikeya Kandula, PhD Student in Computer Science, Center for Information Technology Policy, Princeton UniversityCameron Raymond, MSc Social Data Science, Oxford Internet Institute, University of OxfordKatlyn Turner, Research Scientist, MIT Media LabRachel Bergmann, Research Assistant, Social Media Collective, Microsoft Research New EnglandAndrea Mannocci, Postdoctoral Researcher, ISTI-CNRVioleta Ilik, Dean of University Libraries, Adelphi UniversityW. Michelle Harris, Associate Professor, IGM, Rochester Institute of TechnologyAngie Zhang, PhD Student, University of Texas at AustinStewart Isaacs, PhD Candidate, Aeronautics and Astronautics, Massachusetts Institute of TechnologySerge Assaad, PhD Student, Duke UniversityWendy Norris, Asst. Professor of Social Computing and the Institute for Technology, AI, and Society, Nazareth CollegeIsabela ConstantinCaitlin Stanton, Masters of Engineering, Cornell UniversityMorten Bay, Research Fellow / Lecturer, Annenberg School of Communication, University of Southern CaliforniaLucas Lago, Public Interest Technologist alumnus of Ford/MDF Technology Exchange FundPatrick Musau, Vanderbilt University EECSThomas Moore, Staff Software Engineer, MozillaCatherine Wong, PhD Student at Massachusetts Institute of TechnologyChristopher Tessum, Assistant Professor, Civil and Environmental Engineering, University of Illinois Urbana-ChampaignSudeep Duggal, TutanotaCaroline Jarrett, Artificial Intelligence Feasibility Investigator, Effortmark LtdKen Arnold, Assistant Professor, Calvin UniversityAlexandra Mateescu, Researcher, Data & SocietyLorena Regattieri, PhD Student, UFRJSalmana Ahmed, LuminateMengistu Ketema, Computer EngineerPablo Nunes, Center for Studies on Public Security and CitizenshipNick Diakopoulos, Associate Professor, Northwestern UniversityKim de Laat, Research Collaborator, Brookfield Institute for Innovation and Entrepreneurship, Postdoctoral Fellow, Institute for Gender and the EconomyAmanda Lenhart, Program Director, Health and Data, Data & SocietyDiana Kafkes, Artificial Intelligence Associate, FermilabMartin Tisne, LuminateElizabeth Anne Watkins, PhD Candidate in Communications, Columbia UniversityDylan Halpern, Principal Software Engineer, Center for Spatial Data Science @ UChicagoAsko Lehmuskallio, New Social Research, Tampere UniversityCharlie Welch, PhD Candidate, University of MichiganAri Qayumi, Symbolic Systems, Student, Stanford. Computer Science, Researcher, StanfordChinmayi Arun, Berkman Klein Center for Internet and Society at Harvard UniversityElliot Creager, PhD Candidate, University of TorontoDaniel Gayo-Avello, Department of Computer Science at the University of OviedoRafael Soares, Instituto Superior de Tecnologia, BrazilSamuel R. Bowman, Assistant Professor of Data Science, Linguistics, and Computer Science, NYUHeather Zinn Brooks, Assistant Professor of Mathematics, Harvey Mudd CollegeSahana Kribakaran, MD/PhD Student Yale UniversityAaron Shaw, Associate Professor, Communication Studies, Northwestern UniversityElizabeth M. Renieris, Fellow, Carr Center for Human Rights Policy at Harvard Kennedy SchoolBrian Nord, PhD, Scientist, Fermilab and University of ChicagoKevin Werbach, Professor of Legal Studies & Business Ethics, The Wharton School, University of PennsylvaniaBrishen Rogers, Associate Professor of Law  Temple University, Fellow  Roosevelt InstituteJad Esber, ex-Googler and Fellow at the Berkman Klein Centre for Internet & Society at Harvard UniversityDave Rife, founder Dave&Gabe, founder Future SpaceReed Coke, Senior ML Engineer @ KUNGFU.AIFabio Ferreira, Research Scientist, University of FreiburgMatthew Jorke, PhD Student in Computer Science, Stanford UniversityJennifer C. Lena, Associate Professor of Arts Administration, Columbia UniversityBernard Koch, PhD Student, UCLAIfeoma Ajunwa, J.D., Ph.D., Associate Professor (with tenure), Cornell University ILR School, Associate Faculty Member, Cornell Law School, Faculty Associate, Berkman Klein Center at Harvard UniversityAngle Bush, Founder, Black Women In AINeha Ravella, graduate student at the University of WaterlooRebecca Lewis, PhD Candidate, Stanford UniversitySusila Gurusami, PhD, Assistant Professor, Department of Criminology, Law, and Justice, University of Illinois at ChicagoJesse Martinez, Ph.D. Student, University of WashingtonVeni Kunche, Founder, Diversify TechBurr Settles, Research Director, DuolingoNathaniel Poor, Underwood InstituteMirian Silva, Software Engineer, IBMRuth Jebessa, Undergraduate AI Enthusiast and Supporter, Wright State UniversityChris Gilliard, Harvard Kennedy School Shorenstein Center Visiting Research FellowUdo Schuklenk, PhD (He/Him/His), Professor of Philosophy and Ontario Research Chair in BioethicsRamon Vilarino, Brazilian Data ScientistJonathan Lu, MD student, StanfordMolly Waiting, Staff AttorneyDanya Glabau, PhD, NYU Tandon School of EngineeringAllie Lahnala, PhD Candidate, Computer Science and Engineering, University of MichiganMilagros Miceli, Doctoral Researcher, Weizenbaum Institute / TU BerlinAbimbola Olurin, Account Manager & D&I MarketerKaren Borchgrevink, Executive Director, LA Tech4GoodJas Rault, Assistant Professor, Media Studies & Faculty of Information, University of TorontoK. Supriya, Postdoctoral research scholar, Arizona State UniversityJesper Mlgaard, MD, PhD student., Copenhagen University Hospital, DenmarkSteven Wilson, University of EdinburghElizabeth Popp Berman, Associate Professor of Organizational Studies, University of MichiganDaniel Hirschman, Assistant Professor, Brown UniversityJeramie Scott, Electronic Privacy Information CenterNorm Ferns, Machine Learning Architect, SportlogiqChristopher Brown, Interactive Software Development Lead, Museum of ScienceMichael Ekstrand, Assistant Professor, Boise State UniversityGrace Fenton, FraymJad Esber, ex-Googler and Fellow at the Berkman Klein Centre for Internet & Society at Harvard UniversityEmma Day, Human Rights Lawyer, Affiliate Berkman Klein Center for Internet & Society; Edmund Hillary FellowSam Jackson, Assistant Professor, University at AlbanyDanny Hawkins Co-Founder & Chief Technology Officer QuiqupRoderic N. Crooks, Department of Informatics, University of California, IrvineHarini Suresh, MITR. Joshua Scannell, Assistant Professor, The New School School of Media StudiesNakeema Stefflbauer, PhD, Founder & CEO, FrauenLoop, Lead, Tech in Color.euDr. Adugna Mullissa, Wageningen University and Research, The NetherlandsDavid Madden, Head of Innovation, LuminateEvelyn Cupil-Garcia, Undergraduate Student at Duke UniversityJenna Kainic, PhD Candidate, NYU CourantSydney McMuldroch, Software EngineerJean Gallagher, Professor, NYU Tandon SchoolMelissa Valentine, Assistant Professor MS&E, Stanford UniversityElsa Snider, MPH, Washington University in St. LouisAymar Jean Christian, Northwestern UniversityHeather Wiltse, Associate Professor, Umea Institute of Design, Umea UniversityMelanie Sage, Assistant Professor, School of Social Work, University at BuffaloHilde Weerts, AI Engineer, Eindhoven University of TechnologyAnita Gohdes, Professor, Hertie School, BerlinPaige North, Software Engineer, MicrosoftKhalid El-Arini, Research Scientist, FacebookPadmashree Gehl Sampath, Fellow, Berkman Klein Center for Internet and Society, Harvard UniversityJason Weston, Research Scientist, Facebook AI ResearchJames Mickens, Professor, Harvard UniversityC. Estelle Smith, Computer Science PhD Candidate at University of MinnesotaGiada Pistilli, PhD Candidate, Sorbonne UniversiteAmelia Estwick, Ph.D., Computer Scientist, Director of the National Cybersecurity Institute at Excelsior CollegeYann ""shalf"" Heurtaux, Data Protection Officer ad interim, Kargo.bikeMelanie Denyer, postgrad student in Applied AI and Data Analytics at University of BradfordCynthia C. S. Liem, Assistant Professor, Delft University of TechnologyNorma Mollers, Assistant Professor, Dept. of Sociology, Queen's University, Kingston, OntarioTim Niven, Research Scientist, Doublethink Lab, Taipei, TaiwanEarl W. Huff, Jr., Ph.D. Candidate, School of Computing, Clemson UniversityDavid Ryan Polgar, Founder, All Tech Is HumanSakina Hansen, MSc Machine Learning, University College LondonMelissa Byrne, activist + organizer + campaignerDorothy Howard, PhD student, UC San DiegoCeri Riley, Complexly, MIT '16 AlumAlejandro Calcano, AI Now InstituteDr. Jonathan Soffer, Professor of History, Department of Technology, Culture & Society, NYU Tandon School of EngineeringYabebal Fantaye, 10 AcademyStephanie Huf, Division X, Telia Company ABMarlena Bowen, Solution Architect, Slalom ConsultingAbdul Z Abdulrahim, University of OxfordTais Oliveira, Federal University of ABC (Brazil)Joanna Motley, PhD candidate, Western UniversityLibby Falck, Forward Labs, formerly MITJen Kagan, Logic MagazineNicole Simmons, LuminateNoura Howell, PhD, Assistant Professor, Department of Communication, North Carolina State UniversityVicky Zeamer, Design Researcher at IDEOHewan Shemtaga, PHD student, Auburn UniversityBrian Akperi, Data ScientistAlex A. Ahmed, Ph.D., Postdoctoral Scholar | Human-Computer Interaction Institute, Carnegie Mellon University | School of Computer ScienceMike Ananny, Associate Professor, Annenberg School for Communication and Journalism, University of Southern CaliforniaChristo Sims, Associate Professor UC San Diego and member of the Institute for Advanced Study in PrincetonJacqui Ayling, Researcher, University of SouthamptonScott Fitzgerald, Industry Associate Professor, Academic Director Integrated Design and Media, NYU Tandon School of EngineeringR. Buse Cetin, Independent ResearcherAlan F Winfield, Professor of Robot Ethics, University of the West of England, BristolBianca Kremer  Law and Technology Teacher and Research At Infnet InstituteCarol Hansen, Software Engineer, SRAM LLCKrissy Carr Groom, Machine Learning EngineerDillon Reisman, J.D. Candidate, NYU School of LawBogdan Kulynych, PhD student at EPFLBenedetta Piantella, Industry Associate Professor, Integrated Digital Media Program, Department of Technology, Culture and Society, NYU Tandon School of EngineeringJessica Esquivel, PhD, Physicist, FermilabAlex Welcing, Consultant at Manatt HealthErin Heys, PhD, Policy Director, Berkeley Institute for Young Americans, University of California, BerkeleyPeter Akey, CS Masters CandidateWilliam R. Frey, Columbia UniversityAnthony M Cochetti, Software Engineer, CommerceHubSewale Demeke, PhD student at Sakarya UniversityMegan Shearer, PhD Candidate, University of MichiganCynthia Holcomb, FemaleBrain.AiVera Khovanskaya, PhD Candidate, Cornell UniversitySarita Schoenebeck, Associate Professor, University of MichiganEloize Marianny Bonfim da Silva, Student of Psychology at Universidade Federal de RondonopolisAlessandra Souza, lawyerDr. Marya Zlatnik, University of California, SFFederico Ardila, Professor of Mathematics, San Francisco State UniversitySarah Jobalia, researcher, Stanford UniversityMatthew Facciani, Postdoctoral Researcher, Vanderbilt UniversityElizabeth Resor, PhD Student, UC Berkeley School of InformationDr Jaya Klara Brekke, Assistant Professor, Durham University Geography DepartmentLauren M. Black, Attorney at Law, Martinez Law Group, PCMicah Carroll, PhD Student, UC BerkeleySam Ladner, PhD Principal Researcher, WorkdayMarilyn McKinley Parrish, D. Ed., Special Collections Librarian & University Archivist, Subject Librarian for African American Studies, History, Music, Archives & Special Collections, McNairy Library, Millersville UniversityKatrina Ingram, Researcher, University of AlbertaAaron Su, PhD Student, Princeton UniversityJennifer Sanchez, Product Inclusion, NetflixSai Krishna Vennamaneni, Senior Software Engineer at Swiggy, Bangalore, IndiaKen Hoover, PhD student, UC San DiegoSofia Axelrod, Ph.D., Research Associate, Rockefeller UniversityTamires Nobre, Freelance Multimedia Editor and Associate Degree in Multimidia Production (UFPA)Rowan Zellers, PhD candidate, University of WashingtonUrmila Chadayammuri, Harvard-Smithsonian Center for AstrophysicsBruce Schneier, Fellow and Lecturer, Harvard Kennedy SchoolSamantha Dalal, Information Science Ph.D. Student, University of Colorado  BoulderJessica Crees, Software EngineerVeronica Uribe A. , Ph.D. Student, Communication and Science Studies, University of California, San DiegoTalanda Williams, AnitaB.orgRaven L. Veal, PhD, Design Researcher, IBM Watson HealthShelby Perkins, Cyber Policy MA Student, Stanford UniversityJohn Joon Young Chung, PhD student, University of MichiganRobin Berjon, VP Data Governance, The New York Times Naomi Hirst, Digital Threats to Democracy Campaign  Global WitnessTim O'Gorman, Postdoc, University of Massachusetts AmherstTega Brain, Assistant Professor, Integrated Digital Media, NYU Tandon School of EngineeringAnn He, Stanford UniversityManon Gruaz, Product Designer  Element AIJose Hernandez, University of WashingtonAnthony Dardis, Professor, Department of Philosophy, Hofstra UniversityJill Toh, PhD Candidate, University of Amsterdam, Institute for Information LawEmily Mu, PhD Candidate at MITDr. Didem Gurdur Broo, University of CambridgeKristen Lowman, Senior Analyst, Physical Network, Oracle Cloud Infrastructure Alix Johnson, Assistant Professor, University of FloridaMarmoret Axel, PhD Student at Universite Rennes 1Samantha Robertson, PhD Student, U.C. BerkeleyLevin Kim, University of Washington Professor London School of Economics and Faculty Associate, Berkman Klein Center, HarvardAlison Harvey, Glendon College, York Universityrebecca (marks) leopold, IDM, Tandon School of Engineering  NYUDr Eleanor S Armstrong, UCL/University of DelawareBrooke Erin Duffy, Associate Professor, Cornell UniversityAfsaneh Rigot  ARTICLE 19 and Berkman Klein Center at HarvardJessica Thompson, Universite de MontrealCarla Vieira  USP and perifaCodeTemina Madon, Haas School of Business, UC BerkeleyStacee Burton White, Esq., former 11 year Google HRBPMegan Finn, Assistant Professor, University of WashingtonEric Iversen, Ph.D., VP for Learning and Communications, Start EngineeringJames A. R. Marshall, University of SheffieldSamantha D. Gottlieb, PhD, Principal UX Researcher, FitbitDr. Margaret Jack, postdoc at Syracuse University, Information School and research affiliate at the Digital Life Initiative, Cornell TechFritz Riha, Staff Product Designer, Creadit KarmaDamien Patrick Williams, PhD Candidate in the Department of Science, Technology, and Society, Virginia TechMina Tari, PhD Candidate, University of Washington Information SchoolAlexander Hoyle, PhD student, University of Maryland department of Computer ScienceJuan Ortiz Freuler, Affiliate at the Berkman Klein Center for Internet and SocietyNikki L. Stevens, Researcher, Dartmouth College/ PhD Candidate, Arizona State UniversitySampath Kannan, Professor, University of PennsylvaniaDr. Ahmed Ansari, Assistant Professor, Faculty of Technology, Culture & Society, New York UniversityAlex Sayf Cummings, Associate Professor of History, Georgia State UniversitySharon Zanti, MSW, Student, PhD in Social Welfare, Actionable Intelligence for Social Policy, University of Pennsylvania School of Social Policy & PracticeMary Ann Badavi, MFA Design and Technology, Parsons School of Design  The New SchoolMorgan Scheuerman, Doctoral Student, University of ColoradoDr Garfield Benjamin, Postdoctoral Researcher, Solent UniversityMichelle Choi Ausman, Graduate Student, Science and Technology Studies, Rensselaer Polytechnic InstituteChelsey Glasson, Staff User Experience ResearcherAnees Hasnain, DEI ConsultantDr. Cornelia Hedeler, Data ArchitectDr. John Schreck, Machine Learning Scientist, National Center for Atmospheric ResearchDaniel Estrada, Lecturer, New Jersey Institute of TechnologyJacob Quinn Shenker, PhD student, Harvard Medical SchoolGeoff Pleiss, PostDoc Researcher, Zuccerman Institute, Columbia UniversityMatthew Garvin, Research Assistant, University of Michigan School of InformationStephanie Ballard, PhD Candidate, University of WashingtonSean Follmer, Assistant Professor of Mechanical Engineering and Computer Science (by courtesy), Stanford UniversityRobin Stevens, PhD, MPH, Associate Professor, Annenberg School for communication and Journalism, University of Southern CaliforniaAlice Reznickova, Industry Assistant Professor, NYURania Wazir, Ph.D., Data ScientistNaveen Srinivasan, Brown University, MA Computer Science '20Jason St. John, Fermi National Accelerator LabAlexandros Kalousis, Department of Information Systems, University of Applied Sciences, Western SwitzerlandAlexis Elder, Associate Professor of Philosophy, University of Minnesota DuluthSalem Mekuria, Professor Emerita, Art Department, Wellesley CollegeAnne-Laure Fayard, NYU Tandon School of EngineeringKrystal Cooper, TPMRabbi Eli Cohen, Chadeish Yameinu Jewish RenewalSolomon Abebe Haile, PhD, Program Management OfficerStacy Wood, Research Director, UCLA Center for Critical Internet InquiryBenoit Girard, Centre National de la Recherche ScientifiqueNina Yang, Software Engineer, TranspositAJ Alvero, PhD Candidate, Stanford UniversityPaul J. Weiss, PhD candidate, Information School, University of WashingtonHamed Alemohammad, Executive Director and Chief Data Scientist, Radiant Earth FoundationDaniel Zhang, Research Manager, Stanford Institute for Human-Centered AIDevesh Narayanan, AI ethics researcher, National University of SingaporeKari Zacharias, Assistant Professor, Centre for Engineering in Society, Concordia UniversityNeha Kumar, Ph.D., Georgia Institute of TechnologyBrittany Johnson-Matthews, Assistant Professor in CS at George Mason UniversityAli Zaidi, Microsoft, Technology and ResearchAnne-Laure Fayard, NYU Tandon School of EngineeringKrystal Cooper, Technical Producer, Spotlight SFKatie Martin, PhD Student, Massachusetts Institute of TechnologyRabbi Eli Cohen, Chadeish Yameinu Jewish RenewalAnusha Sethuraman, Head of Marketing, Fiddler AI; Member, Women in AI EthicAmber Simpson, PhD, Canada Research Chair in Biomedical Computing & Informatics, Queen's UniversityMichael Katell, Alan Turing InstituteAlyssa Jarvis, Sr. Program Manager Corporate DEI Services, AnitaB.orgRosemary King, Head of Product. FloatApp, FinTech Nick Tasker, QA Associate, WaymoSarah Gilbert, Postdoctoral Scholar, University of Maryland College ParkAadita Chaudhury, PhD Candidate, Department of Science and Technology Studies, York UniversityKathleen McDermott, Industry Assistant Professor, Integrated Digital Media, NYU Tandon Aimi Hamraie, Vanderbilt University, Critical Design LabMelissa Siah, SyntacogShani Jayant, Principal Design Researcher, Volkswagen Group of AmericaDanny Spitzberg, User ResearcherMichelle Feng, CaltechHezekiah Branch, Tufts UniversityCallie Palmer, Doctoral Candidate, Washington State UniversityKatie Martin, PhD Student, Massachusetts Institute of TechnologyAnusha Sethuraman, Head of Marketing Fiddler AI Member, Women in AI EthicsAndromeda Yelton, AI/ML software engineer and Lecturer, San Jose State University iSchoolSusan Kennedy, Postdoctoral Fellow in Embedded EthiCS, Harvard UniversityDaniel Speckhard, AI Resident, XDaniel Gebremeskel, Senior Cyber Security AnalystBlake Pellman, Ph.D., User Experience Researcher, MicrosoftNicolas Valdes-Meller, MIT graduate student in theoretical physicsAsmeret Asefaw Berhe, Professor, Soil Biogeochemistry, Falasco Chair in Earth Sciences, Life & Environmental Sciences Dept., Interim Associate Dean of the Graduate Division, University of California, MercedDorothy Kidd, Professor, Department of Media Studies, University of San FranciscoDavid Su, Audio Software Developer at Output, formerly MIT Media LabElliot Smith, Research Associate, The University of IowaTC Burnett, MSW, Associate Director, Actionable Intelligence for Social Policy, Univeristy of PennsylvaniaTamara Kneese, Ph.D., Assistant Professor, Media Studies Department, Program Director, Gender and Sexualities Studies, University of San FranciscoMeg Young, Postdoctoral Fellow, Cornell TechUrmila Janardan, Research Assistant, UpturnBryan Bischof; Senior Data Scientist, Stitch Fix; Lecturer of Data Science, Rutgers UniversityPeyten Sharp, Berkeley Law School Logan Koepke, UpturnRenata Barreto, JD/PhD Candidate, Berkeley LawDavid Scharbach, founder Toronto Machine Learning SocietyMeera Desai, Computational Research Fellow, American Museum of Natural HistoryArjun Arora, Machine Learning Engineer, Stanford AlumniDawn Barnhart, Senior UX Designer, Adjunct Faculty  Denver UniversityNathan Schneider, Assistant Professor Linguistics and Computer Science, Georgetown UniversitySeny Kamara, Brown UniversitySwee Leng Harris, LuminateMatthew Garrett, Staff Security EngineerAlexi Orchard, PhD Student, Student Chair of Council for Responsible Innovation & Technology, University of Waterloo.Casey Trimm, Digital Marketing CopywriterJwahir K. Sundai, Software Engineer Falaah Arif Khan, Montreal AI Ethics Institute and NYU Center for Responsible AIMadi Whitman, Postdoctoral Research Scholar, Columbia UniversityPriyansi, Student, Kalinga Institute of Industrial TechnologyJanelle Shane, AIWeirdness.comLaura Bacon, Director of Partner Support, LuminateValorie Thomas, Professor of English and Africana Studies, Pomona CollegeKyle Kubler, PhD Candidate, Department of Communication, University of WashingtonAdrian Gonzalez. Ph.D. Student at Cinvestav, MexicoPatrick Love, Assistant Professor of English, Monmouth UniversityMatthew Weinstein, PhD, Professor Science Education, University of Washington-TacomaClara L. Wilkins, Ph.D. Associate Professor, Psychological and Brain Sciences Washington University in St. LouisConlin Durbin, Senior Software EngineerKyle W. Williams, Assistant Director, ESTEEM Graduate Program at the University of Notre DameAnita Say Chan, Associate Professor, School of Information Sciences and College of Media and Institute for Communications Research, University of Illinois, Urbana-Champaign Faculty Affiliate, Data & SocietyJordan Kraemer, PhD, NYU Tandon School of EngineeringArvind Narayanan, Associate Professor of Computer Science, Princeton UniversityRobyn Perry, Analytical Linguist, GrammarlyRebecca Tabasky, Director of Community, Berkman Klein Center for Internet & Society at Harvard UniversitCollins Korir, Lecturer and Research ScientistAlex Chouldechova, Assistant Professor, Carnegie Mellon UniversityProfessor Lynne Baillie, Professor of Computer Science, School of Mathematical & Computer Sciences, Heriot-Watt UniversityLauren Smith, PhD candidate, University of WaterlooKalpana Shankar, Professor, School of Information and Communication Studies, University College DublinSydney Ji, University of California, BerkeleyJosiah Hester, Assistant Professor, Electrical and Computer Engineering and Computer Science, Northwestern UniversityMadelyn Rose Sanfilippo, Assistant Professor, University of Illinois at Urbana-ChampaignSareeta Amrute, Associate Professor of Anthropology, UW SeattleVictoria Firsanova, Graduate Student, Saint Petersburg State University, RussiaAnna Ma, Research Assistant at Social Studies of Computer Science Lab, McGill UniversityNolawit Mulugeta, Research Support Associate I  Manalis Lab, Koch Institute for Integrative Cancer Research  MITVinicius Mesquita de Pinho, Federal University of Rio de JaneiroJennifer BrodyLesley Cordero, Tech Lead at Teachers Pay Teachers, previously on Google ClassroomMaarten Sap, PhD students, University of WashingtonWinifred Poster, Founder, Labor Tech Research NetworkPreeya Phadnis, Data ScientistRabbi Debra KolodnyKatrin Weller, Senior Researcher, GESIS  Leibniz Institute for the Social SciencesVinyas Harish, MD/PhD student, University of TorontoAndrew Ross, PhD Candidate, Harvard UniversityCana Uluak Itchuaqiyaq, Utah State UniversityMario Gomez, Founder, Hackerspace San Salvador. El SalvadorTegan Maharaj, PhD studentAlex Leavitt, Researcher, FacebookDr. Rachael Tatman, former GooglerDr. Andreas Bischof, Chemnitz University of Technology (Germany)Jacqueline Valeri, PhD Candidate, MITKate Devlin, Senior Lecturer in Social and Cultural Artificial Intelligence, King's College LondonSheldon GayKaren Rustad Tolva, Software Engineer, Science Division, Chan Zuckerberg InitiativeMaralie Armstrong, Visiting Assistant Professor, Modern Culture and Media, Brown UniversityPeter Micek, Adjunct Professor, Columbia University School of International and Public AffairsProf. Paul Duguid, School of Information, University of California, BerkeleyEliska Pirkova, Europe Policy Analyst, Access NowJasmine Jones, PhD, Assistant Professor of Computer Science, Berea CollegeJulie Pitt, Director of Machine Learning Infrastructure, NetflixCheryl Holzmeyer, Research Fellow, Institute for Social Transformation, UC-Santa CruzWendy Yindi Pei, Designer, MicrosoftKaitlin L. Costello, Assistant Professor, Department of Library and Information Science, Rutgers UniversityEs Braziel, Senior Design Researcher, MicrosoftAllison Walters, Software EngineerDan Jurafsky, Professor, Stanford UniversityIvana Feldfeber, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaMailen Garcia, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaSabina Bercovich, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaSusana Baquero, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaMartina Cantaro, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaYasmin Quiroga, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaManiuryis Pena Azahares, Observatorio de Datos con Perspectiva de Genero Argentina / Gender Data Observatory ArgentinaInna Arzumanova, Assistant Professor, University of San FranciscoTim Jurka, Director of Artificial Intelligence, LinkedInChristan Grant, Assistant Professor, University of OklahomaTrucy Phan, Lead Product Designer, AddeparAparna Ananthasubramaniam, Ph.D. Student, School of Information, University of MichiganSu Lin Blodgett, Microsoft ResearchGabriel Poesia, PhD student in Computer Science, StanfordHarlan D. Harris, PhD, Data Science ManagerCynthia Khoo, JD, LLM (Law and Technology), Tekhnos LawAlyssa Kann, Research AssistantChinasa T. Okolo, PhD Student, Cornell UniversityShelly Steward, PhD, Associate Director of Research, Aspen Institute Future of Work InitiativeBeza Merid, Ph.D., Research Fellow, University of Michigan Medical SchoolShaun Gittens, Ph.D., Director for Machine Learning Capability, MasterPeace Solutions, LtdAmy McGovern, Professor, University of OklahomaAndrea Fisher, M.D. Candidate at Stanford School of MedicineJudeth Oden Choi, PhD student, HCII, Carnegie Mellon UniversityAl Smith, Software EngineerVianey Leos Barajas, Assistant Professor, University of TorontoAlissa Black, LuminateKaitlyn Carter, Software EngineerSophia Bessias, Data Scientist, UNC HealthAlisha Ehrlich, D&I Lead & RecruiterProfessor Mark Graham, Oxford Internet InstituteChristoph Becker, Associate Professor of Information, University of TorontoDr. Tesfa Tegegne, Bahir Dar UniversityRoberta JantzInes Moreno, Undergraduate Student, McGill UniversityLillian Chin, PhD Candidate, MIT Computer Science and Artificial Intelligence LaboratoryDavid O'Sullivan, Senior Software Engineer Roy Yap, London Westminster CollegeHaley Adams, Vanderbilt UniversityStryder Crown, Software Engineer, former GooglerAlex Markovits, Software Engineer, One Concern, Inc.Henry Lu, Graduate Student, McGill University, MilaTeemu Roos, Professor of Computer Science, University of HelsinkiEzra Ford, Founder, data4humanityJenny Wang, MS Student, University of Texas School of Public HealthAspen Russell, PhD Student, Cornell UniversityMaggie Ambrose, Student, University of Texas at AustinJulia Slupska, DPhil Student, Oxford Internet InstituteLaura Ammon, Ph.D., Associate Professor of Religion, Appalachian State UniversityPoornima Haridas, Courant Institue of Mathematical Sciences, NYUShane Steinert-Threlkeld (he/him), Assistant Professor, Department of Linguistics, University of WashingtonIrina Preda, Data Scientist, Gower Street AnalyticsRon Eglash, Professor, School of Information, University of MichiganDr. Karen Kelsky, Founder and CEO, The Professor Is InAndy Coravos, CEO, Elektra LabsNaomi Elle Beach, Undergraduate Student, Biology Major, Bowie State UniversityPaul Byrne, Business Consultant, FineosStathi Fotiadis, PhD student, Imperial College LondonAvital Vainberg, student, MIT Department of Urban Studies and Planning and MIT Department of Electrical Engineering and Computer ScienceAnasuya Sengupta, Whose Knowledge?Hunter Lang, PhD student, MITSandhini Agarwal, OpenAIBeth, Becker Digital StrategiesLindsey Gray, PhD, Scientist, Fermi National Accelerator LaboratoryAditi Gupta, PhD, Postdoctoral Researcher, MIT, Scientific Citizenship Initiative at Harvard Medical SchoolAmelia Hardy, Graduate Student in Computer Science, Stanford UniversityMegan Price, Human Rights Data Analysis GroupDr Rebekah Cupitt, Lecturer, Dept of Film, Media & Cultural Studies, School of Arts, Birkbeck, University of LondonKatie Huang, Software Engineer, ZocdocJamie Riden, IOActiveDr. Andrea Frome, Former Google Brain Eva Garcia Martin, Ph.D., Data Scientist, Ekkono SolutionsM. A. Kelly, Assistant Professor, Department of Computer Science, Bucknell UniversityDespina Pantazi, PhD Student, University of AthensMercedes Bunz, King's College LondonRobyn Speer, Chief Science Officer, LuminosoNicholas Caverly, Assistant Professor, University of MassachusettsMaria Gargiulo, Statistician, Human Rights Data Analysis GroupHannah Stevens, Graduate Instructor, Utah State UniversitySteven Bedrick, Associate Professor, Oregon Health & Science UniversityJarod Deweese, Graduate student in Data Analytics at Kansas State UniversityDustin J. Mitchell, Staff Software Engineer, MozillaPatrick Ball, PhD, Director of Research, Human Rights Data Analysis GroupMojtaba Khomami Abadi, Stratuscent ResearchWalker Gosrich, PhD Student, University of PennsylvaniaD.J. Sutherland, Assistant Professor, University of British ColumbiaHouda Lamqaddam, KU LeuvenStephen Pfohl, PhD Candidate, Stanford UniversityAruna S, grad student, MITTracy J. Trothen, Professor, Queen's UniversityLindsey Kuper, Assistant Professor, Computer Science and Engineering, UC Santa CruzSylvain Gugger, Research Engineer at Hugging FaceJ. Ereck Jarvis, Ph.D., Assistant Professor of English, Coordinator of Graduate Studies, Northwestern State University Dan Tasse, Data Scientist, Stitch FixBecca Ricks, Researcher, Mozilla FoundationGuhan Ram Venkataraman, Ph.D Student, Biomedical Data Science, Stanford UniversityJared Katzman, Research Assistant at Microsoft ResearchTanvi Sheth, PhD Candidate, UC Santa Barbara Margrete Svareid, Design Research Lead, MicrosoftProfessor Mick Grierson, Research Leader, UAL Creative Computing InstituteMelissa V. Abad, PhD, Research Associate, VMWare Women's Leadership Innovation Lab, StanfordAlex Lu, PhD student, University of Michigan School of InformationErika Szymanski, Assistant Professor of Rhetoric of Science, Colorado State UniversityTheodore Kim, Associate Professor, Computer Science, Yale UniversityDr Tanya Lokot, Assistant Professor, School of Communications, Dublin City University Nam Do, Brown UniversityBriana Vecchione, Cornell UniversityLewis Miles, PhD Candidate, University of MichiganNaji Shajarisales, PhD student, Carnegie Mellon UniversityHannah Wise, News Partnerships Lead, Brown Institute for Media Innovation Local News Lab, Columbia UniversityDr Theresa E Miedema, Ontario Tech UniversityErik Osheim, Staff Software Engineer, StripeEkaterina Babintseva, Postdoctoral Fellow in STS, Harvey Mudd CollegeRobert Giaquinto, Computer Science PhD Candidate at the University of MinnesotaElizabeth Reddy, PhD, Assistant Professor, Engineering, Design, & Society, Colorado School of MinesPeter Alvaro, Assistant Professor, University of California, Santa CruzOlivia J. Mendez, Software Engineer, C2FOChiin-Rui Tan, Tech Entrepreneur, former Head of Data Science at UK Foreign & Commonwealth OfficeMichael Sani, Data Scientist, Bowery FarmingNicole Ersaro, Biomedical Informatics PhD Candidate, Stanford UniversityIris Clever, Postdoctoral Research Fellow, University of Chicago Sarajane Alverson, M.A., Data & Operations Analyst, YOHAudrey Ettinger, Ph.D., Associate Professor of Biological Sciences, Cedar Crest CollegeSarah Moir, Splunk Inc.Jenny Brennan, Researcher, Ada Lovelace InstituteBobby Madamanchi, UMSI Lecturer IIIVid Kocar, PhD, Quality Assurance ManagerDanaja Maldeniya, PhD Candidate in Information, University of MichiganElle O'Brien, Ph.D., Lecturer at University of Michigan School of Information and Data Scientist at Iterative.aiHaitham Gasim, Technology Consultant and Design Justice Principles SignatoryDanielle Leong, Data Engineer, Teachers Pay TeachersNneka Soyinka, Privacy Manager, MozillaChristian Sandvig, Director and Professor, Center for Ethics, Society, and Computing (ESC), University of MichiganDiletta Huyskes, Privacy NetworkLucy Lin, PhD candidate, University of WashingtonLee Butterman, Core Platform Engineer at ScribdDavid Jurgens, Assistant Professor, School of Information, University of MichiganChristopher Santo Domingo Chan, PhD Student, Department of Anthropology, University of WashingtonDylan Phelan, Software Engineer and Graduate Student, Tufts UniversityRobin Brewer, Assistant Professor, University of MichiganAlannah Lejeune, PhD student, NYU School of Medicine Olivia Brode-Roger, Graduate Student at CSAIL, MITKay L. Kirkpatrick, University of Illinois at Urbana-Champaign, Departments of Mathematics and PhysicsOriol Nieto, Staff Scientist, Pandora / SiriusXMJesse Seegers, Adjunct Professor, Integrated Design and Media, Technology, Culture & Society, Tandon School of Engineering, New York University, and Faculty, Parsons School of Design, Creative Publishing and Critical Journalism Programs, The New School for Social ResearchLaura Jakli, Harvard Society of FellowsBrittney Hall, Technical DirectorVictoria Copeland, Ph.D Student, University of California Los AngelesEric Walkingshaw, Assistant Professor, School of EECS, Oregon State UniversityChirag Agarwal, Postdoctoral fellow, Harvard UniversityNiloufar Salehi, Assistant Professor, School of Information, UC BerkeleyMimi Onuoha, Artist Stephanie Wang, Apple AI/ML software engineerRoya Moussapour, MIT Comparative Media StudiesAshwin Rajadesingan, PhD candidate, University of MichiganCassidy Pyle, PhD Student, University of MichiganReshma Saujani, CEO and Founder, Girls Who Code Mustafa Naseem, Clinical Assistant Professor, University of Michigan School of InformationShawn Aldridge, Senior Software Engineer, TinderEvan Krueger, Software Engineer at TokenMelissa Chalmers, Lecturer, University of MichiganMary L. Gray, PhD, Senior Principal Researcher, Microsoft Research, Faculty, Luddy School of Informatics, Computing, and Engineering; Anthropology; Gender Studies, Indiana University, Berkman Klein Center for Internet and Society, Faculty Associate, Harvard UniversityLiza Gak, PhD Student, UC Berkeley School of InformationRui Pereira, xoogler, NYU AdjunctRenee Desjardins, Ph.D., associate professor, Universite de Saint-Boniface, CanadaLisa Dawn Colvin, SXM PandoraAnjali Singh, Ph.D. Student, University of Michigan Abraham Mhaidli, PhD Candidate at the School of Information, University of MichiganHayden McTavish, computer science and statistics student, University of British ColumbiaKatherine Lawrence, PhD, School of Information, University of MichiganSenait Fisseha, MD, JD, Director of Global Programs, Professor of Obstetrics & Gynecology, The Susan Thompson Buffett FoundationSina Fazelpour, Postdoctoral Fellow, Carnegie Mellon UniversityDaniel Sprague, Undergraduate Researcher at Duke Institute for Brain SciencesPaul Dourish, Chancellor's Professor, UC IrvineHeather Huynh, Software Engineer, FacebookYasaman Sefidgar, PhD Student, Allen School, University of WashingtonJoshua Holden, Professor of Mathematics, Rose-Hulman Institute of TechnologyAngel Alexander Cabrera, PhD Student, School of Computer Science, Carnegie Mellon UniversityChristopher Thomas, Software EngineerKate Sim, PhD Candidate, Oxford Internet InstituteLucy Bellwood, Cartoonist and Contractor for Google AIKelly Engel Wells, Esq.Srujana Katta, PhD student, Oxford Internet Institute, University of OxfordLiz Burgess, Systems Engineer, LaserficheEdward L Platt, PhD Candidate, University of Michigan School of InformationNathan Kapoor, PhD, Visiting Assistant Professor, History of Science, Technology and Medicine, Grand Valley State UniversityCarla Gannis, EducatorMinji Kong, PhD Student, University of DelawareBenjamin H. Bradlow, Postdoctoral Fellow, Weatherhead Center For International Affairs, Harvard UniversityGrace Li, PhD student, UCLAJenny Zhang, Staff Software Engineer, MozillaSean Griffin, Founder, Disaster TechDaniel Griffin, PhD student, UC BerkeleyBirna van Riemsdijk, University of TwenteScott Gigante, PhD Candidate, Yale UniversityDr. Angela Xiao Wu (she/her), Assistant Professor, Media, Culture and Communication, New York UniversityGregory Yauney, PhD student in Computer Science, Cornell UniversityFelicia Gershberg PhD, Together We Will  San JoseStuart Geiger, Assistant Professor, Dept of Communication & Halcoglu Data Science Institute, University of California, San DiegoBryan Hadden  Hadden IncJess Jones, Center for User Experience, University of Wisconsin-MadisonLindsay Oliver, Activism Project Manager, Electronic Frontier FoundationJason Axley, Expedia Group, Principal Security Software EngineerRachel Lo, Director, Solutions Engineering, West, ANZ & CommerciaJ. Rosenbaum, RMIT University, Melbourne AustraliaOliver Haimson, Ph.D. Assistant Professor, University of MichiganJessica Lennard, Policy and Regulatory Lisa Vermillion, Director of Product, Health Technology at Johnson & JohnsonJacalyn Brecher MBA, Humane Technology Advocate, Brown University'82, UCLA '84James Cuff, Retired Assistant Dean & Distinguished Engineer, Harvard UniversityBrittany Wilbert, Security Engineer, Former GooglerNovall Khan, Software EngineerBijal Mehta, Undergraduate Student at Northwestern UniversityFrancesca Loiodice, Undergraduate student, Barnard CollegeHanyu Chwe, PhD Student, Northeastern UniversityLemi Baruh, Associate Professor, Koc UniversityKristen Collins, Senior Fellow, Mercatus Center, George Mason UniversityRob Newby  CEO at Procordr Ltd. (United Kingdom)Elsa B. Kania, PhD Candidate, Harvard UniversityValerie Barr, Mount Holyoke CollegeMichelle Wolfe DPO, U.K. local government Mark Ho, Postdoctoral Researcher, Princeton UniversityDr. Buthayna ElHajj, Research ScientistDawn Nafus, Senior Research Scientist, Intel and Visiting Scholar, University of OtagoDevney Hamilton, Software Engineer at Sassafras Tech Collective (Stanford MSCS in AI)Tsega A. Worku, Sr. Director of Marketing & Strategic Partnerships, TurnKey MarketingIsabel J. Rodriguez, Oregon State University, MS Student, Physics & Ethnic StudiesCharles McHenry, partner, Green EconometricsRahul Mittal, Undergraduate Student, Florida International UniversityProfessor, Sciences Po ParisMichaelanne Dye, Assistant Professor, University of Michigan School of InformationMichael Van Veen, senior software engineer / ML operations lead @ Elementary RoboticsMilen, Public Health Contract ManagerLawrence Humphrey, Founder of Tech Can [Do] BetterCatherine Ponte, Adjunct Instructor, Department of Technology, Culture, and Society, NYU-Tandon School of EngineeringColorado Reed, CS PhD, UC BerkeleyKathy Pham, Fellow, Responsible Computer Science, MozillaTadhg O'Sullivan, Sales Manager, SamsaraTom Williams, Colorado School of MinesTruc Nguyen, Product DesignerJosh Guberman, PhD Student, University of Michigan School of InformationRachel Whaley, Technology Lead, LA Tech4GoodLuke Demarest, Associate Lecturer, University of the Arts LondonChetan Ganjihal, Data ScientistRenee Gosline, Massachusetts Institute of TechnologyMaria De-Arteaga, Assistant Professor, University of Texas at AustinColin Gordon, Assistant Professor, Drexel UniversityNathan Tessema Ersumo, PhD Candidate, UC BerkeleyClare Haru Crowston, Professor of History, University of IllinoisTad Mutersbaugh, geographer, University of KentuckyMartin Bulmer, Principal Researcher, Trade MeAriam Mogos, ConsultantRabbi Min Kantrowitz, Congregation Nahalat Shalom, Albuquerque New MexicoMira Schmitz, communication design studentJoan H. Fujimura, Martindale-Bascom Professor of Sociology, President, Society for the Social Studies of Science, Founding Director, STS Program and Holtz Center for the Study of Science and Technology Dr. Anthony J. Rhem, PhD., Founding Editorial Board: AI & Ethics Journal: https://www.springer.com/journal/43681 Magdalena Zapedowska, Assistant Director or Fellowships, Wesleyan UniversityNathan Drezner, Software Developer at Plotly GraphsAlice Thwaite, founder, Hattusia and the Echo Chamber ClubMyra Cheng, CaltechMax Kreminski, PhD Student, University of California, Santa CruzAssociate Professor Maryanne Large, University of SydneyBeth Kolko, Professor and CEOVilna Bashi Treitler, PhD, Professor, University of California Santa BarbaraRabbi Diane ElliotKathy Baxter, Principal Architect of Ethical AI, SalesforceAnahita Bahri, Insights Manager, NetflixKarim Boughida, Dean of University Libraries, University of Rhode IslandQinlan Shen, PhD Candidate, Carnegie Mellon UniversityCamilla Sarang Rettura, LUISS Universitygregory vassie, Sr. Systems Engineer, Pratt InstituteWossen Workneh, Accounting DirectorJennifer Rhee, Virginia Commonwealth UniversityAmy Keelty, Information Strategy & Governance Director, American Family InsuranceAntoinette Burton, Professor of History, Swanlund Endowed Chair, Department of History, University of Illinois Urbana-ChampaignEdo Roth, PhD student, University of PennsylvaniaEmily Tseng, PhD student in Information Science, Cornell UniversityNancy Smith, Assistant Professor, Pratt Institute Shelley Krause, StorytellerMadhurima Das, Graduate Student, MIT Department of Mechanical EngineeringRenjie Butalid, Montreal AI Ethics InstituteSerena Seshadri, Ph.D. Candidate, UCSBRachel Rudinger, Assistant Professor of Computer Science, University of MarylandDr. Philip N. Howard, Director, Oxford Internet Institute, Professor, Oxford UniversityNicholas Schiraldi, Ph.D, University at AlbanyKentaro Toyama, W. K. Kellogg Professor of Community Information,University of MichiganIris Howley, Assistant Professor of Computer Science, Williams CollegeSahara Ali, Salesforce Matthew Johnson, MicrosoftHaben Girma, Esq, Disability Justice Lawyer, Author, and SpeakerAyori Selassie, SalesforceM L Chan, Northwestern University, former GooglerShobita Parthasarathy, Professor and Director, Program in Science, Technology, and Public Policy, University of MichiganTara-Nicholle Nelson, Founder + CEO of SoulTourBrian Borncamp  DevOps Engineer  BookByteSahaana Suri. PhD Candidate, Stanford UniversityJulie Sparks, Security Engineer at CloudflareAmy Zuckerman, Recruiter, Pinterest Zoe Kahn, UC BerkeleyAdam Lopez, Research Manager, Rasa Technologies Inc. / Reader, University of EdinburghMatt Wallaert, Co-Founder, GetRaisedSky Davis, Product Design EngineerLatisha Ross, Research Associate, University of VirginiaSaleem Hue Penny, Diversity Equity Access & Inclusion Museum ProfessionalAline Vignoli de Lima, Full Stack Web Developer StudentCaroline Simard, Stanford UniversityZohaib Karim Noorani, Grad Student & Researcher at University of TorontoJacob Adamson, AmazonLeora Schertzer, Undergraduate Student, McGill UniversityGrace Eamer, Software DeveloperGabrielle Gantos, Associate Scientist, NCARElizabeth de Rham, Instructional CoachGrace Burleson, Design Science Researcher, University of MichiganPatricia Perozo, Software Engineer, PinterestSean Arseo, PhD Candidate, Sociology, University of California, DavisSennay Ghebreab, Professor of Socially-Intelligent AI, University of AmsterdamNima Boscarino, Machine Learning EngineerMoritz Hardt, Assistant Professor in EECS, UC BerkeleyYangfeng Ji, Assistant Professor, University of VirginiaGerasimos (Jerry) Spanakis, Assistant Professor, Maastricht UniversityJacob Adamson, AmazonWill Greenberg, DevOps Engineer at Electronic Frontier FoundationReverend Doctor Virginia Bemis, formerly research assistant to a sociologistSanjay Varma, Princeton University alumnusJess Moyer, Senior Scientist, Advancer Technology & Innovation, Xylem Inc.Yvonne Witter, Business Writer Researcher UKSreela Kodali, PhD Student in Electrical Engineering, Stanford UniversityJames MacGlashan, Sony AIYejin Choi, Associate Professor, University of Washington / Allen Institute for AIPeggy McCracken, Professor, University of MichiganStephanie Lampkin, Founder & CEO, BlendoorMegh Marathe, PhD candidate, University of Michigan School of InformationSharon Traweek, Gender Studies and History, University of California, Los AngelesAleksei Tiulpin, Post-doctoral fellow, KU Leuven, Belgium & University of Oulu, FinlandRisha P, Designer and Researcher Marissa Koors, Acquisitions Editor for Philosophy and Ethics, Wiley BlackwellNicholas Landry, PhD Student, Department of Applied Mathematics, University of Colorado BoulderSandra Siby, PhD student, EPFLEd Lambert, Lead Product Development Engineer, AT&T Research LabsDr Murray Goulden, University of Nottingham, UKDennis McLeod, UberflipNadya Peek, Assistant Professor, HCDE, University of WashingtonSane Gaytan, PhD, Universidad de ColimaAbiy Tasissa, Norbert Wiener Assistant Professor, Tufts UniversityBharath Narayanan, EPFLAdrian Wong, Community Data Clinic, Institute of Communications Research, University of Illinois Urbana-ChampaignLogan Williams, Research Professor, University of Maryland College ParkJennifer Stirrup, CEO and Founder of Data RelishAlice Zhao, Senior Data Scientist, MetisMelanie King-Dollie, UX/UI DesignerJuliette Chevallier, MS/MBA Candidate, MITSafinah Ali, PhD Student, MITJonathan Mboyo Esole, Professor of Mathematics, Northeastern UniversityJeffrey Green, Department of English, Farmingdale State CollegeSepideh Maleki, ResearcherHenry Lieberman, Research Scientist, Computer Science and Artificial Intelligence Lab (CSAIL), Massachusetts Institute of Technology (MIT)Vaishnav Kameswaran, PhD student, University of Michigan Jeff T. Sheng, PhD Candidate (Sociology), MS (Computer Science), Stanford UniversityBatya Friedman, Professor, The Information School, University of WashingtonMonika N. Lind, PhD Candidate in Clinical Psychology, University of OregonSheena Erete, Ph.D., DePaul UniversityRina Friedberg, Machine Learning Researcher, LinkedInAngie Wang, Claremont McKenna CollegeKristin Eaton| Vividion Therapeutics, Head of Intellectual PropertyEmily Martin, NYU, AnthropologyDr. Steve Kramer, Chief Scientist at KUNGFU.AILibby Hemphill, Associate Professor, University of MichiganNatalie Gable, Graduate Student, Stanford UniversityEthan Wilde, Full-time Faculty, CS Department, Santa Rosa Junior CollegeJonathan Scott, Department of Computer Science and Engineering Ph.D. Student, University of California Santa CruzKevin McKee, Professor of Gerontology, Dalarna University Erik Learned-Miller, Professor of Information and Computer Sciences, University of Massachusetts, AmherstDagmar Monett, Prof. Dr. Computer Science, AGISI.org and HWR BerlinKinjal Dave, PhD Student at the University of Pennsylvania's Annenberg School of CommunicationTjeerd Boonstra, Research Scientist, Maastricht UniversityAmy Csizmar Dalal, Professor of Computer Science and STEM Director, Carleton CollegeDonna H. Odierna, DrPH, MS, CIP, Associate Professor, Researcher, Certified IRB ProfessionalNazli Goharian, Ph.D., Clinical Professor of Computer Science, Information Retrieval Lab. Computer Science Department, Georgetown University Amanda Burke, PhD Student | Univ. of OklahomaMariam Asad, PhD, Sassafras Tech Collective Boaz Sender, Principal, BocoupPaul Libbrecht, professor of computer science, web-developer, IUBHKaren Boyd, University of Michigan School of Information Yvonne Lam, software engineeerMaria Ryskina, PhD Student, Carnegie Mellon UniversityChuck Anderson, Department of Computer Science, Colorado State UniversityMariah Peebles, Managing Director, AI Now Institute, New York UniversityLinda Huber, PhD Student, University of MichiganJaslyn Devi Cincotta, Labeling Program Manager, AirbnbCynthia Bailey Lee, Senior Lecturer of Computer Science, Stanford UniversityWendy van Duivenvoorde, Associate Professor, Flinders UniversityWendy Liu, Software EngineerEli Pariser, Civic SignalsMyanna Lahsen, Academic researcher, INPEDonna Gavin, Senior Lecturer, Department of Computer Science and Software Engineering, University of Wisconsin  PlattevilleKatherine Suazo, Software Engineer, Health In Her HUE and LearnabiAnthea Josias, Lecturer & Research Investigator, School of Information, University of MichiganTheresa Cheng, PhD Candidate, University of OregonSusan Etlinger, Senior Analyst, Altimeter, A Prophet Company; Senior Fellow, Centre for International Governance InnovationRuth E. Stern, Principal sternshus.com, University of WashingtonDuncan Faulkner, Founder and CEO, Equall.appDavid A. Wallace, Clinical Associate Professor, University of MichiganMeklit Workneh MD, MPH  Physician, FDAJulia R. DeCook, Assistant Professor, Loyola University ChicagoRegine Gilbert, Industry Assistant Professor, NYU Tandon School of EngineeringFabio Duarte, Principal Research Scientist, MIT Department of Urban Studies and PlanningYang Hong, Independent Data for Society Engineer, Shoshin Insights & South Park CommonsAnirudh Koul, PinterestAdriana Alvarado Garcia, HCC Ph.D. student at Georgia Institute of TechnologyOren Mizrahi, California Institute of TechnologyYoav Schlesinger, Principal, Ethical AI Practice, SalesforceKatherine Furl, Graduate Student, University of North Carolina at Chapel HillLauren Chen, Harvard College '24Forest Richter, Co-Founder & CEO, Uncrowd.ioEvan Shapiro, CEO of O(1) LabsCyrus Hall, PhD, most recently Principal Engineer Amazon Web Services (currently on a break)Ian Arawjo, PhD Candidate, Cornell UniversityRita DeRaedt, Senior Designer at SpotifySun-ha Hong, Assistant Professor, Simon Fraser UniversityErica Greene, Engineering Manager at EtsyFallon Wilson, PhD, #BlackTechFutures Research InstituteDr Jack Stilgoe, Associate Professor, Science and Technology Studies, University College LondonLisa Cowan, PhDJulius von Kugelgen, PhD candidate, Max Planck Institute for Intelligent Systems, Tubingen & University of Cambridge Bari Dzomba MS, PhD, Asst Professor, Health Informatics, Temple University College of Public HealthLindsay Fasser, Community Engagement Lead at Ksana HealthJohan Michalove, Researcher, 3A Institute, The Australian National UniversityLauren Cech, Graduate Student Researcher, University of California, San FranciscoMatthew Faytak, Ph.D., Postdoctoral Fellow and Lecturer, Dept. of Linguistics, UCLADavid Raji, Software Engineer, Platform Services, GE HealthcareDaniel Woodard, Educational Assistant, Montgomery County Educational Service CenterFlarnie Marchan, Staff Software Engineer at CheggAndrew Lison, Assistant Professor of Media Study, University at BuffaloDanay Weldegabriel, EOP STEM Counselor  BEES Program Coordinator, University of California, Santa CruzFolasade Ayoola, PhD Student, Stanford UniversityDaniel Harlow, Assistant Professor, Massachusetts Institute of TechnologyLucy Hu, PhD. Candidate, Massachusetts Institute of TechnologyAnde Reisman, PhD, UX ResearcherMitali Thakor, Assistant Professor, Science in Society Program, Wesleyan UniversityMarkus Giesler, Ph.D., Associate Professor of Marketing, Schulich School of Business, York UniversityRohan Pinto, CTO, 1Kosmos Inc.Abigail Jacobs, University of MichiganAthena Owirodu, Research Assistant, Institute for Development ImpactDylan Hadfield-Menell, PhD Candidate, UC Berkeley EECSSandra Camacho, Inclusive Design Strategist, Former GooglerAddhyan Pandey, Director Data Science, Cars.comStephen C. Rea, Colorado School of MinesKenneth Conley, Former Staff Engineer at Google Research Kat Zhou, product designer @ SpotifyLaura Biester, PhD Candidate, University of MichiganJavier Chen, Diversity, Inclusion, Belonging Talent PartnerCooper Quintin, Senior Security ResearcherBrendan King, Software EngineerAnna P Meyer, PhD student in Computer Science, University of Wisconsin  MadisonAmrit Krishnan, Research EngineerAli ShahMartha Saavedra, PhD, Associate Director, Center for African StudiesCarlotta A. Berry, Professor Electrical and Computer Engineering ROSE-HULMAN INSTITUTE OF TECHNOLOGYClare F McCann, Project CoordinatorSuzanne Sindi, Associate Professor, UC MercedKate Kallot, Head of Emerging Areas Nvidia, Black in AI memberAnvit Srivastav, Web Dev Educator, BranstationAja Grande, MIT, PhD Student in History, Anthropology, Science, Technology & SocietyPauloes Berhe, Systems AdminCharles Earl, Data Scientist, Automattic.comAndrea Villa, Senior User Experience Researcher, WorkdayCheyanne Harris, Graduate Student, University of California DavisGeorge Berry, PhD, Data ScientistMinisters Fellowship-Western AustraliaSarah Kushner, PhD student, University of TorontoL. Elisa Celis, Statistics & Data Science, Yale UniversityIan Goodfellow, former Google Senior Staff Research ScientistJack Hessel, Postdoc, AI2Tina Cheuk, Assistant Professor, California Polytechnic State UniversityRabbi Chaim Schneider, Santa Cruz, CAByron Galbraith, CTO, TallaFabiola Pina, Software Engineering Fellow at General AssemblyMike A. Merrill, Computer Science PhD Student, University of WashingtonCorey O'Malley, Postdoctoral Researcher, UCLAViplav Madasu, Software EngineerNick Alico, Penn State Schreyer Honors College UX StudentBishwas Bhatta, Computer Engineer / Grad student, KU LeuvenDelanie West, Co-Founder Project AdviseHERJennifer Pfeifer, Professor, University of Oregon, and Co-Director, National Scientific Council on AdolescenceJon Bauman, Staff Platform Engineer, MozillaNishanth Anand, PhD student, McGill UniversityJennifer Cobbe, Research Associate, University of CambridgeNisha Devasia, MITYoel Sumitro, tiket.comJoe Scherrer, PhD student, MITTaylor Jacovich, Physics PhD Candidate, The George Washington UniversityShawn K, Computer ScientistMatt Zucker, Associate Professor of Engineering, Swarthmore CollegeRigoberto Lara Guzman, Data & SocietyKathy Reid, Open Source Voice Specialist, Mozilla and PhD Candidate, Australian National UniversityAaron Hamer, University of EdinburghEmory James Edwards, Informatics Ph.D. Student, UC IrvineDoug Berry, Associate Scientist, FermilabAfra Feyza Akyurek, PhD Student, Boston UniversityNathan Ebikwo Aspire PM at MicrosoftChristina Yuan, PhD Student, University of Texas at AustinClint Valentine, Director of Computational Biology, TwinStrand BiosciencesValeri Jean-Pierre, PhD student at Michigan State UniversityAlex Tsiatas, engineering manager at SquareSteven Wang, PhD Candidate, University of Wisconsin-MadisonSeyram Avle, Assistant Professor, UMass AmherstClara Na, Student, University of VirginiaLorena Mesa, Data Engineer & Data ScientistJohnny Tilahun, Student at the University of California, Santa CruzMuhammed Razzak, PhD Student, University of OxfordAisha Ghori Ozaki, Diversity, Equity, Inclusion + Justice colleagueRashima, Sonson/Founder, SONSONAnia Calderon, Executive Director, Open Data Charter Stephen Ware, Assistant Professor, Computer Science, University of KentuckyEsther Mwema, Atlantic Fellow for Social and Economic Equity at London School of EconomicsAllen Gunn, Executive Director, AspirationLev Tsypin, PhD Candidate and NSF Graduate Research Fellow, California Institute of TechnologyHan Na Shin, PhD Student, University of Michigan School of InformationJen Harrison, Research Portfolio, University of SydneyCella Monet Sum, Research Assistant, UC IrvineStacy Branham, Department of Informatics, University of California, IrvineNedah Nemati, PhD Candidate, University of Pittsburgh Carl Youngblood, Sr Solutions Architect, Amazon Managed BlockchainIman Williams, Software EngineerYutong Xie, University of MichiganAnjali Mehta, Chief of Staff, Civic Software FoundationKimberly King , Out Think The Box Bonnard, Shari  Senior Software Engineer, Consultant Information TechnologyKyle Napierkowski, Managing Partner, Greendata.ai John Wu, Postdoctoral Researcher, Space Telescope Science InstituteChrista Keizer, Design Researcher, MicrosoftAutumm Caines, Instructional Designer, University of Michigan  Dearborn Gabriel Velazco, Student, Health Information Management, Temple UniversityLeelan Farhan, PhD Student, Concordia UniversityRahul Jayaraman, MIT Physics PhD StudentCorianne Baker, Data ScientistBranden Born, Associate Professor, Director, Center for Livable Communities, Department of Urban Design & Planning, Co-Director, Livable City Year lcy.uw.edu, University of WashingtonWallace Grace, PhD Student, Education Policy Studies, University of Wisconsin  MadisonNicholas Oberly, Johns Hopkins UniversityKen Akiha, Curriculum Development Manager, Code.orgRaphael Crawford-Marks, CEO, BonuslyWendy Small CPA, Data Literacy AdvocateIan Stewart, University of MichiganJulie Gerlinger, Registered Nurse Care Manager & Nurse InformatacistPaul Roquet, Associate Professor, MITEarl Arvin, ML ResearcherAllegra Fonda-Bonardi, Doctoral student researcher, University of Michigan, School of InformationBrenda Buchanan  Design Researcher  DropboxShems Saleh, Member of AI Technical StaffWilfredo Merced UX designer Riyad Alalami, Software EngineerBenjamin Kodres-O'Brien, PhD Student in Communications, Columbia UniversitySuman Kalyan Bera, Postdoctoral Scholar, UC Santa CruzAlex Rossell Hayes, Graduate Student, UCLAAlok Tripathy, PhD Student, UC BerkeleyDaniel Cerkoney, PhD Candidate, Department of Physics and Astronomy, Rutgers UniversityHalcyon M. Lawrence, Assistant Professor of Technical Communication, Towson University, MarylandNoura Farra, Data Scientist, MicrosoftRyan Steed, PhD Student, Carnegie Mellon UniversityNick Doiron, Software Engineer, MGGG  Tufts UniversityLuke Hillman, Lead UX Designer at CheggMatias Valdenegro-Toro, German Research Center for Artificial Intelligence, Bremen, GermanyNishant Subramani, MasakhaneVeronika Strnadova-Neeley, Montana State UniversityAlemayehu Geda, Professor of Economics, Addis Ababa UniversityEmma Suryani, Data ScientistDylan Flesch, MLIS (2014), University of Washington iSchoolCade Wolcott, Software Engineer and University Innovation FellowElaine Montilla, Chief Information Officer, The Graduate Center, CUNYJustice Amoh, Ph.D, Chief Technology Officer, ClairwaysAl Barrentine, Data Scientist at Florida Rights Restoration CoalitionMaroussia Levesque, Harvard Law SchoolFrida Polli, phd, Cofounder + CEO, pymetrics Michelle Kim, AwakenNicole Sanchez, Founder & CEO, Vaya ConsultingGeoff Desa, Professor, San Francisco State UniversityBrian Clifton, Senior Data Scientist, BuzzFeedRachel Ward, Professor of Mathematics, UT AustinEllen Berrey, SociologistMichael Martin, Director of Community, SignalFireAlex Buraczynski, Software Engineer at Allen Institute of Artificial IntelligenceRob Katz, Office of Ethical and Humane Use of Technology, SalesforceEllen Berrey, Associate Professor, University of Toronto Min Baek, Founder at Philosophy of Computation at Berkeley, former Software Engineer at MicrosoftRicardo Luis de Azevedo da Rocha, Assistant Professor, Universidade de Sao Paulo, Escola Politecnica da USP, Computing Engineering courseEric Jenkins, StanfordIbrahim Dahlstrom-Hakki, Senior Research Scientist, TERCAurelien Serre, Data ScientistPreeti Ramaraj, Ph.D Candidate in Computer Science, University of MichiganMarcelo De Barros, Sports & Esports General Manager, MicrosoftNathaniel Imel, graduate student at the University of WashingtonJuan M. Lavista Ferres, Lab Director, Microsoft AIKhalia Braswell, Ph.D Student, Temple UniversityDaniel N Wilke, Associate Professor, University of PretoriaJulia Hockenmaier, Associate Professor, Computer Science, University of Illinois at Urbana-ChampaignDebbie Reynolds, Debbie Reynolds Consulting, LLC, Founder, CEO, and Chief Data Privacy OfficerJavad Hashemi, Data Scientist, StitchFixMeseret Haileyesus, Executive Director, Canadian Center for Women's Empowerment (CCFWE)Arto Laitinen, Professor, Philosophy, Tampere University, FinlandSeamus Ross CorrFRSE, Professor, Faculty of Information at the University of TorontoHiwot Tesfaye, Data ScientistAlyssa Li, Graduate Student, StanfordA. Feder Cooper, PhD Student, Department of Computer Science, Cornell University Dr Liam Magee, Principal Research Fellow, Institute for Culture and Society, Western Sydney UniversityStephen Larin, Assistant Professor of Political Studies, Queen's University, CanadaDr. Ermias Betemariam, Landscape EcologistKiran Vodrahalli, Ph.D. Student at Columbia UniversityMutahi Waruhiu, Software EngineerIhudiya Finda Williams, PhD candidate, University of MichiganMaria Mango, Colorado State UniversityDouglas Mellon, Researcher, University of Northern ColoradoCostis Dallas, Faculty of Information, University of TorontoCatherine Cronquist Browning, Assistant Dean, University of California, Berkeley, School of InformationJane Im, PhD student, University of MichiganMay-Li Khoe, Co-Founder and Co-CEO, MakeSpace FoundationDeji Adepetun, Software Engineer, WaybridgeRebecca Williams, Technology and Public Purpose Fellow, Belfer Center, Harvard Kennedy School Daryl Graves, Director of DEINeel Banerjee, President Silicon BoxOssi Rahkonen, Emeritus Professor of Medical Sociology, University of HelsinkiFounder & CEO, ColorStackTravis Vachon, CEO, itmeMolly Diesing, Professor of Linguistics, Cornell University, Ithaca, NYRachel (Robbins) Miles, Design Strategist, and Researcher  Visiting Fellow Parsons, School of Design, Design for Social Impact and Sustainability (DESIS) LabHumphrey Obuobi, Product Manager at RecidivizTed Maas  retired Application DeveloperDr. Rhone Fraser, Independent ScholarColin Allen, Distinguished Professor of History & Philosophy of Science, University of PittsburghDilruba Showkat, Research Assistant, Department of CSE, Lehigh UniversitySayash Kapoor, SWE, FacebookKathryn Clancy, Associate Professor of Anthropology, University of IllinoisSuzanne Yuen, Director of Data Science, SalesforceMichel Steuwer, Lecturer in Compilers and Runtime Systems, University of EdinburghRineke Verbrugge, Professor of Logic and Cognition, Department of AI, University of GroningenLuisa Reis Castro, Massachusetts Institute of TechnologyDr. Crystal N. Steltenpohl, Assistant Professor of Psychology at the University of Southern Indiana, Co-Founder of the Online Technologies LabKevin M. Gallagher, Professional Linux sysadmin/SRE/DevOps/Full-Stack/Security engineerYonatan Kogan, signed as selfBrandi Geurkink, Senior Campaigner, Mozilla FoundationPaola Raska, EpidemiologistGlen Weyl, Founder and Chair, RadicalxChange Foundation and Resident Fellow, Harvard Edmund J. Safra Center for EthicsDr. M. Dingemanse, Associate Professor, Language & Communication, Radboud University Nijmegen, The NetherlandsDavid Corney, NLP Engineer, Full FactLeslie Regan Shade, Professor, Faculty of Information, University of TorontoIris van Rooij, Associate Professor, Computational Cognitive Science, Radboud UniversityAmaury Lendasse, Associate Professor, Department of Information and Logistics Technology, College of Technology, University of HoustonMelanie Heath, Associate Professor, McMaster UniversityKodi A. Foster, SVP Data Solutions, ViacomCBS""Mustafa Volkan Bozkurt, PhD, Lecturer(Assistant Professor), EECS Queen Mary University ofLondon""Lorax B. Horne, member of Distributed Denial of Secrets and former Google subcontractorJ. Montgomery, Ph.D., Georgetown UniversityNahema Marchal, PhD Candidate, University of OxfordElizabeth A. Barnes, Professor, Colorado State UniversityChris Short, DevOps'ishBrooke Wurst, CEO & Founder, Remote HarborNeha Verma, Undergraduate Student, Yale UniversitySarah Lim, PhD student at UC Berkeley, Researcher at Ink and SwitchBarbara Fister, Professor Emerita, Gustavus Adolphus CollegeHelena Jaramillo, Design at Coda, Xoogler Amie Stepanovich, Silicon Flatirons Tina Magazzini, Research Associate, Robert Schuman Centre for Advanced Studies, European University InstituteNic Fishman, Stanford University Computer Science, Rhodes ScholarChris Miciek, University Career Development DirectorSilvia Lindtner, Associate Professor, School of Information, University of MichiganDr Peter Chonka, Lecturer in Global Digital Cultures, King's College LondonDawn Bazely, University Professor, Faculty of Science, York UniversityPhilip R Doyle, PhD candidate, UCDThomas Fiolet, R&D Engineer, Robotics/Vision/IADaniel Nemenyi, Department of Digital Humanities, King's College London""Jorge Saldivar, Postdoctoral Researcher, Barcelona Supercomputing Center(BSC)""Dilrukshi Gamage, Research Scholar, University of MoratuwaMarvin Fernandes, Computer Vision Engineer, Industrial Vision Systems ltdKadija Ferryman, PhD, Industry Assistant Professor, NYU Tandon School of Engineering Masha Krol, co-founder and CEO, meetampersand.comCaroline Sinders, visiting research fellow Weizenbaum Institute, and AI Lab fellow with Ars ElectronicaGeronimo Hirschal, Digital OptimistBianca Lopez, Public Policy undergraduate, University of California, RiversideRobin Boast, Professor of Information Science, University of AmsterdamMallika Leuzinger, Postdoctoral Researcher, Princeton UniversityOndrej Dusek, Assistant Professor, Charles University, PragueNathalie Post, Head of Strategy, DEUSMariana Chilton, PhD, MPH, Professor, Health Management and Policy, Director, Center for Hunger-Free Communities, Drexel UniversityDr.Nesredin Mahmud, Volvo Cars, Sweden Charles Hadley King, M.S., Project Manager, HIVE Lab, The George Washington UniversityChristy Hoffman, General Secretary, UNI Global UnionRonald Barzey ,School Counselor, GCA Charter SchoolEthan White, Associate Professor, University of FloridaRichard Beauchamp, Founder and Principal Software Engineer, SprylioLeilani Battle, Assistant Professor of Computer Science, University of MarylandDr. Erica Diya Basu, Independent Researcher, Internet Governance Studies Brittany Wills, Software Engineer, TwitterNathaniel A. Raymond, Lecturer, Jackson Institute, Yale UniversityAbeer Minhas, Product Manager, Foundational Data & Analytics, MerckAngela Bassa, iRobotKaran Desai, University of MichiganSara Soderstrom, Associate Professor, University of MichiganIrina Shklovski, Professor of Communication and Computing, Department of Computer Science, University of CopenhagenDr. Alisa Bokulich, Department of Philosophy, Center for Philosophy & History of Science, Boston UniversityMarita Liontou, Attorney at LawPaige Morgan, Ph.D., University of DelawareFaranak Hardcastle, Research Fellow, University of SouthamptonAlejo Jesus Nevado-Holgado, University of OxfordChristophe Deschamps, Solutions ArchitectDia Kayyali, Associate Director of Advocacy, Mnemonic/Syrian ArchiveDr. Benjamin Paassen, Research Affiliate, The University of SydneyJayd Matyas, Research Environments Designer, DeepMindMelissa Osareniye W, University of AlbertaDr. Anna Jobin, STS Lab, University of LausanneJasmijn Bastings, Research EngineerMelissa Osareniye W, University of AlbertaDr. Anna Jobin, STS Lab, University of LausanneJon Sneyers, PhD, Senior Image Researcher at CloudinaryJulia Shen, PhD candidate, London School of Hygiene and Tropical MedicineDr. Giles Bergel, University of OxfordJevgenij Gamper, PhD student at Warwick University; Senior Statistical Scientist at Cervest LtdDr Andrew King, Reader in Medical Image Analysis, Kings College LondonSebastian Bujwid, PhD Student, KTH Royal Institute of TechnologyDr. Huma Shah, Assistant Professor, School of Computing Electronics and Mathematics, Coventry UniversityMichael Fischler, Security Engineer Michael Szell, Associate Professor, IT University of CopenhagenDr Lai Ma, School of Information and Communication Studies, University College DublinLynn Cherny, Ghostweather R&DDr Andy Lockhart, Research Associate in Urban Automation and Robotics, University of SheffieldJoshua Garcia, Assistant Professor, Department of Informatics, Donald Bren School of Information and Computer Sciences, University of California, Irvine Ana Weller, UX ResearcherFred GiuljTodd Bennings, Product Design, SquareJanis Wong, PhD Candidate, School of Computer Science, University of St AndrewsAdrian Bussone, Senior User ResearcherPhilip Whitham, Social ResearcherMarcela Linkova, Researcher, Czech Academy of SciencesCynthia Habonimana, Machine LearningAlexandra Whitham, Senior Software EngineerJake Stein, MSc Student, Oxford Internet InstituteFangjing Tu, Ph.D candidate, Department of Communication Arts, University of Wisconsin-Madison Sunil Rodger, PhD Student, Newcastle UniversityEsther Chance, Technical RecruiterMatt Mahmoudi, Jo Cox PhD Scholar, University of CambridgeLudwig Schmidt, Affiliate Assistant Professor, University of WashingtonJonathan van Alteren  Object Guild (NL)Maximilian Poschl, Software Engineer/Founder Hackaburg, ratisbona coding e.V.Thomas E. Allen, Assistant Professor of Computer Science, Centre College Diana Jeater, DPhil (Oxon), FRHistS, Emeritus Professor of African History, UWE, Bristol, Associate Dean (Education), School of Histories, Languages & Culture, University of LiverpoolAgnes Cameron, EngineerDr. Mike Duggan, Teaching Fellow in Digital Media and Culture, Department of Digital Humanities , King's College London Marie Smith, CIO, Data 360Mengistu Ketema, Computer EngineerSean Rintel, Microsoft ResearchJessica Newman, Program Lead, UC Berkeley AI Security Initiative, and AI Policy Specialist, Future of Life InstituteJess Dodson (@girlgerms), Customer Engineer, MicrosoftJessica Forbes, VP Product, Kry/LiviBenoit R. Kloeckner, Professor at Universite Paris Est CreteilJoss Moorkens, Assistant Professor at Dublin City UniversityBolin Gao, PhD Student, University of TorontoFelix A. Epp, Doctoral Candidate in Human-computer Interaction, Aalto UniversitySherria Taylor, Assistant Professor, San Francisco State University Tessa Brown, Ph.D., Lecturer, Program in Writing and Rhetoric, Stanford University Merel Noorman, Assistant professor. TILT/LTMS, Tilburg UniversityKat Owyang Christofer, Lead Technical WriterScott M. Schonfeldt-Aultman, Professor, Communication, Saint Mary's College of CaliforniaDaniel B. Neill, Machine Learning for Good Laboratory, New York UniversityAlexandre Maurer, assistant professor in Computer ScienceMarlijn Gelsing, Art & Tech education curriculum and policyLiz McFall, Chancellor's Fellow University of EdinburghHerve Nicolas Nbonsou Tegang, University of Cape TownPashmina Cameron, Microsoft ResearchEmma Lurie, PhD Student, UC Berkeley School of InformationAlison B Lowndes, Artificial Intelligence DevRel | EMEA | NVIDIA Ltd, Remote from North Yorkshire, UKAzzurra Crispino, Associate Professor of Philosophy, Austin Community College, Austin, TX Andrew Hundt, PhD Candidate, The Johns Hopkins UniversityAdrienne Massanari, Associate Professor, University of Illinois at ChicagoBrian J. Tager, Associate Account Manager, Priceline.comFaviola Santana Meetali Jain, AvaazMara Einstein, PhD, Professor and Chair, Media StudiesYuxi Wu, PhD Student, Georgia TechLindsay Tomson, Xoogler, MicrosoftGiacomo Belocchi PhD student @ University of Rome 'Tor Vergata'Charlotte Webb, Acting Course Leader, Creative Computing Institute, UALKarl Sokalski, retired engineerJay Cassano, CEO of CointelegraphDr. Ruth Starkman, Stanford UniversityKate Saenko, Professor and AI Researcher, Boston UniversityMichael Mallari, MS Candidate, Columbia UniversityJie Qi, Project Assistant Professor, University of Tokyo; alum, MIT Media Lab; Berkman Klein Center for Internet and Society at Harvard UniversityBasiliyos BETRU, Jimma UniversityH.Art Taylor, President & CEO, BBB Wise Giving Alliance Dan Adler, PhD Student, Cornell TechDonia Scott, Professor of Computational Linguistics, University of Sussex, UKGiles Douglas, Principal Engineer, NunaHannes Ullrich, Associate Professor of Economics, DIW Berlin and University of CopenhagenMartin Torres, docente Especialidad Tecnologias Digitales y Educacion del Instituto Superior de Estudios Pedagogicos. Tesista en Maestria en Tecnologia, Politicas y Culturas. Universidad Nacional de Cordoba. ArgentinaNick Rycar, Technical Product MarketingZack Finer, Data EngineerJessica James, MEng Computer Science, University College LondonFatima Brown, Founder & CEO Reclassify AIPim Klaassen, Athena Institute, Vrije Universiteit AmsterdamPaul Tulloch, LivingWork AnalyticsRobert Campbell, Technical Recruiter, FacebookShreeharsh Kelkar, Lecturer, University of California, BerkeleyJonah Price, Software EngineerJim Conley, Professor Emeritus, Sociology, Trent UniversityKoen Leurs, Graduate gender programme, Utrecht UniversityDewey Hunt, Principal Application Specialist, Vanderbilt University Medical CenterJeremiah Bill, Software Engineer, AmazonThomas Zeller, Associate Professor of the History of Technology, Department of History, University of MarylandUlrich Junker, Scientist in Advanced Problem SolvingShaka McGlotten, Professor Media Studies & Anthropology, Purchase College-SUNYAnthony Chen, Undergraduate Researcher, University of California, BerkeleyMatthew Cross, Instructor of Political Science, Department Chair, South. Campus, Macomb Community CollegeAlex Okeson, PhD Student, University of WashingtonEmily Lu, software engineerDona Bellow, Responsible Innovation Manager, FacebookLisa LeVasseur, Executive Director, Me2B AllianceRobert Hawkins, Postdoctoral Fellow, Princeton UniversityMark Pohlmann, Cognitive Psychologist, CEO of MacAnima, FranceProfessor Marina Jirotka, University of OxfordChanuwas Aswamenakul, Data Scientist, Siameitrics ConsultingRodrigo Almeida, Data Scientist Engineer, UP42Amine Saboni, Data & Privacy Engineer, Octo TechnologySolange Martinez Demarco, Research Associate, International Center of Ethics in the Sciences and Humanities (IZEW), University of TubingenDr Rachael Grimaldi, Anaesthetist Maarten Derksen, University of GroningenShea Brown, CEO & Founder, BABL AI IncJacqueline Borgstedt, PhD student in socially intelligent artificial agents, University of GlasgowCorban N. Swain, Doctoral Student, National Science Foundation Graduate Research Fellow, Sloan Scholar, Alfred P. Sloan Foundation's Minority Ph.D. Program, Lemelson Engineering Presidential Fellow, Massachusetts Institute of Technology, Department of Biological EngineeringEllen Fernandez-Sacco, Independent Scholar, GenealogistDanielle R. Wood, Assistant Professor, Massachusetts Institute of TechnologyMatthew Ando, Professor of Mathematics, University of Illinois at Urbana-ChampaignMark Anderson, Universidade da CorunaTanaya Guha, Asst. Prof. Computer Science, University of WarwickKaren Yu, Professor of Psychology, Sewanee: The University of the SouthMaria Wolters, School of Informatics, University of EdinburghBenjamin Perryman, MITNoam Finkelstein, Johns Hopkins UniversityAli Haydaroglu, PhD Student, UCLJessica Meyerson, Director for Research & Strategy, Educopia Institute and Berkman Klein Center FellowChris Benner, Dorothy E. Everett Chair in Global Information and Social Entrepreneurship and Professor of Environmental Studies and Sociology, University of California Santa CruzMatt Donatelli, former software engineer and current game developerKristie Wirth, Data ScientistTheresa Enghardt, Senior Software Engineer, NetflixShea Swauger, Researcher Support Services, Department Head, University of Colorado DenverRuth Beresford, PhD Student, University of SheffieldJulie Lai, Data ScientistWilliam Miller, PhD Student, University of the Basque CountryDavid Rolnick, Assistant Professor, School of Computer Science, McGill University & MilaChristopher Reid, Research Lead, High Impact Innovation Programme, Ara Poutama Aotearoa, Department of CorrectionsJeremy Zucker, Computational Scientist, Pacific Northwest National LaboratoryKim Scott, Author, Radical Candor, XooglerParker Abercrombie, Visualization Software EngineerJayshree Sarathy, PhD student, Harvard UniversityEllie Harmon, Senior Instructor, Computer Science, Portland State UniversityMicha Elsner, Associate Professor, The Ohio State UniversityNikoo Ekhtiari, Ph.D., Data scientist at UP42Anne Xie, Product Owner alumnus of Carnegie Mellon University's Human-Computer Interaction InstituteCaitlin Kraft-Buchman, CEO/Founder, Women at the Table, <A+> Alliance for Inclusive AlgorithmsKatelyn Cunningham, Adjunct Faculty  English, Pierce CollegeKapil Tiwari , Software engineer Emily Hu, Product ManagerAssaf P. Oron, Ph.D., Instructor, UW Certificate for Statistical Analysis with RDmitrii Pasechnik, Lecturer, Department of Computer Science, University of OxfordBill Hofmann, former Director of Architecture, Dolby Laboratories; MIT ABD 1983Hanno Gottschalk, Senior Research ScientistLisa Singh, Professor of Computer Science, Georgetown UniversityJenna Burrell, Associate ProfessorDr. Malin Sandstrom, Community Engagement Officer, INCF, Karolinska Institute, SwedenNada Dimashkieh Chehab, University of Central Florida studentJessie Karsif, Customer Success ManagerValentine Goddard, Founder/ED, AI Impact AllianceCasey Fiesler, Assistant Professor of Information Science, University of Colorado BoulderCraig Thorburn, PhD Student, Department of Linguistics, University of Maryland  College ParkDr. Christopher Buttimer, MIT, Post-doctoral AssociateXande Macedo, Product Design Manager, FacebookMeg Carpenter, UX Designer Physicist and Phd student at Universidade Federal do ABC (Brazil)Raja Chatila, Professor Emeritus AI Robotics Ethics, Sorbonne University, ParisAshe Magalhaes, Software EngineerYadira Sanchez, Software Developer & Researcher, University of SouthamptonVeronica Dahl, Professor (Emeritus) | Computing Sciences Department, Professor (Emeritus) | Cognitive Sciences Department, Simon Fraser UniversityBrendan O'Connor, Associate Professor of Information and Computer Sciences, University of Massachusetts AmherstMina Marmpena, PhD Candidate, University of PlymouthMiriam Leeser, Professor, Northeastern UniversityProf*in Mareike Foecking, Peter Behrens School of Arts, Faculties of Architecture and Design, Head of Department of PhotographyRajeswari Parasa, Senior AnalystLei Lani Michel, Adult Education Instructor, Delgado Community College, New Orleans, LouisianaYucheng Tu, Sr Machine Learning Engineer, TwitterMattia Ferrini, KPMG AGPritika Dasgupta, Doctoral StudentGokul Swamy, PhD Student, Carnegie Mellon UniversityDipl. Inform. Ronald D. VogelMarie Adrienne Robles Manalili, CSP-PASP, MSc Student, University College LondonFriedrich Doku, University of Pittsburgh, Undergrad CS MajorKjell Krona, independent AI Ethics researcherCos, SRE, former Google SREMaria E. Kloiber, Master's Student, Boston UniversityJ. Adams, Senior UX DesignerJonah Warren, Associate Professor, Quinnipiac UniversityKatherine Chandler, Georgetown UniversityJia-Bin Huang, Assistant Professor at Virginia TechTerra D'Antonio Dudley, RN-BSN, NP Student at Duke University Juan David Reina-Rozo, Becario Pos-doctoral ZEF-IDEA, Universidad Nacional de ColombiaFern Ramoutar, PhD Student, University of ChicagoMichael W. Busch, PhD, Research Scientist, SETI InstituteRichard R. Renner, Kalijarvi, Chuzi, Newman & FitchAbigail De Kosnik, Director of the Berkeley Center for New Media, Associate Professor in the Berkeley Center for New Media and the Department of Theater, Dance, and Performance Studies at UC BerkeleyBetsy Chou, Product DesignerMelenn Herve, University of California, Irvine, Intersectional Feminist and EnvironmentalistZana Bucinca, PhD Student, Harvard UniversityLaure Thompson, Assistant Professor, University of Massachusetts AmherstAllison Koenecke, PhD Candidate, Stanford UniversityMichelle Murphy, Technoscience Research Unit, University of TorontoElisabeth Piotelat, Systems Administrator, Paris-Saclay UniversityMadeleine IR Brodbeck, PhD Candidate, Western UniversityClaudia Grisales Bohorquez, PhD student, University of Illinois at Urbana-ChampaignSpencer Thompson Brody, Distributed Systems Engineer at 3BoxLabs inc, former GooglerJessica Femenias, Undergraduate, StanfordAnita Coleman, Director of the E.M.White Library and Professor of Bibliography and Research, Louisville Presbyterian Theological SeminaryJohn Sarracino, Cornell UniversityMax Kanwal, PhD Student, Stanford UniversityRenee Elizabeth Neely, MLIS, Visual Artist / ArchivistAllison Laskey, Postdoctoral Fellow, Wayne State UniversityDr Daniel Allington, Senior Lecturer in Social and Cultural Artificial Intelligence, King's College LondonAlemayehu Seyoum, Embedded SW EngineerEvelyn Dais  SWE, NSBENatalia Modjeska, Research Director, Info-Tech Research GroupKatie Byrd, Software EngineerBen Philps, University of CambridgeMolly Kosiarek, University of California Santa CruzEthan Nevidomsky, MITHaiyan Zhang, Chief of Staff & Senior Director, MicrosoftSteffen Grunewalder, Lancaster UniversityHenok Bizuwork, Biomedical EngineerLong Tran-Thanh, Associate Professor, Department of Computer Science, University of Warwick, UKPeter Letmathe, Professor, Chair of Management Accounting, RWTH Aachen University, GermanySara Duke, UX Researcher, FacebookProf. Petar Sarajcev, PhD, Head of Department of Power Engineering, University of Split, FESBDaniel Irabien Peniche, Junior Research Fellow /PhD Student at Tallinn UniversityChristian Bock, PhD Candidate, ETH ZurichChris Kanich, University of Illinois at ChicagoMichael Howles-Banerji, Software EngineerSarah Bittel, PhD Candidate, The Graduate Institute GenevaNancy L. Wayne, PhD, Professor Emerita of Physiology, David Geffen School of Medicine at UCLAChris McCall, Support Escalations Engineer. Pure StorageMuthoni Wanyoike, Data Scientist, At LabsDesi Ivanova, DPhil Student, University of OxfordDr. Christina N. Harrington, Assistant Professor, College of Computing and Digital Media, DePaul UniversityNani Jansen Reventlow, Human Rights LawyerTammi Campbell, Education SpecialistRoger Dove, Book PowerLennart Verhagen, Assistant Professor, Donders Institute, Radboud UniversityKelly Lyons, Associate Professor, University of TorontoMatthias Spielkamp, Executive Director, AlgorithmWatchMichael Castelle, Assistant Professor, Centre for Interdisciplinary Methodologies, University of WarwickMaurizio Santamicone, AI Specialist, Ooredoo Qatar, DohaTom Harwood, Compiler Geek, MathworksMark Stewart, Circle of Collaborative CommunitiesKaterina Papacostas, Software Engineer, One DropChinwendu Enyioha, Assistant Professor,, Electrical Engineering & Comp Sc., University of Central FloridaMichelle Mazurek, Associate Professor of Computer Science, University of MarylandBrent Porter, Research Scientist, Center for Space Research, University of Texas at AustinMartin Bergoffen, Physics and Math Teacher, Wheaton HSEmilie Dufour, PsychiatristKahmali Rose, Software EngineerSevi AGOSSOU, IT ConsultantChelsey R. Carter, PhD/MPH Candidate, Department of Anthropology, George Warren Brown School of Social Work, Washington University in St. LouisPriya Srikumar, Cornell UniversityJamey Harvey, CEO, Agilian.comKelly Gates, Associate Professor, Communication and Science Studies, UC San DiegoTracy Mitrano, Adjunct Senior Lecturer, Department of Information Science, Cornell UniversityCourtney Bonam, Assistant Professor, Psychology and Critical Race & Ethnic Studies, University of California Santa CruzGerrit Muller, Professor Systems Engineering, University of South-Eastern NorwaySahar D. Sattarzadeh, Assistant Professor of Education Studies, DePauw UniversityOlga Molina, Creative Designer Kemi Shoyinka MSHI Student, Temple UniversityAmelia Prior, Technical Project ManagerSara McDonald  EducatorRichard Zemel, University of Toronto & Vector InstituteAndrew Aggabao, MBA/MPA Candidate, Presidio Graduate SchoolSarah Szalavitz, 7RobotChristy Osorio, Presidio Graduate School, Dual MBA & MPA CandidateAmelia Ahl, MBA/MPA Candidate, Presidio Graduate SchoolJuan Jose Tellez, Doctoral Candidate, Melbourne Law SchoolElliot Truslow, PhD Student, Sociology, University of ArizonaKevin Zheng, Undergraduate Student, Computer Engineering, University of Massachusetts AmherstJames Foulds, assistant professor, University of Maryland, Baltimore CountySean MacAvaney, PhD Candidate in Computer Science, Georgetown UniversityMo Corston-Oliver, Senior Language Data Researcher, Amazon Alexa DomainsXin Xin, Assistant Professor of Interaction and Media Design, Parsons School of Design at the New SchoolSimon Benigeri, MSAI student, Northwestern UniversitySarita Parker, Product ManagerDr. Rosie Kar, PhD, Department of Ethnic Studies, Fullerton College, Department of Women's, Gender, and Sexuality Studies, CSU Long BeachGabriel Vigliensoni, Goldmiths University of LondonTulsi Parida, Visa Tobi Shannon, MBA, MPA, Presidio Graduate SchoolAniruddha Vaidya, Computer ArchitectJodie Evans, co-founder CODEPINKRashidul Islam, Ph.D. Student, Dept. of Information Systems, University of Maryland, Baltimore CountyLC Miller, Consultant/Project Manager at Coastal Cloud, NC A&T Alumna, UNCG AlumnaAlex Burka, Robotics Engineer, Exyn TechnologiesShimbi Masengo Wa Umba Papa Levi, Graduate student at the University of Pretoria. Elie Losleben, Co-Founder, Code InnovationKenneth R. Fleischmann, Professor, School of Information, The University of Texas at Austin, and Founding Chair, Good Systems, a UT Grand ChallengeClaire Valluy  M.A Applied Ethics postgraduate, Universiteit Utrecht Pedro Gonnet, PhD, Senior Software Engineer (Research)Katherine Kelm, Associate Professor, Mathematics, California State University, FresnoChari Glogovac-Smith, PhD Student, University of Washington Salvatore Ruggieri, Professor, University of Pisa, ItalySilvia Cantini, Villa di Campolungo AgriturismoJames Garforth, University of EdinburghJoanna White, Collections & Information Developer, British Film InstituteDr Johan Kwisthout, Degree Programme Director Artificial Intelligence, Radboud UniversityNatalia DIAZ RODRIGUEZ, Institut Polytechnique de ParisAjay Juneja, CEO & Founder, Speak With MeNaila Murray, Facebook AI ResearchDonatella Della Ratta, Associate Professor of Communications and Media Studies, John Cabot University, RomeJordan Osserman, Wellcome Trust Postdoctoral Research Fellow, Birkbeck (University of London) Fahad Hussain Khan, M.Sc Student at Hamburg University of TechnologyAndy Almand-Hunter, Software Engineer, Stitch FixMarianne Gunderson, PhD Candidate in Digital Culture, University of BergenAnisha Pai, Data Scientist at MassMutual Life Insurance Company Toniann Pitassi, Professor, Dept of Computer Science, U of Toronto, and, Institute for Advanced. Study, Princeton, and Vector Institute, Toronto CanadaNavrina Singh, CEO & Founder, Credo AIKanika Hedrick, MBA/MPA Candidate, Presidio Graduate SchoolNamik Eker, DaiNamik IT Consulting Jane Margolis, Ed.D, Senior Researcher, UCLA CS Equity Project, UCLA Graduate School of Education and Information StudiesA. Aneesh, Professor of Sociology, University of Wisconsin, MilwaukeeAlvaro Jarrin, Associate Professor of Anthropology, College of the Holy Cross Asli Telli, Associate Professor, Independent Researcher, Universitat SiegenGarrett Wirka, Ph.D. Student, Computer Science and Engineering, University of Nebraska-Lincolnnash, Lead Organizer, Electronic Frontier Foundation David Held, Assistant Professor, Robotics Institute, School of Computer Science, Carnegie Mellon UniversityNaveena Karusala, PhD student, School of Computer Science and Engineering, University of WashingtonDayna Bonds, Mom of a daughter who is a future STEM leaderMichael P. Oman-Reagan, AnthropologistBrandon Williams, PhD student, University of Michigan Tony Wetzsteon, Graphic DesignerShahan Ali Memon, Research Associate, New York University Abu DhabiPaulo Faltay, Federal University of Rio de Janeiro, BrazilPaul Langton, Software Engineer, KlaviyoGeorgia Gkioxari, Facebook AI ResearchDavid Kale, Senior Machine Learning Scientist, NetflixJenny L. Davis, Senior Lecturer, School of Sociology, The Australian National UniversityDr. Elaina A. Hyde, Department of Physics and Astronomy, York University, Toronto Mauricio Giraldo Arteaga, Interaction DesignerMemunat Ajoke Ibrahim, Student, 3A Institute, The Australian National UniversityOlabode Sule PhD, senior deep learning research engineer, Standard AIDr Catherine Flick, Reader in Computing and Social Responsibility, De Montfort University, UK, 100 Brilliant Women in AI 2020Jonathan Walton, PhD Candidate, UC San DiegoA Mani, Indian Statistical Institute, KolkataMichelle Seng Ah Lee, PhD Candidate, Compliant and Accountable Systems, University of CambridgeTina Lasisi, Penn State UniversityJasper Tran O'Leary, PhD Student, University of WashingtonJames Whalen, Former Selectman, Town of Wales, MassachusettsC.G. Acharjya  Engineering ManagerEsther Robb, Virginia TechDanica Sapit, Software Engineer at AdobeReginold Royston, Ph.D., Assistant Professor, University of Wisconsin, Madison, School of Computer, Information and Data Sciencesthembi anne jackson, Director of Events, University of California, Berkeley, School of LawAllegra Stennett, MIT Graduate StudentArielle Theobald, Honors Graduate CSULBSanxing Chen, Student, University of VirginiaS. F. Summers, PhD Student, The University of TexasDavid Matson, Owner, Matson Consulting and Concierge IT ServicesAmy Blumenfield, Senior Privacy Program Manager, MicrosoftCaryn Tran, CS Educator and Software EngineerChristopher Marks, Data Scientist, TouchnoteChris Welch, New Media ArtistDaniela Delgado Ramos, Informatics Ph.D Student, University of Illinois at Urbana-ChampaignJulian Michael, PhD Student, University of WashingtonReda Yaich (PhD), Senior researcher, Cybersecurity Team leader, Institute of Research and Technology  IRT SystemX, France Vinkle Kumar Srivastav, PhD Student, University of Strasbourg, FranceTobie Langel, Founder & Principal, UnlockOpen Priyanka Nanayakkara, PhD Student, Northwestern UniversityJi Su Yoo UC Berkeley PhD StudentJoseph Seering, Postdoctoral Scholar in Computer Science and Affiliated Fellow at the Human-Centered AI Institute, Stanford UniversityLevi Melnick, Applied Scientist, MicrosoftVinay Uday Prabhu, Chief Scientist, UnifyID IncLawrence Oladeji. Co-Founder of BreedsAIDemayne Collins, Systems EngineerKavi Mehta, Nuro AIDaniel Connolly, MD/PhD Student, Perelman School of Medicine at the University of PennsylvaniaRoseanne Romaine, Holistic NutritionistMoiz Sajid, Master's student, Technical University of MunichJay Cunningham, PhD Student, University of Washington, Human Centered Design & EngineeringJordan Menter, Data Scientist, MassMutualMohit Iyyer, University of Massachusetts AmherstSebastian Schuster, Postdoctoral Researcher, Department of Linguistics & Center for Data Science, New York UniversityDr. Oumer Ahmed, Research ScientistAvi Seth  Virginia Tech graduate studentElizabeth Bondi, PhD Candidate, Harvard University Dr. Esther Plomp-Peterson, Delft University of TechnologyRuben Oliveira Chiavone, Software EngineerD. Stephen Lindsay, Ph.D., Professor of Psychology, University of VictoriaLuis Fernando Medina Cardona, Associate Professor, Universidad Nacional de Colombia (Bogota, Colombia)Amanda Pogue, Data ScientistProf. Simon Buckingham Shum, Director Connected Intelligence Centre, University of Technology SydneyAditya Kusupati, University of WashingtonWei-Fang Sun, Undergraduate Student, Computer Science, National Tsing Hua University, Hsinchu, TaiwanRisa Takenaka, Product Marketing Manager, TelnyxDr. Linwood Tauheed, Associate Professor of Economics, University of Missouri Kansas CityKaio Duarte Costa, Director of Science, Technology and Communications, Uniao Colegial de Minas GeraisMert Ozer, Data Scientist, LexisNexisSoumia Fares, Senior Researcher, TwitterDavid McAllester, Professor, Toyota Technological Institute at Chicago (TTIC)Mai Parker-Medina, Registered NurseErika Enomoto, Program Manager, MicrosoftKimberly Auger, MLIS  Communication Librarian, McNairy Library and Learning Forum, Millersville UniversityArt Goldsmith, Washington and Lee UnivesityRonald E. Robertson, PhD Candidate, Network Science Institute, Northeastern UniversityMelanie Moses, UNM Professor of Computer Science, SFI IWG on Algorithmic JusticeBradley Iott, PhD Candidate, University of MichiganInes Abdeljaoued-Tej, PhD, University of Carthage, TunisiaJohn M Gonzalez, Professor, University of Texas at AustinTulsi Achia, Psychologist & PhD candidate, School of Psychology, The University of Queensland, AustraliaSylvio Drouin, Senior VP of Research LabsRakhi Agrawal, Data ScientistHilary Karls, Engineering ManagerCarly Lockard, Senior Research Engineer, Steadman Philippon Research InstituteMari Gilmore, MBA Candidate, Presidio Graduate SchoolClaudia, Doctoral Candidate, CUNY School of Public Health and Health PolicyCristopher Moore, Santa Fe Institute, Interdisciplinary Working Group on Algorithmic JusticeKathleen H. Pine, Assistant Professor, Arizona State UniversityPatricia Murray, Professor at the University of Liverpool, UKMaria Stone, Insights DirectorDr. Anto Mohsin, Assistant Professor of STS, Northwestern University in QatarMallory G. James, PhD, Postdoctoral Research Associate, Munich Center for Technology in Society, Technical University of MunichSreela Sarkar, Associate Professor, Dept of Communication, Santa Clara UniversityMartha-Elizabeth Baylor, Associate Professor of Physics, Carleton CollegeCeren Budak, Assistant Professor, University of MichiganAlexander Alvara, PhD Student, UC BerkeleyKyle Warnecke, Undergraduate Student, Southern Illinois University EdwardsvilleRabbi Paula MarcusNikolaus H. R. Howe, MSc Student, Universite de Montreal & MilaKarine Gentelet, Professor, Universite du Quebec en Outaouais (Quebec, Canada), Holder of Abeona-ENS-OBVIA Chair on AI and Social JusticeArinn Dembo, Acting President, SF-CanadaHenriette Cramer, SpotifyDan Auerbach, Systems IntegratorBrie Bridegum Pierznik, Startup & Technology Lawyer, Stoel Rives LLPKavita Philip, President's Excellence Chair in Network Cultures, Department of English Language and Literatures, University of British Columbia Alex Williams, Postdoc, Statistics Department, Stanford UniversityBao Nguyen, Undergraduate Student, UMass AmherstLeonie Smith, Doctoral candidate in Philosophy, The University of Manchester (UK)Eskedar Gebremedhib, Deltars, The NetherlandsSharon Strover, Philip G. Warner Regents Professor of Communication, School of Journalism and Media, University of Texas at AustinMartha N. Havenith, Research group leader, Ernst Strungmann Institute for Neuroscience in cooperation with Max Planck SocietyGiles Bowkett, Software DeveloperDr. Ariel Guersenzvaig, Senior Lecturer, Elisava Barcelona School of Design and EngineeringHarry Bendekgey, PhD Student, University of California, IrvineZachary Lipton, Assistant Professor, Carnegie Mellon UniversityAmanda Holman, Sr. Data Analyst, Brad's DealsAnne H. Charity Hudley, Ph.D., North Hall Endowed Chair in the Linguistics of African America, Faculty-in-Residence for Santa Catalina Residences and San Joaquin Villages, University of California, Santa BarbaraAnnette Zimmermann, Technology & Human Rights Fellow, Carr Center for Human Rights Policy, Harvard University & Lecturer, Department of Philosophy, University of YorkVeronique Ginouves, AMU-CNRS USR3125, Maison mediterraneenne des sciences de l'homme, Phonotheque MMSHBukenya Lukman, Post Graduate Student, Software Engineer, Makerere University, Kampala UgandaDr. Savita Bailur, Columbia University and Caribou DigitalHongsup Shin, Staff Research Engineer, ARMAbraham Hmiel, PhD, Data Scientist, Quartet HealthZeheng Chen, Senior Software Engineer, AppleLaure Delisle, PhD student, CaltechTomo Lazovich, Machine Learning Scientist, LightmatterClara Vania, Postdoctoral Researcher, Center for Data Science, NYUTim Mwangi, Software Engineer, Traceable IncFaye Oosterhoff, DivorytaurScott Allen Cambo, PhD candidate in the Technology and Social Behavior program at Northwestern University and data scientist at Avalanche InsightsSarah Pratt, University of WashingtonDaniel Greene, Assistant Professor of Information Studies at the University of MarylandAbdelrahman Ahmed, Research Engineer, CerenautMichael Siciliano, Assistant Professor of Sociology at Queen's UniversityKalkidan A. Kebede, Duke UniversityKiko Smith, TwitterAdam DeConinck, Senior Solutions Architect, NVIDIACan Goksen, Software Engineer, MicrosoftEmma Batson, PhD Student, MITMauli Pandey, PhD Student, University of Michigan Halleluyah AWORINDE, PhD, Bowen University, Iwo, NigeriaCody Coleman, PhD candidate, StanfordJustin Norman, YelpYong-Yeol Ahn, Associate Professor, School of Informatics, Computing, and Engineering, Indiana UniversityTim O'Brien, MicrosoftNathan Murthy, (former) Staff Software Engineer, Tesla, incoming StripeMichael Wayne Goodman, Nanyang Technological UniversityDeblina Mukherjee, University of ChicagoDavid Karger, Professor, MIT CSAILYonas Sium, Ph.D. Student, Iowa State UniversityPankaj Mehta, Associate Professor of Physics, Founding Faculty of Data Science, Boston UniversityAmartya Mitra, UC RiversideLauren Gillespie, Computer Science PhD student, Stanford UniversityTobi Bosede, CEO, Ilekun HealthJeylan Erman, PhD Candidate, University of Pennsylvania Simina Branzei, Assistant Professor of Computer Science, Purdue UniversityTracy Kensey, Sports Coach and Business Owner, South AfricaMebiratu Beyene, Lecturer at Faculty of Computing, Head, Wireless and Mobile communication Research Group, ICT4D Research Center, Bahir Dar Institute of Technology, Bahir Dar UniversityRoman Lutz, MicrosoftRodolfo Corona, PhD Student, UC BerkeleyJacob Ritchie, PhD Student, Stanford University Computer Science DepartmentSebastien Levy, ConsultantCatherine Stinson Queen's National Scholar in Philosophical Implications of Artificial Intelligence, Queen's UniversityLidia del Rio, lecturer, ETH ZurichLennart Ottermann, UX Design and Data Science, GermanyChrisitne Y. Chen, PhD, O.K. Earl Postdoctoral Research Fellow, California Institute of TechnologyHarriet May, Senior Product OwnerVida Vakilotojar, PhD, former Software Engineer at GoogleDidac Ferrer, engineer, Technical University of CataloniaSatoru Tokutsu, Founder of AI NEXT IncThao Phan, Research Fellow, Alfred Deakin Institute for Citizenship and GlobalisationDr. Yoda R. Patta, Academic Advising Director & Adjunct Professor, Stanford UniversityDheeraj Pai, Co-Founder of Hyperweb AIAndre Bittar, Research Associate, King's College LondonChloe-Agathe Azencott, Associate Professor at Mines ParisTech, co-founder of Paris WiMLDSCatherine Ordun, PhD student UMBCDr Matthew Cole, Postdoctoral Researcher, Oxford Internet InstituteNicolas Fiorini, Machine Learning Engineer, independentNantas Nardelli, PhD student, University of OxfordJason Katz-Brown, CTO, Data for Progress, former Google ResearchIlse Pouwels, MFA student Umea Insititute of DesignJonathan Dong, Postdoctoral Researcher, EPFLChico Camargo, Oxford Internet Institute, University of OxfordCaitlin Corrigan, Institute for Ethics in Artificial Intelligence, Technical University of MunichKonstantinos Tsakiliotis, President, Institute for Internet & the Just Society, BerlinDr Kate M. Miltner, Marie Skodowska-Curie TRAIN @ Ed Postdoctoral Fellow University of EdinburghFrancesco Vogelezang, Co-Lead Digital Democracy Cycle  Institute for Internet & the Just SocietyMaria Rigaki, PhD student, Czech Technical UniversityMarie Codet, Grenoble Graduate School of Management, masters studentJillian C. York, author, Silicon ValuesPhilippe Beaudoin, CEO at WaverlyJim Cooper, Global Lead  Intelligent WaterFavour Nerrise, University of Maryland, College ParkCarrie Ewins, MSc Student, Biology, Queen's UniversityYung Au, PhD, Oxford Internet Institute, University of OxfordMaria Nagawa, PhD Student Duke UniversityTabitha Goldstaub, Co Founder CogX, Chair UK Government's AI CouncilVandhana Ravi, Beeck Center for Social Impact, Georgetown UniversityFrederike Kaltheuner, 2019-2020 Mozilla tech policy fellowCecily Rieser, Experiential Marketing, Biotech + Human + DEI AdvocateLauri Goldkind, Associate Professor, Fordham UniversitySteven LindsayAmir-Hossein Karimi, PhD candidate, Max Planck Institute for Intelligent Systems, Tubingen & ETH ZurichMandu Reid, Leader of the UK Women's Equality PartyMichele Santamaria, Assistant Professor, Library Department, Millersville University Rachel Zalupski, ValassisMatthew Wright, Professor of Computing Security and Director of Research for the Global Cybersecurity Institute, Rochester Institute of TechnologyJesse Goldstein, Associate Professor of Sociology, Virginia Commonwealth UniversityTiphaine Viard, associate professor, Telecom ParisAndrea Thomer, Assistant Professor, School of Information, University of MichiganBen Winters, Electronic Privacy Information CenterHeba Gowayed, assistant professor of sociology, Boston UniversityTatiana Mac, Software EngineerRaquel Donahue, MLS, Public Services Librarian, Houston Community CollegeNige Willson  Founder awaken AI, ex Microsoft European CTODeeDee Baldwin, Assistant Professor, History Research Librarian, Mississippi State UniversityNicolas Delley Mike Girma, Deluxe Home CareBarbara C. Wallace, PhD., Teachers College, Columbia UniversityLuca Foschini, PhD, Co-founder & Chief Data Scientist; EvidationTeena Hassan, Bielefeld University, Bielefeld, GermanyK. Chmielinski, Affiliate, Berkman Klein Center at Harvard University Jason Johnson, Security Consultant, F-SecureLelia Hampton, Spelman CollegeDarren Vengroff, Ph.D., Two SigmaDavid Landy, NetflixRucha Kulkarni, PhD student, University of Illinois at Urbana-ChampaignElsie Lee, PhD Student, School of Information, University of MichiganMikael Trellet, Software EngineerStacey Svetlichnaya, deep learning engineer, Weights & BiasesBrandon Rohrer, Principal Data Scientist, iRobotDr. Nicholas Eckenstein, Software EngineerNicole Leaver, Research Assistant, Technology and Social Change Project at Harvard UniversityBec M, BA in Human Ecology/Environmental Studies, College of the Atlantic 2020Sarah Sharma, Associate Professor, ICCIT, University of Toronto, Director of the McLuhan Centre for Culture and TechnologyAbbey Ripstra, Human Centered Design ResearcherKristina Nawrocki, Research Program Manager, Amazon Beth Coleman, University of Toronto John Davies, Senior Software Engineer, FacebookStephen Jacobs, Professor, Rochester Institute of TechnologyJaan Altosaar, Postdoctoral Research Scientist, Columbia UniversityJoshua Cook, Technical Team Lead, Databricks Snehesh Shrestha, PhD Student, University of MarylandDr. Jessica Heesen, Head of Reseach Focus Media Ethics and Information Technology, International Center for Ethics in the Sciences and Humanities (IZEW), University of TuebingenHelene Molinier, Senior Manager for the Action Coalition on Innovation and Technology for Gender Equality, UN WomenMaya Ziv, Stanford BS/MSCS '21Devin Wilson, Software Engineer Hussein Mohsen, PhD Student, Yale UniversityThomas Kober, NLP Researcher, RasaSaige Rutherford, Department of Psychiatry, University of Michigan, Ann Arbor, MICarlo Perrotta, researcherEunice Kokor, Senior Software Engineer & Technical LeadVenu Madhav Govindu, Indian Institute of Science, BengaluruYao Li, PhD Student, University of PennsylvaniaZachary P Kilpatrick, Associate Professor, University of Colorado Boulder, Department of Applied MathematicsBrian Cleary, PhD, Broad Fellow, Broad Institute of MIT and HarvardIsaac Tamblyn, Research Scientist, Ottawa, CanadaOrevaoghene Ahia, InstadeepDr. Tanya Clement, Associate Professor, University of Texas AustinMC Forelle, Presidential Postdoctoral Fellow, Cornell UniversityConor McGinn, Assistant Professor of Robotics and Design, Trinity College DublinAntoine de Scorraille, Software engineerHannah Robertson, Sr. DSP Engineer, Beacon BiosignalsJosef Nguyen, Assistant Professor, The University of Texas at DallasYonadav Shavit, Harvard UniversityNima Fatemi, President of KandooJauzey Imam, Principal Software Engineer, PoyntOlawunmi George, Doctoral Candidate, Computer Science Department, Marquette University, USABrandon Haworth, Assistant Professor of Computer Science, University of VictoriaYoehan Oh, Ph.D. student in Department of Science and Technology Studies, Rensselaer Polytechnic InstituteRida Qadri, PhD Candidate, MITXavier Holt, Senior Applied AI Scientist, Harrison AIKagami Sascha Rosylight, Software Engineer, MozillaAlina Selega, Postdoctoral Fellow, University of Toronto, Vector InstituteHailu Gebremenfes, Senior Software EngineerSephora Madjiheurem, University College LondonMeher Kasam, Software Engineer at SquareKelsey Kim, UCLANathaniel Calhoun, Co-Founder, Code InnovationLizzie Siegle, Twilio developer evangelistEyes S. Robson, PhD Student, Center for Computational Biology, UC BerkeleyJoseph Miller, Lead Software ArchitectJon Gillick, PhD Candidate, UC BerkeleyMoussa Koulbou, CEO, Merwel IncSteven Hanna, PhD, Principal Research Engineer, ZeroFoxTina Vanasse, Educator, Acton, MAFrancis Anokye, Graduate Student, AMMISoham S, PhD Student, Department of Computer Science, Boston UniversityNik Marda, MS student, Stanford University Computer ScienceRoxana Daneshjou, postdoc at StanfordScott Niekum, Assistant Professor, Department of Computer Science, University of Texas at AustinAndrew Beers, PhD Student, University of Washington Department of Human Centered Design & EngineeringMariah Lichtenstern, Fellow, Aspen Tech Policy HubElin McCready, Professor, Aoyama Gakuin UniversityWill Shortt, Co-Founder & COO, Ksana HealthMahzarin R. Banaji, Cabot Professor of Social Ethics, Dept of Psychology, Harvard University and SFI IWG on Algorithmic JusticeDr. Vasant G Honavar, Professor and Edward Frymoyer Endowed Chair of Information Sciences and Technology, Pennsylvania State UniversityPreeti Srinivasan, PhD Student, Stanford UniversityKimberley Smith, Product Compliance Engineer, Bloom EnergyJohn Pougue-Biyong, PhD candidate, University of OxfordAdam Kriesberg, Assistant Professor, Simmons University School of Library and Information ScienceNirmala Gnanaratnam, Graduate Student, Department of Electrical Engineering, University of Victoria, BC, CanadaIris Stone, PhD Candidate, Princeton UniversityEvan Green, Data Scientist, Microsoft ResearchEkta Prashnani, University of California, Santa BarbaraG. Matthew Fricke, Research Assistant Professor, University of New Mexico Computer Science Department, SFI IWG on Algorithmic JusticeAndrew Gibiansky, Applied Research Scientist, FacebookPaul Cantrell, Instructor (NTT), Department of Mathematics and Computer Science, Macalester CollegeAyush C, Software EngineerNachiket Kapre, University of WaterlooMichael Albergo, PhD Student, NYUVass Bednar, Executive Director of the Master of Public Policy in Digital Society, McMaster UniversityNebiyat Esubalew, Reed CollegeSeyoum Alemayehu, Manager, Technology Support Center of Excellence, Compugen IncRachel Sterneck, Undergraduate Student, Yale UniversityEffenus Henderson, Stanford SEP 95, President and CEO, HenderWorks, IncAgata Foryciarz, PhD Student in Computer Science, Stanford UniversityJason Wakeman, Sr. Software Engineer, EBSCOFabio Chiusi, Project Manager, AlgorithmWatchAntonio Khalil Moretti, PhD Candidate, CS Department at Columbia UniversityPatrick Davison, Data & Society Research InstituteDr. Vasant G Honavar, Professor and Edward Frymoyer Endowed Chair of Information Sciences and Technology, Pennsylvania State UniversityPaul R. Pival, Research Librarian  Data Analytics, University of CalgaryHonor Bixby, McGill UniversityRoxana Hadad, PhD, Associate Director, UCLA CS Equity Project, UCLAsava saheli singh, postdoctoral fellow, University of OttawaMichael Giancola, PhD Student, Rensselaer AI & Reasoning (RAIR) Lab, Rensselaer Polytechnic InstituteAlan Lundgard, PhD Student, MITBen Bogart, PhD. Emily Carr University of Art and DesignJonathan Reyes, PhD Student, University of Maryland, College ParkPati Ruiz, Digital PromiseTess Posner, AI4ALLKat Sullivan, NYUEmi Baylor, MSc Student, McGill University & MilaOlivier Boucard, Cofounder and CTO, International Privacy MachinesClifton Roberts, Software EngineerAnnabelle Ducret, maths and physics bachelor studentMallika Balakrishnan, Organizer, No Tech For Tyrants Clement Titton, ASHQ Assistance public des hopitaux de parisSatoru Hayasaka, Ph.D., Data Scientist, KNIMEAnoop Kulkarni, Founder and data scientist, Innotomy ConsultingLeah Roh, Adjunct Professor, Department of Technology, Culture & Society, NYU Tandon School of EngineeringThomas Jaafar, French politicianFabrice Brunel, senior lecturer / adjunct professor, Universite de LyonMartina De Castro, PhD Student, RomaTre University Dario Mangano, IT Transformation Officer, European Space AgencyMathis Randl, comp-sci student and research @ EPFLSidney Congard, Software EngineerValmont Puren, student at the University of Toulouse 1 Capitole Faculty of LawNaomi Assaraf, cloudHQMaxime Nicloux, civil engineer Polytechnique MontrealMarco Lourenco, Statistics master student at University of GenevaPhilippe Truillet, Ph.D, Associate Professor in Computer Science, Universite ToulouseRomain Graux, Undergraduate Student, Data Science @ UCLouvain, BelgiumAlexandr Bali, Licence 2 Mathematics Student at Universite Lyon 1Pierre Ceteaud, Data ScientistMathieu Caroff, Software Engineer at OrnessAnthony Beuchey, HCI Research EngineerSaul Ivan Rivas Vega, CS Masters Student, Universidad Nacional Autonoma de MexicoAntoine Bedouch, Physics Graduate Student and Student Researcher, Paris-Meudon ObservatoryNicolas Leonard, EngineerMatteo Tacchi, PhD candidate at CNRSZied GOBJI, Software EngineerThomas Alauzet, Junior Software EngineerMaxime Lemeiter; engineerMaria T. Khan, Data Storyteller Ludovic Patho, AI trainerAstrid Casadei, Software EngineerKavita Ramanan, Brown UniversityMaxime Kohler, Student, University of Neuchatel CHYannick Patois, Software Engineer at CNRS, FranceVincent Guillemet, Msc student in Mathematics, ETHR. Fiori, Master Degree in geophysics at IPGSChristie Hurrell, MA, MLIS, Director, Lab NEXT, Libraries and Cultural Resources, University of CalgaryJean-Simon Lhost, Data Scientist I'm Nicolas Zimmermann, Computer Science Master student at EPFLManon Kobsch, dev ops engineerJoe Mitchell, Professor and researcher in algorithmsSeyi Olojo , PhD student, UC Berkeley ISchoolFred Lallement, Systems & Security Engineer, FranceChristophe Huet, Data ScientistPierre Drege, Software engineerStephen McMurtry, Senior Software Developer, AmazonPierre Musacchio, undergraduate student, ENSC, FranceRemi Bourgeois, PhD student at CEA Saclay  Maison de la simulationSteven Adler, Georgia TechYoann Debain, etudiant en mathematiques et informatique appliquees aux sciences humaines et sociales a l'universite Paul ValeryMatthieu Sinico, Process engineerGabriel Roch, Student in IT SecurityAlexandre Guitry-Azam, Second year biology undergraduate student, University of Versailles St Quentin, UFR of scienceOfurhe Igbinedion, PhD Candidate, Geography UC DavisDamien Moulis, AI StudentMaxime Bedoin, Graduate Student, Telecom SudParisAdrien Milcent, Machine Learning EngineerMarcel Bourg, EPFLRaphael Bruhat, Economics student ENS Lyon, FrancePaul Ferney, Nuclear Engineer, Phd StudentAlim Adam Nait, KERINGDoubiani Mehdi, Machine Learning EngineerLeonard DABIN, Engineer Student in Robotics at ICAMXengie Doan, Sage BionetworksMohamed Gaalich, Software EngineerPierre Mancini, Software EngineerSimon Micheneau, Software DeveloperJordan Marques, Data EngineerDr. William Stedden, Data Scientist, Anthem AIArthur Wuhrmann, StudentVictor Delage, PhD student, Universite de Rennes 1Tauzy Felix, Montpellier UniversityMathieu Zmudz, Software Quality Assurance EngineerOrazio Gallo, Principal Research Scientist, NVIDIADavid Fouhey, Assistant Professor, Computer Science and Engineering, University of MichiganEdouard Francois, Cybersecurity, Software engineerJulie Chen, Assistant Professor, ICCIT & Faculty of Information, University of TorontoAllyson Gunn, StriveAndrew Spielberg, PhD Student  MIT Computer Science And Artificial Intelligence LaboratoryTheo Santos, Data ScientistAlexandre Roger, student, universite de CaenThomas Houriez, PhD Student, Grenoble Institute of TechnologyAlfred Dennis Mathewson, Emeritus Professor of Law, University of New Mexico , Interdisciplinary Working Group for Algorithmic JusticeThierry Challand, MD, Radiation Therapist, FranceMonica Agrawal, PhD Student, Massachusetts Institute of Technology Fabien Querard, Undergraduate Student , Universite Bretagne SudDhanji R. Prasanna, Cash App @ SquarePierre-Francois Monville, operations research engineer at Sopra SteriaJonas Daverio, EPFLPierre Nicolay, Heriot-Watt University, PhD Student, Robotics controlMeg Stalcup, Associate Professor of Anthropology, University of OttawaLeonard Kobilinsky, Undergraduate StudentGirard Theo, PhD studentGuillaume Mougeot, Ph.D. StudentChristophe Plantin, mechanical engineerJosh Gardner, University of WashingtonBadisse Bouabdallah, Master's Degree in Engineering Student, Ecole Nationale de l'Electronique et de ses ApplicationsNathanael Haas, engineering studentValentin Mauerhofer, PhD candidate, Astrophysics, University of GenevaOscar Thenon, math studentNoe Malais, Machine Learning student, ENS de Rennes, FranceBenjamin Cruz, LinkedIn; former GooglerPierre Houdayer, PhD Student in Astrophysics, Observatoire de ParisJerome Savary, Hardware EngineerDorota Jozwicki, PhD employee in Space Physics at UiT The Arctic University of NorwayFranck Popiers, Data Analyst, Orange FranceTom Boellstorff, Professor, Department of Anthropology, University of California, IrvineAlexandre Khoungui, Appareilleur OrthopediqueLucas Burlot, electronics and digital technologies engineering student at Polytech'NantesFay Wang, Ph.D. Candidate at Columbia UniversityTanguy Firinga, Software EngineerDieudonne N., Mechanical designer at FN HerstalGilles Tuffet, Undergrad Physics Student, Sorbonne Universite, FranceBenoit Camus, Software EngineerDipayan Ghosh, Harvard Kennedy SchoolSamuel Pinson, Ph.D, Postdoc Researcher at ENSTA Bretagne, FranceGregoire Postaire, Paris-Saclay University, Master StudentPaul Menager, Software EngineerPhilippe Bonnaveira, Commissioning engineer, FranceFelix Ferrand, AI engineerMariya I. Vasileva, PhD, University of Illinois at Urbana-ChampaignMerwane Hamadi, Senior Software Engineer, Redica SystemsColaianni Guillaume, Building EngineerLyse Michel, Undergraduate Student, Aix-Marseille UniversityPhilippe Demerliac, Cyrob.orgAnselme Revuz, Software EngineerLouis Vivier, Data Developer at Hardis GroupNicolas Gavrilenko, freelance art directorSam Neubardt, Software Engineer at Upstream TechJulien Smets, PhD studentM-E Poncet, EmlyonMike Wolfson, Android GDE  Google Developer ExpertDejaegere Frederic, Software EngineerOlivia Vincenti, Representative of Women's Equality Party Race Equality CaucusChristine Dean, Representative of Women's Equality Party Race Equality CaucusJules Perrin, Student at EPFLNicolas Barre, Web Developer, FranceMathilde Rynkiewicz, UTTSylvie De Geyter, physics teacher in FranceGregoire Duvauchelle, Software EngineerFranklin Lee, Software EngineerLudovic Roulin, proofreaderMaggie Engler, Senior Data Scientist, TwitterSimeon Campos, Economics Undergraduate, ENS LyonGohan Keller, ECALJerome Vanderstichelen, Software engineerFrederick Mollaret, Data AnalystAntoine ADAM, EPITA Toulouse StudentAntoine Paris, Ph.D. student, Electrical EngineerNicolas Marsal, Software EngineerMartin Benito-Rodriguez, Master's student at Institut Informatique d'AuvergneVlad Carare, University of Cambridge, PhD in Quantum Chemistry & Machine LearningLorri Mon, Associate Professor, Florida State University School of InformationAlexandre Danard, Health, Safety and Environnent engineerPhilip SiederKatie Brennan, Postdoctoral Research Fellow, POLSIS, University of QueenslandRandima (Randy) Fernando, Co-Founder & Executive Director, Center for Humane TechnologyNicolas ValotSamuel Boudet, Lecturer, Catholic University of LilleEliott Thomas, Computer Science Student at ENSSATClement SchererJeremie Morel, Herbarium curator, National Museum of Natural History, ParisHerve Huneau, Software EngineerMichele Gilman, Venable Professor of Law, University of Baltimore School of LawZhen Zeng, Ph.D., University of MichiganDr Andrew Flynn, Duke UniversityBenoit Desille, Continuous Improvement EngineerNicolas Micaux, Telecom Paris, FranceBaladi PierreValcasara Bryan, Software EngineerThomas Beuchot, HEC ParisNicolas Pouillard, PhD, Software EngineerDamien Larralde, Application developerSimon Keldermans, TeacherAlexis Giauque, Ecole Centrale de Lyon, FranceGregorio Ramel, Stanford UniversityLaurent Jacques, Professor, UCLouvain, Belgium.Cyril Randriamaro, Assistant Professor, University of Picardie, FranceMathieu Perrin, physicist at INSA Rennes, FranceSylvain Heck, Computer science Teacher  Strasbourg CESI engineering schoolHaleeq Usman, Senior/Lead Full Stack DeveloperClement Lagisquet, PhD Student in Applied MathematicsArthur Gontier, Research ScientistStephane Capo, Software EngineerMisti Yang, PhD Candidate, University of Maryland, College ParkFlorent Vandangeon, Software EngineerPontus Stenetorp, Lecturer, University College LondonThibault Neveu, Machine Learning Researcher and CTO at Visual BehaviorKilian Tozzini, Software EngineerRobin Blanc, Software EngineerJeremy Freixas, Teacher, Yncrea OuestThomas Mangin, PhD candidate in social Psychology, CeRCA, Universite de Poitiers, FrancePierre-Alain Lindemann, Software architectMathieu Simon, Senior Software EngineerBenjamin Loison, student at ENS Paris Saclay.Louis Freneau, Vendeen responsableTristan Lannuzel, Student in computational cognitive science, Universite Grenoble AlpesJonathan Sowler, VP Engineering, Unbabel  Machine TranslationDambreville Jeremy, front-end developerKarri Lybeck, Senior Coordinator / Organiser, UNI Global UnionJustin Deschenaux, MSc student in Data Science at the Swiss Federal Institute of Technology in Lausanne (EPFL)David Ngo, Ph. D, computer vision and image processingJulien Mastrangelo, Student, Mines de St-EtienneSheila Beladinejad, CEO O Canada Tech & Women in AI Ambassador of GermanyJefferson Baudin, Master Student in MathematicsBRULE Herman Jacques RogerBenoit Chanclou, Ph.D., Senior Software EngineerLaurent BERTHELOT, Software EngineerDr Beth Singler, Junior Research Fellow in AI, University of CambridgeZecic Senad, R&D EngineerMarion Zepf, MicrosoftMahi Hardalupas, PhD candidate, University of PittsburghAlexia Skok, Communications and Media Manager, Access NowGregoire AussenacBaptiste Lebourgeois, student in Information and Communication Sciences, Bordeaux Montaigne University, France.Mael Navarro SalcedoMathieu Guinin, Software DeveloperMaari Ross, LuminateJoseph Keita, Data Science StudentDamien Van Aertselaer, Student, Art et Metiers school of Paris ENSAMYves Gageot, Retired, FranceOlivier Bourdoux, Software EngineerBoris Schapira, Web ProfessionalBasit Ayantunde, Student at the University of IlorinTheo De Pinho, Student, Musicology, University of LilleJonathan Cumming, Software EngineerNora Ammann, Effective Altruism GenevaYodit Debela, Mechanical EngineerBran Knowles, Lancaster UniversityBenoit Lambert, studentAmadou Crookes, Senior Software Engineer, Upstream TechKonrad Seifert, Executive Director, Effective Altruism GenevaMichael Brennan, Ford FoundationChantrel Pierre-Yves, EngineerQuentin Rivera, FPGA Design Engineer, Nexvision SASTanaya Srini, Tech Fellow, Ford FoundationChris Brust, Senior Machine Learning Engineer, DuolingoHamza LAADAILI, Network EngineerJennifer Lee, PhD Student, Center for Neural Science, New York UniversityGuillaume ROBBE, MD Neurology, Pitie-Salpetriere Hospital, APHP, Paris (France).Alexandre Toulemonde, TXT Agencia Transmedia, Brand ExpertMathilde Chabeau, AI developerMarcial Goury, student scientistJEAN Robin, math student at Paris-Sud UniversityEdgar Dumont, (former) graduate student at Sciences Po ParisR. Fiori, Master Degree in geophysics at IPGSLena Karam, INP-ENSEEIHTLudovic Danis  Artist/MusicianFlorent Poinsaut, Computer Science Communicator at QTG and Engineer at Solution LibreMukhethwa Phathela, Reinforcement learning Masters student, South AfricaRaphael Salique, Web developer in FranceSyrielle Montariol, PhD Student, LIMSI-CNRS, University Paris-SaclayNatassia Brenman, Social ScientistTim Faverjon, data scientist, student at CentraleSupelecAlice Stollmeyer, Executive Director, Defend DemocracyDotun Opasina, Data Scientist, DotunDataEnguerran Meurisse, TeacherLaurent Spinelle, PhD, Research and study engineer, Ineris, FranceVictor Sousa, computer scientistDaniel Leufer, PhD, Europe Policy Analyst at Access NowEric Lalevee, Software EngineerJoycelyn Longdon, Black Woman, Ph.D. Student, and Climate Science CommunicatorFabrice Muhlenbach, Associate Professor, Lab. Hubert Curien, Univ. of LyonKillian Herveau, PhD Student, Computer science, Karlsruhe Institute of TechnologyParascandola QuentinJonathan Tricard, Data ScientistNayla AMARA KORBA, Head of Marketing, Sales & Communications, National Agency for the Promotion and Development of Technology Parks, AlgeriaEtienne Dumortier, Architecture and Civil Engineering studentCamille Duflos, PhD student UCLouvain, BelgiumVincent Morice, PhD Student in Computer Science, Grenoble Alpes UniversityMike Roberts, Research Scientist, AppleJulien Sellos, Science TeacherJoseph Garnier, PhD candidate in AI, Universite de LyonArthur Klipfel, PhD student in computer science at l'Universite d'Artois (France)Prof. Alex Lascarides, Professor of Semantics, School of Informatics, University of EdinburghJames White, Orebro UniversityDr Daniel Howe, Associate Professor and Head of the Machine Learning Research Group, School of Creative Media, City University Hong KongDanaele Carbonneau, Philosophy and Computer sciences studentThomas RICHARD DE LATOUR, PhD student, University of Nantes, FranceOrval Touitou, Software EngineerPierre OurliacAnna Jonsson, PhD student in CS, Umea universityDr. Andrea Derdzinski, Postdoctoral Researcher in Theoretical Astrophysics, University of ZurichNicolas Sebestyen, FR Software EngineerSTOENS Paul, Citizen & Biologics EngineerInfante Jonas, web developerJean-Francois Erdelyi, Ph.D. Student in Artificial Intelligence, Toulouse 1 Capitole University  IRITWernert David, Aide-medico-psychologique.Gilles Civario, Computational ScientistAlexandra Aspeel, social worker and engineering and social action studentBenoit Kieffer, Digital sobriety engineerGuillaume SCHLOTTERBECK, Software EngineerChelsea Sidrane, PhD Candidate in Aerospace Engineering, Stanford UniversityGiustina Selvelli, Assistant Professor, University of Nova Gorica, SloveniaNathan Lassance, Assistant Professor, UCLouvainNicolas Mabire, industrial robotics engineerGaetan Lepage, PhD student at Inria, FrancePierre Ponsoda, Master's student in Philosophy of Science, Sorbonne UniversityPushpendra Singh, Professor, IIIT-Delhi, IndiaDavid Leslie, The Alan Turing InstituteJulien Fageot, mathematicianNigel Robertson, Centre for Tertiary Teaching & Learning, University of WaikatoAdrien Thabuis, PhD student, Swiss Federal Institute of Technology Lausanne (EPFL)Louis Delporte, Software EngineerJulien Bourdin, Software Engineer and Data Protection Officer, WeezeventVictor Daniel, Teacher and developperTristan Harris, Center for Humane TechnologyJoshua Achiam, OpenAILeon Fauste, PhD StudentDanielle Smith, PhD, Senior Director | XD (eXperience Design), Experience Research & Accessibility, Express ScriptsJulien Lene, IT Archietecture ConsultantPierre Navard, Software EngineerMaxime Lecomte, student in PhysicsSue Felshin, Research Scientist, MIT CSAILHugo Tranin, PhD student in Astrophysics, Universite Toulouse IIICindy Ma, Doctoral Candidate, Oxford Internet InstituteBetty Mohler, Principal Research Scientist, AmazonChristopher Miles, dual PhD Candidate Depts. of Comm & Culture and Informatics, Indiana UniversityYann Aguettaz, M.Sc student at Ecole Normale Superieure de Lyon, FranceArthur Masson, Research EngineerSamuel Benveniste, Broca Living Lab, Paris, FranceDwayne Jeng, Principal Software Engineer, Sage BionetworksJa May, Application Testing EngineerBen Crulis, research engineer at the University of Tours in FranceBrandon Hyman, Senior Product Designer, HuluOscar Barbier, machine learning engineerOliver Prosperi, Educational ScientistWandemberg Gibaut, PhD Student, UnicampGuillaume de Vals, Software EngineerDamian Carmona, informatic student, Polytech GrenobleTiffany Jiang, Product DesignerJean-Francois Riera, System EngineerDr Lionel Germain, Lecturer, University of Lorraine, FranceArnaud Y. Massenet, MSc Applied Mathematics, Ecole Normale Superieure Paris-SaclayLaura Grave de Peralta, PhD candidateJames Othman Massaquoi, Amazon Account Manager and Google Intern 19'Natalie Bernat, PhD Candidate, CaltechSerge Bonny, System EngineerFrancois Bosma, IT engineerJohn Chodera, Associate Member, Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer CenterAlexis Jacquin, MSc student in forensic science, UNILDr. Emmanuelle Giraud, physician biologist, instructional designerAdrien Clerbaut, Software EngineerNathan Quiblier, Sorbonne University and Ecole Polytechnic in ParisNikko Mendoza, Product Manager at WorkdayLucas Schott, Research Engineer, SystemX Institute of Research and TechnologyStephen Ra, Machine Learning Lead, PfizerAnne Jorstad, Computer Science Postdoc, EPFLJonathan Ish-Horowicz, PhD Student, Imperial College LondonRobin Zbinden, Master's student in Data Science at EPFLDr James Stovold, Lecturer in Artificial Intelligence, Department of Computer Science, University of YorkSimon Delage, Data Scientist.Max Arnell, Researcher, Department of Urban Studies and Planning, Massachusetts Institute of TechnologyLeo Karoubi, Software EngineerDidier Masseret enseignant ENPC Stanislas DOZIAS, Computer Science and Applied Mathematics Student, Ecole PolytechniqueAdrien Mialland, Gipsa-lab PhD student, medical technologyRaphael Boeuf, Data ScientistLouis Cotelle, IT student at TelecomSudParisNicolas Poisson, Software Engineer StudentGregory MILA, software engineerMorgan Klein  CTOAron Ricardo Perez-Lopez, Master's Student, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology; former GooglerLouis Notteghem, Master's student, Institut Superieur d'electronique de Paris (ISEP)Guillaume Clerc, Software EngineerAngela Lin, Research Engineer, SalesforceFrancois Probst, Engineer at Laboratoire Hubert Curien, FranceYannick Giner, Receptionnist Club MedLoick DUCARME, Web DeveloperDiane deGraft-Johnson, Software EngineerUmberto Zona, research fellow, Department of Education, Universita degli Studi Roma Tre.Hana Thier, Software Engineer, ClayMartin Croville, IT Consultant, DevoteamOlivier Languin-Cattoen, PhD Candidate, Paris Sciences et Lettres UniversityMarshall Moutenot, Co-Founder of Upstream TechAlden Keefe Sampson, Co-Founder of Upstream Tech Alex Crozat, Mathematics, Physics and Engineering Student, PSI*, Pierre de FermatLouis Xavier BRUSSET, software quality engineerSanja Simonovikj, MEng student at MIT CSAILUlysse Gantois, Science Student in Lycee Saint-Louis (Paris)Jean-Baptiste Sablonniere, consultant seniorJoy Ming, PhD Student, Cornell University; former GooglerLeonard Suslian, DevOps Engineer at PartooAmadis Lemore, Master degree Student in Computer science in University of LorraineSven LOTHE, Student in Data Science, IA SchoolAhmed Bouzid, CEO, WitlingoSharon Heung, PhD Student, Information Science, Cornell University Yannick Prudent, MSc Student, ISAE-SUPAEROMaxime Monfort, french engineering studentJanet Chen, PhD Student, Computer Science, Cornell UniversityGuillaume de Vals, Software EnginneerGregory SCHMID, Data ArchitectEliseo Pascuzzi, Undergraduate Student, Science and technology, University of Picardy Jules Verne, FranceMelchior Thambipillai, Senior Software Engineer, EPFL AlumniAbdoul Azize Yougbare, Software EngineerBaptiste Cadiou, GFX developperCharbel-Raphael Segerie, Research Engineer, Paris-Saclay UniversityLouis-Jacques Etaix, Grenoble undergraduate student in psychologyAlex Louat, Postdoctoral Researcher, TechnionGeoffroy Dubourg-Felonneau, Machine Learning Lead at Shiru.comRosamond Thalken, PhD Student in Information Science, Cornell UniversityGawen FOLLET, engineering studentMatthieu Duflot, Medical and Nuclear EngineerMathieu Jung-Muller, Engineering Student in Computer ScienceLouis Faucon, PhD Student, EPFLGissela Moya, Tech Fellow, Greenlining Institute Eckhard Siegmann, Volt Europe, AI Policy LeadElia Kowalski, Geophysics engineer, UnistraLoic Zaugg, Tutor with Completude  Toulouse  FranceFabien Ducat, Expert product owner, Opal RT TechnologiesDr. Neerupma Bhardwaj, Researcher, Technion Nicolas Claverie, PhD candidate, Universite de ToursDr. Arthur Goldsipe, Principal Engineer, Computational Biology, MathWorksDAVID Tom, Student at Ecole Normale Superieure de LyonMerlin Bonato, Software EngineerTimothe Malahieude, Software Engineer, LecabGabriel Spick, System architectLetizia Milli, Postdoctoral Researcher, University of Pisa, ItalyJoeva Rock, PhD, Lecturer, UC BerkeleyL Jean Camp, Professor, Indiana UniversityThibault Merle, astrophysicist, ULBAbdelhak Bougouffa, Ph.D. Candidate, Paris-Saclay UniversityAncelin Bouchet, Computer Science Engineering Student, FranceDubosc Marius, Informatic Engineer, EpitaSamuel Gervais, software engineerSofiane Benabdallah, CTOYannick Nana, Cognitive Science GraduateGael Royan, Hardware EngineerThomas Saquet, Computer Science Communicator at QTG and Engineer at Solution LibreDr. Alexandra Moylett, Quantum Scientist, River Lane Research, Cambridge, UKMirabelle Jones, Research Assistant  Computer Science, University of CopenhagenLucile Audiot, professeure agregee, Universite Paris 1  Pantheon SorbonneJessica Dai, Machine Learning Engineer, ArthurAIAidan Jungo, Aerospace EngineerPhilippe TORSET, Civil work engineer, freelancerKen Anderson, Professor and Chair, Department of Computer Science, University of Colorado BoulderCalvin Liang, PhD Student, University of WashingtonFabian Holt, Associate Professor, Department of Communication and Arts, Roskilde UniversityAnna Rohrbach, Research Scientist, University of California, BerkeleyErwan Privat, PhD Student, Universite de BourgogneAiha Nguyen, Program Director, Labor Futures, Data & Society Research InstituteAurelien Vermeir, CIOAlexis Cvetkov-Iliev, PhD student at INRIAHector de Pellegars, Master's Student, ENTPECyril Fassotte, Bio-engineer & Science teacherSamuel Damoy, Instrumentation engineerLouison Braun, Telecom ParisGlorianna Jagfeld, PhD Student, Lancaster University, England, UKArthur Lecert, PhD Student, INRIATom Garnier, student at Ensai (France)Steffan Pedersen, Meemo AI  Eugene Ndiaye, Postdoctoral researcher, Riken AIPJoshua Harrower, Ryerson UniversityCorentin GIROUD-ARGOUD, EPITA CS GraduateJoachim Chaupitre, Software EngineerEllena Jonathan, Software EngineerEliott Lavier, Undergraduate Student in Computer Sciences, University of Versailles Saint-Quentin-en-YvelinesArnaud Brendel, Software Engineer, NeoviaGenevieve Macfarlane Smith, Associate Director | Center for Equity, Gender & Leadership, UC Berkeley Haas School of BusinessBertho Gwenegan, Student at EPITAMarechal Theo, PhD Student, INRAEDr Rebecca Defina, University of MelbournePaula Wellings, Design Research, GuideaLudovic Gabellier, MD, University Hospital of Montpellier, FranceAurelien SIMON, student in Cognitive SystemsMatthieu Humbert, Master's student, Anthropology, Paris-Nanterre UniversityKaty Cook, PhD, Ethicist, Author of The Psychology of Silicon Valley: Ethical Threats and Emotional Unintelligence in the Tech IndustryMiriam Aschkenasy MD, MPH, MPA, Program Director, Institutional Antiracism and Accountability (IARA) Project, Shorenstein Center on Media, Politics, and Public Policy, Harvard Kennedy SchoolJeremie CHARLOTTE, CPGE StudentAaron Clauset, Associate Professor, Department of Computer Science, University of Colorado BoulderBoussejra Amir, data science engineering student at Ecole des MinesAlexis Morfin, Research EngineerBenjamin Haioun, Scientific ResearcherCharles FLORIAN, biology teacher in middle school.Franck Miquel, AI / ML StudentYohan Bonafe, Graphic DesignerThomas Devogele, Computer Science ResearcherMarc Faddoul, AI Research Scientist, UC Berkeley School of InformationDamien BAUSSANT, Optical EngineerC Henrik R Aslund, PhD Student, Imperial College LondonJehan-David Pelaez Romero, Software ArchitectSamuel Asserpe, Student in AIFrancois Straet, Undergraduate Student, ULiege, BelgiumPaul Casalonga  Graphic and web designer, Cas-pLuc Rodriguez, MS student at SIGMA ClermontNathanael Asfaw, physics student, ENSLAlexandre RIABTSEV, undergraduate student at Telecom ParisMatthieu VALENTIN, Engineering Student, University of Lorraine Lucas Druart  Student @ Grenoble INP  EnsimagClement Miguet, insurance engineerNathan Jacot, Bachelor Student, EPFLTristan Nerson, Student at Le Mans UniversityValerie Catanzaro, Digital ArtistDr. Greg Copas, DVM, Researcher and CSO-CC.Ferdinand RAPIN, Student at ULBAdrien Matissart, Software EngineerLaura Green, co-founder of the NGO Altruisme Efficace FranceVincent Schellekens  ICTEAM research institute, UCLouvainLudovic LLUCIA, PhD, Software EngineerGwenael COTREZJulien Thomas, student in software engineering, ENSSAT (France)Sabrina Taylor, Technical WriterGuillaume de Vals, Software EngineerNiels rbk Chemnitz, PhD Student, IT University of CopenhagenBarney Durrant, Xoogler and Owner, Bluebell DigitalJin-Ge Yao, MicrosoftRoman Martin, M.Sc., Department of Mathematics & Computer Science, University of MarburgElizabeth Ogburn, Associate Professor of Biostatistics, Johns Hopkins University Benjamin Schulz, Machine Learning EngineerPierre Carette, PharmD student & Computer ScienceSathvik Nair, Software Development Engineer, Amazon Web ServicesAllison Scott, CEO, Kapor CenterAlison Stanton, Co-Founder Stanton Ventures, Inc.I'm Florence NENY, french mathematics and computer science teacherOtto Sahlgren, PhD student, Tampere UniversityNai Ducos, biology student at Paris Saclay UniversityArnaud Da Silva, Technical support, Groupe CorianceFriedger Muffke, PhD  OpenIntentsAMANO Danji, Law student, University of LimogesBeuselinck Maxence, engineering studentAlexander Hicks, University College LondonLucas Mourot, PhD Student, Inria Universite Rennes 1Grin Lord, Clinical Psychologist and Research Scientist, University of Washington Kyle Erf, VP, Moving Pieces, Former GooglerSteven Cherry, Adjunct Professor, NYU Tandon School of Engineering Antoine BONELLI, Programme Grande Ecole Annee 2  emlyon business schoolSnigdha Kumar, University of MinnesotaAlvin Grissom II, Ph.D., Assistant Professor of Computer Science, Haverford CollegeBryce A. Lynch, Research and Development, Special Circumstances, LLCAlyssa V. Loera, Digital Services & Technology Librarian, Cal Poly Pomona Axel Gomez, Professor of Philosophy, Academy of VersaillesMika Wee, student at the University of Toronto (Canada)Munmun De Choudhury, Associate Professor, School of Interactive Computing, Georgia Institute of TechnologyRohit Khandelwal, Product ManagerKhamis Liam, Undergraduate Student, Department of Economics, University of ToulouseGabriel Tem Pass, Data Analyst, 99Mihai Pop, Professor and Director, Department of Computer Science and UMIACS, University of Maryland, College ParkRoss Girshick, Facebook AI ResearchZachary BENSALEM, UX DesignerLoic Cerf, Associate Professor, Federal University of Minas Gerais, BrazilLuke Oakden-Rayner, University of AdelaideHannah Howard, Software Engineer, Carbon FiveSiddharth Jha, CEO, PlotPaul Alexander Hodgetts, Master of Information Candidate in Human-Centred Data Science, Faculty of Information at the University of TorontoTom Goodman SFHEA FRSA FBS MBCS, Researcher, University of Birmingham School of Computer ScienceAnupam Chander, Professor of Law, Georgetown UniversityMichael Palm, Associate Professor, Department of Communication, University of North Carolina at Chapel HillJean-Luc Fernandes Software EngineerSophie Engle, Associate Professor, Department of Computer Science, University of San FranciscoColin Roland  Gaubert, Student at Sciences PoMourad Gridach, ProfessorAlexandre Chea, Thermal Hydraulic EngineerFrancois Potin, engineer, ICAM FranceJean Laplaige, Mechanical Engineer, ESILVLeo Perrin, Software EngineerLucas Einig, Graduate student, Grenoble INP, FranceStephane Beretta, Civil Engineer Technician, SwitzerlandNiall Richard Murphy, Azure SRE DirectorPaul Madelenat, CS Student @ Swiss Federal Institute of Technology Lausanne (EPFL)Rebecca Ulrich, Executive Assistant, UC BerkeleyDr Jessica Korte, TAS DCRC Fellow, The University of Queensland, AustraliaFrederic Dubut, PM Manager, MicrosoftSami HAMINE, IT engineerTessa Adair, Product Manager, LaserficheHenri de Boutray, PhD student in computer science, FEMTO-ST laboratory, FranceTeanna Barrett, Undergraduate CS Student at Howard UniversityMartin Lefebvre, PhD student, Universite catholique de LouvainIgor Krawczuk, PhD student at EPFLJohn Connor Meents, Georgia State UniversityRose Meyers, Software EngineerGabriela Zayas del Rio, Graduate Student, MIT Department of Urban Studies and PlanningSimon Buttet, EngineerMartin Francqueville, PhD Student, Universite de BordeauxClaudia Lutze, Cybersecurity Researcher, AustriaBrian Cantwell Smith, Faculty of Information, University of TorontoMaxime de Pachtere, Software Engineer.Julien TOUZEAU, Lead Software Developer, AubayTheo Zein, mathematic bachelor student, Aix-Marseille UniversiteCarrel Matshitshi, Civil societyLaurent Valentin Jospin, PhD Student, University of Western AustraliaMilan Dasgupta, Senior Software Engineer, MicrosoftMohamad Bdeir, software engineering at ISAE-CNAM LebanonDr Hannah Barham-Brown, Deputy Leader, Women's Equality Party UKMaeva Mathieu, author and IllustratorZoe Husted, Undergraduate Student at UC Berkeley & Incoming Google SWEAman Narayan, Analyst, SPCValentin Decaillet, Research ScientistEric Nguyen, Master's Student, CentraleSupelecFrederic Bouffier, event planner Frames FestivalAnatole Cremel  Schlemer, undergraduate physical ingeenering Sorbonnes UniversiteClement Caffin, Graduate Student, ENSIMAGMelissa Gold, Science Librarian, MILLERSVILLE UNIVERSITYEmmanuel Franquemagne, Senior Engineering Manager, Agile Expert, Senior developerRaphael BELON, Electrical engineer, ESME SudriaPiper Jackson, Assistant Professor, Thompson Rivers UniversityColleen M. Swanson, Chief Scientist, Bolt LabsKrishna Pillutla, PhD student, University of WashingtonGreg Szczyrbak, Associate Professor, Library Department, Millersville UniversityEric Aubourg, Researcher in Astrophysics, Paris, FranceAnanth B, Ph.D candidate, NYUNathan Billard, Software Engineer StudentGebhard Matt , Senior Research Scientist, ETH Zurich, SwitzerlandJustin Dallant, PhD Student, Universite Libre de BruxellesBen Kraft, Software EngineerPalak Dudani, Designer + Researcher, Oslo School of Architecture and DesignCedric Whitney  University of California, BerkeleySlim (Sarah Lim), PhD Student at UC Berkeley, Software Engineer at Notion LabsDimitrios Glynos, Vice President , Institute for Internet & the Just Society, BerlinVincent de Phily, Software EngineerBehdad Esfahbod, Former Senior Staff Software Engineer, Google & FacebookTania Duarte, Co-Founder, We and AIFelix Neutatz, PhD Student, TU BerlinNick Matheson, TwitterThomas CAMY, Creative Director Avellan Olivier, Polytech Engineer, and Art School StudentFrances Mican, therapistZachary BENSALEM, UX Designer, QredenceDjalal M. Hedna, Computer Science Bachelor Student and Software Developer, ESIEE IT and PrestaclicBlake Marques Carrington | Associate Professor of Interactive Arts | Department of Digital Arts, Pratt Institute Peter Satterthwaite, PhD Student, MIT EECS Louis Renaux, Machine Learning EngineerOlivia Mirascain, Undergrad student at the University of WashingtonFrederick Fabre Ferber, Data Science Student Iroro Orife, Senior Software Engineer, NetflixJenine Carron, Summer Researcher, Tauru Ihirangi, CMIC | Te Herenga Waka, Victoria University of Wellington, New ZealandJulie Polk, Technical Writer, Former IBMerPatrice Bois, retired classical literature teacherC Fred Richardson, Grid Central ConsultingBrianna Espino, LCSW, A.M. The University of Chicago, B.S. University of California DavisRobin Dupuis, Lighting designerTayo Akinyemi, Principal Systems Designer, Query InsightsJonathan BeltranHeather Lane, Senior Architect of Data Science at athernahealth, and XooglerLiz Lucas, Data Analyst EngineerEtienne LEFAI, INRAE Senior Research Scientist, FranceMegan E. Mansfield, M.A., social scientistFrancois Generau, PhD, Universite Grenoble AlpesMaximilien Chau, Student at an Engineering schoolChristophe Mazzara, physicien medicalTimothee Bigex | Etudiant 2nde annee | EpitechOctavia Maria Sulea, NLP Data ScientistMatthieu DELACOMMUNE, Undergraduate Student at CentraleSupelec, Universite Paris SaclayMaria Coste, Data ScientistBastien Zimmermann, engineering student, EURECOMMichael J. Oghia | Advocacy & Engagement Manager, Global Forum for Media Development (GFMD)Jean-Pierre SCHLOTTERBECK : Retired research engineer (Thales)Mouhamadou Seck, University of Nantes, FranceLeo-Paul Charlet, Undergraduate Student, Paris-Saclay University Yun Zeng, Software EngineerAlexis Queen, Harvard Undergraduate '23Natasha Borgen, MPH, University of California San FranciscoRoberta Dousa, PhD student, UC BerkeleyAngela Pham, graduate student in Big Data & Analytics @ ECE.ParisTom Boissonnet, PhD student, European Molecular Biology LaboratoryFrederic Caparros, Software EngineerVeena Dubal, J.D., Ph.D., Professor of Law, Harry & Lillian Hastings Research Chair, University of California, HastingsVictor Yon, PhD student, Sherbrooke UniversityVincent Paugam, Design EngineerAlgorithmWatchGuillaume Deslandes, bid manager, SPHEREAAmalie Trewartha, Postdoctoral Scholar, Lawrence Berkeley National LaboratorySelam Abraham, Student, University of AlbertaOttavio Khalifa, AI StudentOlivier Cornelis, French Medical StudentBenjamin Martin, Engineering studentManon Michel, Master's student in Computer and Communication Sciences at EPFLBasile Rulh, student at Telecom ParisMarie Mansour, Ocupational TherapistHugo Simon, IA Engineer, FranceLucas Muller, student, University of Lyon Claire Descombes, student in mathematics and physics, University of BernCharlotte Blease, PhD | OpenNotes Keane Scholar, OpenNotes | Beth Israel Deaconess Medical CenterTheo D'Antuono, Masters Student, University of Lorraine Jessica Verran-Lingard, Technology and Startup LawyerGregoire DOAT, student, UPPA (Universite de Pau et des Pays de l'Adour)Gregory Baltus, PhD candidate, university of LiegeLeo Sakri, administrative officer, CcomptesAndrea Alarcon, PhD candidate, Annenberg School for Communication and Journalism, University of Southern CaliforniaAndrea RIGAL-CASTA, Environmental LawyerBenoit Wallon, software engineer Steven Challe, developerEtienne Hodille, researcher associateAnastasiia Kucherenko, PhD student at EPFLTanguy Gernigon, trade union employeePhilippe Namias, Software Engineer, GED INVESTValerie Moy, Software Engineer, PinterestBaptiste Lafoux, PhD student in fluid mechanics (ESPCI, Sorbonne Universite)Xavier Loux, Generalist Game DeveloperDaniel Smolyak, PhD Student, Department of Computer Science, University of Maryland, College ParkBertrand Lagarde, Web DesignerAxel Rousselot, Student at Ecole polytechniqueMuthu Annamalai, Software Engineer, Stealth Startup Palo Alto, CASagar Hugar, Grad Student, MA New Media & Digital Culture, University of AmsterdamAlexis Chazard, Creative Digital Design Professor, ESAD-GV Grenoble-Valence Art & Design Graduate SchoolGuillaume Colin, Energy and Climate ConsultantDominik Golle, Coordinator, Hertie Network on Digitalization (HNoD)Eric Lecomte, Government officialEliot Deneux, university computer science studentCedric Rozzes, Master student at IPI Toulouse in network and Cybersecurity / FRANCELoic Doubinine, Software EngineerYonatan Tekleab, PhD Candidate, MITArsany Guirguis, EPFLMoumita Das, Associate Professor, Rochester Institute of TechnologyKayleen Keefer, Middle School Teacher, Hawthorne Middle School CUSSET Swann, Physiotherapist Julien BOUCHER Gerant, NC DecorSimon GUSTIN, Software Engineer, Arhs Developments BelgiumRalph Pastel, Product Manager, Dalia ResearchMatheo Dumont, Student Master 2 Computer Graphics at Universite Claude Bernard Lyon FranceAdam D.  Software Engineer / PhD Student in AI at Intel CorporationTori Culler, NC State University Libraries FellowYoucef Ammar-Khodja, Algerian political analyst and human rights activistMathieu Jacquin, engineerAnny Gakhokidze, Software Engineer, Mozilla CorporationBlanchard AllanDaniel Ricciardelli, Machine Learning Researcher, LinkedInDanielle Olson, Massachusetts Institute of TechnologyDr Carla Bonina, Senior Lecturer (Associate Professor) in innovation and entrepreneurship, Core Member  Surrey Centre of Digital Economy, Surrey Business SchoolLaetitia Chuine, orthophoniste (speech therapist)Moudrik Chamoux, Engineering Student CentraleSupelec, Mathematics Student Paris-SaclayWendy TaySydney Christian, Graduate StudentNathanael Tepakbong, MSc. Student, ISAE-SupaeroRudy Patard, former PhD student at Lille1Xavier FORUNA, Enterprise Architect & CISOVincent Cauchois, MSc Engineering, ISAE-Supaero, FranceJason Edward Lewis, Director, Initiative for Indigenous Futures, Co-director, Aboriginal Territories in Cyberspace, Concordia University Menno Deij  van Rijswijk, Researcher/ProgrammerOkki Berendschot, MCP Student, Department of Urban Studies and Planning, MITThito Wisambodhi, Graduate Student, MITLaurent Sacco, Redacteur Sciences Physiques | Futura-Sciencesdorian luzineauHannah ChapmanEmmylou NicolleMichael Shawn MckibbinLeah BordenGuillaume Bouillon, Business and Economics studentTim Peyron, Engineering ApprenticeMarlon-Bradley Paniah, Undergraduate Telecommunications Engineering Student, INSA Lyonsridipta, PhD student in Public PolicyAmber M. DohertyScarlet Benson, MDLuwa Matthews, Software Engineer, AppleRaphael Dordeins, Master degree in chemistry and chemical engineering; former HSE engineerkatarpilar, IT Security EngineerMarius Jassoud, video game studentEmilie Neveu Research Scientist UNILAlicia Drucker, CEXOBenoit Lejeune, Attache SPF Finance BelgiumAlexandre Bugnard, Student at EPFL LausanneGwendoline De Bie, Research ScientistJeanine MergensKala Barnes, Automation Test EngineerCori Crider, co-Founder and Director, FoxgloveMaxime Tarrasse, Master's student in Astrophysics, Universite Paul Sabatier ToulouseMichele Markstein, Assistant Professor, Biology Department, University of Massachusetts AmherstBoris LB, process engineerOmosola Odetunde, Founder & CTO AdvisorYouval Aharon, Undergraduate Student at UCLouvainMarie BOSC, cybersecurity engineerAlois Blarre, Data Science Graduate Student at ISAE-SUPAEROBeth Tellman, Chief Science Officer, Cloud to StreetEstelle Inack, Perimeter InstituteLois Castets, studentAlex Chen, Table XI Product DesignerDr Simon Bouget, Senior Researcher in Distributed Systems and CybersecuritySebastien Cobos, Spirulina Producer, FranceHugo Feliciano, MINES Saint-Etienne Energy efficiency applied to buildings engineerNeel Shah, Principal EngineerSebastien Beffa, software engineerRobb Viens, Stanford University 21'Pierrot Cauchy, IT engineerFavre AlexisEva Thelisson, CEO AI Transparency Institute (AITI)Daniel M. Negusse, Pharmacist (Walgreens)Quentin Brabant, Data ScientistKalpana Mandal, PhD, Postdoctoral ResearcherLucas BibolletEric Steckx, MDTheresa Duringer, CEO, Temple Gates GamesWilliam Porreweck, Master's Student in Educational SciencesLucas Rhetat  Student, INSABuraaq Alrawi, UC MercedSamuel Benard, human being, welder, google userVincent Cauchois, MSc Engineering, ISAE-Supaero, FranceJorge Rabanal-Arabach, Ph.D., Assistant Professor, Faculty of Engineering, University of AntofagastaAllan POINT, studentSaya Kim-Suzuki, Fieldston High School StudentAnnie Biby RaphealMadeline Klinger, PhD Candidate, Helen Wills Neuroscience Institute, UC BerkeleySofia Lemons, Assistant Professor of Computer Science, Earlham CollegeKidist Alemu, CS student at Harvard UniversityAnna Peckinpah, Data Scientist, Consumer Experience & InsightsJoana Francener, designerSiddha Ganju, NvidiaDr. Conor O'Kelly, Teaching Fellow, Department of Film Studies, The School of Creative Arts, Trinity College Dublin, the University of DublinKyra Yee, ML Research Engineer, TwitterFabien Moutarde, professor, MINES ParisTech, PSL UniversityB. Allen, Software Engineer in TestNino Segala, Student in Machine Learning, KTH StockholmAlexandria RangoussisPui Man Kam, PhD Candidate ETH ZurichNeda Ghafourian, Rights Management AnalystAnnabeth Nix, University of Arizona undergraduateMarcel Yiao, PhD, researcherEmily PrastIsabel RothCharles Edouard HesseKen Holstein, Assistant Professor at Carnegie Mellon UniversityOlorundamilola 'Dami' KazeemFlorentin Brisebard, Geomatics Student, ENSGGuerinet Remi, mechanical engineering student, Federal Institute of Technology Lausanne (EPFL)Damien Desjardin, Game DeveloperLoic Vanden Broeck, MSc in Economics at Solvay Brussels School of Economics and ManagementSinai Teffera, Student, University of California, RiversideEliaz Pitavy, Computer Science student at EPITAAlain Park Suk SwierkowskiClaire TomanDr. Luette Forrest research scientistNina GentlesGrace DoyleToni Sagayaraj, Brown UniversityBethany Radcliff, Master's student in English and Information Studies at the University of Texas at AustinHongfang Liu, PhD, FACMIGenesis Bookard, Carolina International SchoolShreya Singh, Planned Parenthood Federation of AmericaCharles Cadet, Health information technicianAnthony Cadillon, professeur-documentalisteCollin Stoffel, AI Researcher Allanah Rolph, Undergraduate Student, Harvard College '23Catherine Lai, Lecturer in Speech and Language Technology, University of EdinburghLouis de Monterno, PhD student in computer science, LIX, France.Dominique Climenti, Security Engineer, Kyos SA, ACMAmaury Jorant, Data Scientist, Arvalis Agathe Michot, Student in Engineering, Ecole Centrale de LyonJulia Villarroel, Novo Nordisk Foundation Center for Basic Metabolic Research, University of CopenhagenMax Dunitz, PhD candidate in mathematics, Universite Paris-SaclayJerome Avrillon, Statistician, Transgene S.A.Frank-H. Welz-Detroy, Geschaftsfuhrender Vorstand, Berufsbildungswerk Dr. Fritz BauerMarianna Dikaiakou, Principal Engineering Manager, Microsoft Myles Lewando, Committee Member, United Tech & Allied Workers (UTAW)Sara (""Meg"") Davis, Ph.D., Research fellow, Graduate Institute of International and Development StudiesThomas Billet, computer engineering student, Polytech MarseilleAbdimajid Osman, Associate Professor of Medical Genetics, PhD, Linkoping University Marius Henry, Msc Student in Mathematics, ETH ZurichFlorian Bouillet, Software EngineerAugustin BAR, IT Engineer student, CPE Lyon, FranceKatharine Beaumont, University College Dublin, IrelandBastien Carel, University of Lyon, Ecole Centrale de LyonAndrea Brizzi, PHD student, Imperial CollegeBenoit Fuentes, Research Scientist, Telecom ParisMike Vola, driving manager and maintenance technicianAlberto M. Chierici, Ph.D. Candidate in Computer Science & Engineering, New York University Abu DhabiRaphael Laurenceau, Researcher at Institut PasteurFranck MIHALIC, Software EngineerBarbara Paes, Minas ProgramamMichal Fabinger, University of TokyoAri Font, Director ML Platform and ML Ethics, Transparency and Accountability, TwitterWaris Radji, Engineering Student at Polytechnic Institute of BordeauxDr. Nicolas Simon MD, Private practitioner, Poissy, FranceSebastien Colla, PhD Student, UCLouvain, BelgiumGabriel Thil, Economics Student at Pantheon-Assas UniversityCharles Rey, Software EngineerValentin Durning, Film Editor, FranceCarlos Mercado, Senior Data Scientist, GuidehouseJacinta Conrad, Professor of Chemical Engineering, University of HoustonMichel Limoges, engineer, FranceSamy Rezzouqi, Software EngineerFrancis Kintz, Technical System EngineerLouis-David Deschenes, Computer Science student, McGill UniversityJohn Chadfield, Product Manager, Who Targets MeAlexander HerrDaniel BlumenthalEmily Lubar, University of Texas at Austin graduate research assistantJonathan Dion, Developper at Healthcare Systems Group  DedalusAlex Pouliquen, Software DeveloperMatthew Shapiro, Associate Professor of Political Science, Illinois Institute of TechnologyJoshua Cambria, MPA | Data Scientist, CVS HealthAmritha Jayanti, Research Assistant, Technology and Public Purpose Project, Harvard Kennedy SchoolVictor Escorcia, PhDAlexander Meulemans, PhD student ETH ZurichMara Fennema, Graduate Student in Artificial Intelligence, Utrecht UniversityDeborah Brown, Senior Researcher on Digital Rights, Human Rights WatchGhislaine van den Boogerd, AI Masters student, Utrecht UniversityAlexandra Saizan, Data Scientist, GuidehouseRachel Wells, DataKindStefano Vrizzi, Research Assistant, Ecole Normale SuperieureChristine Liu, UC BerkeleyRania TafatVincent Tartaglione, Ph. D. student, University of BordeauxJulien Guerin, Data ScientistLeah Buechley, Associate Professor, Department of Computer Science, University of New MexicoMonica Jean Henderson, Ph.D. Student, Faculty of Information, University of TorontoHAFS AbdelwahebSofia Alarcon, Academic Advisor at the University of Texas at AustinStephen Lewis, Software EngineerRackeb Tesfaye PhD Candidate, Integrated Program in Neuroscience McGill UniversityJean-Matthieu Schaffhauser, Senior Software EngineerEric Sans d'Agut, Senior Software EngineerRobert Ellis, Associate Professor, Illinois Institute of TechnologyMark Freeman, Data ScientistAditya Devarakonda, Assistant Research Scientist, Johns Hopkins UniversityValentin Duchene, artistJeffrey Dripps, Senior Enterprise Java/JavaFX ArchitectLouis Billiet, Md student in Public health medicine, FranceThomas RASTOUIL, Senior Software EngineerRyan Williams Sr., Founder of the Other Side of the Firewall PodcastNoa Visser, Graduate student in Artificial intelligence, Utrecht UniversityArthur Grimonpont, Food security researcher, Universite de LyonYouri Bosque, a concerned young ladBennett Alterman, Georgia Institute of Technology Thomas Treml, Data ScientistAdriana Casarez, Librarian, University of Texas at AustinThomas de Chevigny, Master Student, Data Science, EPFLSergio Graziosi, Information Systems Manager at the Social Science Research Unit, part of the UCL Social Research Institute (London, UK).Vincent Hamel, physiotherapist, post-graduate teacher of pain neuroscience (I/O Sport)Kara Woo, Principal Bioinformatics Engineer, Sage BionetworksLea KarimmasihiEthan Sichel, undergraduate student at the University of North Carolina at Chapel HillTilo Hartmann, Full Professor VR and communication, Department Chair, Department of Communication Science, Vrije Universiteit Amsterdam (VU)Maxime Delorme  Research Engineer, CEA SaclayDanica Li, Attorney at Leonard Carder LLCZoey Rebecca Boisen, Oregon State University College of ForestryYanis Marchand, PhD studentRomain Bellanger, Site Reliability EngineerNils Raymond, grad student at Aix-Marseille UniversiteLauren Hall-Lew, University of EdinburghNicolas Merli, Master's degree student in NLP at Universite de ParisLynette Shaw, Ph.D., Fellow, Insight Data ScienceZulema Valdez, Ph.D., Professor, University of California at MercedKate Auman, Cranbrook Academy of Art, 2D Masters CandidateEric Pilote, PhD candidate, lecturer, Universite de MontrealAbdulahi Mohammed, Environmental & Civil Engineer, Addis AbabaElaine Hsieh, Columbia GSAPPKelsey Campbell, Data ScientistLaurane Castiaux, NLP student, UCLouvainTyler Dae Devlin, Senior Machine Learning Engineer, LinkedInMatt Weiner, Associate Professor of Philosophy, University of VermontSasha Perez, Director of Program ManagementJulieann Murphy, Undergraduate Student, Stevens Institute of TechnologyRae'ven G.Pol Dellaiera, Software engineer @ European CommissionBasile Pillet, teacherJulien Guay, Mechanical & Automation EngineerStephanie Allen, Ph.D. Student, University of Maryland, College ParkWendelyn Oslock, medical studentLaura Davis, student, Bronx Community CollegeAmanda MakDeepak Kapur, Distinguished Professor of Computer Science, The University of New MexicoAlaisha Sharma, Forward Deployed Engineer at Palantir TechnologiesCory Jog, Sr. Product OwnerMadeline Friend, Data Engineer, Distributor Data SolutionsAva EastmanPamela Dunsmore, English professor, Fullerton CollegeHamima Halim, Kensho Technologies, SWEAndre Ferreira, Software EngineerKathryn Bonnen, New York UniversityFatiha Sadat, UQAM Montreal, QC, CanadaMaggie MacDonald, PhD Student  University of Toronto Faculty of InformationRylan Peery, Co-Founder, CoLab CooperativeDr Tereza Hendl, Research Associate, Ludwig-Maximilians-University in MunichJosie Williams; Founder of Ashe StudiosDelphine MichaudFelipe Ventura software engineer and organizer at techqueriaErica M. Zacharie, J.D., M.P.A., Founder, AfriCentered, L.L.C.Halidou Cisse, Software Engineer Microsoft Mixed RealityAdrien Weil, Head of Digital MarketingSophie Mourre  Software Engineer  CNRS  FranceAnuka Pokharel, Talent Acquisition Manager, RasaMarzena Karpinska, Postdoctoral Researcher, University of Massachusetts AmherstMary Elizabeth Luka, PhD, Assistant Professor, Department of Arts, Culture, Media (UTSC) & Faculty of Information, University of Toronto, and Director, Knowledge Media Design Collaborative Specialization (KMD-CS), KMDI-SemaphoreNadia Caidi, Associate Professor, Faculty of Information, Univ. of TorontoLoic Coulet, Software engineer & development managerAudrey Evans, Data & Society Research InstituteZac Anger, Senior SRE, Jane.comDeMarcus Edwards, Howard UniversitySwetha Shankar, Research Engineer, Inria-SaclayDr. Nick Schuster, Research Fellow in AI Ethics, The Australian National UniversityCharlene ALEXANDRE, Supply Chain CoordinatorQuincy Childs, School of Geography and the Environment, University of OxfordGraeme Smith, Assistant Professor of Physics and JILA Fellow, University of Colorado BoulderBenedict DourlensJason Norwood-Young, Open Data AdvocateFlorian Morgan, Software Engineer, MipihSimon Bovard, GardenerAmirah Cisse, Senior Manager at WHY Strategy Group, Horizon MediaVenance Joffrion, StudentAnnuska Zolyomi, PhD Candidate, University of WashingtonVlad Nitu, CNRS Research Scientist, INSA Lyon, FranceEric Ghysels, UNC Chapel HillDr. Lionel Bloch, EPFL, CHHarry Hochheiser, Associate Professor, Department of Biomedical Informatics , University of PittsburghJanus Sanders  Founder of Janus InnovationsErin Case, Software EngineerPiotr Indyk, Professor, MITGennie Gebhart, Acting Activism Director, Electronic Frontier FoundationDr. Julian Pfeifle, Mathematician, Universitat Politecnica de Catalunya, BarcelonaLionel HILLION, Software EngineerAlan Galey, Associate Professor, Faculty of Information, University of TorontoJanna Z. Huang, PhD Student in Sociology, UC BerkeleyGina Hsieh, Stanford University AlumShion Guha, Assistant Professor, Department of Computer Science, Marquette UniversityPaul van Gent, MSc, PostDoctoral Researcher, TU DelftAlex Rudnick, XooglerSimon Ducatez, Educator & Technology passionateWilliam Chan, AI Engineer, LinkedInVelian Pandeliev, Assistant Professor (Teaching Stream), Faculty of Information, University of TorontoCarolin M. Schuster, Technical University of Munich, GermanyNathan Cahill, DPhil, Associate Professor of Mathematical Sciences, Rochester Institute of TechnologyP. Takis Metaxas, Professor and Chair of Computer Science, Wellesley CollegeJason Harris, Founder, The BlkRobot ProjectFlorent Defay, Software engineerYannick Stadtfeld, Researcher, Institute for Internet & the Just Society, BerlinMathieu Halef, Student in political scienceCecilia O. Alm, Associate Professor, Rochester Institute of TechnologyVictor Alberca, Graduate, Paris Pantheon-Sorbonne University.Renee di Cherri, Senior Product Marketing Manager, AdobeEden Fesseha, Harvard UndergradRoss Wang, independent researcher, former Google senior software engineerDeborah Harrison, Microsoft, Senior Content Experience Manager, Content IntelligenceNaian Baron, sociology studentEmily Fan, Undergraduate Student, Massachusetts Institute of Technology (MIT)Dr. Negin Dahya, Assistant Professor, Institute of Communication, Culture, Information and Technology & Faculty of Information, University of Toronto Jayson Lynch, Postdoctoral Researcher, University of WaterlooDr. Sara M. Grimes, Director, Knowledge Media Design Institute (KMDI), University of TorontoKristen Sheets, Machine Learning Engineer, SpotifyProfessor Jean Burgess, Centre Director, Digital Media Research Centre, Queensland University of Technology, AustraliaThomas SENTUCQ, Engineering student at ISAE-SUPAERO, deep learning intern.Stuart Coulson, Adjunct Professor, Stanford UniversityPete Nuwayser, Solutions Architect, IBMTim Turner, Nonprofit Digital Marketing Consultant, Business OwnerCalvin Mercer, ProfessorGian Maria Campedelli, Postdoctoral Research Fellow in Computational Criminology  University of Trento, ItalyJules MARIE, PhD Student, IAE Jean MonnetChitti Srinivasa Phani, Research Associate and PhD Student, School of Computer Science, Electrical and Electronic Engineering, and Engineering Maths (SCEEM), University of BristolMarcel Baumann, LecturerFranck Giton, maths teacher, FranceBilal Botte, Medical studentNico Amiri, Member Neuroethics Cycle  Institute for Internet & the Just SocietyQuentin Duchemin, Software EngineerMaria Smith, JD Student at Harvard Law SchoolHenry Prickett-Morgan, Undergraduate Engineering Student, University of TorontoMatilde Marcolli, Robert F. Christy Professor of Mathematics and Computing and Mathematical Sciences, California Institute of TechnologyMiranda Wei, PhD student, University of WashingtonKsenia H, NBUPhillip Rogaway, Professor of Computer Science, UC DavisLogan Stapleton, PhD Student, University of MinnesotaMichelle S Brown, Data Scientist & CEO of Curious Fox LabsPamela Mahler, Hunter University Student + American CitizenJodi F. Paroff, Adjunct Asst Professor of Public Service, NYU Robert F Wagner Graduate SchoolVassiki Chauhan, Graduate Student, Psychological and Brain Sciences, Dartmouth CollegeMehul Patel, Program ManagerDr Pierre GIROD, PhD Musicology, FranceJeandre Verster, Hyperion DevelopmentGuillaume Cogan, resident in medical genetics, University of Paris.Mathias Bossaerts, Engineering studentDalia Papuc, EPFLDonat E Grant, President of Missions for HealthLaurent Despeyroux, Software EngineerKaren Voght, Wellness, Inc.Raphael TEITGEN, Phd studentCarolyne R. Meehan, The Pennsylvania State UniversityJosep Curto Diaz, Estudis d'Informatica, Multimedia i Telecomunicacio, Director del Programa Master Intelligencia de Negoci i Big Data Analytics, Universitat Oberta de CatalunyaGeoffrey Huck, CEO at GEOT S.A.S, FranceDorian Guyot, MSc student in computer science at EPFLGiacomo Stroffolini, University of Torino, ItalySylvain Millard, IT Infrastructure engineerZahra Ahmadi, Ph.D., Research Associate, Johannes Gutenberg University Mainz, GermanyJane Hsu, Professor, National Taiwan UniversityGerry Chng  SingaporeMarie-Alice Mathis, neuroscience PhD, UX specialist in the video game industry and concerned Google user.Priyanka Sinha, PhD Student, Indian Institute of Technology, Kharagpur, Scientist, Tata Consultancy Services LimitedKatrina Morris, ActorSimin Li, Undergraduate Student, University of Maryland-College ParkAlexander Latev, MD MS, Emergency medicine physician, Clinical researcherSebastien A. Krier, Dataphysix Ltd (former AI policy adviser to the UK Government)Kipp Jones, Skyhook, Board of Advisors, Georgia Tech College of ComputingGeorgios Karras, PhD student on AI Ethics, Free University of Brussels (VUB)David Maginot, software engineerNoe Zufferey, University of Lausanne, PhD StudentEnrico Prevosti, electronic engineer, University of BresciaWandrille Legras, Undergraduate Student at ESIEE Paris, Gustave-Eiffel UniversityQuynn EvansDavid GONZALEZ, Mobile applications engineerMira Ruder-Hook, PMCassidy Henry, PhD Student, University of Maryland, College Park Department of LinguisticsAmaury Faure, Student at Ecole Centrale de Lille.Amos Toh, Senior researcher, artificial intelligence and human rights, Human Rights WatchKhushi SainiIonel Chila, Cybersecurity ManagerQuentin Chenevier, AI product manager & data scientistYanis CallipolitisMonica Hannush, MAT USC Rossier School of EducationSai Joseph, Graduate Student, Northeastern UniversityAndualem Workneh, Enrolled AgentAja nicholson, teacherMs. Julia S. Butler, A.A.S., B.A. [2021]Connor Atkins, Computer Science student, Swansea UniversityPatrick Keilty, PhD, Associate Professor, Faculty of Information, Centre for Sexual Diversity Studies, Archives Director, Sexual Representation Collection, University of TorontoAkshat Mahajan, SWEAdam BluefordCornelia Fjelkestam, Energy EngineerAliza Saraco-Polner, Operations Manager and MBA CandidateHayat Alkadir, Harvard CollegeHailey Brown, Software EngineerSydney Lewis, Harvard College UndergraduateJohn DeTreville, Ph.D. Computer systems researcher, and Xoogler.Brian Altenhofen, PhD, Truman State UniversityDaniel Roy, University of TorontoKatta Spiel, TU WienOle Winther, Professor, University of Copenhagen and Technical University of DenmarkNalini K. Singh, User Services Librarian, Robarts Library, University of TorontoZvi Effron, Staff Software Engineer, Riot GamesMeier Vanni, Student at EPFLStephanie Jordan, Assistant Professor, Center for Gender in a Global Context, Michigan State UniversityDiogo Machado, MD, DESA, AnesthesiologistGabriela FelderLorren Butterwick, MBA Candidate, Presidio Graduate SchoolEric Yu, Professor, University of Toronto Faculty of InformationSamir Nurmohamed, Assistant Professor of Management, The Wharton School, University of Pennsylvania Derek Eder, Partner, DataMadeJessica Morgan, Ex-GooglerNoah Jones, Ph.D student at MIT and data scientist at Harvard FASEric Li, Harvard CollegeEmily Lin, FacebookLeah Caudell-Feagan, Social Impact, NianticRupa Dachere, CEO & Founder, Thrive-WiSEChelsea Hanson, American Medical AssociationKrysta Harrison, Sr. UX Researcher, ComcastMelanie Lei, Senior Product Manager, SurveyMonkeyAlex Quach, Undergraduate Student, MITVeronica Appleton, PhD-abd, Lecturer of Intercultural Communications, DePaul UniversitySarah Mathew, PhD Student in HCC, Georgia Institute of TechnologyMarine Carpuat, Associate Professor of Computer Science, University of MarylandMarianna Ghirardelli, Student at Davidson College, Incoming SDE AWSDurand de Gevigney Valentin, PhD Student in Machine LearningAhmed Azzi, student in commutation at UFR SLHS Besancon, FranceJustin Smith, Lafayette CollegePhillip Weber, Software DesignerStella Linda Holt, Computer Science Student, Winston Salem, North CarolinaEthan Brockmann, Full-Stack Data AnalystAndrea McClure, Systems Administrator, Enterprise SolutionsCatherine Blaikie, Senior AdvisorJered Higgins, SEO ManagerJason Hembree, Composer, HopecrashFilsan B. Abikar, Fellowship Program Manager, Code for AmericaManal Siddiqui, Director of Strategic Health PartnershipsLiam O'Donnell, Energy and Environmental EngineerVolha Litvinets, PhD Candidate, Sorbonne UniversityAlexandre Pelletier, PhD Student in Computational Biology Thierry Wattier, Software Engineer / Project ManagerMonique Anderson, MD PhD, Emory Neurology Resident PhysicianMimi Fox Melton, Acting CEO, General Manager | CODE2040Peter Harrington, Machine Learning Engineer, Lawrence Berkeley National LaboratoryAlisha Ukani, PhD Student at UC San DiegoEmma Humphries, Engineering Generalist, Developer Technologies, bandcamp.comTimothy J. Hazen, Senior Staff Machine Learning Researcher, TwitterMorgan Gallant, Undergraduate Student at the University of WaterlooHaileyesus Kassaye, Senior Software Developer, Nationwide InsuranceDr. Georges Ryschenkow, PhD in Solid State Physics, Laboratory SVI, Aubervilliers, FranceShimelis Assefa, Associate Professor of Information Science, University of Denver Edip Yuksel, J.D., Philosophy ProfessorVedant Nanda, PhD student University of Maryland, College Park and MPI-SWSAnteneh Addisu Yimer, Data Science (MSc) student, Computer Science, Loughborough University Heather Berlin, MIT, graduate student Alexandria Volkening, Northwestern UniversityLynette (Kvasny) Yarger, Associate Professor of Information Sciences and Technology, Penn StateEmma Sheridan, Pomona CollegeMarzia Polito, Applied Science ManagerAmy Petris, Data Science Master's Student, San Diego State UniversityDominique Burrell-Paige, PhD Candidate, The Chicago School of Professional PsychologyMicol Marchetti-Bowick, PhD, Senior Software EngineerDr. Shannon Stark Guss, Postdoctoral Research Fellow, University of DenverAdam Heriban, PhD, LIP6, Sorbonne UniversiteKaterina Papamihail, Data ScientistDr Olivia Guest, Postdoctoral Research Associate, Donders Centre for Cognitive Neuroimaging, Radboud University, NetherlandsDimitri Labat, Physicist, EntrepreneurAndy Nguyen, Senior Software Engineer, CourseraDakksh Nandrajog, Undergraduate student, McGill UniversityTristan Pinaudeau, Cybersecurity Student at INP-ENSEEIHTVicky Hallam, Senior Delivery ManagerHamza Mogni, Student at Abdelmalek Essaadi UniversityDr. Joran van Apeldoorn, PostDoc, Institute for Information Law & QuSoft, University of AmsterdamAnna Monreale, Associate Professor, University of PisaDr. Mahlet (Milly) Zimeta, Head of Public Policy, the Open Data Institute (ODI)Josh D'Addario, ODIShannon Macika, Smart Cities ConsultantAndrea Welty Peachey, Public school teacherLawrence B. Almeida, Software DeveloperJoseph Renner, INRIALuke Slater, Research Fellow, Institute of Cancer and Genomics, University of BirminghamVirgile Longo, physics student, I.U.T. De LilleSreenu Yedavalli, Devops EngineerChelsea Chandler, Ph.D., Learning Experience Designer, University of MichiganDr Jeni Tennison, Vice President and Chief Strategy Adviser, Open Data InstituteMike Beggs, PhD, VP Product Development, MagArray, Inc.Sarah Huntington, English Teacher, Webster House, Fairfield Ludlowe High SchoolMoira Courseaux, M.Sc student at Ecole Normale Superieure de Lyon, FranceSarah Jamgotch, social work policy practice student, Columbia UniversityMarianne Maloy, EducatorSomalee Datta, PhD, Director, Technology & Digital Solutions, Stanford UniversityKevin Duffy, Science Teacher, Guilderland Central School DistrictKaitlyn Ouverson, M.S., PhD student in Human Computer Interaction at Iowa State UniversityAlec Olschner, UX Designer, Varsity Tutors LLCErin Beasley, Graduate Student, Emory UniversitySamuel Morgan | QPA | RP Ops Analyst, The Standard, Standard Insurance CompanyClaire Williams, Doctoral Student and Student Researcher; Indiana UniversitySusan Cooper, Sr. Director of Data Analytics and Performance, Emory UniversityKaren Warkentin, Professor of Biology, Boston UniversityAdam Carriker, M.S. Student, Computer Science, Duke UniversityGracie Ermi, Research Software Engineer II, Machine Learning, Product Team | Vulcan Inc.Janelle Duda-Banwar, Professor and Senior Research Associate, Center for Public Safety Initiatives, Rochester Institute of TechnologyLisa Dempsey, CEO of Leadership LabsRyan Need, Ph.D., Assistant Professor, Department of Materials Science and Engineering, Herbert Wertheim College of Engineering, University of FloridaRachel Marchant, Snap Inc.lene D. Alexander, PhD, Center for Educational Innovation, University of MinnesotaAmy Epperson, Behavior TherapistMelissa Trunk, Behavior Therapist, Academy of Whole LearningLeigh Schlecht, University of Cambridge postgradJenna Tishler, Software Engineer, DuolingoVictoria Parker, Research Coordinator at Tulane UniversityEmily R. Pellerin, Design Writer Bart Stofberg, Strategisch adviseurBaptiste Lorenzi, System EngineerLauren Salig, graduate studentStephanie Burrows, MSW Candidate, Boston CollegeMadison London, Computer Science Student, University of RichmondAmanda Elend, Bright Red PixelsAna Korolkova, ResearcherYaro Kaminskiy, Software EngineerTanya Munroe, Centering Healthcare InstituteAmy Drayer, User Interface Developer, University of MinnesotaSarah ChanderMelissa Haskell, University of Michigan, Postdoc in Electrical EngineeringLeah Haas Sanborn  Senior Software Developer, Ginkgo BioworksShahid ButtarMegan Bulloch, PhDGrace Modisett, Video Software EngineerFabio Manganiello, Senior Software Engineer at EclecticIQErica Ellis, Inclusive Design LeadMaria Cipollone, Ph.D., Quantitative Research Manager, Experience Design at ComcastEmily Booth, Postdoctoral Research FellowCasey Yanos, researcherEve Novak, Mother of future professionalsMargaret Reichard, software engineerYvonne Chart, DPhil Candidate, University of OxfordAngela V, Court Administrator for Ethics in the JudiciaryThomas Butler, Bioinformatics ContractorBrooke Giger, Student at University of AkronAdrienne Wootters, Vice President of Academic AffairsJeffrey OtteRachel Gordon, Undergraduate Student, Loyola University ChicagoBonnie Richardson, LCSWJulia Kenneally, Brandeis UniversityIsabel Asha Penzlien, PhotographerMichelle Walkup, University of MinnesotaRachel Ryan, Clinical Research CoordinatorChantal Roche, Diversity & Inclusion ManagerEllyn S. Derman, M.S. EducatorEmily Brudner, PhD Candidate, Rutgers UniversityReiss WilliamsBrittany Emmanuel, Contact TracerChristena Lutz, PastorDenis FOLCHER, AnalystDr. Anna Slavina, Lecturer in Human Computer Interaction, Iowa State UniversityRebecca Hunsaker, Director of Research Administration, University of MarylandKatie Aman, Research AssistantAyla Martinez, PhD student, Biology, Northern Arizona UniversityMara Santilli, journalistBarbara Kim, Professor, California State University, Long BeachMervyn Naidoo, PhDMarley Bonacquist-Currin, PhD Student, Cornell UniversityAmanda Sugimoto, Ph.D. Assistant Professor, College of Education, Portland State UniversityIsabella Ramirez, Contractor at Facebook Reality LabsDinah Strange, Independent Freelance Graphic Designer Taylor BurkeyEmily Oldham, Undergraduate, Industrial EngineeringEmilia Valente, university undergradJerry Registre, Undergrad in Biology & Computer Science, Harvard UniversityAlice K. Zelman, PhD Candidate, University of ConnecticutJade Huang, Software EngineerCB Bowman-Ottomanelli, CEO, Workplace Equity & EqualityChristine DeZelar-Tiedman, Metadata Librarian, University of MinnesotaRachael Jean MoenchMadeleine Page, instructional designerJuan Gallardo, retired physicistBen Schwartz, organizerRuth LoganTing Zhang, Senior Engineer and Technical Lead, Brown Institute for Media Innovation Local News Lab, Columbia UniversityDiana MartinBenoit Knott, Undergraduate Student, University of Liege, BelgiumJulia Friend, Software Engineer, MicrosoftCaitlin Doyle, Technical Program Manager, Great Place to Work USIsabel Yannatos, student, University of PennsylvaniaAudrey Seo, PhD Student, Paul G. Allen School of Computer Science and Engineering, University of WashingtonKelsey McBeain, M.S. University of Hawai'i at ManoaKatie RobertsGreta Behnke, Research & Product MarketingFujita Michiko, student, teacherJonah Bossewitch, PhD Software ArchitectRachel Diaz, student, Millikin UniversityValentin Perez, student in computer science.Carly Penrose, Feed Nova ScotiaErendira Garcia, Graphics and Media Relations TechnicianBethany Suter, Ph.D candidate, UC Berkeleykim ellnerAtanu Basu, CEO & President, AyataCassandra Casias, Ph.D.Lori Mitchell, Founder and CEO,Black Women in Technology Larry Zitnick, Facebook AI ResearchMartin Takac, PhD, Associate Professor of cognitive science, Comenius University in BratislavaKaty Williams, PhD student at the University of ArizonaHugo Chateau-Laurent, PhD student, InriaCesar Debeunne  ISAE-SUPAERODavid Cairns, Software EngineerLaurent Rohrbasser, University of GenevaMarylou Lenhart, Software Engineer, Stitch FixGuilhem SAIZ, PhD student in condensed matter at ESPCITegan Wilson, PhD Student in Computer Science, Cornell UniversityRamon van Alteren, Director of EngineeringT.L. Cowan, Assistant Professor, Media Studies, University of TorontoAgathe G.Ivan Lopez-Espejo, Aalborg UniversityFestus Ojo, Harvard Computer ScienceKristen Nadaraja, MBA/MPA Candidate, Presidio Graduate SchoolNatasha Taraporevala, Interaction DesignerHannah Bast, Professor of Algorithms and Data Structures, University of Freiburg, GermanyTanguy VernetMarcos Sanchez-Elez Martin Assistant Professor of Computer EthicsDaniella Scalice, Educator and Research Investigator, Blue Marble Space Institute of ScienceAntonio B. Moniz, Professor of Universidade NOVA Lisbon (Portugal)Amie Ninh, Diversity, Equity & Inclusion PractitionerJane Yang, Head of Data & Policy, BasecampMaxime Peyrard, Post-doc at EPFLMaria Jesus Gonzalez-EspejoAly EvansMarkus Reisenleitner, Professor of Humanities, York University, TorontoAustyn Laures, IBM Cloud & Cognitive SoftwareHyunyoung Kim, LG ElectronicsAndreas Grun, ZDF, Head of TechnologyDr. Denise Dudek, Senior Software ConsultantNicolas Graves, French student, Ecole Nationale des Ponts et ChausseesScott Myers-Lipton, Ph.D., Professor, Sociology Department, San Jose State UniversityRaul Maldonado, Data AnalystDr. Debra Chittur, Learning Designer, Dixie State UniversitySamuel Bonnet, PhD/PharmD, Research Scientist, University of Angers  PRISMDanny SallisAaron Joshua Maxwell, PhD, Facilitator at Durham College and Data Scientist at Paladin AI, Inc.Lingyu Wang, Doctoral Student, University of North Carolina at Chapel HillAudrey Kriva, MBA Candidate, Presidio Graduate SchoolAkash Gupta, Graduate Student, University of California, RiversideAyo Oluwole, Founder, 11xFrancisco R. Ortega, Ph.D., Assistant Professor of Computer Science, 3D User Interfaces, Colorado State University.Rob Arbon, School of Chemistry, University of Bristol, UKLayla Bouzoubaa, Lead Research Analyst, University of Miami Miller School of MedicineVarun Tekur, Harvard CS StudentOscar Ortega, PhD studentDumas YanisNarcisa C., Software EngineerAmberNechole Hart, Product DesignerAditya Kausik, Think SurgicalSylvain Breton, PhD student, Universite de Paris/CEA de SaclayGerard Rozsavolgyi, Maths & CS  Orleans UniversityJeremy Henault, Senior Software Engineer, AppleDr. Anderson D. Prewitt, Data Science Research Consultant at Dr. A.D. Prewitt & Associates, Co-Author of STEM Navigators: Pathways to Achievement in Science Technology Engineering & MathProfessor Michael Winikoff, Victoria University of WellingtonAntoine Barras, Media & Interaction Design StudentSusan Ingram, Professor, Dept of Humanities, York UniversityOmiros Pantazis, PhD student, University College LondonAntoine Pichon, EngineerNatalie Byfield, Professor, Department of Sociology & Anthropology, St. John's University; Director, Race & Big-Data Driven Policing Lab.Serge Mwembo, network engineer and Google userVincent Beaudenon, Ph.D.Samuel Buchet, PhD student, Ecole Centrale de Nantes/LS2NDaniel Seita, PhD Student, UC BerkeleyTeresa Padgett, Graduate Student, George Mason UniversityLuci Baker, University of MinnesotaClement Bonnet, graduate student at Ecole Normale Superieure.Kelsey Gross, Product ManagerAndrea Nus, Marriage & Family TherapistPierre Noel, Software engineerCarla B, Creative ContourEllen Yates, Business AnalystJean Stillemans, Professor, Faculty of Architecture, University of Louvain-la-NeuveBetsy Book, User Experience Researcher, LinkedInTheo BoutronAnja Dietmann, EditorProfessor of Technology, Ensemble Scolaire St Francois de Sales  Middle SchoolValerie J. Meier, University of GenevaMiriam EdwinMathurshan Vimalesvaran, Technical LeadJanelle Liceaga, DEI Employment Brand StrategistJami FowlieTricia HollandIssiakhem Massinissa Said, AI StudentMarc Belderbos, Professor UCL &KUL BelgiumGitta YeboahTony Samuel, CEO/Founder Talented BooksSreecharan Gullapalli, Independent researcher, Data scienceSergio Celaschi, Researcher CTI Renato Archer, Stanford University AlumniPaula A.Alexander Katler, Emerging Technologies and Real Estate InvestorEric ForgetEsther Bier, Graduate student at University of Illinois, ChicagoAbigail HoughtonAviv Ovadya, The Thoughtful Technology ProjectHomer Aalfs, Seattle Children'sBrianna Mills, PhD Candidate, Department of Astronomy University of VirginiaKyle Thayer, Assistant Teaching Professor, The Information School, University of Washington, SeattleDr Sue Chadwick, Research Fellow ODIChristiana Moore, Brilliance Consulting Services LLC, Principal and OwnerLauren Scopaz DaunaisDarith KlibanowMaxime Lafond, Ph.D. Postdoctoral Fellow.Dr Ariel E Marcy | Postdoctoral Research Associate at the University of Nebraska-LincolnLevi Cai, PhD student, MITKerry Lane, PhD Candidate, University of California, Santa BarbaraIvan Villegas // California State University, SacramentoLisa M Austin, Chair in Law and Technology, University of Toronto Faculty of LawKelly Hummell  Customer Support RepresentativeChristina Maerlender, Program Assistant- BridgeUP: STEM, American Museum of Natural HistoryNaomi Rubalcava Levinthal, studentAnneliese Paulson, M.Ed-Social Studies, University of Minnesota Twin CitiesJulian M. KleberHana Kurihara, UX DesignerAnthony ROBILLARDLaura CatherwoodYvette Pasqua, CTO, EXOSJonathan VARELA, Software EngineerErica J. Baker, Director of EngineeringSara Cannon, M.Sc., PhD Candidate, University of British ColumbiaMike Salvato, PhD Candidate, Stanford UniversityJohanne Antoine, Undergraduate Student, Boston University '22Ebony Utley, Professor, California State University Long BeachMaria Campbell, EducatorChris Jenkins, Marketer & Data Scientist at Rank.lySebastian Lasse, journalistMegan Comfort, University of California, San FranciscoKatie CosciaShelby N LafrinereJulia Larrabee, Capital One Experience DesignTewodros GebreselassieMeghan Kosowski, Graduate Student of Bioethics, University of LouisvilleGeo Atherton, Senior Designer at MicrosoftAndrea ZhangCyril Guidoux, Ph.D., Senior EngineerLamesha Porter, Accountant/InvestorRobert Sim, Microsoft ResearchBrandie Nonnecke (@BNonnecke), PhD, Director, CITRIS Policy Lab, UC BerkeleySpyridon Thermos, Postdoctoral Researcher, University of EdinburghGuillaume Le BailGuillaume Mouillet, Software EngineerAlana Kapust, LCSWNushin Yazdani, Artist & Designer, dgtl fmnsm & Design Justice NetworkJulianna Devillers, MSc Engineering, ISAE-Supaero, FranceBereket Mamo, PhD student, University of Texas at San AntonioKevin Chavez, Software Engineer, FacebookShanda Hunt, MPH, Health Sciences Library, University of MinnesotaLouis RICHARDLindsay Shea, Graduate School Student, Hazelden Betty Ford Graduate School of Addiction StudiesGrace Lotti Sydney SchillerJanice Burch, Founder, Before DiversityAmanda Buddemeyer, PhD student, Department of Computer Science, University of PittsburghJelani Nelson, Professor, Department of EECS, UC BerkeleyBrooke Mason, Engineering PhD Student, Civil Engineering, University of MichiganKerry Francesca Nisco, Ed.D., Assistant Vice President for Marketing and Brand Management, SEATTLE UNIVERSITYTsedale M. Melaku, Ph.D. Postdoctoral Research Fellow, The Graduate Center, City University of New YorkJulia Pitts, software engineerKarli RobertsonMichele Slawson, financial systems analystIsabel Nicole Tuason, Research Assistant, UC IrvineBrian Forney, software engineer, ClumioAlexandre FRASSETO, Buyer and EntreprenorVictoria Okuneye, Ph.D., MD Candidate at Uchicago Pritzker School of MedicineSarah Royal, Enough WickerDaniel RoussyLindsay BrissonJustin Sherman, Co-Founder and Senior Fellow, Ethical Tech, Duke UniversityRobin L Zebrowski, Professor of Cognitive Science, Beloit CollegeSimon Woldemichael, Solutions Architect, AWSRachel Dortin, PhD, University of Central ArkansasHugo Belhomme, PhD Student, Mines Saint-EtiennesHal Cropp, Producing Artistic Director, Commonweal Theatre CompanyFernando Perez, Associate Professor, Dept. of Statistics, UC BerkeleyLauren deLisa Coleman, Cultural Analyst, Entrepreneur, ActivistRoy PankaAlexandre Netchaieff, electronic engineer at HelicoideeVincent James GalganoHahn Lheem, Harvard CollegeCatherine Cannizzo, Research Assistant Professor, Mathematics, Simons Center for Geometry and PhysicsLaura Sayendr rani makwanaTheo MerlevedeKevin D. WilliamsMatthieu BrocardUlrich EbnerMia SloanErica FeidelseitEsteban Quintard, Independent Social EntrepreneurQuint GregoryCoralie CornouVeronica MakYani Ioannou, Sessional Lecturer, University of TorontoSaleh HassenYani Ioannou, Sessional Lecturer, University of TorontoDr. Nicole Wyatt, Associate Professor and Head, Department of Philosophy, University of CalgaryAnna Marburger, Training Program Specialist, Better.comAofei Liu, PhD Student, Stanford UniversityTaylor Jones IV, PhD Candidate, Stanford UniversityFrancisco R. Ortega, Ph.D., Assistant Professor of Computer Science, 3D User Interfaces, Colorado State UniversityAdil YusufAlexander Su, Ph.D. candidate, Stanford UniversityBaptiste Boisseau, Arts et Metiers engineering school student Erica LiuAlexis Rockley; positive psychology coach & authorJames Conant, Harvard College '21Hugo Chouraqui, CNRS Ph.D. studentElias Ayana, PhD, Engineering ManagerSamantha Witte, PhD candidate, Oxford UniversityValerian Hall-Chen, University of OxfordThibaud Veron, Engineer & Concerned civilianKatie RichardsonWill Peck, Software Engineer, Xactaware LimitedSascha Pappinghaus, IT Consultant and TU Munich AlumnusMichelle Yeo, PhD Candidate, IST AustriaTeresa Heiss, Insititute of Science and Technology AustriaBrittney Ellis, Portland State UniversityLydia Bryan-Smith, University of HullEric Wise, Senior Video Editor, Advanced Systems GroupTarryn Balsdon, Postdoctoral researcher, Department of cognitive studies, Ecole normale superieure, Paris, FranceAriana Johnson, MPH, PhD Candidate University of MiamiEmmett Smith, PhD- Biologist, Earlham College
",67
https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b?source=tag_archive---------6-----------------------,Visualising high-dimensional datasets using PCA and t-SNE in Python,"Update: April 29, 2019. Updated some of the code to not use ggplot but instead use seaborn and matplotlib. I also added an example for a...",Luuk Derksen,10,"Update: April 29, 2019. Updated some of the code to not use ggplot but instead use seaborn and matplotlib. I also added an example for a 3d-plot. I also changed the syntax to work with Python3.
The first step around any data related challenge is to start by exploring the data itself. This could be by looking at, for example, the distributions of certain variables or looking at potential correlations between variables.
The problem nowadays is that most datasets have a large number of variables. In other words, they have a high number of dimensions along which the data is distributed. Visually exploring the data can then become challenging and most of the time even practically impossible to do manually. However, such visual exploration is incredibly important in any data-related problem. Therefore it is key to understand how to visualise high-dimensional datasets. This can be achieved using techniques known as dimensionality reduction. This post will focus on two techniques that will allow us to do this: PCA and t-SNE.
More about that later. Lets first get some (high-dimensional) data to work with.
We will use the MNIST-dataset in this write-up. There is no need to download the dataset manually as we can grab it through using Scikit Learn.
First let's get all libraries in place.
and let's then start by loading in the data
We are going to convert the matrix and vector to a Pandas DataFrame. This is very similar to the DataFrames used in R and will make it easier for us to plot it later on.
Because we dont want to be using 70,000 digits in some calculations we'll take a random subset of the digits. The randomisation is important as the dataset is sorted by its label (i.e., the first seven thousand or so are zeros, etc.). To ensure randomisation we'll create a random permutation of the number 0 to 69,999 which allows us later to select the first five or ten thousand for our calculations and visualisations.
We now have our dataframe and our randomisation vector. Lets first check what these numbers actually look like. To do this we'll generate 30 plots of randomly selected images.
Now we can start thinking about how we can actually distinguish the zeros from the ones and two's and so on. If you were, for example, a post office such an algorithm could help you read and sort the handwritten envelopes using a machine instead of having humans do that. Obviously nowadays we have very advanced methods to do this, but this dataset still provides a very good testing ground for seeing how specific methods for dimensionality reduction work and how well they work.
The images are all essentially 28-by-28 pixel images and therefore have a total of 784 'dimensions', each holding the value of one specific pixel.
What we can do is reduce the number of dimensions drastically whilst trying to retain as much of the 'variation' in the information as possible. This is where we get to dimensionality reduction. Lets first take a look at something known as Principal Component Analysis.
PCA is a technique for reducing the number of dimensions in a dataset whilst retaining most information. It is using the correlation between some dimensions and tries to provide a minimum number of variables that keeps the maximum amount of variation or information about how the original data is distributed. It does not do this using guesswork but using hard mathematics and it uses something known as the eigenvalues and eigenvectors of the data-matrix. These eigenvectors of the covariance matrix have the property that they point along the major directions of variation in the data. These are the directions of maximum variation in a dataset.
I am not going to get into the actual derivation and calculation of the principal components  if you want to get into the mathematics see this great page  instead we'll use the Scikit-Learn implementation of PCA.
Since we as humans like our two- and three-dimensional plots lets start with that and generate, from the original 784 dimensions, the first three principal components. And we'll also see how much of the variation in the total dataset they actually account for.
Now, given that the first two components account for about 25% of the variation in the entire dataset lets see if that is enough to visually set the different digits apart. What we can do is create a scatterplot of the first and second principal component and color each of the different types of digits with a different color. If we are lucky the same type of digits will be positioned (i.e., clustered) together in groups, which would mean that the first two principal components actually tell us a great deal about the specific types of digits.
From the graph we can see the two components definitely hold some information, especially for specific digits, but clearly not enough to set all of them apart. Luckily there is another technique that we can use to reduce the number of dimensions that may prove more helpful. In the next few paragraphs we are going to take a look at that technique and explore if it gives us a better way of reducing the dimensions for visualisation. The method we will be exploring is known as t-SNE (t-Distributed Stochastic Neighbouring Entities).
For a 3d-version of the same plot
t-Distributed Stochastic Neighbor Embedding (t-SNE) is another technique for dimensionality reduction and is particularly well suited for the visualization of high-dimensional datasets. Contrary to PCA it is not a mathematical technique but a probablistic one. The original paper describes the working of t-SNE as:
""t-Distributed stochastic neighbor embedding (t-SNE) minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding"".
Essentially what this means is that it looks at the original data that is entered into the algorithm and looks at how to best represent this data using less dimensions by matching both distributions. The way it does this is computationally quite heavy and therefore there are some (serious) limitations to the use of this technique. For example one of the recommendations is that, in case of very high dimensional data, you may need to apply another dimensionality reduction technique before using t-SNE:
The other key drawback is that it:
""Since t-SNE scales quadratically in the number of objects N, its applicability is limited to data sets with only a few thousand input objects; beyond that, learning becomes too slow to be practical (and the memory requirements become too large)"".
We will use the Scikit-Learn Implementation of the algorithm in the remainder of this writeup.
Contrary to the recommendation above we will first try to run the algorithm on the actual dimensions of the data (784) and see how it does. To make sure we don't burden our machine in terms of memory and power/time we will only use the first 10,000 samples to run the algorithm on. To compare later on I'll also run the PCA again on the subset.
x
Now that we have the two resulting dimensions we can again visualise them by creating a scatter plot of the two dimensions and coloring each sample by its respective label.
This is already a significant improvement over the PCA visualisation we used earlier. We can see that the digits are very clearly clustered in their own sub groups. If we would now use a clustering algorithm to pick out the seperate clusters we could probably quite accurately assign new points to a label. Just to compare PCA & T-SNE:
We'll now take the recommendations to heart and actually reduce the number of dimensions before feeding the data into the t-SNE algorithm. For this we'll use PCA again. We will first create a new dataset containing the fifty dimensions generated by the PCA reduction algorithm. We can then use this dataset to perform the t-SNE on
Amazingly, the first 50 components roughly hold around 85% of the total variation in the data.
Now lets try and feed this data into the t-SNE algorithm. This time we'll use 10,000 samples out of the 70,000 to make sure the algorithm does not take up too much memory and CPU. Since the code used for this is very similar to the previous t-SNE code I have moved it to the Appendix: Code section at the bottom of this post. The plot it produced is the following one:
From this plot we can clearly see how all the samples are nicely spaced apart and grouped together with their respective digits. This could be an amazing starting point to then use a clustering algorithm and try to identify the clusters or to actually use these two dimensions as input to another algorithm (e.g., something like a Neural Network).
So we have explored using various dimensionality reduction techniques to visualise high-dimensional data using a two-dimensional scatter plot. We have not gone into the actual mathematics involved but instead relied on the Scikit-Learn implementations of all algorithms.
Before closing off with the appendix...
Together with some likeminded friends we are sending out weekly newsletters with some links and notes that we want to share amongst ourselves (why not allow others to read them as well?).
Code: t-SNE on PCA-reduced data
And for the visualisation
",68
https://towardsdatascience.com/how-to-study-for-the-google-data-analytics-professional-certificate-25e5e13b1f04?source=tag_archive---------3-----------------------,How to Study for the Google Data Analytics Professional Certificate,An overview of how to succeed in this world-class data analytics program.,Madison Hunter,9,"In March of 2021, Google made a bold claim: with no relevant experience required, you will get a job in data analytics, with help from Google, by completing their Data Analytics Professional Certificate.
Just like that, after completion, you are thrust into a world of $67,900 starting salaries and hundreds of thousands of job opportunities.
This widely anticipated course will become fully available starting in April, which makes this the perfect time to begin preparing for this course.
There is no better time than right now to challenge yourself and to develop new skills. Plus, if this pandemic has taught us anything, it's that upskilling is never a waste of time and can help guarantee you better employment options. With 337,400 job openings in data analytics in the United States alone, this reasonably-priced, self-paced, and easily-accessible course is the perfect way to get your foot in the door without having to splurge on a bootcamp or Masters degree.
The Google Data Analytics Professional Certificate is professional training designed by Google that prepares you to become a junior data analyst, a database administrator, and many other positions.
This certificate aims to fill the gap in data analytics positions seen in all industries.
Google has created 8 courses that can be completed in less than six months with less than 10 hours put in every week.
The courses are completed through the online learning platform, Coursera.
Over the course of the Google Data Analytics Professional Certificate, you will learn the basics and fundamental skills to enter an entry-level data analyst position. This begins by learning about the practices and processes that people in these positions use in their daily work. Following that, you will learn how to use the tools necessary for data cleaning, organizing, analyzing, and visualizing, including:
Google also supports you through the whole job preparation process with a resume-building tool, mock interviews, and career networking support to help you get a job after program completion.
Google has developed 8 courses to help you become proficient in data analysis. The first five courses are currently available, with the last three projected to be released in April.
This introductory course starts at the very beginning with a gentle introduction into what the data analysis job position entails, what a data analyst does daily, the skills a data analyst needs to be successful, and a description of the first basic terms and concepts that you will need to complete the course.
The second course in the program focuses on helping you learn how to ask the right questions to make a data-driven decision. This course covers the basics of effective questioning, how to apply those questioning techniques to real-world business situations, and the importance of using structured thinking and clear communication with stakeholders to achieve business objectives.
The third course covers everything you need to know about data preparation and extraction. This course covers the use of spreadsheets and SQL to extract data, how to organize and protect data, and the basics of data ethics and privacy.
This course covers data cleaning using spreadsheets and SQL, and the development of data cleaning reports.
The fifth course focuses entirely on the data analysis process using spreadsheets and SQL. In this course, you will learn how to use formulas, functions, and SQL queries to conduct an analysis.
The sixth course in this program covers the storytelling aspect of data analysis. This course focuses on helping you understand how to bring your data to life. You will learn how to use Tableau to create dashboards and visualizations that you will then learn how to present to an audience.
This course covers one of the programming languages often used in data analysis. You will learn how to use R and RStudio to clean, organize, analyze, visualize, and report data analyses.
The final course in the program begins with teaching you the benefits of case studies and portfolios for your job search and goes over job-hunting and interview skills. In the end, you are given the option to complete a capstone project that can be used for your professional portfolio.
It's important to make sure you check a few boxes before jumping head first into this program.
This course is for people who: are complete beginners with data analysis and have no prerequisite knowledge; have 10 hours a week to dedicate to studying; want to learn the basics of data analysis.
This course is not for people who: are already proficient in data analysis (this course is at a very beginner level, and therefore is not for those who already have experience or have taken other data analytics MOOCs); are looking to work in Python (though Python is easily learned after the fact); don't have time to dedicate to the course (data analysis is best learned using a regular schedule).
The Google Data Analytics Professional Certificate is accessed through Coursera, which has a $39 (USD) monthly subscription fee. Therefore, if you take six months to complete the course, the cost will be approximately $234 (USD).
Because of this monthly subscription fee, it makes sense to try to complete the course as quickly as possible to reduce the cost. However, it's important to bear in mind that this cost is much less than if you were to attend a bootcamp (which can be anywhere between $3,000 to $15,000) or if you were to get a Masters's in data science.
Coursera has financial aid available for those who need it.
With MOOCs having an average completion rate of less than 10%, it's important to set yourself up for success early on by figuring out how you plan to study for this certificate.
Having a study plan, especially if self-study isn't really your thing, will help you get through this course and retain everything that you learned.
As someone who is self-studying their entire university degree and currently has a 3.8 GPA, I feel particularly qualified to write about how to set yourself up for success when it comes to self-studying.
Google suggests that you can complete the course in 6 months if you dedicate 10 hours of your week to studying and going through the course material. For your average working professional looking to make a career change, this timeline is reasonable. With that being said, this course can be done in a shorter amount of time (2-3 months) if you have no other commitments.
With the program clocking in at 180 hours of instruction and hundreds of practice-based assessments, you can't just sit there and absorb the information as you watch the videos. To get the most out of the course and the small amount of time you have every day to complete it, you need to be actively learning through the whole process.
The first step in completing this course is to set up a study schedule for yourself that you actually follow.
It's easy to tell yourself that you will study for ten hours a day until you actually get to that day and realize how grueling ten hours of studying really is. Within a week, you'll be burnt out and one of the 90% of people who don't finish MOOCs.
Instead, pace yourself by creating a balanced schedule.
Ten hours per week of study can be broken up into 4 days with 2.5 hours of study each day. When you realize that the average person spends three and a half hours on their phone every day, it suddenly becomes more feasible to use your time more constructively.
From there, you plan out what parts of the course you want to get accomplished in those 2.5 hours.
For example:
Monday: Complete Week 1 lectures, readings, and quizzes.
Wednesday: Complete Week 1 practice exercises.
Friday: Complete Week 2 lectures, readings, and quizzes.
Sunday: Complete Week 2 practice exercises.
By printing out this schedule and keeping it in a prominent place, you're always reminded of your commitment to completing this program.
Making a schedule is all well and good, but only if you stick to it and complete everything you need to for the week.
Whether you keep yourself accountable by live-streaming or filming your study sessions, joining a study group, blogging about your studying, or having a trusted friend remind you of your need to study, the important thing to remember is to keep your accountability insurance (the thing that keeps you motivated to study) consistent.
Tech subjects are best learned consistently and regularly, so you must establish a disciplined study habit early on.
Studying tech is completely different than studying anything else. Where normally you would need to have a deep understanding of topics to be able to succeed in a given field, in tech, you simply need a broad understanding of topics (and the willingness to admit when you don't know something).
Due to the open nature of tech education, it's incredibly easy to Google anything you don't know. Any lack of depth of knowledge can be counteracted by the ability to learn on the fly by searching for anything you don't understand.
Therefore, your time is better spent on getting a broad understanding of all of the topics in this program and learning how to apply them effectively.
As mentioned earlier, studying tech is completely different than anything you've probably studied before. As such, the type of notes you take will also be different.
To go along with our idea of having a broad understanding of concepts, it's important to take notes that help with this broad understanding. The key is to not take extensive notes. Instead, focus on grasping general concepts and writing notes that help you apply them.
Writing notes has been a proven learning tool to help students retain information, which is why it's highly recommended  especially when self-studying. Not only does it help you learn the important stuff, but it also helps you remain engaged and actively learning.
Google has made the final capstone project for their program optional, which means that you don't have to complete it to pass the course.
While going through 180 hours of learning material and completing all of the learning exercises can make you feel like you know everything there is to know about data analysis, it's important to remember that the best way to learn is by doing.
With that, comes completing the capstone project.
While it's easy to complete all of the different steps of data analysis on their own, it's a whole new thing to put them all together and complete a full analysis.
Most MOOCs don't offer a capstone project due to the demands on the instructors and the extra effort that has to be put in by the course provider. In short, it would be silly to pass up the chance to work on a capstone project with Google employees available to you to answer any questions you come across.
By putting all the pieces together, you allow yourself to learn by doing. This action will stay with you far more than if you were to just complete each section of analysis and forego the capstone project.
Only such a claim of guaranteed job success could be touted by a company such as Google, though it remains unfounded. Time will tell if this program is effective in getting people with no previous experience into data analyst roles.
Regardless, this well-rounded program offers an amazing opportunity for a world-class education at a fraction of the cost it would normally be for a comparable bootcamp or Master's degree.
By going into this program with your eyes wide open and a solid plan in place, there's no reason why you can't complete this program and become one of those lucky individuals who make it into this competitive industry.
",69
https://towardsdatascience.com/is-data-science-still-a-rising-career-in-2021-722281f7074c?source=tag_archive---------3-----------------------,Is Data Science Still a Rising Career in 2021,"Examining the Demand, Supply, and Growth of the Data Science industry to see if it's  still a rising career in 2021",Chris Zaire,5,"2020 was the first year since 2016, Data Scientist was not the number one job in America, according to Glassdoor's annual ranking. That title would belong to Front End Engineer, followed by Java Developer, followed by Data Scientist.
Now being 3rd isn't bad at all, in fact you're still on the podium. But is the shiny title of Data Scientist finally falling off, or did it just become another victim of 2020?
To determine this we'll look at 3 economic factors that have an influence on job ranking:
Examining these factors will give us the answer if data science is still a rising career in 2021.
If you go back years ago, the shiny titles in the early 2010s were programmers and web designers. The salaries for the two were great back then, but have plateaued since as supply caught up with demand.
That is not the case for Data Scientists yet as demand is still quite high.
There is a reason Data Scientist is in the top 3 for job rankings, and it's because their demand is absolutely ridiculous and in no sight of slowing down.
Data-driven decision making. That is the simple answer to this question. To be a successful company in the 21st century you have to use data to your advantage.
Before many were doing this by using excel to analyze data, but now anyone can have access to and use data-crunching tools like:
The largest companies in the entire world are data science fueled enterprises. Take a look at Google, Amazon, and Facebook. Each use data science to create algorithms that improve customer satisfaction and maximize profits.
In the end, the main reason demand is still high is because if your competitors are relying on data-driven decision making and you aren't, they will surpass you and steal your market share.
Therefore companies have to adapt and employ data science tools and techniques or they will simply be forced out of business.
Meaning... Data Scientists are a must in 2021.
The supply of Data Scientists is low, and it's because the field of data science is still relatively new even in 2021.
You see 20 years ago it was impossible to learn data science because of slow internet connection, and low computational primitive programming languages. As the years went on though, the power of computers started to grow exponentially and data science became possible.
This exponential growth and interest in the field were impossible to predict, and traditional education was not ready to meet the needs of those who wanted to learn this growing field.
Very few programs were created to educate aspiring Data Scientists. This shows as research suggests those who get into the field usually transition from other fields such as business, psychology, and life sciences.
Most who transitioned learned their skills through self-preparation by reading books, and taking online courses...
Not through the traditional education system.
Those who get into data science have the advantage of starting a career path in which there are more open jobs than qualified candidates to fill them.
In fact, data science jobs remain open 5 days longer than the average for all other jobs. This points to the fact that there is less competition which results in the recruiters needing extra time to find the correct candidates.
These correct candidates are in luck as most will only need a bachelor's degree to get hired. The low supply has resulted in 61% of data scientist positions be available to those with a bachelor's degree, while only 39% will require a master's degree or a PhD.
If you've been following this article along, then you probably have a good assumption on the trajectory of the growth of data science jobs.
Per LinkedIn, there has been a 650% increase in data science jobs since 2012. Glassdoor gives evidence to this claim as they had about 1700 job postings with data science being the primary role in 2016. That number rose to 4500 in 2018, and sort of flattened out in 2020 at around 6500.
COVID-19 was the big story in 2020, and presumably, the reason for this flattening out. Overall though tech jobs have proven to be resilient during the pandemic, which is now in its tenth month.
Demand for Data Scientists is still high while supply is low. According to IBM, this tendency will continue to be strong for years to come. Another credible source that agrees with this statement is the U.S. Bureau of Labor Statistics.
The U.S. Bureau of Labor Statistics sees strong growth in the data science field and predicts the number of jobs will increase by about 28% through 2026. To give that 28% a number, that is roughly 11.5 million new jobs in the field.
In the long term, it would probably be unwise to bet against data science as a career move, especially when you widen the field to include related positions like research engineers and machine learning engineers.
So is data science still a rising career in 2021? The answer is a resounding YES! Demand across the world for Data Scientists are in no way of slowing down, and the lack of competition for these jobs makes data science a very lucrative option for a career path.
Hey! If you want to read unlimited articles on Medium  and support me at the same time  you can subscribe through the link below for just $5.
christopherzita.medium.com
[1] A. Woodie, Why Data Science Is Still a Top Job (2020), https://www.datanami.com/2020/11/16/why-data-science-is-still-a-top-job/
[2] California University of Pennsylvania, Data Science Careers, Jobs, Salaries | Data Scientists (2019), https://www.calu.edu/academics/undergraduate/bachelors/data-science/jobs-career-salaries.aspx#:~:text=Data%20science% 20was%20named%20the,engineer%2C%20and%20business%20analyst.""
[3] 365 Data Science, Is Data Science Really a Rising Career (2020), https://www.youtube.com/watch?v=PXLVLS1vJHY
",70
https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------5-----------------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,"For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms...",Arthur Juliani,6,"For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1-3). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).
Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.
For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn't choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.
In it's simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.
We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:
Eq 1. Q(s,a) = r + (max(Q(s',a'))
This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted () future reward expected according to our own table for the next state (s') we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:
(Thanks to Praneet D for finding the optimal hyperparameters for this approach)
Now, you may be thinking: tables are great, but they don't really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don't work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.
In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the ""target"" value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.
Eq2. Loss = (Q-target - Q)2
Below is the Tensorflow walkthrough of implementing our simple Q-Network:
While the network learns to solve the FrozenLake problem, it turns out it doesn't do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!
If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!
If you'd like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on Twitter @awjliani.
More from my Simple Reinforcement Learning with Tensorflow series:
Exploring frontier technology through the lens of...
15.1K 
111
",71
https://towardsdatascience.com/how-to-grow-from-non-coder-to-data-scientist-in-6-months-197f465dfa9f?source=tag_archive---------7-----------------------,How To Grow From Non-Coder to Data Scientist in 6 Months,A complete guide with all required resources,Sharan Kumar Ravindran,10,"Many people from a diverse range of backgrounds reach me on LinkedIn. Asking for tips and suggestions for transitioning their career into data science. I used to think, A ton of resources are available online to learn data science for free. So, Why are they asking me? After speaking to some I clearly understood,
Also going through some of the job descriptions for entry-level positions in data science. It was clear the expectations are being set very high. This is further causing a lot of stress and anxiety especially among people with non-technical backgrounds. It is also the reason why many are unable to keep up the level of motivation throughout their learning.
In this article, I am going to show you a clear plan. To help you not only learn data science but also to get a job.
The most common mistake made by people learning data science is starting off with a complex topic. It is natural to get attracted to an interesting topic. The problem is if you hit a roadblock too soon in your learning journey. It will be enough to drop your motivation and make you quit. The probability of running into issues is very high when you start off with a complex topic.
My number one advice to people starting with learning data science is to start small. Below are some basic skillsets that would be used on daily basis in a data science job.
OK, But how much expertise is required and Where to learn? For each of these topics, I will show you reference resources from where you can learn.
To become a data scientist you need to write code. From reading the data, exploring the dataset, creating a visualization, performing feature engineering, and building a model everything needs coding. There are many AutoML and No-Code ML tools coming up to automate repetitive tasks but to grow as a data scientist you need to learn to code.
If you are new to coding then web-based platforms are the best place to start learning to code. The interactive web-based environment makes your learning very easy. You would be learning more about the topic by doing it yourself. Also, there is no stress of setting up the environment and installing the tools.
One fantastic web-based platform to learn basic Python programming is Codecademy. It is a wonderful platform to learn coding in an interactive way. It is easy to continue from where you left last time and also helps to track your progress. Thus it will be good to do some learning while you are traveling or when you got some spare time. If you have decided to go with R then you could still learn here from Codecademy.
Once you complete the online tutorial for either python or R then you can try programming from your system. Install the required tools and try it out. It will be sufficient if you are comfortable with the below to start with,
The other must-have skill set for a data scientist is SQL. When you move into a corporate environment all the data would be generally stored in DBs. Unlike the learning environment, the datasets are not readily provided to the data scientists. It is the primary role of the data science team to understand and extract the required dataset. To perform that you need to have a good knowledge of SQL.
Again, the course here in codecademy provides an interactive platform to learn basic SQL. This will help you with the basics functionalities and syntax of SQL. There are online platforms like learnsql that provide practice scenarios replicating real-life challenges.
Like, python/R once you are comfortable with SQL you can install the open-source database management system MySQL. It comes with an in-built database. You can refer to my learning SQL playlist below. It would help you with the common functionalities of SQL that are used in the day-to-day job of a data scientist.
The basic statistics knowledge helps in getting a better understanding of the data. As you progress and level up your skills in data science. The basic statistics knowledge will help in coming up with better solutions.
The Statistics and probability course in Khan Academy will cover all the key statistics topics required for a data scientist. If you find this course overwhelming or you are keen to get started with data science with just enough statistics then check out the below article,
towardsdatascience.com
It covers the basic must-know statistics concepts that are required for a data scientist. If you are more inclined towards tutorial videos then check this playlist.
It is enough to start small while learning data science. But it is more important to ensure consistency, especially in the early days. If you learn for a week or two and take a long break then it doesn't help with the learning. You need to dedicate a fixed amount of time every week to your learning.
Once you are comfortable with the basic skillsets captured above it's time to scale up. It is time to start working on datasets and learn about data visualization, data exploration, and model building.
Visualization is a key skill set for a data scientist. It helps in clearly communicating the insights and to bring out the patterns present in the dataset. It is easy for people to pick patterns from visual data as compared to tabular or just text data. Visualization plays important role in both data analysis and communication.
To learn more about building some amazing visualization from scratch, check out the below article.
towardsdatascience.com
There are many data science-related courses in Coursera. It can be very confusing to choose the one best suitable for your skillset. Below are some courses you can choose based on your current level of expertise.
If you are a beginner and haven't performed any data analysis then start with this short 2 hours course from Coursera.
www.coursera.org
If you have a basic idea about data analysis or if you have completed the above course then you can sign-up for the below course. This course has some amazing feedback and many have acknowledged getting tangible benefits due to this course.
www.coursera.org
If you are keen to learn more then you could learn about feature engineering from kaggle using the below course.
www.kaggle.com
Now it's time to level up your skills. Below are some courses from Coursera that could help in building a strong foundation. It helps to understand and learn about activities performed by the data science team across the entire project pipeline. You will learn about understanding the problem, running analysis to extract inferences and insights, choosing the right model for the business problem. These courses also have projects part of the curriculum that would provide first-hand project experience.
Most of these projects are long-term, it requires at least a few weeks of effort. But these are critical and it helps in making you ready for data science interviews and challenges in the job.
Here are some details about the below courses to help you choose the one best suitable for you. Also, don't hesitate to skim through the topics already covered.
www.coursera.org
www.coursera.org
www.coursera.org
The most important part of your learning journey is to have goals and religiously tracking your goals. It is very easy to get distracted while learning data science. The best way to avoid any distractions or divergence from the original plan is to write down the goals. Most of the resources mentioned in this article have a progress tracker. Set some goals for each week and stick to them. To increase accountability share your learning progress on a blog or LinkedIn posts.
Also, sharing your achievements and progress helps in creating positive energy and motivation to pursue further. Learning data science is a long journey, you need to have your motivations high enough to sustain and complete it.
One important reason for people to quit learning data science is a lack of confidence. Having a plan and tracking the progress helps in building confidence.
Transitioning your way into data science is not going to be a smooth one. Expect some technical issues and other possible obstacles that could shake your confidence. Just remember one thing even experienced data scientists run into issues and it is part of the career. The data science space is a fast-evolving field so it is not uncommon to run into issues. But the positive thing is there is a very good support system in place. There are a number of online communities that will help you out. Consistency plays an important role, going to bed every day with a little more knowledge will have a compounding impact on your career.
Reach out to people with a background similar to you and have found their way into data science. It helps in understanding the challenges of the job and skillsets that need to be focused on. It will give you a good picture of things you need to work on. Networking also plays a key role in the job search. A number of data science opportunities get filled directly through referrals without going to the job portals. Apart from reaching people with a similar background. Attending meet-ups and other public data science events could help in building your professional network.
Many people learning data science are struggling to find a suitable job. I am going to share some tips that could help in increasing your chances of getting hired.
towardsdatascience.com
towardsdatascience.com
The below story explains the journey of the author from Neuroscience into Data Science. After 8 years of study and work in Neuroscience, the author found her way into data science.
towardsdatascience.com
Below is my personal journey into data science from a sales career. I had no idea about R or Python. I didn't have a formal education in data science or statistics. In fact, I wasn't good at programming but now I have authored 2 books in data science and have successfully executed some turnkey data science projects.
",72
https://towardsdatascience.com/apples-new-m1-chip-is-a-machine-learning-beast-70ca8bfa6203?source=tag_archive---------7-----------------------,Apple's New M1 Chip is a Machine Learning Beast,Speed testing a (nearly) top-spec Intel-based 16-inch MacBook Pro versus the new Apple silicon MacBook Air and 13-inch MacBook Pro.,Daniel Bourke,11,"I watched the keynote and saw the graphs, the battery life, the instant wake. And they got me. I started to think, how could one of these new M1-powered MacBooks make their way into my life?
Of course, I didn't need one but I kept wondering what story could I tell myself to justify purchasing another computer? Then I had it. My 16-inch MacBook Pro is too heavy to carry around all the time. Yeah, that'll do. This 2.0 kg aluminium powerhouse is too much to be galavanting.
Wait... 2.0 kg, as in, 4.4 pounds?
That's it?
Yes.
Wow. It's not even that heavy.
C'mon now... let's not let the truth get in the way of a good story.
I had it. My reason for placing an order on a shiny new M1 MacBook (or two). My 16-inch MacBook is too heavy to lug around to cafes and write code, words, edit videos and check emails sporadically.
And Apple seems to think their new M1 chip is 11x, 15x, 12x, 3x faster on a bunch of different things. Thought-provoking numbers but I've never measured any of these in the past.
All I care about is: can I do what I need to do, fast.
The last word of the previous sentence is the most important. I've become conditioned. Speed is a part of me now. Ever since the transition from hard drives to solid-state drives. And I'm not going back.
I bought the 16-inch in February 2020. I'd just completed a large project and was flush with cash, so I decided to future proof my work station. Since I edit videos daily and hate lag, I opted for the biggest dawg I could buy and basically maxed everything except for the storage (see the specs below).
Thankfully I've still got a friend at Apple who was able to apply their employee discount to the beast (shout out to Joey).
Anyway, we've discussed my primary criteria: speed. Let's consider the others:
Why not?
But really, I'm a nerd. And an Apple fan. Plus, I wanted to see how my big-dawg-almost-top-of-the-line 16-inch MacBook Pro faired against the new M1 chip-powered MacBook's.
Plus, I can't remember being this excited for a new computing device since the original iPhone.
Other reasons include: carrying around a lighter laptop and tax benefits (if I buy another machine before the end of the year, I can claim it on tax).
Whenever I buy a new machine, I usually upgrade the RAM and the storage at least a step or two from baseline.
512GB storage and 16GB RAM seems to be the minimum for me these days (seriously, who is running a 128GB MacBook effectively?).
So for the M1 MacBook's, I upgraded both of their RAM from 8GB to 16GB and for the 13-inch Pro, I upgraded from 256GB to 512GB storage.
The 16-inch MacBook is my current machine, which I've never had a problem with until running the tests below.
Apple's graphs were impressive. And the GeekBench scores were even more impressive. But these are just numbers on a page to me. I wanted to see how these machines performed doing what I'd actually do day-to-day:
Reflecting on the above, I devised three tests:
Why not test more extensively?
These are enough for me. I've got other sh*t to do.
Alright, time for the results. The best results for each experiment have been highlighted in bold.
For this one, all machines were given ample time to pre-render the raw footage. So when the export button got clicked, they all should've been relatively on the same page.
Experiment details:
No surprise here, the 16-inch MacBook Pro exported in the fastest time. Most likely because of the dedicated 8GB GPU or 64GB of RAM.
However, it seems using the dedicated GPU came at the cost of battery life drain and fan speed (in the video you can hear the fans going off like a jet).
During the video export, the M1-powered MacBook Air and MacBook Pro 13-inch remained completely silent (the MacBook Air had no choice, it doesn't have a fan but the MacBook Pro 13-inch's fan never turned on).
I've never actually used a machine learning model trained by CreateML. However, I decided to see how one of Apple's custom apps would leverage their new silicon.
For this test, each MacBook was setup with the following CreateML settings:
Potentially the most surprising result of all the tests is that the M1 MacBook Air won this one by a clear margin, both in training time and battery life. All the while without a fan and one less GPU than the MacBook Pro 13-inch (the MacBook Air I used has an 8-core CPU, 7-core GPU M1 chip).
It's also quite clear the CreateML app has potentially been optimised for the M1 chip. As, despite having 8 CPU cores and a dedicated GPU, the 16-inch Intel-powered MacBook Pro ran out of juice before finishing the experiment.
During the M1 announcement keynote, Apple claimed their new silicon was capable of ""running popular deep learning frameworks such as TensorFlow"" at much greater speeds than previous generations.
Hearing this forced me to sit up a little straighter.
Did they just say TensorFlow? Natively?
I read back through the announcement.
Yes. They did. They said TensorFlow.
Then I found the blog posts by the TensorFlow team and Apple Machine Learning team showcasing the new results on the M1 chips and Intel-based Macs. Turns out, Apple recently released a fork of TensorFlow, tensorflow_macos which allows you to run native TensorFlow code right on your Mac (something which was previously a pain in the ass, actually, not really, I hear PlaidML has made it easier but I haven't tried that).
Naturally, after hearing this news, I had to try it.
By some miracle, I installed Apple's fork of TensorFlow into a Python 3.8 environment without 8-10 hours of troubleshooting and created the following mini-experiments:
I copied the CNN architecture on the CNN explainer website (TinyVGG). And used similar data to the CreateML test.
These days I rarely build models from scratch. I use either an existing untrained architecture and train it on my own data or a pretrained architecture like EfficientNet and fine-tune it on my own data.
Browsing Apple's tensorflow_macos GitHub, I came across an issue thread containing a benchmark a fair few people had run on their various machines. So I decided to include it in my testing.
As well as running the above three experiments on all of the MacBook's I had, I also ran them on a GPU-powered Google Colab instance as a control (my usual workflow is: experiment on Google Colab, scale up on larger cloud servers when needed).
The Google Colab GPU-powered instance performed the fastest across all three tests.
Notably, the M1 machines significantly outperformed the Intel machine in the Basic CNN and Transfer learning experiments.
However, the Intel-powered machine clawed back some ground on the tensorflow_macos benchmark. I believe this was due to explicitly telling TensorFlow to use the GPU, using the lines:
I tried running these lines with the previous two experiments and it didn't make a difference. Perhaps, it's something to do with the different data loading schemes used between the Basic CNN/Transfer learning setup and the tensorflow_macosbenchmark.
See all code experiments in the accompanying Google Colab Notebook.
If you're buying a laptop, you want to be able to move it around. Perhaps write some words while looking at the beach or code up your latest experiment whilst sipping coffee at your local cafe.
So what we have here is a compilation of the various amounts of battery used over the three experiments, plus a new portability score I've invented.
The M1 Macs crushed it here. Finishing all tests with over 30% of battery left each. The MacBook Air was again the standout, using the least amount of power and scoring the lowest on the portability score.
Anyone want to buy a second-hand close-to-top-spec MacBook Pro 16-inch? Its been well looked after, I promise.
After running the above experiments and using the M1 MacBooks for a couple of weeks, it looks like the graphs Apple showed were pretty close to reality. And all those raving reviews? Well, in my experience, they're correct too.
I originally bought the 16-inch MacBook Pro to be a powerhouse, a speed machine. And it is that. But so are the M1 machines. Except the M1 machines are lighter, quieter and have better battery life.
The only test the MacBook Pro 16-inch won on was the video rendering test (and of course screen size). And even then, the results weren't dramatically better, certainly not ""this machine costs 2.5x more better"".
So what next?
I'm keeping the 13-inch MacBook Pro. The little extra boost for video editing won it over the 13-inch Air (plus, I edited the entire video version of this article on the 13-inch Pro without a single hiccup). I'll use it as a portable machine and probably set my 16-inch up as a permanent desktop.
But the Air... Oh the Air... I remember when I worked at Apple Retail, I'd tell customers to get the Air if they were only concerned with word processing and browsing the web. Now you can train machine learning models on it.
Which one should you get?
""I write code, words and browse the web."" Or ""I just want a portable, capable machine.""
Get the MacBook Air, you won't be disappointed. It's what I'd get if I didn't edit videos daily.
""I write code, words, browse the web and edit videos.""
From my tests, the 13-inch M1 MacBook Pro or M1 MacBook Air will perform at 70-90% of what a nearly-top-spec Intel 16-inch MacBook Pro can offer, so either of those would be phenomenal. For a slight edge on video editing, you might opt for the 13-inch M1 MacBook Pro.
""I want a larger screen size and don't care about cost.""
As of now, the only valid reason I'd consider buying an Intel-based 16-inch MacBook Pro would be if screen size was of utmost importance to you and you didn't have a budget.
Screen size doesn't matter so much to me as most of the time I'm running a single full-screen app or two apps split down the middle of the screen. Or if I do want to run multiple apps, I'll plug my machine into an external monitor (note: for now the M1 Macs only support a single external monitor, so for the 3+ monitor fans out there, you'll need an Intel-based Mac).
This being said, even if you were after a larger screen, I'd hold off and wait for the Apple silicon 16-inch.
As I said, I had no problems with 16-inch MacBook Pro. But in comparison to these new M1 MacBook's, it feels dated.
I haven't been this impressed with a new computer since I first switched from hard drive to solid-state drive.
I'm writing this on the 13-inch M1 MacBook Pro and it feels like butter. The little things, the instance wake, the new keyboard style, the native Apple apps, the battery life, they all add up.
If Apple were capable of pulling off these kinds of performance improvements with a 1st-generation chip in a laptop (even one without a fan), I can't imagine what's going to happen on the machines without power constraints (the Mac mini hints at the potential here).
How about a 16-inch with an M2?
Hopefully they wait at least another year, I mean, my Intel-based 16-inch isn't even a year old yet...
PS you can see the video version of this article, including all of the tests above being run on YouTube:
",73
https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6?source=tag_archive---------3-----------------------,Train/Test Split and Cross Validation in Python,"Hi everyone! After my last post on linear regression in Python, I thought it would only be natural to write a post about Train/Test Split...",Adi Bronshtein,9,"Hi everyone! After my last post on linear regression in Python, I thought it would only be natural to write a post about Train/Test Split and Cross Validation. As usual, I am going to give a short overview on the topic and then give an example on implementing it in Python. These are two rather important concepts in data science and data analysis and are used as tools to prevent (or at least minimize) overfitting. I'll explain what that is  when we're using a statistical model (like linear regression, for example), we usually fit the model on a training set in order to make predications on a data that wasn't trained (general data). Overfitting means that what we've fit the model too much to the training data. It will all make sense pretty soon, I promise!
As mentioned, in statistics and machine learning we usually split our data into two subsets: training data and testing data (and sometimes to three: train, validate and test), and fit our model on the train data, in order to make predictions on the test data. When we do that, one of two thing might happen: we overfit our model or we underfit our model. We don't want any of these things to happen, because they affect the predictability of our model  we might be using a model that has lower accuracy and/or is ungeneralized (meaning you can't generalize your predictions on other data). Let's see what under and overfitting actually mean:
Overfitting means that model we trained has trained ""too well"" and is now, well, fit too closely to the training dataset. This usually happens when the model is too complex (i.e. too many features/variables compared to the number of observations). This model will be very accurate on the training data but will probably be very not accurate on untrained or new data. It is because this model is not generalized (or not AS generalized), meaning you can generalize the results and can't make any inferences on other data, which is, ultimately, what you are trying to do. Basically, when this happens, the model learns or describes the ""noise"" in the training data instead of the actual relationships between variables in the data. This noise, obviously, isn't part in of any new dataset, and cannot be applied to it.
In contrast to overfitting, when a model is underfitted, it means that the model does not fit the training data and therefore misses the trends in the data. It also means the model cannot be generalized to new data. As you probably guessed (or figured out!), this is usually the result of a very simple model (not enough predictors/independent variables). It could also happen when, for example, we fit a linear model (like linear regression) to data that is not linear. It almost goes without saying that this model will have poor predictive ability (on training data and can't be generalized to other data).
It is worth noting the underfitting is not as prevalent as overfitting. Nevertheless, we want to avoid both of those problems in data analysis. You might say we are trying to find the middle ground between under and overfitting our model. As you will see, train/test split and cross validation help to avoid overfitting more than underfitting. Let's dive into both of them!
As I said before, the data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model's prediction on this subset.
Let's see how to do this in Python. We'll do this using the Scikit-Learn library and specifically the train_test_split method. We'll start with importing the necessary libraries:
Let's quickly go over the libraries I've imported:
OK, all set! Let's load in the diabetes dataset, turn it into a data frame and define the columns' names:
Now we can use the train_test_split function in order to make the split. The test_size=0.2 inside the function indicates the percentage of the data that should be held over for testing. It's usually around 80/20 or 70/30.
Now we'll fit the model on the training data:
As you can see, we're fitting the model on the training data and trying to predict the test data. Let's see what (some of) the predictions are:
Note: because I used [0:5] after predictions, it only showed the first five predicted values. Removing the [0:5] would have made it print all of the predicted values that our model created.
Let's plot the model:
And print the accuracy score:
There you go! Here is a summary of what I did: I've loaded in the data, split it into a training and testing sets, fitted a regression model to the training data, made predictions based on this data and tested the predictions on the test data. Seems good, right? But train/test split does have its dangers  what if the split we make isn't random? What if one subset of our data has only people from a certain state, employees with a certain income level but not other income levels, only women or only people at a certain age? (imagine a file ordered by one of these). This will result in overfitting, even though we're trying to avoid it! This is where cross validation comes in.
In the previous paragraph, I mentioned the caveats in the train/test split method. In order to avoid this, we can perform something called cross validation. It's very similar to train/test split, but it's applied to more subsets. Meaning, we split our data into k subsets, and train on k-1 one of those subset. What we do is to hold the last subset for test. We're able to do it for each of the subsets.
There are a bunch of cross validation methods, I'll go over two of them: the first is K-Folds Cross Validation and the second is Leave One Out Cross Validation (LOOCV)
In K-Folds Cross Validation we split our data into k different subsets (or folds). We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the test set.
Here is a very simple example from the Sklearn documentation for K-Folds:
And let's see the result  the folds:
As you can see, the function split the original data into different subsets of the data. Again, very simple example but I think it explains the concept pretty well.
This is another method for cross validation, Leave One Out Cross Validation (by the way, these methods are not the only two, there are a bunch of other methods for cross validation. Check them out in the Sklearn website). In this type of cross validation, the number of folds (subsets) equals to the number of observations we have in the dataset. We then average ALL of these folds and build our model with the average. We then test the model against the last fold. Because we would get a big number of training sets (equals to the number of samples), this method is very computationally expensive and should be used on small datasets. If the dataset is big, it would most likely be better to use a different method, like kfold.
Let's check out another example from Sklearn:
And this is the output:
Again, simple example, but I really do think it helps in understanding the basic concept of this method.
So, what method should we use? How many folds? Well, the more folds we have, we will be reducing the error due the bias but increasing the error due to variance; the computational price would go up too, obviously  the more folds you have, the longer it would take to compute it and you would need more memory. With a lower number of folds, we're reducing the error due to variance, but the error due to bias would be bigger. It's would also computationally cheaper. Therefore, in big datasets, k=3 is usually advised. In smaller datasets, as I've mentioned before, it's best to use LOOCV.
Let's check out the example I used before, this time with using cross validation. I'll use the cross_val_predict function to return the predicted values for each data point when it's in the testing slice.
As you remember, earlier on I've created the train/test split for the diabetes dataset and fitted a model. Let's see what is the score after cross validation:
As you can see, the last fold improved the score of the original model  from 0.485 to 0.569. Not an amazing result, but hey, we'll take what we can get :)
Now, let's plot the new predictions, after performing cross validation:
You can see it's very different from the original plot from earlier. It is six times as many points as the original plot because I used cv=6.
Finally, let's check the R2 score of the model (R2 is a ""number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s)"". Basically, how accurate is our model):
That's it for this time! I hope you enjoyed this post. As always, I welcome questions, notes, comments and requests for posts on topics you'd like to read. See you next time!
",74
https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=tag_archive---------5-----------------------,Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT,"Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models",Mauro Di Pietro,22,"In this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned Bag-of-Words (with Tf-Idf ), the famous Word Embedding (with Word2Vec), and the cutting edge Language models (with BERT).
NLP (Natural Language Processing) is the field of artificial intelligence that studies the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. NLP is often applied for classifying text data. Text classification is the problem of assigning categories to text data according to its content.
There are different techniques to extract information from raw text data and use it to train a classification model. This tutorial compares the old school approach of Bag-of-Words (used with a simple machine learning algorithm), the popular Word Embedding model (used with a deep learning neural network), and the state of the art Language models (used with transfer learning from attention-based transformers) that have completely revolutionized the NLP landscape.
I will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example (link to the full code below).
github.com
I will use the ""News category dataset"" in which you are provided with news headlines from the year 2012 to 2018 obtained from HuffPost and you are asked to classify them with the right category, therefore this is a multiclass classification problem (link below).
www.kaggle.com
In particular, I will go through:
First of all, I need to import the following libraries:
The dataset is contained into a json file, so I will first read it into a list of dictionaries with json and then transform it into a pandas Dataframe.
The original dataset contains over 30 categories, but for the purposes of this tutorial, I will work with a subset of 3: Entertainment, Politics, and Tech.
In order to understand the composition of the dataset, I am going to look into the univariate distribution of the target by showing labels frequency with a bar plot.
The dataset is imbalanced: the proportion of Tech news is really small compared to the others, this will make for models to recognize Tech news rather tough.
Before explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set.
That function removes a set of words from the corpus if given. I can create a list of generic stop words for the English vocabulary with nltk (we could edit this list by adding or removing words).
Now I shall apply the function I wrote on the whole dataset and store the result in a new column named ""text_clean"" so that you can choose to work with the raw corpus or the preprocessed text.
If you are interested in a deeper text analysis and preprocessing, you can check this article. With this in mind, I am going to partition the dataset into training set (70%) and test set (30%) in order to evaluate the models performance.
Let's get started, shall we?
The Bag-of-Words model is simple: it builds a vocabulary from a corpus of documents and counts how many times the words appear in each document. To put it another way, each word in the vocabulary becomes a feature and a document is represented by a vector with the same length of the vocabulary (a ""bag of words""). For instance, let's take 3 sentences and represent them with this approach:
As you can imagine, this approach causes a significant dimensionality problem: the more documents you have the larger is the vocabulary, so the feature matrix will be a huge sparse matrix. Therefore, the Bag-of-Words model is usually preceded by an important preprocessing (word cleaning, stop words removal, stemming/lemmatization) aimed to reduce the dimensionality problem.
Terms frequency is not necessarily the best representation for text. In fact, you can find in the corpus common words with the highest frequency but little predictive power over the target variable. To address this problem there is an advanced variant of the Bag-of-Words that, instead of simple counting, uses the term frequency-inverse document frequency (or Tf-Idf). Basically, the value of a word increases proportionally to count, but it is inversely proportional to the frequency of the word in the corpus.
Let's start with the Feature Engineering, the process to create features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. ""new"" and ""york"") and bigrams (i.e. ""new york""). I will provide the code for the classic count vectorizer as well:
Now I will use the vectorizer on the preprocessed corpus of the train set to extract a vocabulary and create the feature matrix.
The feature matrix X_train has a shape of 34,265 (Number of documents in training) x 10,000 (Length of vocabulary) and it's pretty sparse:
In order to know the position of a certain word, we can look it up in the vocabulary:
If the word exists in the vocabulary, this command prints a number N, meaning that the Nth feature of the matrix is that word.
In order to drop some columns and reduce the matrix dimensionality, we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:
I reduced the number of features from 10,000 to 3,152 by keeping the most statistically relevant ones. Let's print some:
We can refit the vectorizer on the corpus by giving this new set of words as input. That will produce a smaller feature matrix and a shorter vocabulary.
The new feature matrix X_train has a shape of is 34,265 (Number of documents in training) x 3,152 (Length of the given vocabulary). Let's see if the matrix is less sparse:
It's time to train a machine learning model and test it. I recommend using a Naive Bayes algorithm: a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.
I'm going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step.
We can now evaluate the performance of the Bag-of-Words model, I will use the following metrics:
The BoW model got 85% of the test set right (Accuracy is 0.85), but struggles to recognize Tech news (only 252 predicted correctly).
Let's try to understand why the model classifies news with a certain category and assess the explainability of these predictions. The lime package can help us to build an explainer. To give an illustration, I will take a random observation from the test set and see what the model predicts and why.
That makes sense: the words ""Clinton"" and ""GOP"" pointed the model in the right direction (Politics news) even if the word ""Stage"" is more common among Entertainment news.
Word Embedding is the collective name for feature learning techniques where words from the vocabulary are mapped to vectors of real numbers. These vectors are calculated from the probability distribution for each word appearing before or after another. To put it another way, words of the same context usually appear together in the corpus, so they will be close in the vector space as well. For instance, let's take the 3 sentences from the previous example:
In this tutorial, I'm going to use the first model of this family: Google's Word2Vec (2013). Other popular Word Embedding models are Stanford's GloVe (2014) and Facebook's FastText (2016).
Word2Vec produces a vector space, typically of several hundred dimensions, with each unique word in the corpus such that words that share common contexts in the corpus are located close to one another in the space. That can be done using 2 different approaches: starting from a single word to predict its context (Skip-gram) or starting from the context to predict a word (Continuous Bag-of-Words).
In Python, you can load a pre-trained Word Embedding model from genism-data like this:
Instead of using a pre-trained model, I am going to fit my own Word2Vec on the training data corpus with gensim. Before fitting the model, the corpus needs to be transformed into a list of lists of n-grams. In this particular case, I'll try to capture unigrams (""york""), bigrams (""new york""), and trigrams (""new york city"").
When fitting the Word2Vec, you need to specify:
We have our embedding model, so we can select any word from the corpus and transform it into a vector.
We can even use it to visualize a word and its context into a smaller dimensional space (2D or 3D) by applying any dimensionality reduction algorithm (i.e. TSNE).
That's pretty cool and all, but how can the word embedding be useful to predict the news category? Well, the word vectors can be used in a neural network as weights. This is how:
Let's start with the Feature Engineering by transforming the same preprocessed corpus (list of lists of n-grams) given to the Word2Vec into a list of sequences using tensorflow/keras:
The feature matrix X_train has a shape of 34,265 x 15 (Number of sequences x Sequences max length). Let's visualize it:
Every text in the corpus is now an id sequence with length 15. For instance, if a text had 10 tokens in it, then the sequence is composed of 10 ids + 5 0s, which is the padding element (while the id for word not in the vocabulary is 1). Let's print how a text from the train set has been transformed into a sequence with the padding and the vocabulary.
Before moving on, don't forget to do the same feature engineering on the test set as well:
We've got our X_train and X_test, now we need to create the matrix of embedding that will be used as a weight matrix in the neural network classifier.
That code generates a matrix of shape 22,338 x 300 (Length of vocabulary extracted from the corpus x Vector size). It can be navigated by word id, which can be obtained from the vocabulary.
It's finally time to build a deep learning model. I'm going to use the embedding matrix in the first Embedding layer of the neural network that I will build and train to classify the news. Each id in the input sequence will be used as the index to access the embedding matrix. The output of this Embedding layer will be a 2D matrix with a word vector for each word id in the input sequence (Sequence length x Vector size). Let's use the sentence ""I like this article"" as an example:
My neural network shall be structured as follows:
Now we can train the model and check the performance on a subset of the training set used for validation before testing it on the actual test set.
Nice! In some epochs, the accuracy reached 0.89. In order to complete the evaluation of the Word Embedding model, let's predict the test set and compare the same metrics used before (code for metrics is the same as before).
The model performs as good as the previous one, in fact, it also struggles to classify Tech news.
But is it explainable as well? Yes, it is! I put an Attention layer in the neural network to extract the weights of each word and understand how much those contributed to classify an instance. So I'll try to use Attention weights to build an explainer (similar to the one seen in the previous section):
Just like before, the words ""clinton"" and ""gop"" activated the neurons of the model, but this time also ""high"" and ""benghazi"" have been considered slightly relevant for the prediction.
Language Models, or Contextualized/Dynamic Word Embeddings, overcome the biggest limitation of the classic Word Embedding approach: polysemy disambiguation, a word with different meanings (e.g. "" bank"" or ""stick"") is identified by just one vector. One of the first popular ones was ELMO (2018), which doesn't apply a fixed embedding but, using a bidirectional LSTM, looks at the entire sentence and then assigns an embedding to each word.
Enter Transformers: a new modeling technique presented by Google's paper Attention is All You Need (2017) in which it was demonstrated that sequence models (like LSTM) can be totally replaced by Attention mechanisms, even obtaining better performances.
Google's BERT (Bidirectional Encoder Representations from Transformers, 2018) combines ELMO context embedding and several Transformers, plus it's bidirectional (which was a big novelty for Transformers). The vector BERT assigns to a word is a function of the entire sentence, therefore, a word can have different vectors based on the contexts. Let's try it using transformers:
If we change the input text into ""bank money"", we get this instead:
In order to complete a text classification task, you can use BERT in 3 different ways:
I'm going with the latter and do transfer learning from a pre-trained lighter version of BERT, called Distil-BERT (66 million of parameters instead of 110 million!).
As usual, before fitting the model there is some Feature Engineering to do, but this time it's gonna be a little trickier. To give an illustration of what I'm going to do, let's take as an example our beloved sentence ""I like this article"", which has to be transformed into 3 vectors (Ids, Mask, Segment):
First of all, we need to select the sequence max length. This time I'm gonna choose a much larger number (i.e. 50) because BERT splits unknown words into sub-tokens until it finds a known unigrams. For example, if a made-up word like ""zzdata"" is given, BERT would split it into [""z"", ""##z"", ""##data""]. Moreover, we have to insert special tokens into the input text, then generate masks and segments. Finally, put all together in a tensor to get the feature matrix that will have the shape of 3 (ids, masks, segments) x Number of documents in the corpus x Sequence length.
Please note that I'm using the raw text as corpus (so far I've been using the clean_text column).
The feature matrix X_train has a shape of 3 x 34,265 x 50. We can check a random observation from the feature matrix:
You can take the same code and apply it to dtf_test[""text""] to get X_test.
Now, I'm going to build the deep learning model with transfer learning from the pre-trained BERT. Basically, I'm going to summarize the output of BERT into one vector with Average Pooling and then add two final Dense layers to predict the probability of each news category.
If you want to use the original versions of BERT, here's the code (remember to redo the feature engineering with the right tokenizer):
As I said, I'm going to use the lighter version instead, Distil-BERT:
Let's train, test, evaluate this bad boy (code for evaluation is the same):
The performance of BERT is slightly better than the previous models, in fact, it can recognize more Tech news than the others.
This article has been a tutorial to demonstrate how to apply different NLP models to a multiclass classification use case. I compared 3 popular approaches: Bag-of-Words with Tf-Idf, Word Embedding with Word2Vec, and Language model with BERT. I went through Feature Engineering & Selection, Model Design & Testing, Evaluation & Explainability, comparing the 3 models in each step (where possible).
I hope you enjoyed it! Feel free to contact me for questions and feedback or just to share your interesting projects.
LinkedIn | Instagram | Twitter | GitHub
This article is part of the series NLP with Python, see also:
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
",75
https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=tag_archive---------3-----------------------,The 5 Clustering Algorithms Data Scientists Need to Know,"Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group...",George Seif,11,"I write a newsletter for learners called Mighty Knowledge. Each new issue contains links and key lessons from the very best content including quotes, books, articles, podcasts, and videos. Each and every one is picked out specifically for learning how to live a wiser, happier, and fuller life. Sign up for it here.
Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.
In Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Today, we're going to look at 5 popular clustering algorithms that data scientists need to know and their pros and cons!
K-Means is probably the most well-known clustering algorithm. It's taught in a lot of introductory data science and machine learning classes. It's easy to understand and implement in code! Check out the graphic below for an illustration.
K-Means has the advantage that it's pretty fast, as all we're really doing is computing the distances between points and group centers; very few computations! It thus has a linear complexity O(n).
On the other hand, K-Means has a couple of disadvantages. Firstly, you have to select how many groups/classes there are. This isn't always trivial and ideally with a clustering algorithm we'd want it to figure those out for us because the point of it is to gain some insight from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.
K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.
Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration.
An illustration of the entire process from end-to-end with all of the sliding windows is shown below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.
In contrast to K-means clustering, there is no need to select the number of clusters as mean-shift automatically discovers this. That's a massive advantage. The fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data-driven sense. The drawback is that the selection of the window size/radius ""r"" can be non-trivial.
DBSCAN is a density-based clustered algorithm similar to mean-shift, but with a couple of notable advantages. Check out another fancy graphic below and let's get started!
DBSCAN poses some great advantages over other clustering algorithms. Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises, unlike mean-shift which simply throws them into a cluster even if the data point is very different. Additionally, it can find arbitrarily sized and arbitrarily shaped clusters quite well.
The main drawback of DBSCAN is that it doesn't perform as well as others when the clusters are of varying density. This is because the setting of the distance threshold  and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold  becomes challenging to estimate.
One of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. We can see why this isn't the best way of doing things by looking at the image below. On the left-hand side, it looks quite obvious to the human eye that there are two circular clusters with different radius' centered at the same mean. K-Means can't handle this because the mean values of the clusters are very close together. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center.
Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! Taking an example in two dimensions, this means that the clusters can take any kind of elliptical shape (since we have a standard deviation in both the x and y directions). Thus, each Gaussian distribution is assigned to a single cluster.
To find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation), we will use an optimization algorithm called Expectation-Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. Then we can proceed with the process of Expectation-Maximization clustering using GMMs.
There are 2 key advantages to using GMMs. Firstly GMMs are a lot more flexible in terms of cluster covariance than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster's covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support mixed membership.
Hierarchical clustering algorithms fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm steps
Hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can't do this. These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of O(n3), unlike the linear complexity of K-Means and GMM.
",76
https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8?source=tag_archive---------0-----------------------,"Building A Logistic Regression in Python, Step by Step",Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent...,Susan Li,9,"Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.
Keeping the above assumptions in mind, let's look at our dataset.
The dataset comes from the UCI Machine Learning repository, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (1/0) to a term deposit (variable y). The dataset can be downloaded from here.
The dataset provides the bank customers' information. It includes 41,188 records and 21 fields.
Input variables
Predict variable (desired target):
y  has the client subscribed a term deposit? (binary: ""1"", means ""Yes"", ""0"" means ""No"")
The education column of the dataset has many categories and we need to reduce the categories for a better modelling. The education column has the following categories:
Let us group ""basic.4y"", ""basic.9y"" and ""basic.6y"" together and call them ""basic"".
After grouping, this is the columns:
percentage of no subscription is 88.73458288821988
percentage of subscription 11.265417111780131
Our classes are imbalanced, and the ratio of no-subscription to subscription instances is 89:11. Before we go ahead to balance the classes, let's do some more exploration.
Observations:
We can calculate categorical means for other categorical variables such as education and marital status to get a more detailed sense of our data.
The frequency of purchase of the deposit depends a great deal on the job title. Thus, the job title can be a good predictor of the outcome variable.
The marital status does not seem a strong predictor for the outcome variable.
Education seems a good predictor of the outcome variable.
Day of week may not be a good predictor of the outcome.
Month might be a good predictor of the outcome variable.
Most of the customers of the bank in this dataset are in the age range of 30-40.
Poutcome seems to be a good predictor of the outcome variable.
That is variables with only two values, zero and one.
Our final data columns will be:
With our training data created, I'll up-sample the no-subscription using the SMOTE algorithm(Synthetic Minority Oversampling Technique). At a high level, SMOTE:
We are going to implement SMOTE in Python.
Now we have a perfect balanced data! You may have noticed that I over-sampled only on the training data, because by oversampling only on the training data, none of the information in the test data is being used to create synthetic observations, therefore, no information will bleed from test data into the model training.
Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.
The RFE has helped us select the following features: ""euribor3m"", ""job_blue-collar"", ""job_housemaid"", ""marital_unknown"", ""education_illiterate"", ""default_no"", ""default_unknown"", ""contact_cellular"", ""contact_telephone"", ""month_apr"", ""month_aug"", ""month_dec"", ""month_jul"", ""month_jun"", ""month_mar"", ""month_may"", ""month_nov"", ""month_oct"", ""poutcome_failure"", ""poutcome_success"".
The p-values for most of the variables are smaller than 0.05, except four variables, therefore, we will remove them.
Predicting the test set results and calculating the accuracy
Accuracy of logistic regression classifier on test set: 0.74
[[6124 1542]
[2505 5170]]
The result is telling us that we have 6124+5170 correct predictions and 2505+1542 incorrect predictions.
To quote from Scikit Learn:
The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.
The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.
The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.
The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.
The support is the number of occurrences of each class in y_test.
Interpretation: Of the entire test set, 74% of the promoted term deposit were the term deposit that the customers liked. Of the entire test set, 74% of the customer's preferred term deposits that were promoted.
The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).
The Jupyter notebook used to make this post is available here. I would be pleased to receive feedback or questions on any of the above.
Reference: Learning Predictive Analytics with Python book
",77
https://towardsdatascience.com/springer-has-released-65-machine-learning-and-data-books-for-free-961f8181f189?source=tag_archive---------0-----------------------,Springer has released 65 Machine Learning and Data books for free,Hundreds of books are now free to download,Uri Eliabayev,4,"Hundreds of books are now free to download
Springer has released hundreds of free books on a wide range of topics to the general public. The list, which includes 408 books in total, covers a wide range of scientific and technological topics. In order to save you some time, I have created one list of all the books (65 in number) that are relevant to the data and Machine Learning field.
Among the books, you will find those dealing with the mathematical side of the domain (Algebra, Statistics, and more), along with more advanced books on Deep Learning and other advanced topics. You also could find some good books in various programming languages such as Python, R, and MATLAB, etc.
If you are looking for more recommended books about Machine Learning and data you can check my previous article about it.
The Elements of Statistical Learning
Trevor Hastie, Robert Tibshirani, Jerome Friedman
http://link.springer.com/openurl?genre=book&isbn=978-0-387-84858-7
Introductory Time Series with R
Paul S.P. Cowpertwait, Andrew V. Metcalfe
http://link.springer.com/openurl?genre=book&isbn=978-0-387-88698-5
A Beginner's Guide to R
Alain Zuur, Elena N. Ieno, Erik Meesters
http://link.springer.com/openurl?genre=book&isbn=978-0-387-93837-0
Introduction to Evolutionary Computing
A.E. Eiben, J.E. Smith
http://link.springer.com/openurl?genre=book&isbn=978-3-662-44874-8
Data Analysis
Siegmund Brandt
http://link.springer.com/openurl?genre=book&isbn=978-3-319-03762-2
Linear and Nonlinear Programming
David G. Luenberger, Yinyu Ye
http://link.springer.com/openurl?genre=book&isbn=978-3-319-18842-3
Introduction to Partial Differential Equations
David Borthwick
http://link.springer.com/openurl?genre=book&isbn=978-3-319-48936-0
Fundamentals of Robotic Mechanical Systems
Jorge Angeles
http://link.springer.com/openurl?genre=book&isbn=978-3-319-01851-5
Data Structures and Algorithms with Python
Kent D. Lee, Steve Hubbard
http://link.springer.com/openurl?genre=book&isbn=978-3-319-13072-9
Introduction to Partial Differential Equations
Peter J. Olver
http://link.springer.com/openurl?genre=book&isbn=978-3-319-02099-0
Methods of Mathematical Modelling
Thomas Witelski, Mark Bowen
http://link.springer.com/openurl?genre=book&isbn=978-3-319-23042-9
LaTeX in 24 Hours
Dilip Datta
http://link.springer.com/openurl?genre=book&isbn=978-3-319-47831-9
Introduction to Statistics and Data Analysis
Christian Heumann, Michael Schomaker, Shalabh
http://link.springer.com/openurl?genre=book&isbn=978-3-319-46162-5
Principles of Data Mining
Max Bramer
http://link.springer.com/openurl?genre=book&isbn=978-1-4471-7307-6
Computer Vision
Richard Szeliski
http://link.springer.com/openurl?genre=book&isbn=978-1-84882-935-0
Data Mining
Charu C. Aggarwal
http://link.springer.com/openurl?genre=book&isbn=978-3-319-14142-8
Computational Geometry
Mark de Berg, Otfried Cheong, Marc van Kreveld, Mark Overmars
http://link.springer.com/openurl?genre=book&isbn=978-3-540-77974-2
Robotics, Vision and Control
Peter Corke
http://link.springer.com/openurl?genre=book&isbn=978-3-319-54413-7
Statistical Analysis and Data Display
Richard M. Heiberger, Burt Holland
http://link.springer.com/openurl?genre=book&isbn=978-1-4939-2122-5
Statistics and Data Analysis for Financial Engineering
David Ruppert, David S. Matteson
http://link.springer.com/openurl?genre=book&isbn=978-1-4939-2614-5
Stochastic Processes and Calculus
Uwe Hassler
http://link.springer.com/openurl?genre=book&isbn=978-3-319-23428-1
Statistical Analysis of Clinical Data on a Pocket Calculator
Ton J. Cleophas, Aeilko H. Zwinderman
http://link.springer.com/openurl?genre=book&isbn=978-94-007-1211-9
Clinical Data Analysis on a Pocket Calculator
Ton J. Cleophas, Aeilko H. Zwinderman
http://link.springer.com/openurl?genre=book&isbn=978-3-319-27104-0
The Data Science Design Manual
Steven S. Skiena
http://link.springer.com/openurl?genre=book&isbn=978-3-319-55444-0
An Introduction to Machine Learning
Miroslav Kubat
http://link.springer.com/openurl?genre=book&isbn=978-3-319-63913-0
Guide to Discrete Mathematics
Gerard O'Regan
http://link.springer.com/openurl?genre=book&isbn=978-3-319-44561-8
Introduction to Time Series and Forecasting
Peter J. Brockwell, Richard A. Davis
http://link.springer.com/openurl?genre=book&isbn=978-3-319-29854-2
Multivariate Calculus and Geometry
Sean Dineen
http://link.springer.com/openurl?genre=book&isbn=978-1-4471-6419-7
Statistics and Analysis of Scientific Data
Massimiliano Bonamente
http://link.springer.com/openurl?genre=book&isbn=978-1-4939-6572-4
Modelling Computing Systems
Faron Moller, Georg Struth
http://link.springer.com/openurl?genre=book&isbn=978-1-84800-322-4
Search Methodologies
Edmund K. Burke, Graham Kendall
http://link.springer.com/openurl?genre=book&isbn=978-1-4614-6940-7
Linear Algebra Done Right
Sheldon Axler
http://link.springer.com/openurl?genre=book&isbn=978-3-319-11080-6
Linear Algebra
Jorg Liesen, Volker Mehrmann
http://link.springer.com/openurl?genre=book&isbn=978-3-319-24346-7
Algebra
Serge Lang
http://link.springer.com/openurl?genre=book&isbn=978-1-4613-0041-0
Understanding Analysis
Stephen Abbott
http://link.springer.com/openurl?genre=book&isbn=978-1-4939-2712-8
Linear Programming
Robert J Vanderbei
http://link.springer.com/openurl?genre=book&isbn=978-1-4614-7630-6
Understanding Statistics Using R
Randall Schumacker, Sara Tomek
http://link.springer.com/openurl?genre=book&isbn=978-1-4614-6227-9
An Introduction to Statistical Learning
Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
http://link.springer.com/openurl?genre=book&isbn=978-1-4614-7138-7
Statistical Learning from a Regression Perspective
Richard A. Berk
http://link.springer.com/openurl?genre=book&isbn=978-3-319-44048-4
Applied Partial Differential Equations
J. David Logan
http://link.springer.com/openurl?genre=book&isbn=978-3-319-12493-3
Robotics
Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo
http://link.springer.com/openurl?genre=book&isbn=978-1-84628-642-1
Regression Modeling Strategies
Frank E. Harrell , Jr.
http://link.springer.com/openurl?genre=book&isbn=978-3-319-19425-7
A Modern Introduction to Probability and Statistics
F.M. Dekking, C. Kraaikamp, H.P. Lopuhaa, L.E. Meester
http://link.springer.com/openurl?genre=book&isbn=978-1-84628-168-6
The Python Workbook
Ben Stephenson
http://link.springer.com/openurl?genre=book&isbn=978-3-319-14240-1
Machine Learning in Medicine  a Complete Overview
Ton J. Cleophas, Aeilko H. Zwinderman
http://link.springer.com/openurl?genre=book&isbn=978-3-319-15195-3
Object-Oriented Analysis, Design and Implementation
Brahma Dathan, Sarnath Ramnath
http://link.springer.com/openurl?genre=book&isbn=978-3-319-24280-4
Introduction to Data Science
Laura Igual, Santi Segui
http://link.springer.com/openurl?genre=book&isbn=978-3-319-50017-1
Applied Predictive Modeling
Max Kuhn, Kjell Johnson
http://link.springer.com/openurl?genre=book&isbn=978-1-4614-6849-3
Python For ArcGIS
Laura Tateosian
http://link.springer.com/openurl?genre=book&isbn=978-3-319-18398-5
Concise Guide to Databases
Peter Lake, Paul Crowther
http://link.springer.com/openurl?genre=book&isbn=978-1-4471-5601-7
Digital Image Processing
Wilhelm Burger, Mark J. Burge
http://link.springer.com/openurl?genre=book&isbn=978-1-4471-6684-9
Bayesian Essentials with R
Jean-Michel Marin, Christian P. Robert
http://link.springer.com/openurl?genre=book&isbn=978-1-4614-8687-9
Robotics, Vision and Control
Peter Corke
http://link.springer.com/openurl?genre=book&isbn=978-3-642-20144-8
Foundations of Programming Languages
Kent D. Lee
http://link.springer.com/openurl?genre=book&isbn=978-3-319-70790-7
Introduction to Artificial Intelligence
Wolfgang Ertel
http://link.springer.com/openurl?genre=book&isbn=978-3-319-58487-4
Introduction to Deep Learning
Sandro Skansi
http://link.springer.com/openurl?genre=book&isbn=978-3-319-73004-2
Linear Algebra and Analytic Geometry for Physical Sciences
Giovanni Landi, Alessandro Zampini
http://link.springer.com/openurl?genre=book&isbn=978-3-319-78361-1
Applied Linear Algebra
Peter J. Olver, Chehrzad Shakiban
http://link.springer.com/openurl?genre=book&isbn=978-3-319-91041-3
Neural Networks and Deep Learning
Charu C. Aggarwal
http://link.springer.com/openurl?genre=book&isbn=978-3-319-94463-0
Data Science and Predictive Analytics
Ivo D. Dinov
http://link.springer.com/openurl?genre=book&isbn=978-3-319-72347-1
Analysis for Computer Scientists
Michael Oberguggenberger, Alexander Ostermann
http://link.springer.com/openurl?genre=book&isbn=978-3-319-91155-7
Excel Data Analysis
Hector Guerrero
http://link.springer.com/openurl?genre=book&isbn=978-3-030-01279-3
A Beginners Guide to Python 3 Programming
John Hunt
http://link.springer.com/openurl?genre=book&isbn=978-3-030-20290-3
Advanced Guide to Python 3 Programming
John Hunt
http://link.springer.com/openurl?genre=book&isbn=978-3-030-25943-3
",78
https://medium.com/free-code-camp/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------9-----------------------,"Every single Machine Learning course on the internet, ranked by your reviews","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science...",David Venturi,20,"A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master's program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost.
I'm almost finished now. I've taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science.
For the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization.
For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below.
For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews.
Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.
Each course must fit three criteria:
We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only.
There's always a chance that we missed something, though. So please let us know in the comments section if we left a good course out.
We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings.
We made subjective syllabus judgment calls based on three factors:
A popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives ""computers the ability to learn without being explicitly programmed."" In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience.
A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you'll find helpful visualization of these core steps:
The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves.
First off, let's define deep learning. Here is a succinct description:
""Deep learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.""
 Jason Brownlee from Machine Learning Mastery
As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we've got you covered with the following article:
medium.freecodecamp.com
My top three recommendations from that list would be:
Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline.
Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar.
Stanford University's Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews.
Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available.
Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning.
Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice:
In the past, I've tried to teach machine learning using a large variety of different programming languages including C++, Java, Python, NumPy, and also Octave ... And what I've seen after having taught machine learning for almost a decade is that you learn much faster if you use Octave as your programming environment.
Though Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn't stop you from taking the course.
A few prominent reviewers noted the following:
Of longstanding renown in the MOOC world, Stanford's machine learning course really is the definitive introduction to this topic. The course broadly covers all of the major areas of machine learning ... Prof. Ng precedes each segment with a motivating discussion and examples.
Andrew Ng is a gifted teacher and able to explain complicated subjects in a very intuitive and clear way, including the math behind all concepts. Highly recommended.
The only problem I see with this course if that it sets the expectation bar very high for other courses.
Columbia University's Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn't have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews.
The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia's is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding).
Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course's total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase.
Below are a few of the aforementioned sparkling reviews:
Over all my years of [being a] student I've come across professors who aren't brilliant, professors who are brilliant but they don't know how to explain the stuff clearly, and professors who are brilliant and know how explain the stuff clearly. Dr. Paisley belongs to the third group.
This is a great course ... The instructor's language is precise and that is, to my mind, one of the strongest points of the course. The lectures are of high quality and the slides are great too.
Dr. Paisley and his supervisor are ... students of Michael Jordan, the father of machine learning. [Dr. Paisley] is the best ML professor at Columbia because of his ability to explain stuff clearly. Up to 240 students have selected his course this semester, the largest number among all professors [teaching] machine learning at Columbia.
Machine Learning A-ZTM on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can't be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered.
It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an ""intuition"" video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R.
As a ""bonus,"" the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren't the strong points of the course.
Eremenko and the SuperDataScience team are revered for their ability to ""make the complex simple."" Also, the prerequisites listed are ""just some high school mathematics,"" so this course might be a better option for those daunted by the Stanford and Columbia offerings.
A few prominent reviewers noted the following:
The course is professionally produced, the sound quality is excellent, and the explanations are clear and concise ... It's an incredible value for your financial and time investment.
It was spectacular to be able to follow the course in two different programming languages simultaneously.
Kirill is one of the absolute best instructors on Udemy (if not the Internet) and I recommend taking any class he teaches. ... This course has a ton of content, like a ton!
Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let's look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide  you can find those here.
The Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews.
Python for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews.
Data Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla's above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews.
Machine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course's description. Uses Python. Cost varies depending on Udemy discounts, which are frequent.
Machine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity's Machine Learning Engineer Nanodegree and Georgia Tech's Online Master's Degree (OMS). Bite-sized videos, as is Udacity's style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews.
Implementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews.
Data Science and Machine Learning with Python  Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews.
Scala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): ""Big data"" focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews.
Machine Learning Engineer Nanodegree (Udacity): Udacity's flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews.
Learning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech's independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews.
Learning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): ""A real Caltech course, not a watered-down version."" Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn't as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews.
Mining Massive Datasets (Stanford University): Machine learning with a focus on ""big data."" Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews.
AWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews.
Introduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews.
StatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, ""An Introduction to Statistical Learning, with Applications in R"" and taught by the professors who wrote it. Reviewers note that the MOOC isn't as good as the book, citing ""thin"" exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews.
Machine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford's or Caltech's). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews.
From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): ""A down-to-earth, shy but confident take on machine learning techniques."" Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews.
Principles of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews.
Big Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews.
Genomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD's Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews.
Intro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity's Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews.
Machine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan's Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews.
Programming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews.
Machine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity's Machine Learning Engineer Nanodegree and Georgia Tech's Online Master's Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews.
Practical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU's Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews.
Machine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews.
Recommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning  recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews.
Machine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD's Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews.
Practical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW's Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews.
The following courses had one or no reviews as of May 2017.
Machine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review.
Applied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available.
Applied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase.
Machine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free.
Machine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free.
Machine Learning for Data Science (University of California, San Diego/edX): Doesn't launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase.
Introduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase.
Predictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise's Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase.
Introduccion al Machine Learning (Universitas Telefonica/Miriada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks.
Machine Learning Path Step (Dataquest): Taught in Python using Dataquest's interactive in-browser platform. Multiple guided projects and a ""plus"" project where you build your own machine learning system using your own data. Subscription required.
The following six courses are offered by DataCamp. DataCamp's hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course.
Introduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours.
Supervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours.
Unsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours.
Machine Learning Toolbox (DataCamp): Teaches the ""big ideas"" in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours.
Machine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school's budget. DataCamp's ""Supervised Learning with scikit-learn"" is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours.
Unsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours.
Machine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon's graduate introductory machine learning course. A prerequisite to their second graduate level course, ""Statistical Machine Learning."" Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free.
Statistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon's Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free.
Undergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below).
Machine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas' undergraduate course (above) apply here as well.
This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth.
medium.freecodecamp.com
The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering.
If you're looking for a complete list of Data Science online courses, you can find them on Class Central's Data Science and Big Data subject page.
If you enjoyed reading this, check out some of Class Central's other pieces:
medium.freecodecamp.com
medium.freecodecamp.com
If you have suggestions for courses I missed, let me know in the responses!
If you found this helpful, click the  so more people will see it here on Medium.
This is a condensed version of my original article published on Class Central, where I've included detailed course syllabi.
We've moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
22K 
71
",79
https://medium.com/@ilblackdragon/tensorflow-tutorial-part-1-c559c63c0cb1?source=tag_archive---------8-----------------------,TensorFlow Tutorial Part 1,"UPD (April 20, 2016): Scikit Flow has been merged into TensorFlow since version 0.8 and now called TensorFlow Learn or tf.learn.",Illia Polosukhin,3,"UPD (April 20, 2016): Scikit Flow has been merged into TensorFlow since version 0.8 and now called TensorFlow Learn or tf.learn.
Google released a machine learning framework called TensorFlow and it's taking the world by storm. 10k+ stars on Github, a lot of publicity and general excitement in between AI researchers.
Now, but how you to use it for something regular problem Data Scientist may have? (and if you are AI researcher  we will build up to interesting problems over time).
A reasonable question, why as a Data Scientist, who already has a number of tools in your toolbox (R, Scikit Learn, etc), you care about yet another framework?
The answer is two part:
Let's start with simple example  take Titanic dataset from Kaggle.
First, make sure you have installed TensorFlow and Scikit Learn with few helpful libs, including Scikit Flow that is simplifying a lot of work with TensorFlow:
You can get dataset and the code from http://github.com/ilblackdragon/tf_examples
Quick look at the data (use iPython or iPython notebook for ease of interactive exploration):
Let's test how we can predict Survived class, based on float variables in Scikit Learn:
We separate dataset into features and target, fill in N/A in the data with zeros and build a logistic regression. Predicting on the training data gives us some measure of accuracy (of cause it doesn't properly evaluate the model quality and test dataset should be used, but for simplicity we will look at train only for now).
Now using tf.learn (previously Scikit Flow):
Congratulations, you just built your first TensorFlow model!
TF.Learn is a library that wraps a lot of new APIs by TensorFlow with nice and familiar Scikit Learn API.
TensorFlow is all about a building and executing graph. This is a very powerful concept, but it is also cumbersome to start with.
Looking under the hood of TF.Learn, we just used three parts:
Even as you get more familiar with TensorFlow, pieces of Scikit Flow will be useful (like graph_actions and layers and host of other ops and tools). See future posts for examples of handling categorical variables, text and images.
Part 2  Deep Neural Networks, Custom TensorFlow models with Scikit Flow and Digit recognition with Convolutional Networks.
Since writing this post, I founded NEAR Protocol. Read more about our journey.
",80
https://medium.com/swlh/the-7-best-data-science-and-machine-learning-podcasts-e8f0d5a4a419?source=tag_archive---------7-----------------------,The 7 Best Data Science and Machine Learning Podcasts,Learn the basics and keep up with the latest news by listening to these podcasts...,Matt Fogel,4,"Data science and machine learning have long been interests of mine, but now that I'm working on Fuzzy.ai and trying to make AI and machine learning accessible to all developers, I need to keep on top of all the news in both fields.
My preferred way to do this is through listening to podcasts. I've listened to a bunch of machine learning and data science podcasts in the last few months, so I thought I'd share my favorites:
A great starting point on some of the basics of data science and machine learning. Every other week, they release a 10-15 minute episode where hosts, Kyle and Linda Polich give a short primer on topics like k-means clustering, natural language processing and decision tree learning, often using analogies related to their pet parrot, Yoshi. This is the only place where you'll learn about k-means clustering via placement of parrot droppings.
Website | iTunes
Hosted by Katie Malone and Ben Jaffe of online education startup Udacity, this weekly podcast covers diverse topics in data science and machine learning: teaching specific concepts like Hidden Markov Models and how they apply to real-world problems and datasets. They make complex topics extremely accessible.
Website | iTunes
Each week, hosts Chris Albon and Jonathon Morgan, both experienced technologists and data scientists, talk about the latest news in data science over drinks. Listening to Partially Derivative is a great way to keep up on the latest data news.
Website | iTunes
This podcast features Ben Lorica, O'Reilly Media's Chief Data Scientist speaking with other experts about timely big data and data science topics. It can often get quite technical, but the topics of discussion are always really interesting.
Website | iTunes
Data Stories is a little more focused on data visualization than data science, but there is often some interesting overlap between the topics. Every other week, Enrico Bertini and Moritz Stefaner cover diverse topics in data with their guests. Recent episodes about smart cities and Nicholas Felton's annual reports are particularly interesting.
Website | iTunes
Billing itself as ""A Gentle Introduction to Artificial Intelligence and Machine Learning"", this podcast can still get quite technical and complex, covering topics like: ""How to Reason About Uncertain Events using Fuzzy Set Theory and Fuzzy Measure Theory"" and ""How to Represent Knowledge using Logical Rules"".
Website | iTunes
The newest podcasts on this list, with 8 episodes released as of this writing. Every other week, hosts Katherine Gorman and Ryan Adams speak with a guest about their work, and news stories related to machine learning.
Website | iTunes
Feel I've unfairly left a podcast off this list? Leave me a note to let me know.
Published in Startups, Wanderlust, and Life Hacking
-
Get smarter at building your thing. Join The Startup's +745K followers.
1K 
12
",81
https://towardsdatascience.com/random-forest-in-python-24d0893d51c0?source=tag_archive---------2-----------------------,Random Forest in Python,A Practical End-to-End Machine Learning Example,Will Koehrsen,21,"A Practical End-to-End Machine Learning Example
There has never been a better time to get into machine learning. With the learning resources available online, free open-source tools with implementations of any algorithm imaginable, and the cheap availability of computing power through cloud services such as AWS, machine learning is truly a field that has been democratized by the internet. Anyone with access to a laptop and a willingness to learn can try out state-of-the-art algorithms in minutes. With a little more time, you can develop practical models to help in your daily life or at work (or even switch into the machine learning field and reap the economic benefits). This post will walk you through an end-to-end implementation of the powerful random forest machine learning model. It is meant to serve as a complement to my conceptual explanation of the random forest, but can be read entirely on its own as long as you have the basic idea of a decision tree and a random forest. A follow-up post details how we can improve upon the model built here.
There will of course be Python code here, however, it is not meant to intimate anyone, but rather to show how accessible machine learning is with the resources available today! The complete project with data is available on GitHub, and the data file and Jupyter Notebook can also be downloaded from Google Drive. All you need is a laptop with Python installed and the ability to start a Jupyter Notebook and you can follow along. (For installing Python and running a Jupyter notebook check out this guide). There will be a few necessary machine learning topics touched on here, but I will try to make them clear and provide resources for learning more for those interested.
The problem we will tackle is predicting the max temperature for tomorrow in our city using one year of past weather data. I am using Seattle, WA but feel free to find data for your own city using the NOAA Climate Data Online tool. We are going to act as if we don't have access to any weather forecasts (and besides, it's more fun to make our own predictions rather than rely on others). What we do have access to is one year of historical max temperatures, the temperatures for the previous two days, and an estimate from a friend who is always claiming to know everything about the weather. This is a supervised, regression machine learning problem. It's supervised because we have both the features (data for the city) and the targets (temperature) that we want to predict. During training, we give the random forest both the features and targets and it must learn how to map the data to a prediction. Moreover, this is a regression task because the target value is continuous (as opposed to discrete classes in classification). That's pretty much all the background we need, so let's start!
Before we jump right into programming, we should lay out a brief guide to keep us on track. The following steps form the basis for any machine learning workflow once we have a problem and model in mind:
Step 1 is already checked off! We have our question: ""can we predict the max temperature tomorrow for our city?"" and we know we have access to historical max temperatures for the past year in Seattle, WA.
First, we need some data. To use a realistic example, I retrieved weather data for Seattle, WA from 2016 using the NOAA Climate Data Online tool. Generally, about 80% of the time spent in data analysis is cleaning and retrieving data, but this workload can be reduced by finding high-quality data sources. The NOAA tool is surprisingly easy to use and temperature data can be downloaded as clean csv files which can be parsed in languages such as Python or R. The complete data file is available for download for those wanting to follow along.
The following Python code loads in the csv data and displays the structure of the data:
The information is in the tidy data format with each row forming one observation, with the variable values in the columns.
Following are explanations of the columns:
year: 2016 for all data points
month: number for month of the year
day: number for day of the year
week: day of the week as a character string
temp_2: max temperature 2 days prior
temp_1: max temperature 1 day prior
average: historical average max temperature
actual: max temperature measurement
friend: your friend's prediction, a random number between 20 below the average and 20 above the average
If we look at the dimensions of the data, we notice only there are only 348 rows, which doesn't quite agree with the 366 days we know there were in 2016. Looking through the data from the NOAA, I noticed several missing days, which is a great reminder that data collected in the real-world will never be perfect. Missing data can impact an analysis as can incorrect data or outliers. In this case, the missing data will not have a large effect, and the data quality is good because of the source. We also can see there are nine columns which represent eight features and the one target ('actual').
To identify anomalies, we can quickly compute summary statistics.
There are not any data points that immediately appear as anomalous and no zeros in any of the measurement columns. Another method to verify the quality of the data is make basic plots. Often it is easier to spot anomalies in a graph than in numbers. I have left out the actual code here, because plotting is Python is non-intuitive but feel free to refer to the notebook for the complete implementation (like any good data scientist, I pretty much copy and pasted the plotting code from Stack Overflow).
Examining the quantitative statistics and the graphs, we can feel confident in the high quality of our data. There are no clear outliers, and although there are a few missing points, they will not detract from the analysis.
Unfortunately, we aren't quite at the point where you can just feed raw data into a model and have it return an answer (although people are working on this)! We will need to do some minor modification to put our data into machine-understandable terms. We will use the Python library Pandas for our data manipulation relying, on the structure known as a dataframe, which is basically an excel spreadsheet with rows and columns.
The exact steps for preparation of the data will depend on the model used and the data gathered, but some amount of data manipulation will be required for any machine learning application.
One-Hot Encoding
The first step for us is known as one-hot encoding of the data. This process takes categorical variables, such as days of the week and converts it to a numerical representation without an arbitrary ordering. Days of the week are intuitive to us because we use them all the time. You will (hopefully) never find anyone who doesn't know that 'Mon' refers to the first day of the workweek, but machines do not have any intuitive knowledge. What computers know is numbers and for machine learning we must accommodate them. We could simply map days of the week to numbers 1-7, but this might lead to the algorithm placing more importance on Sunday because it has a higher numerical value. Instead, we change the single column of weekdays into seven columns of binary data. This is best illustrated pictorially. One hot encoding takes this:
and turns it into
So, if a data point is a Wednesday, it will have a 1 in the Wednesday column and a 0 in all other columns. This process can be done in pandas in a single line!
Snapshot of data after one-hot encoding:
The shape of our data is now 349 x 15 and all of the column are numbers, just how the algorithm likes it!
Features and Targets and Convert Data to Arrays
Now, we need to separate the data into the features and targets. The target, also known as the label, is the value we want to predict, in this case the actual max temperature and the features are all the columns the model uses to make a prediction. We will also convert the Pandas dataframes to Numpy arrays because that is the way the algorithm works. (I save the column headers, which are the names of the features, to a list to use for later visualization).
Training and Testing Sets
There is one final step of data preparation: splitting data into training and testing sets. During training, we let the model 'see' the answers, in this case the actual temperature, so it can learn how to predict the temperature from the features. We expect there to be some relationship between all the features and the target value, and the model's job is to learn this relationship during training. Then, when it comes time to evaluate the model, we ask it to make predictions on a testing set where it only has access to the features (not the answers)! Because we do have the actual answers for the test set, we can compare these predictions to the true value to judge how accurate the model is. Generally, when training a model, we randomly split the data into training and testing sets to get a representation of all data points (if we trained on the first nine months of the year and then used the final three months for prediction, our algorithm would not perform well because it has not seen any data from those last three months.) I am setting the random state to 42 which means the results will be the same each time I run the split for reproducible results.
The following code splits the data sets with another single line:
We can look at the shape of all the data to make sure we did everything correctly. We expect the training features number of columns to match the testing feature number of columns and the number of rows to match for the respective training and testing features and the labels :
It looks as if everything is in order! Just to recap, to get the data into a form acceptable for machine learning we:
Depending on the initial data set, there may be extra work involved such as removing outliers, imputing missing values, or converting temporal variables into cyclical representations. These steps may seem arbitrary at first, but once you get the basic workflow, it will be generally the same for any machine learning problem. It's all about taking human-readable data and putting it into a form that can be understood by a machine learning model.
Before we can make and evaluate predictions, we need to establish a baseline, a sensible measure that we hope to beat with our model. If our model cannot improve upon the baseline, then it will be a failure and we should try a different model or admit that machine learning is not right for our problem. The baseline prediction for our case can be the historical max temperature averages. In other words, our baseline is the error we would get if we simply predicted the average max temperature for all days.
We now have our goal! If we can't beat an average error of 5 degrees, then we need to rethink our approach.
After all the work of data preparation, creating and training the model is pretty simple using Scikit-learn. We import the random forest regression model from skicit-learn, instantiate the model, and fit (scikit-learn's name for training) the model on the training data. (Again setting the random state for reproducible results). This entire process is only 3 lines in scikit-learn!
Our model has now been trained to learn the relationships between the features and the targets. The next step is figuring out how good the model is! To do this we make predictions on the test features (the model is never allowed to see the test answers). We then compare the predictions to the known answers. When performing regression, we need to make sure to use the absolute error because we expect some of our answers to be low and some to be high. We are interested in how far away our average prediction is from the actual value so we take the absolute value (as we also did when establishing the baseline).
Making predictions with out model is another 1-line command in Skicit-learn.
Our average estimate is off by 3.83 degrees. That is more than a 1 degree average improvement over the baseline. Although this might not seem significant, it is nearly 25% better than the baseline, which, depending on the field and the problem, could represent millions of dollars to a company.
To put our predictions in perspective, we can calculate an accuracy using the mean average percentage error subtracted from 100 %.
That looks pretty good! Our model has learned how to predict the maximum temperature for the next day in Seattle with 94% accuracy.
In the usual machine learning workflow, this would be when start hyperparameter tuning. This is a complicated phrase that means ""adjust the settings to improve performance"" (The settings are known as hyperparameters to distinguish them from model parameters learned during training). The most common way to do this is simply make a bunch of models with different settings, evaluate them all on the same validation set, and see which one does best. Of course, this would be a tedious process to do by hand, and there are automated methods to do this process in Skicit-learn. Hyperparameter tuning is often more engineering than theory-based, and I would encourage anyone interested to check out the documentation and start playing around! An accuracy of 94% is satisfactory for this problem, but keep in mind that the first model built will almost never be the model that makes it to production.
At this point, we know our model is good, but it's pretty much a black box. We feed in some Numpy arrays for training, ask it to make a prediction, evaluate the predictions, and see that they are reasonable. The question is: how does this model arrive at the values? There are two approaches to get under the hood of the random forest: first, we can look at a single tree in the forest, and second, we can look at the feature importances of our explanatory variables.
One of the coolest parts of the Random Forest implementation in Skicit-learn is we can actually examine any of the trees in the forest. We will select one tree, and save the whole tree as an image.
The following code takes one tree from the forest and saves it as an image.
Let's take a look:
Wow! That looks like quite an expansive tree with 15 layers (in reality this is quite a small tree compared to some I've seen). You can download this image yourself and examine it in greater detail, but to make things easier, I will limit the depth of trees in the forest to produce an understandable image.
Here is the reduced size tree annotated with labels
Based solely on this tree, we can make a prediction for any new data point. Let's take an example of making a prediction for Wednesday, December 27, 2017. The (actual) variables are: temp_2 = 39, temp_1 = 35, average = 44, and friend = 30. We start at the root node and the first answer is True because temp_1  59.5. We move to the left and encounter the second question, which is also True as average  46.8. Move down to the left and on to the third and final question which is True as well because temp_1  44.5. Therefore, we conclude that our estimate for the maximum temperature is 41.0 degrees as indicated by the value in the leaf node. An interesting observation is that in the root node, there are only 162 samples despite there being 261 training data points. This is because each tree in the forest is trained on a random subset of the data points with replacement (called bagging, short for bootstrap aggregating). (We can turn off the sampling with replacement and use all the data points by setting bootstrap = False when making the forest). Random sampling of data points, combined with random sampling of a subset of the features at each node of the tree, is why the model is called a 'random' forest.
Furthermore, notice that in our tree, there are only 2 variables we actually used to make a prediction! According to this particular decision tree, the rest of the features are not important for making a prediction. Month of the year, day of the month, and our friend's prediction are utterly useless for predicting the maximum temperature tomorrow! The only important information according to our simple tree is the temperature 1 day prior and the historical average. Visualizing the tree has increased our domain knowledge of the problem, and we now know what data to look for if we are asked to make a prediction!
In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables. The importances returned in Skicit-learn represent how much including a particular variable improves the prediction. The actual calculation of the importance is beyond the scope of this post, but we can use the numbers to make relative comparisons between variables.
The code here takes advantage of a number of tricks in the Python language, namely list comprehensive, zip, sorting, and argument unpacking. It's not that important to understand these at the moment, but if you want to become skilled at Python, these are tools you should have in your arsenal!
At the top of the list is temp_1, the max temperature of the day before. This tells us the best predictor of the max temperature for a day is the max temperature of the day before, a rather intuitive finding. The second most important factor is the historical average max temperature, also not that surprising. Your friend turns out to not be very helpful, along with the day of the week, the year, the month, and the temperature 2 days prior. These importances all make sense as we would not expect the day of the week to be a predictor of maximum temperature as it has nothing to do with weather. Moreover, the year is the same for all data points and hence provides us with no information for predicting the max temperature.
In future implementations of the model, we can remove those variables that have no importance and the performance will not suffer. Additionally, if we are using a different model, say a support vector machine, we could use the random forest feature importances as a kind of feature selection method. Let's quickly make a random forest with only the two most important variables, the max temperature 1 day prior and the historical average and see how the performance compares.
This tells us that we actually do not need all the data we collected to make accurate predictions! If we were to continue using this model, we could only collect the two variables and achieve nearly the same performance. In a production setting, we would need to weigh the decrease in accuracy versus the extra time required to obtain more information. Knowing how to find the right balance between performance and cost is an essential skill for a machine learning engineer and will ultimately depend on the problem!
At this point we have covered pretty much everything there is to know for a basic implementation of the random forest for a supervised regression problem. We can feel confident that our model can predict the maximum temperature tomorrow with 94% accuracy from one year of historical data. From here, feel free to play around with this example, or use the model on a data set of your choice. I will wrap up this post by making a few visualizations. My two favorite parts of data science are graphing and modeling, so naturally I have to make some charts! In addition to being enjoyable to look at, charts can help us diagnose our model because they compress a lot of numbers into an image that we can quickly examine.
The first chart I'll make is a simple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables. Plotting in Python is kind of non-intuitive, and I end up looking up almost everything on Stack Overflow when I make graphs. Don't worry if the code here doesn't quite make sense, sometimes fully understanding the code isn't necessary to get the end result you want!
Next, we can plot the entire dataset with predictions highlighted. This requires a little data manipulation, but its not too difficult. We can use this plot to determine if there are any outliers in either the data or our predictions.
A little bit of work for a nice looking graph! It doesn't look as if we have any noticeable outliers that need to be corrected. To further diagnose the model, we can plot residuals (the errors) to see if our model has a tendency to over-predict or under-predict, and we can also see if the residuals are normally distributed. However, I will just make one final chart showing the actual values, the temperature one day previous, the historical average, and our friend's prediction. This will allow us to see the difference between useful variables and those that aren't so helpful.
It is a little hard to make out all the lines, but we can see why the max temperature one day prior and the historical max temperature are useful for predicting max temperature while our friend is not (don't give up on the friend yet, but maybe also don't place so much weight on their estimate!). Graphs such as this are often helpful to make ahead of time so we can choose the variables to include, but they also can be used for diagnosis. Much as in the case of Anscombe's quartet, graphs are often more revealing than quantitative numbers and should be a part of any machine learning workflow.
With those graphs, we have completed an entire end-to-end machine learning example! At this point, if we want to improve our model, we could try different hyperparameters (settings) try a different algorithm, or the best approach of all, gather more data! The performance of any model is directly proportional to the amount of valid data it can learn from, and we were using a very limited amount of information for training. I would encourage anyone to try and improve this model and share the results. From here you can dig more into the random forest theory and application using numerous online (free) resources. For those looking for a single book to cover both theory and Python implementations of machine learning models, I highly recommend Hands-On Machine Learning with Scikit-Learn and Tensorflow. Moreover, I hope everyone who made it through has seen how accessible machine learning has become and is ready to join the welcoming and helpful machine learning community.
As always, I welcome feedback and constructive criticism! My email is wjk68@case.edu.
",82
https://towardsdatascience.com/how-to-land-a-data-analytics-job-in-6-months-58f05311b905?source=tag_archive---------9-----------------------,How to Land a Data Analytics Job in 6 Months,Go from zero to hero in under six months,Natassha Selvaraj,7,"Data analysts are some of the most sought after professionals in the world. These are people who help companies make informed business decisions with the help of data.
There is a lot of hype surrounding data science right now.
However, data science has a very high barrier of entry. It is a very competitive field that everybody from different educational backgrounds are looking to get into.
Most data science positions require you to have a post-graduate degree in a quantitative field. However, most data analysts I know come from a completely unrelated background and do not possess technical degrees.
Data analytic skills can easily be gained by taking online courses and doing boot camps. The learning curve isn't as steep as that in data science, and it can be learned in a shorter span of time.
Even if you have no previous programming or technical experience, you can gain the skills required to become a data analyst in just a few months.
After doing a 3-month internship, I received an offer to join the company as a data analyst.
In this article, I will describe the steps I took to learn data analytics. It took a lot of trial and error to find these resources and create a roadmap for myself.
If you follow these steps, you can learn the skills required to get an entry level data analytics job in just a few months. You can even do it faster than six months depending on the amount of time you spend studying everyday.
To get into the field of analytics, you will first need to learn a programming language. Python and R are the two most commonly used languages in this domain.
If you're just starting out, I strongly suggest learning Python. It is a lot more user-friendly than R and it is easier to pick up. Python also has a wide array of libraries that make tasks like data pre-processing a lot easier.
Python is also more widely used than R. If you were to move into a field like web development or machine learning in the future, you won't need to learn a new language.
a) 2020 Complete Python Bootcamp: From Zero to Hero in Python:
Take this course if you are a complete beginner with no programming experience whatsoever. This course will take you through the basics of Python syntax, and you will learn about variables, conditional statements, and loops. This course is taught by Jose Portilla, one of the best instructors on Udemy.
b) Learning Python for Data Analysis and Visualization:
Once you have an understanding of Python basics and syntax, you can start learning how to analyze data with it. This course will walk you through libraries specific to data analytics such as Numpy, Matplotlib, Pandas, and Seaborn.
After taking these two courses, you will have a basic understanding of Python and its use in the field of analytics. Then, I suggest moving on to get some hands on practice with the language.
To gain hands on practice, visit coding challenge sites like HackerRank and LeetCode. I strongly suggest HackerRank. They have coding challenges with varying levels of difficulty. Start out with the easiest ones, then work your way up.
When you start working in analytics, you will face programming issues on a daily basis. Sites like HackerRank will help improve your problem solving skills.
Spend around 4-5 hours daily solving Python HackerRank problems. Do this for around a month, and your Python programming skills will be good enough to get a job.
SQL skills are necessary to get a job in analytics. Your daily task would usually involve querying large amounts of data from a database, and manipulating the data according to business requirements.
Many companies integrate SQL with other frameworks, and will expect you to know how to query data using these frameworks.
SQL can be used within languages like Python, Scala, and Hadoop. This will differ depending on the company you are working with. However, if you know SQL for data manipulation, you will be able to pick up on other SQL integrated frameworks easily.
I took this free course by Udacity to learn SQL for data analysis. DataCamp also has a popular SQL for data analytics track that you can try out.
You will need to know how to analyze data and derive insights from it. Knowing how to code or query data isn't enough. You need to be able to answer questions and solve problems with this data.
To learn data analysis in Python, you can take this Udemy course I mentioned above. You can also pursue the data analyst career track at DataCamp.
After deriving insights from data, you should be able to present these insights. Stakeholders need to make business decisions based on the insights you present, so you need to make sure your presentation is clear and concise.
These insights are usually presented with the help of data visualization tools. Visualizations can be created using Excel, Python libraries, or business intelligence tools like Tableau.
If you want to become a data analyst, I suggest learning Tableau. It is one of the most commonly used reporting tool and is sought after by most employers.
This Udemy course by Kirill Eremenko is one of the best resource to learn Tableau.
After completing the first three steps, you already have all the necessary skills to get an entry level job in data analytics.
Now, you will need to present these skills to a potential employer. If you don't come from a technical background, you will need to show recruiters that you have the necessary skillset to become an analyst.
To do this, I strongly suggest building a data analytics portfolio. Build dashboards in Tableau, use Python to analyze Kaggle datasets, and write articles on your newly honed skills.
You can take a look at my portfolio site here.
Here are some examples of data analytic projects you can showcase on your portfolio:
Showcasing projects like these on your resume will make you stand out to potential employers.
Make sure to tell stories around the projects you create. Document every step you took to create the project and write an article about it. You can even create your own blog and publish these articles.
This increases the chances of your article getting into the hands of another person, which means that there are higher chances of it being seen by a potential employer.
If you are looking to break into the data industry, data analytics is a good place to start. It has a lower barrier of entry compared to fields like machine learning.
You will enjoy working in the field of analytics if you like storytelling and creating presentations. Your daily work will involve explaining technical concepts to non-technical people, and you will need to work on improving your communication skills.
Remember, data analysis is a field that people spend their entire lives trying to learn. Even the individual skills required to become an analyst can take a lifetime to learn, so it is impossible to master in just a few months.
This article is only aimed towards people trying to get an entry level job in data analytics.
I managed to get a job in analytics in around 6 months by following the steps above. Even if you have no previous data experience, put in around 5-6 hours per day, and you will be able to do the same.
Education is the most powerful weapon which you can use to change the world  Nelson Mandela
",83
https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04?source=tag_archive---------3-----------------------,What is a Transformer?,An Introduction to Transformers and Sequence-to-Sequence Learning for Machine Learning,Maxime,13,"New deep learning models are introduced at an increasing rate and sometimes it's hard to keep track of all the novelties. That said, one particular neural network model has proven to be especially effective for common natural language processing tasks. The model is called a Transformer and it makes use of several methods and mechanisms that I'll introduce here. The papers I refer to in the post offer a more detailed and quantitative description.
The paper 'Attention Is All You Need' describes transformers and what is called a sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence. (Well, this might not surprise you considering the name.)
Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A popular choice for this type of model is Long-Short-Term-Memory (LSTM)-based models. With sequence-dependent data, the LSTM modules can give meaning to the sequence while remembering (or forgetting) the parts it finds important (or unimportant). Sentences, for example, are sequence-dependent since the order of the words is crucial for understanding the sentence. LSTM are a natural choice for this type of data.
Seq2Seq models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector). That abstract vector is fed into the Decoder which turns it into an output sequence. The output sequence can be in another language, symbols, a copy of the input, etc.
Imagine the Encoder and Decoder as human translators who can speak only two languages. Their first language is their mother tongue, which differs between both of them (e.g. German and French) and their second language an imaginary one they have in common. To translate German into French, the Encoder converts the German sentence into the other language it knows, namely the imaginary language. Since the Decoder is able to read that imaginary language, it can now translates from that language into French. Together, the model (consisting of Encoder and Decoder) can translate German into French!
Suppose that, initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples.
A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them.
You're wondering when the Transformer will finally come into play, aren't you?
We need one more technical detail to make Transformers easier to understand: Attention. The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important. It sounds abstract, but let me clarify with an easy example: When reading this text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context.
An attention-mechanism works similarly for a given sequence. For our example with the human Encoder and Decoder, imagine that instead of only writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords that are important to the semantics of the sentence, and gives them to the Decoder in addition to the regular translation. Those new keywords make the translation much easier for the Decoder because it knows what parts of the sentence are important and which key terms give the sentence context.
In other words, for each input that the LSTM (Encoder) reads, the attention-mechanism takes into account several other inputs at the same time and decides which ones are important by attributing different weights to those inputs. The Decoder will then take as input the encoded sentence and the weights provided by the attention-mechanism. To learn more about attention, see this article. And for a more scientific approach than the one provided, read about different attention-based approaches for Sequence-to-Sequence models in this great paper called 'Effective Approaches to Attention-based Neural Machine Translation'.
The paper 'Attention Is All You Need' introduces a novel architecture called Transformer. As the title indicates, it uses the attention-mechanism we saw earlier. Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).
Recurrent Networks were, until now, one of the best ways to capture the timely dependencies in sequences. However, the team presenting the paper proved that an architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks! One improvement on Natural Language Tasks is presented by a team introducing BERT: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
So, what exactly is a Transformer?
An image is worth thousand words, so we will start with that!
The Encoder is on the left and the Decoder is on the right. Both Encoder and Decoder are composed of modules that can be stacked on top of each other multiple times, which is described by Nx in the figure. We see that the modules consist mainly of Multi-Head Attention and Feed Forward layers. The inputs and outputs (target sentences) are first embedded into an n-dimensional space since we cannot use strings directly.
One slight but important part of the model is the positional encoding of the different words. Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word.
Let's have a closer look at these Multi-Head Attention bricks in the model:
Let's start with the left description of the attention-mechanism. It's not very complicated and can be described by the following equation:
Q is a matrix that contains the query (vector representation of one word in the sequence), K are all the keys (vector representations of all the words in the sequence) and V are the values, which are again the vector representations of all the words in the sequence. For the encoder and decoder, multi-head attention modules, V consists of the same word sequence than Q. However, for the attention module that is taking into account the encoder and the decoder sequences, V is different from the sequence represented by Q.
To simplify this a little bit, we could say that the values in V are multiplied and summed with some attention-weights a, where our weights are defined by:
This means that the weights a are defined by how each word of the sequence (represented by Q) is influenced by all the other words in the sequence (represented by K). Additionally, the SoftMax function is applied to the weights a to have a distribution between 0 and 1. Those weights are then applied to all the words in the sequence that are introduced in V (same vectors than Q for encoder and decoder but different for the module that has encoder and decoder inputs).
The righthand picture describes how this attention-mechanism can be parallelized into multiple mechanisms that can be used side by side. The attention mechanism is repeated multiple times with linear projections of Q, K and V. This allows the system to learn from different representations of Q, K and V, which is beneficial to the model. These linear representations are done by multiplying Q, K and V by weight matrices W that are learned during the training.
Those matrices Q, K and V are different for each position of the attention modules in the structure depending on whether they are in the encoder, decoder or in-between encoder and decoder. The reason is that we want to attend on either the whole encoder input sequence or a part of the decoder input sequence. The multi-head attention module that connects the encoder and decoder will make sure that the encoder input-sequence is taken into account together with the decoder input-sequence up to a given position.
After the multi-attention heads in both the encoder and decoder, we have a pointwise feed-forward layer. This little feed-forward network has identical parameters for each position, which can be described as a separate, identical linear transformation of each element from the given sequence.
How to train such a 'beast'? Training and inferring on Seq2Seq models is a bit different from the usual classification problem. The same is true for Transformers.
We know that to train a model for translation tasks we need two sentences in different languages that are translations of each other. Once we have a lot of sentence pairs, we can start training our model. Let's say we want to translate French to German. Our encoded input will be a French sentence and the input for the decoder will be a German sentence. However, the decoder input will be shifted to the right by one position. ..Wait, why?
One reason is that we do not want our model to learn how to copy our decoder input during training, but we want to learn that given the encoder sequence and a particular decoder sequence, which has been already seen by the model, we predict the next word/character.
If we don't shift the decoder sequence, the model learns to simply 'copy' the decoder input, since the target word/character for position i would be the word/character i in the decoder input. Thus, by shifting the decoder input by one position, our model needs to predict the target word/character for position i having only seen the word/characters 1, ..., i-1 in the decoder sequence. This prevents our model from learning the copy/paste task. We fill the first position of the decoder input with a start-of-sentence token, since that place would otherwise be empty because of the right-shift. Similarly, we append an end-of-sentence token to the decoder input sequence to mark the end of that sequence and it is also appended to the target output sentence. In a moment, we'll see how that is useful for inferring the results.
This is true for Seq2Seq models and for the Transformer. In addition to the right-shifting, the Transformer applies a mask to the input in the first multi-head attention module to avoid seeing potential 'future' sequence elements. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position.
The process of feeding the correct shifted input into the decoder is also called Teacher-Forcing, as described in this blog.
The target sequence we want for our loss calculations is simply the decoder input (German sentence) without shifting it and with an end-of-sequence token at the end.
Inferring with those models is different from the training, which makes sense because in the end we want to translate a French sentence without having the German sentence. The trick here is to re-feed our model for each position of the output sequence until we come across an end-of-sentence token.
A more step by step method would be:
We see that we need multiple runs through our model to translate our sentence.
I hope that these descriptions have made the Transformer architecture a little bit clearer for everybody starting with Seq2Seq and encoder-decoder structures.
We have seen the Transformer architecture and we know from literature and the 'Attention is All you Need' authors that the model does extremely well in language tasks. Let's now test the Transformer in a use case.
Instead of a translation task, let's implement a time-series forecast for the hourly flow of electrical power in Texas, provided by the Electric Reliability Council of Texas (ERCOT). You can find the hourly data here.
A great detailed explanation of the Transformer and its implementation is provided by harvardnlp. If you want to dig deeper into the architecture, I recommend going through that implementation.
Since we can use LSTM-based sequence-to-sequence models to make multi-step forecast predictions, let's have a look at the Transformer and its power to make those predictions. However, we first need to make a few changes to the architecture since we are not working with sequences of words but with values. Additionally, we are doing an auto-regression and not a classification of words/characters.
The available data gives us hourly load for the entire ERCOT control area. I used the data from the years 2003 to 2015 as a training set and the year 2016 as test set. Having only the load value and the timestamp of the load, I expanded the timestamp to other features. From the timestamp, I extracted the weekday to which it corresponds and one-hot encoded it. Additionally, I used the year (2003, 2004, ..., 2015) and the corresponding hour (1, 2, 3, ..., 24) as the value itself. This gives me 11 features in total for each hour of the day. For convergence purposes, I also normalized the ERCOT load by dividing it by 1000.
To predict a given sequence, we need a sequence from the past. The size of those windows can vary from use-case to use-case but here in our example I used the hourly data from the previous 24 hours to predict the next 12 hours. It helps that we can adjust the size of those windows depending on our needs. For example, we can change that to daily data instead of hourly data.
As a first step, we need to remove the embeddings, since we already have numerical values in our input. An embedding usually maps a given integer into an n-dimensional space. Here instead of using the embedding, I simply used a linear transformation to transform the 11-dimensional data into an n-dimensional space. This is similar to the embedding with words.
We also need to remove the SoftMax layer from the output of the Transformer because our output nodes are not probabilities but real values.
After those minor changes, the training can begin!
As mentioned, I used teacher forcing for the training. This means that the encoder gets a window of 24 data points as input and the decoder input is a window of 12 data points where the first one is a 'start-of-sequence' value and the following data points are simply the target sequence. Having introduced a 'start-of-sequence' value at the beginning, I shifted the decoder input by one position with regard to the target sequence.
I used an 11-dimensional vector with only -1's as the 'start-of-sequence' values. Of course, this can be changed and perhaps it would be beneficial to use other values depending of the use case but for this example, it works since we never have negative values in either dimension of the input/output sequences.
The loss function for this example is simply the mean squared error.
The two plots below show the results. I took the mean value of the hourly values per day and compared it to the correct values. The first plot shows the 12-hour predictions given the 24 previous hours. For the second plot, we predicted one hour given the 24 previous hours. We see that the model is able to catch some of the fluctuations very well. The root mean squared error for the training set is 859 and for the validation set it is 4,106 for the 12-hour predictions and 2,583 for the 1-hour predictions. This corresponds to a mean absolute percentage error of the model prediction of 8.4% for the first plot and 5.1% for the second one.
The results show that it would be possible to use the Transformer architecture for time-series forecasting. However, during the evaluation, it shows that the more steps we want to forecast the higher the error will become. The first graph (Figure 3) above has been achieved by using the 24 hours to predict the next 12 hours. If we predict only one hour, the results are much better as we see on the second the graph (Figure 4).
There's plenty of room to play around with the parameters of the Transformer, such as the number of decoder and encoder layers, etc. This was not intended to be a perfect model and with better tuning and training, the results would probably improve.
It can be a big help to accelerate the training using GPUs. I used the Watson Studio Local Platform to train my model with GPUs and I let it run there rather than on my local machine. You can also accelerate the training using Watson's Machine Learning GPUs which are free up to a certain amount of training time! Check out my previous blog to see how that can be integrated easily into your code.
Thank you very much for reading this and I hope I was able to clarify a few notions to the people who are just starting to get into Deep Learning!
Deep-dive articles about machine learning and data.
6.5K 
28
",84
https://towardsdatascience.com/fundamentals-of-statistics-for-data-scientists-and-data-analysts-69d93a05aae7?source=tag_archive---------8-----------------------,Fundamentals Of Statistics For Data Scientists and Analysts,Key statistical concepts for your data science or data analytics journey,Tatev Karen,31,"As Karl Pearson, a British mathematician has once stated, Statistics is the grammar of science and this holds especially for Computer and Information Sciences, Physical Science, and Biological Science. When you are getting started with your journey in Data Science or Data Analytics, having statistical knowledge will help you to better leverage data insights.
""Statistics is the grammar of science."" Karl Pearson
The importance of statistics in data science and data analytics cannot be underestimated. Statistics provides tools and methods to find structure and to give deeper data insights. Both Statistics and Mathematics love facts and hate guesses. Knowing the fundamentals of these two important subjects will allow you to think critically, and be creative when using the data to solve business problems and make data-driven decisions. In this article, I will cover the following Statistics topics for data science and data analytics:
If you have no prior Statistical knowledge and you want to identify and learn the essential statistical concepts from the scratch, to prepare for your job interviews, then this article is for you. This article will also be a good read for anyone who wants to refresh his/her statistical knowledge.
The concept of random variables forms the cornerstone of many statistical concepts. It might be hard to digest its formal mathematical definition but simply put, a random variable is a way to map the outcomes of random processes, such as flipping a coin or rolling a dice, to numbers. For instance, we can define the random process of flipping a coin by random variable X which takes a value 1 if the outcome if heads and 0 if the outcome is tails.
In this example, we have a random process of flipping a coin where this experiment can produce two possible outcomes: {0,1}. This set of all possible outcomes is called the sample space of the experiment. Each time the random process is repeated, it is referred to as an event. In this example, flipping a coin and getting a tail as an outcome is an event. The chance or the likelihood of this event occurring with a particular outcome is called the probability of that event. A probability of an event is the likelihood that a random variable takes a specific value of x which can be described by P(x). In the example of flipping a coin, the likelihood of getting heads or tails is the same, that is 0.5 or 50%. So we have the following setting:
where the probability of an event, in this example, can only take values in the range [0,1].
The importance of statistics in data science and data analytics cannot be underestimated. Statistics provides tools and methods to find structure and to give deeper data insights.
To understand the concepts of mean, variance, and many other statistical topics, it is important to learn the concepts of population and sample. The population is the set of all observations (individuals, objects, events, or procedures) and is usually very large and diverse, whereas a sample is a subset of observations from the population that ideally is a true representation of the population.
Given that experimenting with an entire population is either impossible or simply too expensive, researchers or analysts use samples rather than the entire population in their experiments or trials. To make sure that the experimental results are reliable and hold for the entire population, the sample needs to be a true representation of the population. That is, the sample needs to be unbiased. For this purpose, one can use statistical sampling techniques such as Random Sampling, Systematic Sampling, Clustered Sampling, Weighted Sampling, and Stratified Sampling.
The mean, also known as the average, is a central value of a finite set of numbers. Let's assume a random variable X in the data has the following values:
where N is the number of observations or data points in the sample set or simply the data frequency. Then the sample mean defined by , which is very often used to approximate the population mean, can be expressed as follows:
The mean is also referred to as expectation which is often defined by E() or random variable with a bar on the top. For example, the expectation of random variables X and Y, that is E(X) and E(Y), respectively, can be expressed as follows:
The variance measures how far the data points are spread out from the average value, and is equal to the sum of squares of differences between the data values and the average (the mean). Furthermore, the sample variance defined by sigma squared, which can be used to approximate the population variance, can be expressed as follows:
For deriving expectations and variances of different popular probability distribution functions, check out this Github repo.
The standard deviation is simply the square root of the variance and measures the extent to which data varies from its mean. The standard deviation defined by sigma can be expressed as follows:
Standard deviation is often preferred over the variance because it has the same unit as the data points, which means you can interpret it more easily.
The covariance is a measure of the joint variability of two random variables and describes the relationship between these two variables. It is defined as the expected value of the product of the two random variables' deviations from their means. The covariance between two random variables X and Z can be described by the following expression, where E(X) and E(Z) represent the means of X and Z, respectively.
Covariance can take negative or positive values as well as value 0. A positive value of covariance indicates that two random variables tend to vary in the same direction, whereas a negative value suggests that these variables vary in opposite directions. Finally, the value 0 means that they don't vary together.
The correlation is also a measure for relationship and it measures both the strength and the direction of the linear relationship between two variables. If a correlation is detected then it means that there is a relationship or a pattern between the values of two target variables. Correlation between two random variables X and Z are equal to the covariance between these two variables divided to the product of the standard deviations of these variables which can be described by the following expression.
Correlation coefficients' values range between -1 and 1. Keep in mind that the correlation of a variable with itself is always 1, that is Cor(X, X) = 1. Another to keep in mind when interpreting correlation is to not confuse it with causation, given that a correlation is not causation. Even if there is a correlation between two variables, you cannot conclude that one variable causes a change in the other. This relationship could be coincidental, or a third factor might be causing both variables to change.
A function that describes all the possible values, the sample space, and the corresponding probabilities that a random variable can take within a given range, bounded between the minimum and maximum possible values, is called a probability distribution function (pdf) or probability density. Every pdf needs to satisfy the following two criteria:
where the first criterium states that all probabilities should be numbers in the range of [0,1] and the second criterium states that the sum of all possible probabilities should be equal to 1.
Probability functions are usually classified into two categories: discrete and continuous. Discrete distribution function describes the random process with countable sample space, like in the case of an example of tossing a coin that has only two possible outcomes. Continuous distribution function describes the random process with continuous sample space. Examples of discrete distribution functions are Bernoulli, Binomial, Poisson, Discrete Uniform. Examples of continuous distribution functions are Normal, Continuous Uniform, Cauchy.
The binomial distribution is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each with the boolean-valued outcome: success (with probability p) or failure (with probability q = 1  p). Let's assume a random variable X follows a Binomial distribution, then the probability of observing k successes in n independent trials can be expressed by the following probability density function:
The binomial distribution is useful when analyzing the results of repeated independent experiments, especially if one is interested in the probability of meeting a particular threshold given a specific error rate.
Binomial Distribution Mean & Variance
The figure below visualizes an example of Binomial distribution where the number of independent trials is equal to 8 and the probability of success in each trial is equal to 16%.
The Poisson distribution is the discrete probability distribution of the number of events occurring in a specified time period, given the average number of times the event occurs over that time period. Let's assume a random variable X follows a Poisson distribution, then the probability of observing k events over a time period can be expressed by the following probability function:
where e is Euler's number and  lambda, the arrival rate parameter is the expected value of X. Poisson distribution function is very popular for its usage in modeling countable events occurring within a given time interval.
Poisson Distribution Mean & Variance
For example, Poisson distribution can be used to model the number of customers arriving in the shop between 7 and 10 pm, or the number of patients arriving in an emergency room between 11 and 12 pm. The figure below visualizes an example of Poisson distribution where we count the number of Web visitors arriving at the website where the arrival rate, lambda, is assumed to be equal to 7 minutes.
The Normal probability distribution is the continuous probability distribution for a real-valued random variable. Normal distribution, also called Gaussian distribution is arguably one of the most popular distribution functions that are commonly used in social and natural sciences for modeling purposes, for example, it is used to model people's height or test scores. Let's assume a random variable X follows a Normal distribution, then its probability density function can be expressed as follows.
where the parameter  (mu) is the mean of the distribution also referred to as the location parameter, parameter  (sigma) is the standard deviation of the distribution also referred to as the scale parameter. The number  (pi) is a mathematical constant approximately equal to 3.14.
Normal Distribution Mean & Variance
The figure below visualizes an example of Normal distribution with a mean 0 ( = 0) and standard deviation of 1 ( = 1), which is referred to as Standard Normal distribution which is symmetric.
The Bayes Theorem or often called Bayes Law is arguably the most powerful rule of probability and statistics, named after famous English statistician and philosopher, Thomas Bayes.
Bayes theorem is a powerful probability law that brings the concept of subjectivity into the world of Statistics and Mathematics where everything is about facts. It describes the probability of an event, based on the prior information of conditions that might be related to that event. For instance, if the risk of getting Coronavirus or Covid-19 is known to increase with age, then Bayes Theorem allows the risk to an individual of a known age to be determined more accurately by conditioning it on the age than simply assuming that this individual is common to the population as a whole.
The concept of conditional probability, which plays a central role in Bayes theory, is a measure of the probability of an event happening, given that another event has already occurred. Bayes theorem can be described by the following expression where the X and Y stand for events X and Y, respectively:
In the case of the earlier example, the probability of getting Coronavirus (event X) conditional on being at a certain age is Pr (X|Y), which is equal to the probability of being at a certain age given one got a Coronavirus, Pr (Y|X), multiplied with the probability of getting a Coronavirus, Pr (X), divided to the probability of being at a certain age., Pr (Y).
Earlier, the concept of causation between variables was introduced, which happens when a variable has a direct impact on another variable. When the relationship between two variables is linear, then Linear Regression is a statistical method that can help to model the impact of a unit change in a variable, the independent variable on the values of another variable, the dependent variable.
Dependent variables are often referred to as response variables or explained variables, whereas independent variables are often referred to as regressors or explanatory variables. When the Linear Regression model is based on a single independent variable, then the model is called Simple Linear Regression and when the model is based on multiple independent variables, it's referred to as Multiple Linear Regression. Simple Linear Regression can be described by the following expression:
where Y is the dependent variable, X is the independent variable which is part of the data, 0 is the intercept which is unknown and constant, 1 is the slope coefficient or a parameter corresponding to the variable X which is unknown and constant as well. Finally, u is the error term that the model makes when estimating the Y values. The main idea behind linear regression is to find the best-fitting straight line, the regression line, through a set of paired ( X, Y ) data. One example of the Linear Regression application is modeling the impact of Flipper Length on penguins' Body Mass, which is visualized below.
Multiple Linear Regression with three independent variables can be described by the following expression:
The ordinary least squares (OLS) is a method for estimating the unknown parameters such as 0 and 1 in a linear regression model. The model is based on the principle of least squares that minimizes the sum of squares of the differences between the observed dependent variable and its values predicted by the linear function of the independent variable, often referred to as fitted values. This difference between the real and predicted values of dependent variable Y is referred to as residual and what OLS does, is minimizing the sum of squared residuals. This optimization problem results in the following OLS estimates for the unknown parameters 0 and 1 which are also known as coefficient estimates.
Once these parameters of the Simple Linear Regression model are estimated, the fitted values of the response variable can be computed as follows:
The residuals or the estimated error terms can be determined as follows:
It is important to keep in mind the difference between the error terms and residuals. Error terms are never observed, while the residuals are calculated from the data. The OLS estimates the error terms for each observation but not the actual error term. So, the true error variance is still unknown. Moreover, these estimates are subject to sampling uncertainty. What this means is that we will never be able to determine the exact estimate, the true value, of these parameters from sample data in an empirical application. However, we can estimate it by calculating the sample residual variance by using the residuals as follows.
This estimate for the variance of sample residuals helps to estimate the variance of the estimated parameters which is often expressed as follows:
The squared root of this variance term is called the standard error of the estimate which is a key component in assessing the accuracy of the parameter estimates. It is used to calculating test statistics and confidence intervals. The standard error can be expressed as follows:
It is important to keep in mind the difference between the error terms and residuals. Error terms are never observed, while the residuals are calculated from the data.
OLS estimation method makes the following assumption which needs to be satisfied to get reliable prediction results:
A1: Linearity assumption states that the model is linear in parameters.
A2: Random Sample assumption states that all observations in the sample are randomly selected.
A3: Exogeneity assumption states that independent variables are uncorrelated with the error terms.
A4: Homoskedasticity assumption states that the variance of all error terms is constant.
A5: No Perfect Multi-Collinearity assumption states that none of the independent variables is constant and there are no exact linear relationships between the independent variables.
Under the assumption that the OLS criteria A1  A5 are satisfied, the OLS estimators of coefficients 0 and 1 are BLUE and Consistent.
Gauss-Markov theorem
This theorem highlights the properties of OLS estimates where the term BLUE stands for Best Linear Unbiased Estimator.
The bias of an estimator is the difference between its expected value and the true value of the parameter being estimated and can be expressed as follows:
When we state that the estimator is unbiased what we mean is that the bias is equal to zero, which implies that the expected value of the estimator is equal to the true parameter value, that is:
Unbiasedness does not guarantee that the obtained estimate with any particular sample is equal or close to . What it means is that, if one repeatedly draws random samples from the population and then computes the estimate each time, then the average of these estimates would be equal or very close to .
The term Best in the Gauss-Markov theorem relates to the variance of the estimator and is referred to as efficiency. A parameter can have multiple estimators but the one with the lowest variance is called efficient.
The term consistency goes hand in hand with the terms sample size and convergence. If the estimator converges to the true parameter as the sample size becomes very large, then this estimator is said to be consistent, that is:
Under the assumption that the OLS criteria A1  A5 are satisfied, the OLS estimators of coefficients 0 and 1 are BLUE and Consistent.
Gauss-Markov Theorem
All these properties hold for OLS estimates as summarized in the Gauss-Markov theorem. In other words, OLS estimates have the smallest variance, they are unbiased, linear in parameters, and are consistent. These properties can be mathematically proven by using the OLS assumptions made earlier.
The Confidence Interval is the range that contains the true population parameter with a certain pre-specified probability, referred to as the confidence level of the experiment, and it is obtained by using the sample results and the margin of error.
The margin of error is the difference between the sample results and based on what the result would have been if one had used the entire population.
The Confidence Level describes the level of certainty in the experimental results. For example, a 95% confidence level means that if one were to perform the same experiment repeatedly for 100 times, then 95 of those 100 trials would lead to similar results. Note that the confidence level is defined before the start of the experiment because it will affect how big the margin of error will be at the end of the experiment.
As it was mentioned earlier, the OLS estimates of the Simple Linear Regression, the estimates for intercept 0 and slope coefficient 1, are subject to sampling uncertainty. However, we can construct CI's for these parameters which will contain the true value of these parameters in 95% of all samples. That is, 95% confidence interval for  can be interpreted as follows:
95% confidence interval of OLS estimates can be constructed as follows:
which is based on the parameter estimate, the standard error of that estimate, and the value 1.96 representing the margin of error corresponding to the 5% rejection rule. This value is determined using the Normal Distribution table, which will be discussed later on in this article. Meanwhile, the following figure illustrates the idea of 95% CI:
Note that the confidence interval depends on the sample size as well, given that it is calculated using the standard error which is based on sample size.
The confidence level is defined before the start of the experiment because it will affect how big the margin of error will be at the end of the experiment.
Testing a hypothesis in Statistics is a way to test the results of an experiment or survey to determine how meaningful they the results are. Basically, one is testing whether the obtained results are valid by figuring out the odds that the results have occurred by chance. If it is the letter, then the results are not reliable and neither is the experiment. Hypothesis Testing is part of the Statistical Inference.
Firstly, you need to determine the thesis you wish to test, then you need to formulate the Null Hypothesis and the Alternative Hypothesis. The test can have two possible outcomes and based on the statistical results you can either reject the stated hypothesis or accept it. As a rule of thumb, statisticians tend to put the version or formulation of the hypothesis under the Null Hypothesis that that needs to be rejected, whereas the acceptable and desired version is stated under the Alternative Hypothesis.
Let's look at the earlier mentioned example where the Linear Regression model was used to investigating whether a penguins' Flipper Length, the independent variable, has an impact on Body Mass, the dependent variable. We can formulate this model with the following statistical expression:
Then, once the OLS estimates of the coefficients are estimated, we can formulate the following Null and Alternative Hypothesis to test whether the Flipper Length has a statistically significant impact on the Body Mass:
where H0 and H1 represent Null Hypothesis and Alternative Hypothesis, respectively. Rejecting the Null Hypothesis would mean that a one-unit increase in Flipper Length has a direct impact on the Body Mass. Given that the parameter estimate of 1 is describing this impact of the independent variable, Flipper Length, on the dependent variable, Body Mass. This hypothesis can be reformulated as follows:
where H0 states that the parameter estimate of 1 is equal to 0, that is Flipper Length effect on Body Mass is statistically insignificant whereas H0 states that the parameter estimate of 1 is not equal to 0 suggesting that Flipper Length effect on Body Mass is statistically significant.
When performing Statistical Hypothesis Testing one needs to consider two conceptual types of errors: Type I error and Type II error. The Type I error occurs when the Null is wrongly rejected whereas the Type II error occurs when the Null Hypothesis is wrongly not rejected. A confusion matrix can help to clearly visualize the severity of these two types of errors.
As a rule of thumb, statisticians tend to put the version the hypothesis under the Null Hypothesis that that needs to be rejected, whereas the acceptable and desired version is stated under the Alternative Hypothesis.
Once the Null and the Alternative Hypotheses are stated and the test assumptions are defined, the next step is to determine which statistical test is appropriate and to calculate the test statistic. Whether or not to reject or not reject the Null can be determined by comparing the test statistic with the critical value. This comparison shows whether or not the observed test statistic is more extreme than the defined critical value and it can have two possible results:
The critical value is based on a prespecified significance level  (usually chosen to be equal to 5%) and the type of probability distribution the test statistic follows. The critical value divides the area under this probability distribution curve into the rejection region(s) and non-rejection region. There are numerous statistical tests used to test various hypotheses. Examples of Statistical tests are Student's t-test, F-test, Chi-squared test, Durbin-Hausman-Wu Endogeneity test, White Heteroskedasticity test. In this article, we will look at two of these statistical tests.
The Type I error occurs when the Null is wrongly rejected whereas the Type II error occurs when the Null Hypothesis is wrongly not rejected.
One of the simplest and most popular statistical tests is the Student's t-test. which can be used for testing various hypotheses especially when dealing with a hypothesis where the main area of interest is to find evidence for the statistically significant effect of a single variable. The test statistics of the t-test follows Student's t distribution and can be determined as follows:
where h0 in the nominator is the value against which the parameter estimate is being tested. So, the t-test statistics are equal to the parameter estimate minus the hypothesized value divided by the standard error of the coefficient estimate. In the earlier stated hypothesis, where we wanted to test whether Flipper Length has a statistically significant impact on Body Mass or not. This test can be performed using a t-test and the h0 is in that case equal to the 0 since the slope coefficient estimate is tested against value 0.
There are two versions of the t-test: a two-sided t-test and a one-sided t-test. Whether you need the former or the latter version of the test depends entirely on the hypothesis that you want to test.
The two-sided or two-tailed t-test can be used when the hypothesis is testing equal versus not equal relationship under the Null and Alternative Hypotheses that is similar to the following example:
The two-sided t-test has two rejection regions as visualized in the figure below:
In this version of the t-test, the Null is rejected if the calculated t-statistics is either too small or too large.
Here, the test statistics are compared to the critical values based on the sample size and the chosen significance level. To determine the exact value of the cutoff point, the two-sided t-distribution table can be used.
The one-sided or one-tailed t-test can be used when the hypothesis is testing positive/negative versus negative/positive relationship under the Null and Alternative Hypotheses that is similar to the following examples:
One-sided t-test has a single rejection region and depending on the hypothesis side the rejection region is either on the left-hand side or the right-hand side as visualized in the figure below:
In this version of the t-test, the Null is rejected if the calculated t-statistics is smaller/larger than the critical value.
F-test is another very popular statistical test often used to test hypotheses testing a joint statistical significance of multiple variables. This is the case when you want to test whether multiple independent variables have a statistically significant impact on a dependent variable. Following is an example of a statistical hypothesis that can be tested using the F-test:
where the Null states that the three variables corresponding to these coefficients are jointly statistically insignificant and the Alternative states that these three variables are jointly statistically significant. The test statistics of the F-test follows F distribution and can be determined as follows:
where the SSRrestricted is the sum of squared residuals of the restricted model which is the same model excluding from the data the target variables stated as insignificant under the Null, the SSRunrestricted is the sum of squared residuals of the unrestricted model which is the model that includes all variables, the q represents the number of variables that are being jointly tested for the insignificance under the Null, N is the sample size, and the k is the total number of variables in the unrestricted model. SSR values are provided next to the parameter estimates after running the OLS regression and the same holds for the F-statistics as well. Following is an example of MLR model output where the SSR and F-statistics values are marked.
F-test has a single rejection region as visualized below:
If the calculated F-statistics is bigger than the critical value, then the Null can be rejected which suggests that the independent variables are jointly statistically significant. The rejection rule can be expressed as follows:
Another quick way to determine whether to reject or to support the Null Hypothesis is by using p-values. The p-value is the probability of the condition under the Null occurring. Stated differently, the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. The smaller the p-value, the stronger is the evidence against the Null Hypothesis, suggesting that it can be rejected.
The interpretation of a p-value is dependent on the chosen significance level. Most often, 1%, 5%, or 10% significance levels are used to interpret the p-value. So, instead of using the t-test and the F-test, p-values of these test statistics can be used to test the same hypotheses.
The following figure shows a sample output of an OLS regression with two independent variables. In this table, the p-value of the t-test, testing the statistical significance of class_size variable's parameter estimate, and the p-value of the F-test, testing the joint statistical significance of the class_size, and el_pct variables parameter estimates, are underlined.
The p-value corresponding to the class_size variable is 0.011 and when comparing this value to the significance levels 1% or 0.01 , 5% or 0.05, 10% or 0.1, then the following conclusions can be made:
So, this p-value suggests that the coefficient of the class_size variable is statistically significant at 5% and 10% significance levels. The p-value corresponding to the F-test is 0.0000 and since 0 is smaller than all three cutoff values; 0.01, 0.05, 0.10, we can conclude that the Null of the F-test can be rejected in all three cases. This suggests that the coefficients of class_size and el_pct variables are jointly statistically significant at 1%, 5%, and 10% significance levels.
Although, using p-values has many benefits but it has also limitations. Namely, the p-value depends on both the magnitude of association and the sample size. If the magnitude of the effect is small and statistically insignificant, the p-value might still show a significant impact because the large sample size is large. The opposite can occur as well, an effect can be large, but fail to meet the p<0.01, 0.05, or 0.10 criteria if the sample size is small.
Inferential statistics uses sample data to make reasonable judgments about the population from which the sample data originated. It's used to investigate the relationships between variables within a sample and make predictions about how these variables will relate to a larger population.
Both Law of Large Numbers (LLN) and Central Limit Theorem (CLM) have a significant role in Inferential statistics because they show that the experimental results hold regardless of what shape the original population distribution was when the data is large enough. The more data is gathered, the more accurate the statistical inferences become, hence, the more accurate parameter estimates are generated.
Suppose X1, X2, . . . , Xn are all independent random variables with the same underlying distribution, also called independent identically-distributed or i.i.d, where all X's have the same mean  and standard deviation . As the sample size grows, the probability that the average of all X's is equal to the mean  is equal to 1. The Law of Large Numbers can be summarized as follows:
Suppose X1, X2, . . . , Xn are all independent random variables with the same underlying distribution, also called independent identically-distributed or i.i.d, where all X's have the same mean  and standard deviation . As the sample size grows, the probability distribution of X converges in the distribution in Normal distribution with mean  and variance -squared. The Central Limit Theorem can be summarized as follows:
Stated differently, when you have a population with mean  and standard deviation  and you take sufficiently large random samples from that population with replacement, then the distribution of the sample means will be approximately normally distributed.
Dimensionality reduction is the transformation of data from a high-dimensional space into a low-dimensional space such that this low-dimensional representation of the data still contains the meaningful properties of the original data as much as possible.
With the increase in popularity in Big Data, the demand for these dimensionality reduction techniques, reducing the amount of unnecessary data and features, increased as well. Examples of popular dimensionality reduction techniques are Principle Component Analysis, Factor Analysis, Canonical Correlation, Random Forest.
Principal Component Analysis or PCA is a dimensionality reduction technique that is very often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller set that still contains most of the information or the variation in the original large dataset.
Let's assume we have a data X with p variables; X1, X2, ...., Xp with eigenvectors e1, ..., ep, and eigenvalues 1,..., p. Eigenvalues show the variance explained by a particular data field out of the total variance. The idea behind PCA is to create new (independent) variables, called Principal Components, that are a linear combination of the existing variable. The ith principal component can be expressed as follows:
Then using Elbow Rule or Kaiser Rule, you can determine the number of principal components that optimally summarize the data without losing too much information. It is also important to look at the proportion of total variation (PRTV) that is explained by each principal component to decide whether it is beneficial to include or to exclude it. PRTV for the ith principal component can be calculated using eigenvalues as follows:
The elbow rule or the elbow method is a heuristic approach that is used to determine the number of optimal principal components from the PCA results. The idea behind this method is to plot the explained variation as a function of the number of components and pick the elbow of the curve as the number of optimal principal components. Following is an example of such a scatter plot where the PRTV (Y-axis) is plotted on the number of principal components (X-axis). The elbow corresponds to the X-axis value 2, which suggests that the number of optimal principal components is 2.
Factor analysis or FA is another statistical method for dimensionality reduction. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality. Let's assume we have a data X with p variables; X1, X2, ...., Xp. FA model can be expressed as follows:
where X is a [p x N] matrix of p variables and N observations,  is [p x N] population mean matrix, A is [p x k] common factor loadings matrix, F [k x N] is the matrix of common factors and u [pxN] is the matrix of specific factors. So, put it differently, a factor model is as a series of multiple regressions, predicting each of the variables Xi from the values of the unobservable common factors fi:
Each variable has k of its own common factors, and these are related to the observations via factor loading matrix for a single observation as follows: In factor analysis, the factors are calculated to maximize between-group variance while minimizing in-group variance. They are factors because they group the underlying variables. Unlike the PCA, in FA the data needs to be normalized, given that FA assumption that the dataset follows Normal Distribution.
towardsdatascience.com
towardsdatascience.com
Check out this book that introduced essential ML algorithms, topics, and practical Labs in R, Introduction to Statistical Learning
Check out Data Analytics Certification Google where you follow 8 Coursera courses that not only provide theoretical background but also practical guidance about how to create your personal profile, and how to prepare for Data Analytics job interviews.
Check out Data Science Popular Algorithms Github repository containing basic and advanced Data Science algorithms in R, Python, and Scala
medium.com
medium.com
Thanks for the read!
Follow me up on Medium to read more articles about various Data Science and Data Analytics topics. For more hands-on applications of Mathematical and Statistical concepts check out my Github account. I welcome feedback and can be reached out on LinkedIn.
Happy learning!
",85
https://medium.com/@soccermatics/is-googles-ai-research-about-to-implode-4e1ab194fc0e?source=tag_archive---------5-----------------------,Is Google's AI research about to implode?,What does Timnit Gebru's firing and the recent papers coming out of Google tell us about the state of research at the world's biggest AI...,David Sumpter,9,"What does Timnit Gebru's firing and the recent papers coming out of Google tell us about the state of research at the world's biggest AI research department.
The high point for Google's research in to Artifical Intelligence may well turn out to be the 19th of October 2017. This was the date that David Silver and his co-workers at DeepMind published a report, in the journal Nature, showing how their deep-learning algorithm AlphaGo Zero was a better Go player than not only the best human in the world, but all other Go-playing computers.
What was most remarkable about AlphaGo Zero was that it worked without human assistance. The researchers set up a neural network, let it play lots of games of Go against itself and a few days later it was the best Go player in the world. Then they showed it chess and it took only four hours to become the best chess player in the world. Unlike previous game-playing algorithms there was no rulebook built in to the algorithm or specialised search algorithm, just a machine playing game after game, from novice up to master level, all the way up to a level where nobody, computer or person, could beat it.
But there was a problem.
Maybe it wasn't Silver and his colleagues' problem, but it was a problem all the same. The DeepMind research program had shown what deep neural networks could do, but it had also revealed what they couldn't do. For example, although they could train their system to win at Atari games Space Invaders and Breakout, it couldn't play games like Montezuma Revenge where rewards could only be collected after completing a series of actions (for example, climb down ladder, get down rope, get down another ladder, jump over skull and climb up a third ladder). For these types of games, the algorithms can't learn because they require an understanding of the concept of ladders, ropes and keys. Something us humans have built in to our cognitive model of the world. But also something that can't be learnt by the reinforcement learning approach that DeepMind applied.
Another example of the limitations of the deep learning approach can be found in language models. One approach to getting machines to understand language, pursued at Google Brain as well as Open AI and other research institutes, is to train models to predict sequences of words and sentences in large corpuses of text. This approach goes all the way back to 1913 and the work of Andrej Markov, who used it to predict the order of vowels and consonants in Pushkin's novel in verse Eugene Onegin. There are well defined patterns within language and by 'learning' those patterns an algorithm can speak that language.
The pattern detecting approach to langauge is interesting in the sense that it can reproduce paragraphs that seem to make sense, at least superficially. A nice example of this was published in The Guardian in September 2020, where an AI mused on whether computers could bring world peace. But, as Emily Bender, Timnit Gebru and co-workers point out in their recent paper 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?' these techniques do not understand what we are writing. They are simply storing language in a convenient form and the outputs they produce are just parroting the data. And, as the example below shows, these ouputs can be dangerously untrue. Primed with data about QAnon, the GPT-3 language model produces lies and conspiracy theories.
Bender and her colleagues explain that many of the corpuses of text available to researchers  from, for example, Reddit and entertainment news sites  contain incorrect information and are highly biased in the way they represent the world. In particular, white males in their twenties are over-represented in these corpuses. Furthermore, in making certain ""corrections"" to large datasets, for example removing references to sex, the voices of LGBTQ people will be given less prominence. The lack of transparency and accountability in the data makes these models useless for anything other than generating amusing Guardian articles (my words, not the authors). But they have substantial negative consequences: in producing reems of factually incorrect texts and requiring computing resources that can have a major environmental impact.
When Gebru sent the paper with Bender for internal review at Google it was rejected, although it was susequently accepted after rigorous external peer-review for publication. Gebru was later fired for objecting to her manager's request to retract or remove her name from the paper.
Yet she is not alone in critising the way machine learning research is playing out at Google.
In an article that came out a few weeks before Gebru's firing, 40 Google researchers, from throughout the organisation, raised serious issues around machine learning approaches to a wide-range of problems. They used an example of modelling an epidemic to illustrate their concerns. The image below shows two epidemic curves predicted by a machine learning model, but based on different assumptions about disease spread. Both models are equally good, accordning to the algorithm, but they make very different predictions about the eventual size of an epidemic.
This is an example of what is known as an underspecification problem: many models explain the same data. As the 40 researchers wrote, underspecification presents significant challenges for the credibility of modern machine learning. It affects everything from tumour detection to self-driving cars, and (of course) language models.
From my own point of view, from 25 years experience working as an applied mathematician, what I find fascinating about the 40 researchers' article is that applied mathematicians have been aware of the underspecification or identifiction problem for decades. Often there are a whole range of models that might explain a particular data set: how can we identify which one is best? The answer is that we can't. Or at least we can't do so without making and documenting further assumptions. If we want to model the spread of a disease, we make assumptions about peoples behaviour  how they will respond to a lockdown or to a vaccine's arrival, etc. We know that not everything we assume can be measured. Assumptions thus document what we don't know, but think might be the case.
In general, mathematical modelling consists of three components:
1, Assumptions: These are taken from our experience and intuition to be the basis of our thinking about a problem.
2, Model: This is the representation of our assumptions in a way that we can reason (i.e. as an equation or a simulation).
3, Data: This is what we measure and understand about the real world.
Where Google's machine learning programme has been strong in the last five years is on the model (step 2). In particular, they have mastered one particular model: the neural network model of pictures and words. But this is just one model of many, possibly infinite many, alternatives. It is one way of looking at the world. In emphasising the model researchers persuing a pure machine learning approach make a very strong implicit assumption: that their model doesn't need assumptions. As applied mathematicians have long known (and Google's own group of 40 now argue) this simply isn't the case. All models need assumptions.
The 'model free' assumption then leads to the stochastic parrot problem. One of the strengths that Deep Mind emphasise in their neural network approach is that it learns directly from the data. As a result, what their neural network models ultimately learn is nothing more (or less) than the contents of that data. For board games like chess and go or computer games like breakout, this isn't a problem. The researchers can generate an infinite supply of data by simulating the games and win.
But for learning from words or from images, the data is a severely limited for two reasons. Firstly, human conversations are much more complex than games, involving our unspoken understanding of the meaning of objects (like the keys and the skulls in Montezuma Revenge). Secondly, we have access to only very limited data sets. Even billions of words on Reddit and gossip news sites  the favourite source for data  is only a very narrow representation of our language. You don't have to be Wittgenstein to see that there is no way of learning our true language game in this way. The neural networks are nothing more than a compact representation of a gigantic database of words. So when GPT-3 or BERT 'says' something, all it is telling us is about some groups of sentences and grammar structures happen to occur together in the text. And since no assumptions are made by the model, then the only thing the neural networks learn, as Bender et al. point out, is to randomly parrot the contents of Reddit.
The important insight of, in particular, Timnit Gebru, Margaret Mitchell (another Google AI researcher) and Joy Buolamwini (at MIT) is that the technical challenges lie in finding ways to organise and construct language and image datasets in a way that is as 'neutral' as possible (one example from Gebru's work on face recognition is shown in the figure below). And, even more importantly, we have to be honest that true neutrality in language and image data is impossible. If our text and image libraries are formed by and document sexism, systemic racism and violence, how can we expect to find neutrality in this data? The answer is that we can't. If we use models that learn from Reddit with no assumed model, then our assumptions will come from Reddit. Bender, Gebru, Mitchell and Buolamwini show us that, if we are going to take this approach, then our focus should turn to finding ways of documenting data and models in a transparent and auditable way.
Is there hope for Google Brain? One glimmer can be found in the fact that most of the articles I cite here, criticising Google's overall approach, are written by their own researchers. But the fact that they fired Gebru  the author of at least three of the most insightful articles ever produced by their research department and in modern AI as a whole  is deeply worrying. If the leaders who claim to be representing the majority of 'Googlers' can't accept even the mildest of critiques from co-workers, then it does not bode well for the company's innovation. The risk is that the 'assumption free' neural network proponents will double down on their own mistakes.
All great research must rise and then fall, as one model assumption takes over from another. As I wrote at the start of the article, the Deep Mind programme has generated impressive results. Many of the ideas which underlay this research were developed outside of Google  starting with decades of research by, for example, Geoffrey Hinton and colleagues  and were made primarily within the University system. It took those decades to realise the true potential in Hinton's approach. Google gave the final push, providing time and computer resources to make the researchers' dreams a reality.
What concerns me is that when Google's own researchers start to produce novel ideas then the company perceives these as a threat. So much of a threat that they fire their most innovative researcher and shut down the groups that are doing truly novel work. I don't want to downplay the deep instutionalised sexism and racism that is at play in Gebru's firing  that is there for all to see. But I want to emphasise that Google has also done something deeply stupid. Instead of providing time and resources to their greatest minds, they are closing ranks and looking inwards at past achievements.
Its not often a clickbait question headline ends with an affirmative, but in this case it does: Google Brain's research looks like it is on its way to implode.
Maybe one day we will see the transition from Hinton (ways of representing complex data in a neural network) to Gebru (accountable representation of those data sets) in the same way as we see the transition from Newton to Einstein. And when we do, it won't be Google that should take the credit.
",86
https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------3-----------------------,Machine Learning is Fun! Part 2,Using Machine Learning to generate Super Mario Maker levels,Adam Geitgey,15,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in Italiano, Espanol, Francais, Turkce, ,  Portugues, , Tieng Viet or .
Giant update: I've written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
In Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven't already read part 1, read it now!).
This time, we are going to see one of these generic algorithms do something really cool  create video game levels that look like they were made by humans. We'll build a neural network, feed it existing Super Mario levels and watch new ones pop out!
Just like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone  which means that there's a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished.
Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this:
We ended up with this simple estimation function:
In other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house's value.
Instead of using code, let's represent that same function as a simple diagram:
However this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn't so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn't matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model?
To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases:
Now we have four different price estimates. Let's combine those four price estimates into one final estimate. We'll run them through the same algorithm again (but using another set of weights)!
Our new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model.
Let's combine our four attempts to guess into one big diagram:
This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions.
There's a lot that I'm skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click:
It's just like LEGO! We can't model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together:
The neural network we've seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it's a stateless algorithm.
In many cases (like estimating the price of house), that's exactly what you want. But the one thing this kind of model can't do is respond to patterns in data over time.
Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess?
I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter.
Our model might look like this:
But let's make the problem harder. Let's say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem.
Let's use the first few words of Ernest Hemingway's The Sun Also Rises as an example:
Robert Cohn was once middleweight boxi
What letter is going to come next?
You probably guessed 'n'  the word is probably going to be boxing. We know this based on the letters we've already seen in the sentence and our knowledge of common words in English. Also, the word 'middleweight' gives us an extra clue that we are talking about boxing.
In other words, it's easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English.
To solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently.
Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters.
This is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory.
Predicting the next letter in a story might seem pretty useless. What's the point?
One cool use might be auto-predict for a mobile phone keyboard:
But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over  forever? We'd be asking it to write a complete story for us!
We saw how we could guess the next letter in Hemingway's sentence. Let's try generating a whole story in the style of Hemingway.
To do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github.
We'll create our model from the complete text of The Sun Also Rises  362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway's style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example.
As we just start to train the RNN, it's not very good at predicting letters. Here's what it generates after a 100 loops of training:
hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h'e btentmuhgehi bcgdltt. gey heho grpiahe.Ddelnss.eelaishaner"" cot AAfhB ht ltnyehbih a""on bhnte ectrsnae abeahngyamo k ns aeo?cdse nh a taei.rairrhelardr er deffijha
You can see that it has figured out that sometimes words have spaces between them, but that's about it.
After about 1000 iterations, things are looking more promising:
hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been somsinick stalling that aid. ""Hon't me and acrained on .Hw's don't you for the roed,"" In's pair."" ""Alough marith him.""
The model has started to identify the patterns in basic sentence structure. It's adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there's also still a lot of nonsense.
But after several thousand more training iterations, it looks pretty good:
He went over to the gate of the cafe. It was like a country bed. ""Do you know it's been me."" ""Damned us,"" Bill said. ""I was dangerous,"" I said. ""You were she did it and think I would a fine cape you,"" I said. ""I can't look strange in the cab."" ""You know I was this is though,"" Brett said. ""It's a fights no matter?"" ""It makes to do it."" ""You make it?"" ""Sit down,"" I said. ""I wish I wasn't do a little with the man."" ""You found it."" ""I don't know."" ""You see, I'm sorry of chatches,"" Bill said. ""You think it's a friend off back and make you really drunk.""
At this point, the algorithm has captured the basic pattern of Hemingway's short, direct dialog. A few sentences even sort of make sense.
Compare that with some real text from the book:
There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave. ""Sit down,"" said Harvey, ""I've been looking for you."" ""What's the matter?"" ""Nothing. Just looking for you."" ""Been out to the races?"" ""No. Not since Sunday."" ""What do you hear from the States?"" ""Nothing. Absolutely nothing."" ""What's the matter?""
Even by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing!
We don't have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters.
For fun, let's make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of ""Er"", ""He"", and ""The S"":
Not bad!
But the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern.
In 2015, Nintendo released Super Mario MakerTM for the Wii U gaming system.
This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It's like a virtual LEGO set for people who grew up playing Super Mario Brothers.
Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels?
First, we need a data set for training our model. Let's take all the outdoor levels from the original Super Mario Brothers game released in 1985:
This game has 32 levels and about 70% of them have the same outdoor style. So we'll stick to those.
To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game's memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game's memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime.
Here's the first level from the game (which you probably remember if you ever played it):
If we look closely, we can see the level is made of a simple grid of objects:
We could just as easily represent this grid as a sequence of characters with one character representing each object:
We've replaced each object in the level with a letter:
...and so on, using a different letter for each different kind of object in the level.
I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don't really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well.
To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up:
Just like we saw when creating the model of Hemingway's prose, a model improves as we train it.
After a little training, our model is generating junk:
It sort of has an idea that '-'s and '='s should show up a lot, but that's about it. It hasn't figured out the pattern yet.
After several thousand iterations, it's starting to look like something:
The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the ""P""s in the data should appear in 2x2 clusters. That's pretty cool!
With a lot more training, the model gets to the point where it generates perfectly valid data:
Let's sample an entire level's worth of data from our model and rotate it back horizontal:
This data looks great! There are several awesome things to notice:
Finally, let's take this level and recreate it in Super Mario Maker:
Play it yourself!
If you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9-0000-0157-F3C3.
The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a 'toy' instead of cutting-edge is that our model is generated from very little data. There just aren't enough levels in the original Super Mario Brothers game to provide enough data for a really good model.
If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can't  because Nintendo won't let us have them. Big companies don't give away their data for free.
As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That's why companies like Google and Facebook need your data so badly!
For example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate.
But without Google's massive trove of data in every language, you can't create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you've ever been.
In machine learning, there's never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach.
Readers have sent me links to other interesting approaches to generating Super Mario levels:
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I'll only email you when I have something new and awesome to share. It's the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I'd love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 3!
",88
https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29?source=tag_archive---------9-----------------------,Understanding Generative Adversarial Networks (GANs),"Building, step by step, the reasoning that leads to GANs.",Joseph Rocca,20,"This post was co-written with Baptiste Rocca.
Yann LeCun described it as ""the most interesting idea in the last 10 years in Machine Learning"". Of course, such a compliment coming from such a prominent researcher in the deep learning area is always a great advertisement for the subject we are talking about! And, indeed, Generative Adversarial Networks (GANs for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article Generative Adversarial Nets.
So what are Generative Adversarial Networks ? What makes them so ""interesting"" ? In this post, we will see that adversarial training is an enlightening idea, beautiful by its simplicity, that represents a real conceptual progress for Machine Learning and more especially for generative models (in the same way as backpropagation is a simple but really smart trick that made the ground idea of neural networks became so popular and efficient).
Before going into the details, let's give a quick overview of what GANs are made for. Generative Adversarial Networks belong to the set of generative models. It means that they are able to produce / to generate (we'll see how) new content. To illustrate this notion of ""generative models"", we can take a look at some well known examples of results obtained with GANs.
Naturally, this ability to generate new content makes GANs look a little bit ""magic"", at least at first sight. In the following parts, we will overcome the apparent magic of GANs in order to dive into ideas, maths and modelling behind these models. Not only we will discuss the fundamental notions Generative Adversarial Networks rely on but, more, we will build step by step and starting from the very beginning the reasoning that leads to these ideas.
Without further ado, let's re-discover GANs together!
Note: Although we tried to make this article as self-contained as possible, a basic prior knowledge in Machine Learning is still required. Nevertheless, most of the notions will be remained when needed and some references will be given otherwise. We really tried to make this article as smooth to read as possible. Do not hesitate to mention in the comment section what you would have liked to read more about (for possible further articles on the subject).
In the first following section we will discuss the process of generating random variables from a given distribution. Then, in section 2 we will show, through an example, that the problems GANs try to tackle can be expressed as random variable generation problems. In section 3 we will discuss matching based generative networks and show how they answer problems described in section 2. Finally in section 4 we will introduce GANs. More especially, we will present the general architecture with its loss function and we will make the link with all the previous parts.
https://drive.google.com/drive/folders/1lHtjHQ8K7aemRQAnYMylrrwZp6Bsqqrb
In this section, we discuss the process of generating random variables: we remind some existing methods and more especially the inverse transform method that allows to generate complex random variables from simple uniform random variables. Although all this could seems a little bit far from our subject of matter, GANs, we will see in the next section the deep link that exists with generative models.
Computers are fundamentally deterministic. So, it is, in theory, impossible to generate numbers that are really random (even if we could say that the question ""what really is randomness ?"" is a difficult one). However, it is possible to define algorithms that generate sequences of numbers whose properties are very close to the properties of theoretical random numbers sequences. In particular, a computer is able, using a pseudorandom number generator, to generate a sequence of numbers that approximatively follows a uniform random distribution between 0 and 1. The uniform case is a very simple one upon which more complex random variables can be built in different ways.
There exist different techniques that are aimed at generating more complex random variables. Among them we can find, for example, inverse transform method, rejection sampling, Metropolis-Hasting algorithm and others. All these methods rely on different mathematical tricks that mainly consist in representing the random variable we want to generate as the result of an operation (over simpler random variables) or a process.
Rejection sampling expresses the random variable as the result of a process that consist in sampling not from the complex distribution but from a well known simple distribution and to accept or reject the sampled value depending on some condition. Repeating this process until the sampled value is accepted, we can show that with the right condition of acceptance the value that will be effectively sampled will follow the right distribution.
In the Metropolis-Hasting algorithm, the idea is to find a Markov Chain (MC) such that the stationary distribution of this MC corresponds to the distribution from which we would like to sample our random variable. Once this MC found, we can simulate a long enough trajectory over this MC to consider that we have reach a steady state and then the last value we obtain this way can be considered as having been drawn from the distribution of interest.
We won't go any further into the details of rejection sampling and Metropolis-Hasting because these methods are not the ones that will lead us to the notion behind GANs (nevertheless, the interested reader can refer to the pointed Wikipedia articles and links therein). However, let's focus a little bit more on the inverse transform method.
The idea of the inverse transform method is simply to represent our complex  in this article ""complex"" should always be understood in the sense of ""not simple"" and not in the mathematical sense  random variable as the result of a function applied to a uniform random variable we know how to generate.
We consider in what follows a one dimensional example. Let X be a complex random variable we want to sample from and U be a uniform random variable over [0,1] we know how to sample from. We remind that a random variable is fully defined by its Cumulative Distribution Function (CDF). The CDF of a random variable is a function from the domain of definition of the random variable to the interval [0,1] and defined, in one dimension, such that
In the particular case of our uniform random variable U, we have
For simplicity, we will suppose here that the function CDF_X is invertible and its inverse is denoted
(the method can easily be extended to the non-invertible case by using the generalised inverse of the function but it is really not the main point we want to focus on here). Then if we define
we have
As we can see, Y and X have the same CDF and then define the same random variable. So, by defining Y as above (as a function of a uniform random variable) we have managed to define a random variable with the targeted distribution.
To summarise, inverse transform method is a way to generate a random variable that follows a given distribution by making a uniform random variable goes through a well designed ""transform function"" (inverse CDF). This notion of ""inverse transform method"" can, in fact, be extended to the notion of ""transform method"" that consists, more generally, in generating random variables as function of some simpler random variables (not necessarily uniform and then the transform function is no longer the inverse CDF). Conceptually, the purpose of the ""transform function"" is to deform/reshape the initial probability distribution: the transform function takes from where the initial distribution is too high compared to the targeted distribution and puts it where it is too low.
Suppose that we are interested in generating black and white square images of dogs with a size of n by n pixels. We can reshape each data as a N=nxn dimensional vector (by stacking columns on top of each others) such that an image of dog can then be represented by a vector. However, it doesn't mean that all vectors represent a dog once shaped back to a square! So, we can say that the N dimensional vectors that effectively give something that look like a dog are distributed according to a very specific probability distribution over the entire N dimensional vector space (some points of that space are very likely to represent dogs whereas it is highly unlikely for some others). In the same spirit, there exists, over this N dimensional vector space, probability distributions for images of cats, birds and so on.
Then, the problem of generating a new image of dog is equivalent to the problem of generating a new vector following the ""dog probability distribution"" over the N dimensional vector space. So we are, in fact, facing a problem of generating a random variable with respect to a specific probability distribution.
At this point, we can mention two important things. First the ""dog probability distribution"" we mentioned is a very complex distribution over a very large space. Second, even if we can assume the existence of such underlying distribution (there actually exists images that looks like dog and others that doesn't) we obviously don't know how to express explicitly this distribution. Both previous points make the process of generating random variables from this distribution pretty difficult. Let's then try to tackle these two problems in the following.
Our first problem when trying to generate our new image of dog is that the ""dog probability distribution"" over the N dimensional vector space is a very complex one and we don't know how to directly generate complex random variables. However, as we know pretty well how to generate N uncorrelated uniform random variables, we could make use of the transform method. To do so, we need to express our N dimensional random variable as the result of a very complex function applied to a simple N dimensional random variable!
Here, we can emphasise the fact that finding the transform function is not as straightforward as just taking the closed-form inverse of the Cumulative Distribution Function (that we obviously don't know) as we have done when describing the inverse transform method. The transform function can't be explicitly expressed and, then, we have to learn it from data.
As most of the time in these cases, very complex function naturally implies neural network modelling. Then, the idea is to model the transform function by a neural network that takes as input a simple N dimensional uniform random variable and that returns as output another N dimensional random variable that should follow, after training, the the right ""dog probability distribution"". Once the architecture of the network has been designed, we still need to train it. In the next two sections, we will discuss two ways to train these generative networks, including the idea of adversarial training behind GANs!
Disclaimer: The denomination of ""Generative Matching Networks"" is not a standard one. However, we can find in the literature, for example, ""Generative Moments Matching Networks"" or also ""Generative Features Matching Networks"". We just want here to use a slightly more general denomination for what we describe bellow.
So far, we have shown that our problem of generating a new image of dog can be rephrased into a problem of generating a random vector in the N dimensional vector space that follows the ""dog probability distribution"" and we have suggested to use a transform method, with a neural network to model the transform function.
Now, we still need to train (optimise) the network to express the right transform function. To this purpose, we can suggest two different training methods: a direct one and an indirect one. The direct training method consists in comparing the true and the generated probability distributions and backpropagating the difference (the error) through the network. This is the idea that rules Generative Matching Networks (GMNs). For the indirect training method, we do not directly compare the true and generated distributions. Instead, we train the generative network by making these two distributions go through a downstream task chosen such that the optimisation process of the generative network with respect to the downstream task will enforce the generated distribution to be close to the true distribution. This last idea is the one behind Generative Adversarial Networks (GANs) that we will present in the next section. But for now, let's start with the direct method and GMNs.
As mentioned, the idea of GMNs is to train the generative network by directly comparing the generated distribution to the true one. However, we do not know how to express explicitly the true ""dog probability distribution"" and we can also say that the generated distribution is far too complex to be expressed explicitly. So, comparisons based on explicit expressions are not possible. However, if we have a way to compare probability distributions based on samples, we can use it to train the network. Indeed, we have a sample of true data and we can, at each iteration of the training process, produce a sample of generated data.
Although, in theory, any distance (or similarity measure) able to compare effectively two distributions based on samples can be used, we can mention in particular the Maximum Mean Discrepancy (MMD) approach. The MMD defines a distance between two probability distributions that can be computed (estimated) based on samples of these distributions. Although it is not fully out of the scope of this article, we have decided not to spend much more time describing the MMD. However, we have the project to publish soon an article that will contains more details about it. The reader that would like to know more about MMD right now can refer to these slides, this article or this article.
So, once we have defined a way to compare two distributions based on samples, we can define the training process of the generative network in GMNs. Given a random variable with uniform probability distribution as input, we want the probability distribution of the generated output to be the ""dog probability distribution"". The idea of GMNs is then to optimise the network by repeating the following steps:
As written above, when following these steps we are applying a gradient descent over the network with a loss function that is the distance between the true and the generated distributions at the current iteration.
The ""direct"" approach presented above compare directly the generated distribution to the true one when training the generative network. The brilliant idea that rules GANs consists in replacing this direct comparison by an indirect one that takes the form of a downstream task over these two distributions. The training of the generative network is then done with respect to this task such that it forces the generated distribution to get closer and closer to the true distribution.
The downstream task of GANs is a discrimination task between true and generated samples. Or we could say a ""non-discrimination"" task as we want the discrimination to fail as much as possible. So, in a GAN architecture, we have a discriminator, that takes samples of true and generated data and that try to classify them as well as possible, and a generator that is trained to fool the discriminator as much as possible. Let's see on a simple example why the direct and indirect approaches we mentioned should, in theory, lead to the same optimal generator.
In order to better understand why training a generator to fool a discriminator will lead to the same result as training directly the generator to match the target distribution, let's take a simple one dimensional example. We forget, for the moment, how both generator and discriminator are represented and consider them as abstract notions (that will be specified in the next subsection). Moreover, both are supposed ""perfect"" (with infinite capacities) in the sense that they are not constrained by any kind of (parametrised) model.
Suppose that we have a true distribution, for example a one dimensional gaussian, and that we want a generator that samples from this probability distribution. What we called ""direct"" training method would then consist in adjusting iteratively the generator (gradient descent iterations) to correct the measured difference/error between true and generated distributions. Finally, assuming the optimisation process perfect, we should end up with the generated distribution that matches exactly the true distribution.
For the ""indirect"" approach, we have to consider also a discriminator. We assume for now that this discriminator is a kind of oracle that knows exactly what are the true and generated distribution and that is able, based on this information, to predict a class (""true"" or ""generated"") for any given point. If the two distributions are far appart, the discriminator will be able to classify easily and with a high level of confidence most of the points we present to it. If we want to fool the discriminator, we have to bring the generated distribution close to the true one. The discriminator will have the most difficulty to predict the class when the two distributions will be equal in all points: in this case, for each point there are equal chances for it to be ""true"" or ""generated"" and then the discriminator can't do better than being true in one case out of two in average.
At this point, it seems legit to wonder whether this indirect method is really a good idea. Indeed, it seems to be more complicated (we have to optimise the generator based on a downstream task instead of directly based on the distributions) and it requires a discriminator that we consider here as a given oracle but that is, in reality, neither known nor perfect. For the first point, the difficulty of directly comparing two probability distributions based on samples counterbalances the apparent higher complexity of indirect method. For the second point, it is obvious that the discriminator is not known. However, it can be learned!
Let's now describe the specific form that take the generator and the discriminator in the GANs architecture. The generator is a neural network that models a transform function. It takes as input a simple random variable and must return, once trained, a random variable that follows the targeted distribution. As it is very complicated and unknown, we decide to model the discriminator with another neural network. This neural network models a discriminative function. It takes as input a point (in our dog example a N dimensional vector) and returns as output the probability of this point to be a ""true"" one.
Notice that the fact that we impose now a parametrised model to express both the generator and the discriminator (instead of the idealised versions in the previous subsection) has, in practice, not a huge impact on the theoretical argument/intuition given above: we just then work in some parametrised spaces instead of ideal full spaces and, so, the optimal points that we should reach in the ideal case can then be seen as ""rounded"" by the precision capacity of the parametrised models.
Once defined, the two networks can then be trained jointly (at the same time) with opposite goals :
So, at each iteration of the training process, the weights of the generative network are updated in order to increase the classification error (error gradient ascent over the generator's parameters) whereas the weights of the discriminative network are updated so that to decrease this error (error gradient descent over the discriminator's parameters).
These opposite goals and the implied notion of adversarial training of the two networks explains the name of ""adversarial networks"": both networks try to beat each other and, doing so, they are both becoming better and better. The competition between them makes these two networks ""progress"" with respect to their respective goals. From a game theory point of view, we can think of this setting as a minimax two-players game where the equilibrium state corresponds to the situation where the generator produces data from the exact targeted distribution and where the discriminator predicts ""true"" or ""generated"" with probability 1/2 for any point it receives.
Note: This section is a little bit more technical and not absolutely necessary for the overall understanding of GANs. So, the readers that don't want to read some mathematics right now can skip this section for the moment. For the others, let's see how the intuitions given above are mathematically formalised.
Disclaimer: The equations in the following are not the ones of the article of Ian Goodfellow. We propose here an other mathematical formalisation for two reasons: first, to stay a little bit closer to the intuitions given above and, second, because the equations of the original paper are already so clear that it would not have been useful to just rewrite them. Notice also that we absolutely do not enter into the practical considerations (vanishing gradient or other) related to the different possible loss functions. We highly encourage the reader to also take a look at the equations of the original paper: the main difference is that Ian Goodfellow and co-authors have worked with cross-entropy error instead of absolute error (as we do bellow). Moreover, in the following we assume a generator and a discriminator with unlimited capacity.
Neural networks modelling essentially requires to define two things: an architecture and a loss function. We have already described the architecture of Generative Adversarial Networks. It consists in two networks:
Let's take now a closer look at the ""theoretical"" loss function of GANs. If we send to the discriminator ""true"" and ""generated"" data in the same proportions, the expected absolute error of the discriminator can then be expressed as
The goal of the generator is to fool the discriminator whose goal is to be able to distinguish between true and generated data. So, when training the generator, we want to maximise this error while we try to minimise it for the discriminator. It gives us
For any given generator G (along with the induced probability density p_g), the best possible discriminator is the one that minimises
In order to minimise (with respect to D) this integral, we can minimise the function inside the integral for each value of x. It then defines the best possible discriminator for a given generator
(in fact, one of the best because x values such that p_t(x)=p_g(x) could be handled in another way but it doesn't matter for what follows). We then search G that maximises
Again, in order to maximise (with respect to G) this integral, we can maximise the function inside the integral for each value of x. As the density p_t is independent of the generator G, we can't do better than setting G such that
Of course, as p_g is a probability density that should integrate to 1, we necessarily have for the best G
So, we have shown that, in an ideal case with unlimited capacities generator and discriminator, the optimal point of the adversarial setting is such that the generator produces the same density as the true density and the discriminator can't do better than being true in one case out of two, just like the intuition told us. Finally, notice also that G maximises
Under this form, we better see that G wants to maximise the expected probability of the discriminator to be wrong.
The main takeaways of this article are:
Even if the ""hype"" that surrounds GANs is maybe a little bit exaggerated, we can say that the idea of adversarial training suggested by Ian Goodfellow and its co-authors is really a great one. This way to twist the loss function to go from a direct comparison to an indirect one is really something that can be very inspiring for further works in the deep learning area. To conclude, let's say that we don't know if the idea of GANs is really ""the most interesting idea in the last 10 years in Machine Learning""... but it's pretty obvious that it is, at least, one of the most interesting!
Thanks for reading!
Note: We highly recommend the interested readers to both read the initial paper ""Adversarial Neural Nets"", that is really a model of clarity for a scientific paper, and watch the lecture video about GANs of Ali Ghodsi, who is truly an amazing lecturer/teacher. Additional explanation can be found in the tutorial about GANs written by Ian Goodfellow.
Other articles written with Baptiste Rocca:
towardsdatascience.com
towardsdatascience.com
",89
https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681?source=tag_archive---------2-----------------------,17 types of similarity and dissimilarity measures used in data science.,"The following article explains various methods for computing distances and showing their instances in our daily lives. Additionally, it...",Mahmoud Harmouch,25,"""There is no Royal Road to Geometry.""  Euclid
Quick note: Everything written and visualized has been created by the author unless it was specified. Illustrations and equations were generated using tools like Matplotlib, Tex, Scipy, Numpy and edited using GIMP.
 Similarity and dissimilarity:
In data science, the similarity measure is a way of measuring how data samples are related or closed to each other. On the other hand, the dissimilarity measure is to tell how much the data objects are distinct. Moreover, these terms are often used in clustering when similar data samples are grouped into one cluster. All other data samples are grouped into different ones. It is also used in classification(e.g. KNN), where the data objects are labeled based on the features' similarity. Another example is when we talk about dissimilar outliers compared to other data samples(e.g., anomaly detection).
The similarity measure is usually expressed as a numerical value: It gets higher when the data samples are more alike. It is often expressed as a number between zero and one by conversion: zero means low similarity(the data objects are dissimilar). One means high similarity(the data objects are very similar).
Let's take an example where each data point contains only one input feature. This can be considered the simplest example to show the dissimilarity between three data points A, B, and C. Each data sample can have a single value on one axis(because we only have one input feature); let's denote that as the x-axis. Let's take two points, A(0.5), B(1), and C(30). As you can tell, A and B are close enough to each other in contrast to C. Thus, the similarity between A and B is higher than A and C or B and C. In other terms, A and B have a strong correlation. Therefore, the smaller the distance is, the larger the similarity will get.
 Metric:
A given distance(e.g. dissimilarity) is meant to be a metric if and only if it satisfies the following four conditions:
1- Non-negativity: d(p, q)  0, for any two distinct observations p and q.
2- Symmetry: d(p, q) = d(q, p) for all p and q.
3- Triangle Inequality: d(p, q)  d(p, r) + d(r, q) for all p, q, r.
4- d(p, q) = 0 only if p = q.
Distance measures are the fundamental principle for classification, like the k-nearest neighbor's classifier algorithm, which measures the dissimilarity between given data samples. Additionally, choosing a distance metric would have a strong influence on the performance of the classifier. Therefore, the way you compute distances between the objects will play a crucial role in the classifier algorithm's performance.
The technique used to measure distances depends on a particular situation you are working on. For instance, in some areas, the euclidean distance can be optimal and useful for computing distances. Other applications require a more sophisticated approach for calculating distances between points or observations like the cosine distance. The following enumerated list represents various methods of computing distances between each pair of data points.
The most common distance function used for numeric attributes or features is the Euclidean distance which is defined in the following formula:
As you may know, this distance metric presents well-known properties, like symmetrical, differentiable, convex, spherical...
In 2-dimensional space, the previous formula can be expressed as:
which is equal to the length of the hypotenuse of a right-angle triangle.
Moreover, the Euclidean distance is a metric because it satisfies its criterion, as the following illustration shows.
Furthermore, the distance calculated using this formula represents the smallest distance between each pair of points. In other words, it is the shortest path to go from point A to point B(2-D Cartesian coordinate system), as the following figure illustrates:
Thus, it is useful to use this formula whenever you want to compute the distance between 2 points in the absence of obstacles on the pathway. This can be considered one of the situations where you don't want to compute the euclidean distance; instead, you want to use other metrics like the Manhattan distance, which will be explained later throughout this article.
Another scenario where the euclidean distance fails to give us useful information is in a plane's flight path that follows the earth's curvature, not a straight line(unless the earth is flat, which is not!).
However, before going any further, let's explain how we can use the euclidean distance in the context of machine learning.
One of the most famous classification algorithms, the KNN algorithm, can benefit from using the euclidean distance to classify data. To demonstrate how the KNN works with the euclidean metric, the popular iris dataset from the Scipy package has been chosen.
As you may know, this dataset contains three kinds of flowers: Iris-Setosa, Iris-Versicolor, and Iris-Virginica, having the following four features: sepal length, sepal width, petal length, petal width. Therefore, we have a 4-dimensional space where each data point can be represented in.
For simplicity and demonstration purposes, let's choose only two features: petal length, petal width, and excluding Iris-virginica data. In this way, we can plot the data points in a 2-D space where the x-axis and the y-axis represent the petal length and the petal width, respectively.
Each data point came along with its own label: Iris-Setosa or Iris-versicolor(0 and 1 in the dataset). Thus, the dataset can be used in KNN classification because it is a supervised ML algorithm by its nature. Let's assume that our ML model(KNN with k = 4) has been trained on this dataset where we have selected two input features and only twenty data points, as the previous graph shows.
Until this point, everything looks great, and our KNN classifier is ready to classify a new data point. Therefore, we need a way to let the model decide where the new data point can be classified.
As you may be thinking, the euclidean distance has been chosen to let each trained data point vote where the new data sample can fit: Iris-Setosa or Iris-versicolor. Thus, the euclidean distance has been calculated from the new data point to each point of our training data, as the following figure shows:
With k = 4, The KNN classifier requires to chose the smallest four distances, which represents the distance from the new point to the following points: point1, point5, point8, and point9 as the graph shows:
Therefore, the new data sample has been classified as Iris-Setosa. Using this analogy, you can imagine higher dimensions and other classifiers. Hopefully, You get the idea!
As stated earlier, each domain requires a specific way of computing distances. As we progress more throughout this article, you will find out what is meant by stating this.
Computing distances using this approach avoids the need to use the squared root function. As the name reflects, the SED is equal to the euclidean distance squared. Therefore, SED can reduce computational work while calculating distances between observations. For instance, it can be used in clustering, classification, image processing, and other domains.
This metric is very useful in measuring the distance between two streets in a given city, where the distance can be measured in terms of the number of blocks that separate two different places. For instance, according to the following image, the distance between point A and point B is roughly equal to 4 blocks.
This method was created to solve computing the distance between source and destination in a given city where it is nearly impossible to move in a straight line because of the buildings grouped into a grid that blocks the straight pathway. Hence the name City Block.
You could say that the distance between A and B is the euclidean one. But, as you may notice that this distance is not useful. For instance, you need to have a useful distance to estimate the travel time or how long you have to drive. Instead, it would help if you had the shortest path using streets. So it depends on the situation on how you can define and use distance.
In n-dimensional space, the Manhattan distance is expressed as:
For a 2-dimensional grid, the previous formula can be written as:
Recall from the previous KNN example, Computing the manhattan distance from a new data point to the training data will produce the following values:
As you can see, two data points have voted for Iris-Setosa, and another two points have voted for Iris-versicolor, which means a tie.
I think you may have encountered this problem somewhere. An intuitive solution would be changing the value of k, decreasing by one if k is larger than one, or increasing by one, otherwise.
However, you will get different behavior for the KNN classifier for each of the previous solutions. For instance, in our example, k=4. Changing it to k=3 would result in the following values:
And the flower has been classified as Iris-versicolor. In the same manner, Changing it to k=5 would result in the following values:
And the flower is classified as Iris-Setosa. So, it is up to you to decide whether you need to increase or decrease the value of k.
However, someone would argue that you can change the metric of measure if it is not a constraint for the problem. For example, computing the euclidean distance would solve this:
And the flower is strongly classified as Iris-Setosa.
In my opinion, if you don't have to change the Manhattan distance and use the same value for k, adding a new dimension or feature, if available, would also break the tie. For instance, adding the sepal width as a new dimension would lead to the following results:
And the flower is classified as Iris-Versicolor.
And here goes the plot in 3-D space where x-axis, y-axis, and z-axis represent sepal width, petal length, and petal width, respectively:
Computing Manhattan distance is computationally faster than the previous two methods. As the formula shows, it only requires additions and subtractions, which turns out to be much faster than computing square root and the power of two.
If you have ever played chess, The Manhattan distance is used by bishops in order to move between two horizontal or vertical blocks of the same color:
In other terms, the number of moves(distance) required to get the bishop over the red squares is equal to the Manhattan distance, which is two.
Aside from that, the Manhattan distance would be preferred over the Euclidean distance if the data present many outliers.
Moreover, L1-norm gives a more sparse estimation than l2-norm.
Besides that, L1-norm and L2-norm are commonly used in Regularization for a neural network to minimize the weights or zero out some values, like the one used on lasso regression.
As you can see in the figure above, L1-norm tries to zero out the W1 weight and minimize the other one. However, L2-norm tries to minimize both W1 and W2 weights(like W1 = W2).
In the meantime, I don't want this article to take a deep dive into regularization because its main goal is to explain common distance functions while stating some usage here and there and making it as digestible as it would be. Thus, let's move on.
It is a weighted version of manhattan distance used in Clustering, like Fuzzy Clustering, classification, computer security, and ham/spam detection systems. It is more robust to outliers in contrast to the previous metric.
The Chebyshev distance among two n-D observations or vectors is equal to the maximum absolute value of the variations between the data samples' coordinates. In a 2-D world, the Chebyshev distance between data points can be determined as the sum of absolute differences of their 2-dimensional coordinates.
The Chebyshev distance between two points P and Q is defined as:
The Chebyshev distance is a metric because it satisfies the four conditions for being a metric.
However, you may be wondering if the min function can also be a metric!
The min function is not a metric because there is a counterexample(e.g. horizontal or vertical line) where d(A, B) = 0 and A != B. But, It should be equal to zero only if A = B!!!!
One of the use cases you can think of that uses Chebyshev metric is trading stocks, cryptocurrencies where the features are like volume, Bid, Ask... For instance, you need to find a way that tells the most cryptocurrency that has a big gap between rewards and losses. And it turns out that Chebyshev distance is a good fit for that particular situation.
Another common scenario where the Chebyshev distance is used in chessboard where the number of moves for a king, or queen, is equal to the distance in order to reach a neighbor square as the following figure shows:
The Minkowski distance is just a generalization of the previous distance metrics: Euclidean, Manhattan, and Chebyshev. It is defined as the distance between two observations in the n-D space as the following formula demonstrate:
Where P, Q are two given n-D points, and p represents the Minkowski metric. For a particular value of p, you can derive the following metrics:
This metric is widely used in text mining, natural language processing, and information retrieval systems. For instance, it can be used to measure the similarity between two given documents. It can also be used to identify spam or ham messages based on the length of the message.
The Cosine distance can be measured as follows:
Where P and Q represent two given points. These two points can represent the frequencies of words in documents which is explained in the following example.
Let's take, for instance, three documents that contain the following phrases:
Computing the frequency, occurrence for each word would result in the following:
Before computing the number of occurrences, you already knew a priori that documents A and B are very similar in the meaning: ""I love to drink coffee."" However, document C contains all the words of document A but very dissimilar in the meaning, from the frequencies table. To solve this issue, you need to compute the cosine similarity to find whether they are similar or not.
On the other hand, this can illustrate how information retrieval, or search engine, works. Think of document A as a query(short message) for a given source(image, text, video...) and document C as the web page that needs to be fetched and returned as a response to the query.
On the other side, the euclidean distance would fail to give the correct distance between short and large documents because it would be huge in this situation. Using the cosine similarity formula would compute the difference between the two documents in terms of directions and not magnitude.
To illustrate this, let's take the following two documents:
And let's denote by the word ""Bitcoin"" as the x-axis and the word ""Money"" as the y-axis. This means that document A can be represented as a vector A(3,1) and document B as B(2,2).
Computing the cosine similarity would result in the following value:
Cosine_Similarity = 0.894 means that documents A and B, are very similar. The cos(angle) is large(close to one) means the angle is small(26.6), the two documents A and B are closed to each other.
However, you can't interpret the value of the Cosine Similarity as a percentage. For instance, the value 0.894 doesn't mean that document A is 89.4%, similar to B. It means that documents A and B are very similar, but we don't know how much percentage! There is no threshold for that value. In other words, You can interpret the value of the Cosine Similarity as follows:
The larger it gets, the more likely that documents A and B are similar, and vice versa.
Let's take another example for A(1, 11) and B(22, 3)
However, the euclidean distance would give a large number like 22.4, which doesn't tell the relative similarity between the vectors.
On the other hand, the cosine similarity also works well for higher dimensions.
Another interesting application of cosine similarity is the OpenPose project.
Congrats ! You have made it halfway . Keep it up!
The Correlation distance quantifies the strength of the linear, monotonic relationship between two attributes. Furthermore, It uses the covariance value as an initial computational step. However, the covariance itself is hard to interpret and doesn't show how much the data are close or far from the line representing the trend between the measurements.
To show what correlation means, let's go back to our Iris dataset and plot the Iris-Setosa samples to show the relationship between the two features: The petal length and the petal width.
The sample mean values and the variance for the two features for the same flower samples have been estimated, as the next figure shows.
Generally speaking, we can say that flowers with relatively low petal length values also have relatively low values in petal width. Also that flowers with relatively high values in petal length also have relatively high values in petal width. Additionally, we can summarize this relationship with a line.
This line represents the positive trend where both values of petal length and petal width increase together.
The Covariance value can classify three types of relationships:
The correlation distance can be calculated using the following formula:
Where the numerator represents the covariance value of the observations, and the denominator represents the square root of the variance for each feature.
Let's take a simple example to demonstrate how we can compute this formula.
The red and blue points have the following coordinates, respectively:
A(1.2, 0.6) and B (3.0, 1.2).
The estimated sample means for both measurements are equals to:
One last point on this metric is that correlation doesn't mean causation. For instance, an iris-Setosa with a relatively small petal length value doesn't mean that the petal width's value should also be small. It is a sufficient but not Necessary Condition! One could say that small petal length probably contributes to small petal width, but not the only cause!
Like Pearson correlation, Spearman correlation is used whenever we are dealing with bivariate analysis. However, unlike Pearson correlation, Spearman correlation is used when both variables are rank-ordered. It can be used for both categorical and numerical attributes.
Spearman correlation index can be calculated using the following formula:
Spearman correlation is often used in hypothesis testing.
It is a metric of measure mostly used in multivariate statistical testing where the euclidean distance fails to give the real distance between observations. It measures how far away a data point from the distribution.
As shown in the previous image, the red and blue points have the same Euclidean distance from the mean. However, they don't fall in the same region or cluster: The red point is more likely similar to the data set. But the blue one is considered an outlier because it is far away from the line that represents the direction of the greatest variability in the dataset(major axis regression). Therefore, the Mahalanobis metric was introduced to solve this issue.
The Mahalanobis metric tries to decrease the covariance between the two features or attributes in the sense that you can rescale the previous plot into new axes. And these new axes represent the eigenvectors like the first eigenvector as previously shown.
This first direction of the eigenvector greatly influences the data classification because it has the largest eigenvalue. Furthermore, the dataset is spread out along this direction more than the other perpendicular direction.
Using this technique, we can shrink the dataset along with this direction and rotate it around the mean(PCA). Then we can use the euclidean distance, which gives different distances from the mean between the previous two data points. And that's what the Mahalanobis metric does.
Where C represents the covariance matrix between the attributes or features.
To demonstrate this formula's usage, let's compute the distance between A(1.2, 0.6) and B (3.0, 1.2) from our previous example in the correlation distance section.
Let's now evaluate the covariance matrix, which is defined as follows:
Where Cov[P,P] = Var[P] and Cov[Q,Q]= Var[Q], and
Therefore, the Mahalanobis distance between the two objects A and B can be calculated as follows:
In addition to its use cases, The Mahalanobis distance is used in the Hotelling t-square test.
Standardization or normalization is a technique used in the preprocessing stage when building a machine learning model. The dataset presents a high difference between the minimum and the maximum ranges of features. This scale distance would affect the ML model when clustering data, leading to a wrong interpretation.
For instance, let's take a situation where we have two different features that present a large difference in range variations. For example, let's say we have a feature that varies from 0.1 to 2 and another feature that goes from 50 to 200. Computing distance using these values would make the second feature more dominant, leading to incorrect results. In other terms, the euclidean distance will be highly influenced by attributes that have the largest values.
That's why standardization is a necessity in order to let the features contribute in an equal manner. It is done by transforming variables to all have the same variance that is equal to one and centralize the features around the mean like the following formula show:
The standardized Euclidean distance can be expressed as follows:
We can apply this formula to compute the distance between A and B.
Chi-square distance is commonly used in computer vision while doing texture analysis in order to find (dis)similarities between normalized histograms, known as ""Histogram matching"".
A face recognition algorithm would be a great example that uses this metric in order to compare two histograms. For instance, in the prediction step of a new face, the model computes the histogram from the newly captured image, compared it with the saved histograms(often stored in a .yaml file), and then tries to find the best match for it. This comparison is made via computing the Chi-square distance between each pair of histograms of n bins.
As you may know, this formula is different from the Chi-square statistics test for standard normal distributions, where it is used to decide whether to retain or reject the null hypothesis using the following formula:
Where O and E represent the observed and expected data values, respectively.
To illustrate how it is used, let's say a survey was done on 1000 persons to test a given vaccine's side effect and see if there any significant difference based on gender. Therefore, each person can be one of four categories:
1- Male without side effects.
2- Male with side effects.
3- Female without side effects.
4- Female with side effects.
The null hypothesis is: there is no significant difference in side effects between the two genders.
In order to retain or reject this hypothesis, you compute the Chi-square test value for the following data:
By plugging these values into the Chi-square test formula, you will get 1.7288.
Using a Chi-square table with a degree of freedom equal to 1, you will get a probability between 0.2 and 0.1 > 0.05  you retain the null hypothesis.
Note that degree of freedom = (number of columns -1) x (number or rows -1)
Besides, I just wanted to give you a quick refresher on the hypothesis testing; I hope you find it helpful. Anyway, let's keep things moving.
The Jensen-Shannon distance computes the distance between two probability distributions. It uses the Kullback Leibler divergence(The relative entropy) formula in order to find the distance.
Where R is the midpoint between P and Q.
Besides, Just a quick note on how you can interpret the value of entropy:
Low entropy for event A means that there is a high knowing that this event will occur; in other terms, I am less surprised if event A is going to happen, and I am highly confident that it will happen. The same analogy for High entropy.
On the other hand, the Kullback Leibler divergence itself is not a distance metric since it is not symmetric: D(P || Q) != D(Q || P).
A metric for measuring similarity between two strings. It is equal to the minimum number of operations required to transform a given string into another one. There are three types of operations:
For Levenshtein distance, the substitution cost is two units and one for the other two operations.
For instance, let's take two strings s= ""Bitcoin"" and t = ""Altcoin"". To go from s to t, you need two substitutions of the letters ""B"" and ""I"" by the letters ""A"" and ""l"". Thus, the d(t, s) = 2 * 2 = 4.
There are many use cases for the Levenshtein distance like spam filtering, computational biology, Elastic search, and many more.
The hamming distance is equal to the number of digits where two codewords of the same length differ. In the binary world, it is equal to the number of different bits between two binary messages.
For example, the hamming distance between two messages can be calculated using:
As you may notice, it looks like the manhattan distance in the context of categorical data.
For messages of length 2 bits, this formula represents the number of edges that separate two given binary messages. It can be at most equal to two.
In the same manner, for messages of length 3 bits, this formula represents the number of edges that separates two given binary messages. It can be at most equal to three.
Let's take some example to illustrate how the hamming distance is calculated:
H(100001, 010001) = 2
H(110, 111) = 1
If one of the messages contains all zeros, the hamming distance is called hamming weight and equal to the number of non-zero digits in a given message. In our case, it is equal to the total number of ones.
H(110111,000000) = W (110111) = 5
The hamming distance is used to detect and correct, if possible, errors in received messages that have been transmitted over an unreliable noisy channel.
A metric used to measure the similarity between two sets of data. One may argue that in order to measure similarity is to compute the size(cardinality, number of elements.) of the intersection between two given sets.
However, the number of common elements alone cannot tell us how relative it is compared to the sets' size. Hence the intuition behind the Jaccard coefficient.
So Jaccard proposes that, in order to measure similarity, you need to divide the size of the intersection by the size of the union for the two sets of data.
Jaccard distance is complementary to the Jaccard coefficient to measures dissimilarity between data sets and is calculated by:
The following illustration explains how this formula can be used for non-binary data.
For binary attributes, the Jaccard similarity is calculated using the following formula:
Jaccard index can be useful in some domains like semantic segmentation, text mining, E-Commerce, and recommendation systems.
Now you may be thinking: ""Ok, but you have just mentioned earlier that cosine distance can also be used in text mining. What do you prefer to use as a metric for a given clustering algorithm? What the difference between both metrics anyway?""
Glad you asked this question. In order to answer this, we need to compare each term of the two formulas.
The only difference between the two formulas is the denominator term. Instead of computing the union's size between the two sets with Jaccard, you are computing the magnitude of the dot product between P and Q. Instead of adding the terms in the Jaccard formula's denominator; you are computing the product between the two in the cosine formula. I don't know what the interpretation of that is. As far as I know, The dot product gives us how much one vector goes in the direction of another. Other than that, if you have something to add, I am so grateful to read your thoughts down below in the comments.
The Srensen-Dice distance is a statistical metric used to measure the similarity between sets of data. It is defined as two times the size of the intersection of P and Q, divided by the sum of elements in each data set P and Q.
Like Jaccard, the similarity values range from zero to one. However, unlike Jaccard, this dissimilarity measure is not a metric since it doesn't satisfy the triangle inequality condition.
Srensen-Dice is used in lexicography, image segmentation, and other applications.
In the past few weeks, while I was researching similarity and dissimilarity measures, I thought it would be a fun/great idea to reimplement these types of measures in python as a coding practice. I took the inspiration from Piotr Dollar's toolbox repo written in Matlab and can also be found on the MathWorks website. Therefore, pydist2 is a python package, 1:1 code adoption of pdist and pdist2 Matlab functions, for computing distance between observations. The list of methods of measuring the distance currently supported by pydist2 is available at read the docs.
Hooray!!! You have just reached the end of this article. Hopefully, you didn't get overwhelmed by its content at this point, which I tried to make as concise and clear as possible.
Throughout this article, you have learned different types of metrics used in data science and their applications in many areas.
For me, writing this article was an enjoyable experience in my data science journey. It can stress the fact of spending countless hours researching and exploring a mathematical branch of data science and machine learning as well as practicing my coding skills while writing my first python package: pydist2.
If you want to contribute to this project, I am very welcoming to see your pull requests on pydist2 repo. You can Checkout this tutorial if you want to contribute.
If you have noticed any mistakes while reading this article, please mention them in the comments below In order to improve the content.
If you have any suggestions, drop me a message on LinkedIn or send me an email.
If you want to use anything from this article, please cite it as:
Mahmoud Harmouch, 17 types of similarity and dissimilarity measures used in data science, medium.com. Mar-14-2021
If my analytical skills have convinced you and you want to see more, following me and/or sharing this article would help me gain exposure in the data science community and spread useful information.
And that's it for today's blog. Good luck in your own data science journey! Thanks for reading, and see you in the next one!
",90
https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49?source=tag_archive---------2-----------------------,Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,(An alternate version of this article was originally published in the Boston Globe),Scott Santens,14,"(An alternate version of this article was originally published in the Boston Globe)
On December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as Chicago Pile-1, it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words.
Now, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it's vital we understand this new language, and what it's increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it.
The language is a new class of machine learning known as deep learning, and the ""whispered word"" was a computer's use of it to seemingly out of nowhere defeat three-time European Go champion Fan Hui, not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world's best living Go players, if not the best. Imagining such a grand duel of man versus machine, China's top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most.
What actually ended up happening when they faced off? Lee went on to lose all but one of their match's five games. An AI named AlphaGo is now a better Go player than any human and has been granted the ""divine"" rank of 9 dan. In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue.
""AlphaGo's historic victory is a clear signal that we've gone from linear to parabolic.""
So, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what's happening is no game.
AlphaGo's historic victory is a clear signal that we've gone from linear to parabolic. Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income.
This may all sound like exaggeration, so let's take a few decade steps back, and look at what computer technology has been actively doing to human employment so far:
Let the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It's already here. Computer technology is already eating jobs and has been since 1990.
All work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn't change, and that work can be better handled by machines.
Distressingly, it's exactly routine work that once formed the basis of the American middle class. It's routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it's routine cognitive work that once filled US office spaces. Such jobs are now increasingly unavailable, leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them.
If we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That's what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to learn.
I'm a writer at heart, but my educational background happens to be in psychology and physics. I'm fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as cognitive neuroscience. I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me.
As a quick primer in the way our brains function, they're a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It's all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we've been learning about how we work, we've now begun applying to the way machines work.
One of these applications is the creation of deep neural networks - kind of like pared-down virtual brains. They provide an avenue to machine learning that's made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It's not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka big data.
Big data isn't just some buzzword. It's information, and when it comes to information, we're creating more and more of it every day. In fact we're creating so much that a 2013 report by SINTEF estimated that 90% of all information in the world had been created in the prior two years. This incredible rate of data creation is even doubling every 1.5 years thanks to the Internet, where in 2015 every minute we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets. Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn to learn. Why?
Imagine programming a computer to recognize a chair. You'd need to enter a ton of instructions, and the result would still be a program detecting chairs that aren't, and not detecting chairs that are. So how did we learn to detect chairs? Our parents pointed at a chair and said, ""chair."" Then we thought we had that whole chair thing all figured out, so we pointed at a table and said ""chair"", which is when our parents told us that was ""table."" This is called reinforcement learning. The label ""chair"" gets connected to every chair we see, such that certain neural pathways are weighted and others aren't. For ""chair"" to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains.
The power of deep learning is that it's a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing ""chairness"" to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of ""chairness."" Next we test it with even more images. Where it's wrong, we correct it, which further improves its ""chairness"" detection. Repetition of this process results in a computer that knows what a chair is when it sees it, for the most part as well as we can. The important difference though is that unlike us, it can then sort through millions of images within a matter of seconds.
This combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo, Google's DeepMind AI learned how to read and comprehend what it read through hundreds of thousands of annotated news articles. DeepMind also taught itself to play dozens of Atari 2600 video games better than humans, just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions, attaining International Master level status in just 72 hours by repeatedly playing itself. In 2015, an AI even passed a visual Turing test by learning to learn in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all major milestones in AI.
However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to the announcement by Google of AlphaGo's victory, was by experts essentially, ""Maybe in another ten years."" A decade was considered a fair guess because Go is a game so complex I'll just let Ken Jennings of Jeopardy fame, another former champion human defeated by AI, describe it:
Go is famously a more complex game than chess, with its larger board, longer games, and many more pieces. Google's DeepMind artificial intelligence team likes to say that there are more possible Go boards than atoms in the known universe, but that vastly understates the computational problem. There are about 10170 board positions in Go, and only 1080 atoms in the universe. That means that if there were as many parallel universes as there are atoms in our universe (!), then the total number of atoms in all those universes combined would be close to the possibilities on a single Go board.
Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what feels like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and playing itself millions of times. So the answer to when the game of Go would fall to machines wasn't even close to ten years. The correct answer ended up being, ""Any time now.""
Any time now. That's the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it.
We need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean nothing humans do as a job is uniquely safe anymore. From hamburgers to healthcare, machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans.
Amelia is just one AI out there currently being beta-tested in companies right now. Created by IPsoft over the past 16 years, she's learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she's able to learn, she's able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it's been estimated that she can put 250 million people out of a job, worldwide.
Viv is an AI coming soon from the creators of Siri who'll be our own personal assistant. She'll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she'll know we'll like best. In doing all of this for us, we'll see far fewer ads, and that means the entire advertising industry  that industry the entire Internet is built upon  stands to be hugely disrupted.
A world with Amelia and Viv  and the countless other AI counterparts coming online soon  in combination with robots like Boston Dynamics' next generation Atlas portends, is a world where machines can do all four types of jobs and that means serious societal reconsiderations. If a machine can do a job instead of a human, should any human be forced at the threat of destitution to perform that job? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when jobs for many are entirely unobtainable? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them, where does that money go instead? And what does it no longer buy? Is it even possible that many of the jobs we're creating don't need to exist at all, and only do because of the incomes they provide? These are questions we need to start asking, and fast.
Fortunately, people are beginning to ask these questions, and there's an answer that's building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is universal basic income. By adopting UBI, aside from immunizing against the negative effects of automation, we'd also be decreasing the risks inherent in entrepreneurship, and the sizes of bureaucracies necessary to boost incomes. It's for these reasons, it has cross-partisan support, and is even now in the beginning stages of possible implementation in countries like Switzerland, Finland, the Netherlands, and Canada.
The future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will. The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million. That's a net loss, not a net gain of 5 million jobs. In a frequently cited paper, an Oxford study estimated the automation of about half of all existing jobs by 2033. Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies  especially the US economy as I wrote last year about automating truck driving  by eliminating millions of jobs within a short span of time.
And now even the White House, in a stunning report to Congress, has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable ""duck and cover"" strategies for avoiding nuclear blasts during the Cold War.
All of this is why it's those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist Jeremy Howard asked ""Do you want half of people to starve because they literally can't add economic value, or not?"" before going on to suggest, ""If the answer is not, then the smartest way to distribute the wealth is by implementing a universal basic income.""
AI pioneer Chris Eliasmith, director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, ""AI is already having a big impact on our economies... My suspicion is that more countries will have to follow Finland's lead in exploring basic income guarantees for people.""
Moshe Vardi expressed the same sentiment after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, ""we need to rethink the very basic structure of our economic system... we may have to consider instituting a basic income guarantee.""
Even Baidu's chief scientist and founder of Google's ""Google Brain"" deep learning project, Andrew Ng, during an onstage interview at this year's Deep Learning Summit, expressed the shared notion that basic income must be ""seriously considered"" by governments, citing ""a high chance that AI will create massive labor displacement.""
When those building the tools begin warning about the implications of their use, shouldn't those wishing to use those tools listen with the utmost attention, especially when it's the very livelihoods of millions of people at stake? If not then, what about when Nobel prize winning economists begin agreeing with them in increasing numbers?
No nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let's ask ourselves, what's the purpose of the technologies we're creating? What's the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose how we work, and to decline any pay/hours we deem insufficient because we're already earning the incomes that machines aren't?
What's the big lesson to learn, in a century when machines can learn?
I offer it's that jobs are for machines, and life is for people.
This article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with a monthly patron pledge of $1+.
Are you a creative? Become a creator on Patreon. Join me in taking the BIG Patreon Creator Pledge for basic income
Special thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O'Sullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli Niinimaki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith.
Would you like to see your name here too?
Scott Santens writes about basic income on his blog. You can also follow him here on Medium, on Twitter, on Facebook, or on Reddit where he is a moderator for the /r/BasicIncome community of over 30,000 subscribers.
If you feel others would appreciate this article, please click the green heart.
Articles about Universal Basic Income (UBI)
9.3K 
191
Some rights reserved

",91
https://medium.com/@XiaohanZeng/i-interviewed-at-five-top-companies-in-silicon-valley-in-five-days-and-luckily-got-five-job-offers-25178cf74e0f?source=tag_archive---------5-----------------------,"I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers",Here's how I did it and what I thought,Xiaohan Zeng,13,"In the five days from July 24th to 28th 2017, I interviewed at LinkedIn, Salesforce Einstein, Google, Airbnb, and Facebook, and got all five job offers.
It was a great experience, and I feel fortunate that my efforts paid off, so I decided to write something about it. I will discuss how I prepared, review the interview process, and share my impressions about the five companies.
I had been at Groupon for almost three years. It's my first job, and I have been working with an amazing team and on awesome projects. We've been building cool stuff, making impact within the company, publishing papers and all that. But I felt my learning rate was being annealed (read: slowing down) yet my mind was craving more. Also as a software engineer in Chicago, there are so many great companies that all attract me in the Bay Area.
Life is short, and professional life shorter still. After talking with my wife and gaining her full support, I decided to take actions and make my first ever career change.
Although I'm interested in machine learning positions, the positions at the five companies are slightly different in the title and the interviewing process. Three are machine learning engineer (LinkedIn, Google, Facebook), one is data engineer (Salesforce), and one is software engineer in general (Airbnb). Therefore I needed to prepare for three different areas: coding, machine learning, and system design.
Since I also have a full time job, it took me 2-3 months in total to prepare. Here is how I prepared for the three areas.
While I agree that coding interviews might not be the best way to assess all your skills as a developer, there is arguably no better way to tell if you are a good engineer in a short period of time. IMO it is the necessary evil to get you that job.
I mainly used Leetcode and Geeksforgeeks for practicing, but Hackerrank and Lintcode are also good places. I spent several weeks going over common data structures and algorithms, then focused on areas I wasn't too familiar with, and finally did some frequently seen problems. Due to my time constraints I usually did two problems per day.
Here are some thoughts:
This area is more closely related to the actual working experience. Many questions can be asked during system design interviews, including but not limited to system architecture, object oriented design,database schema design,distributed system design,scalability, etc.
There are many resources online that can help you with the preparation. For the most part I read articles on system design interviews, architectures of large-scale systems, and case studies.
Here are some resources that I found really helpful:
Although system design interviews can cover a lot of topics, there are some general guidelines for how to approach the problem:
With all that said, the best way to practice for system design interviews is to actually sit down and design a system, i.e. your day-to-day work. Instead of doing the minimal work, go deeper into the tools, frameworks, and libraries you use. For example, if you use HBase, rather than simply using the client to run some DDL and do some fetches, try to understand its overall architecture, such as the read/write flow, how HBase ensures strong consistency, what minor/major compactions do, and where LRU cache and Bloom Filter are used in the system. You can even compare HBase with Cassandra and see the similarities and differences in their design. Then when you are asked to design a distributed key-value store, you won't feel ambushed.
Many blogs are also a great source of knowledge, such as Hacker Noon and engineering blogs of some companies, as well as the official documentation of open source projects.
The most important thing is to keep your curiosity and modesty. Be a sponge that absorbs everything it is submerged into.
Machine learning interviews can be divided into two aspects, theory and product design.
Unless you are have experience in machine learning research or did really well in your ML course, it helps to read some textbooks. Classical ones such as the Elements of Statistical Learning and Pattern Recognition and Machine Learning are great choices, and if you are interested in specific areas you can read more on those.
Make sure you understand basic concepts such as bias-variance trade-off, overfitting, gradient descent, L1/L2 regularization,Bayes Theorem,bagging/boosting,collaborative filtering,dimension reduction, etc. Familiarize yourself with common formulas such as Bayes Theorem and the derivation of popular models such as logistic regression and SVM. Try to implement simple models such as decision trees and K-means clustering. If you put some models on your resume, make sure you understand it thoroughly and can comment on its pros and cons.
For ML product design, understand the general process of building a ML product. Here's what I tried to do:
Here I want to emphasize again on the importance of remaining curious and learning continuously. Try not to merely using the API for Spark MLlib or XGBoost and calling it done, but try to understand why stochastic gradient descent is appropriate for distributed training, or understand how XGBoost differs from traditional GBDT, e.g. what is special about its loss function, why it needs to compute the second order derivative, etc.
I started by replying to HR's messages on LinkedIn, and asking for referrals. After a failed attempt at a rock star startup (which I will touch upon later), I prepared hard for several months, and with help from my recruiters, I scheduled a full week of onsites in the Bay Area. I flew in on Sunday, had five full days of interviews with around 30 interviewers at some best tech companies in the world, and very luckily, got job offers from all five of them.
All phone screenings are standard. The only difference is in the duration: For some companies like LinkedIn it's one hour, while for Facebook and Airbnb it's 45 minutes.
Proficiency is the key here, since you are under the time gun and usually you only get one chance. You would have to very quickly recognize the type of problem and give a high-level solution. Be sure to talk to the interviewer about your thinking and intentions. It might slow you down a little at the beginning, but communication is more important than anything and it only helps with the interview. Do not recite the solution as the interviewer would almost certainly see through it.
For machine learning positions some companies would ask ML questions. If you are interviewing for those make sure you brush up your ML skills as well.
To make better use of my time, I scheduled three phone screenings in the same afternoon, one hour apart from each. The upside is that you might benefit from the hot hand and the downside is that the later ones might be affected if the first one does not go well, so I don't recommend it for everyone.
One good thing about interviewing with multiple companies at the same time is that it gives you certain advantages. I was able to skip the second round phone screening with Airbnb and Salesforce because I got the onsite at LinkedIn and Facebook after only one phone screening.
More surprisingly, Google even let me skip their phone screening entirely and schedule my onsite to fill the vacancy after learning I had four onsites coming in the next week. I knew it was going to make it extremely tiring, but hey, nobody can refuse a Google onsite invitation!
LinkedIn
This is my first onsite and I interviewed at the Sunnyvale location. The office is very neat and people look very professional, as always.
The sessions are one hour each. Coding questions are standard, but the ML questions can get a bit tough. That said, I got an email from my HR containing the preparation material which was very helpful, and in the end I did not see anything that was too surprising. I heard the rumor that LinkedIn has the best meals in the Silicon Valley, and from what I saw if it's not true, it's not too far from the truth.
Acquisition by Microsoft seems to have lifted the financial burden from LinkedIn, and freed them up to do really cool things. New features such as videos and professional advertisements are exciting. As a company focusing on professional development, LinkedIn prioritizes the growth of its own employees. A lot of teams such as ads relevance and feed ranking are expanding, so act quickly if you want to join.
Salesforce Einstein
Rock star project by rock star team. The team is pretty new and feels very much like a startup. The product is built on the Scala stack, so type safety is a real thing there! Great talks on the Optimus Prime library by Matthew Tovbin at Scala Days Chicago 2017 and Leah McGuire at Spark Summit West 2017.
I interviewed at their Palo Alto office. The team has a cohesive culture, and work life balance is great there. Everybody is passionate about what they are doing and really enjoys it. With four sessions it is shorter compared to the other onsite interviews, but I wish I could have stayed longer. After the interview Matthew even took me for a walk to the HP garage :)
Google
Absolutely the industry leader, and nothing to say about it that people don't already know. But it's huge. Like, really, really HUGE. It took me 20 minutes to ride a bicycle to meet my friends there. Also lines for food can be too long. Forever a great place for developers.
I interviewed at one of the many buildings on the Mountain View campus, and I don't know which one it is because it's HUGE.
My interviewers all look very smart, and once they start talking they are even smarter. It would be very enjoyable to work with these people.
One thing that I felt special about Google's interviews is that the analysis of algorithm complexity is really important. Make sure you really understand what Big O notation means!
Airbnb
Fast expanding unicorn with a unique culture and arguably the most beautiful office in the Silicon Valley. New products such as Experiences and restaurant reservation, high end niche market, and expansion into China all contribute to a positive prospect. Perfect choice if you are risk tolerant and want a fast growing, pre-IPO experience.
Airbnb's coding interview is a bit unique because you'll be coding in an IDE instead of whiteboarding, so your code needs to compile and give the right answer. Some problems can get really hard.
And they've got the one-of-a-kind cross functional interviews. This is how Airbnb takes culture seriously, and being technically excellent doesn't guarantee a job offer. For me the two cross functionals were really enjoyable. I had casual conversations with the interviewers and we all felt happy at the end of the session.
Overall I think Airbnb's onsite is the hardest due to the difficulty of the problems, longer duration, and unique cross-functional interviews. If you are interested, be sure to understand their culture and core values.
Facebook
Another giant that is still growing fast, and smaller and faster-paced compared to Google. With its product lines dominating the social network market and big investments in AI and VR, I can only see more growth potential for Facebook in the future. With stars like Yann LeCun and Yangqing Jia, it's the perfect place if you are interested in machine learning.
I interviewed at Building 20, the one with the rooftop garden and ocean view and also where Zuckerberg's office is located.
I'm not sure if the interviewers got instructions, but I didn't get clear signs whether my solutions were correct, although I believed they were.
By noon the prior four days started to take its toll, and I was having a headache. I persisted through the afternoon sessions but felt I didn't do well at all. I was a bit surprised to learn that I was getting an offer from them as well.
Generally I felt people there believe the company's vision and are proud of what they are building. Being a company with half a trillion market cap and growing, Facebook is a perfect place to grow your career at.
This is a big topic that I won't cover in this post, but I found this article to be very helpful.
Some things that I do think are important:
All successes start with failures, including interviews. Before I started interviewing for these companies, I failed my interview at Databricks in May.
Back in April, Xiangrui contacted me via LinkedIn asking me if I was interested in a position on the Spark MLlib team. I was extremely thrilled because 1) I use Spark and love Scala, 2) Databricks engineers are top-notch, and 3) Spark is revolutionizing the whole big data world. It is an opportunity I couldn't miss, so I started interviewing after a few days.
The bar is very high and the process is quite long, including one pre-screening questionnaire, one phone screening, one coding assignment, and one full onsite.
I managed to get the onsite invitation, and visited their office in downtown San Francisco, where Treasure Island can be seen.
My interviewer were incredibly intelligent yet equally modest. During the interviews I often felt being pushed to the limits. It was fine until one disastrous session, where I totally messed up due to insufficient skills and preparation, and it ended up a fiasco. Xiangrui was very kind and walked me to where I wanted to go after the interview was over, and I really enjoyed talking to him.
I got the rejection several days later. It was expected but I felt frustrated for a few days nonetheless. Although I missed the opportunity to work there, I wholeheartedly wish they will continue to make greater impact and achievements.
From the first interview in May to finally accepting the job offer in late September, my first career change was long and not easy.
It was difficult for me to prepare because I needed to keep doing well at my current job. For several weeks I was on a regular schedule of preparing for the interview till 1am, getting up at 8:30am the next day and fully devoting myself to another day at work.
Interviewing at five companies in five days was also highly stressful and risky, and I don't recommend doing it unless you have a very tight schedule. But it does give you a good advantage during negotiation should you secure multiple offers.
I'd like to thank all my recruiters who patiently walked me through the process, the people who spend their precious time talking to me, and all the companies that gave me the opportunities to interview and extended me offers.
Lastly but most importantly, I want to thank my family for their love and support  my parents for watching me taking the first and every step, my dear wife for everything she has done for me, and my daughter for her warming smile.
Thanks for reading through this long post.
You can find me on LinkedIn or Twitter.
10/22/17
PS: Since the publication of this post, it has (unexpectedly) received some attention. I would like to thank everybody for the congratulations and shares, and apologize for not being able to respond to each of them.
This post has been translated into some other languages:
It has been reposted in Tech In Asia.
Breaking Into Startups invited me to a live video streaming, together with Sophia Ciocca.
CoverShr did a short QnA with me.
",92
https://towardsdatascience.com/advantages-and-disadvantages-of-artificial-intelligence-182a5ef6588c?source=tag_archive---------4-----------------------,Advantages and Disadvantages of Artificial Intelligence,Artificial Intelligence is one of the emerging technologies which tries to simulate human reasoning in AI systems. John McCarthy invented...,sunil kumar,5,"Artificial Intelligence is one of the emerging technologies which tries to simulate human reasoning in AI systems. John McCarthy invented the term Artificial Intelligence in the year 1950.
He said, 'Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions, and concepts, solve kinds of problems now reserved for humans, and improve themselves.'
Artificial Intelligence is the ability of a computer program to learn and think. Everything can be considered Artificial intelligence if it involves a program doing something that we would normally think would rely on the intelligence of a human.
The advantages of Artificial intelligence applications are enormous and can revolutionize any professional sector. Let's see some of them
1) Reduction in Human Error:
The phrase ""human error"" was born because humans make mistakes from time to time. Computers, however, do not make these mistakes if they are programmed properly. With Artificial intelligence, the decisions are taken from the previously gathered information applying a certain set of algorithms. So errors are reduced and the chance of reaching accuracy with a greater degree of precision is a possibility.
Example: In Weather Forecasting using AI they have reduced the majority of human error.
2) Takes risks instead of Humans:
This is one of the biggest advantages of Artificial intelligence. We can overcome many risky limitations of humans by developing an AI Robot which in turn can do the risky things for us. Let it be going to mars, defuse a bomb, explore the deepest parts of oceans, mining for coal and oil, it can be used effectively in any kind of natural or man-made disasters.
Example: Have you heard about the Chernobyl nuclear power plant explosion in Ukraine? At that time there were no AI-powered robots that can help us to minimize the effect of radiation by controlling the fire in early stages, as any human went close to the core was dead in a matter of minutes. They eventually poured sand and boron from helicopters from a mere distance.
AI Robots can be used in such situations where intervention can be hazardous.
3) Available 24x7:
An Average human will work for 4-6 hours a day excluding the breaks. Humans are built in such a way to get some time out for refreshing themselves and get ready for a new day of work and they even have weekly offed to stay intact with their work-life and personal life. But using AI we can make machines work 24x7 without any breaks and they don't even get bored, unlike humans.
Example: Educational Institutes and Helpline centers are getting many queries and issues which can be handled effectively using AI.
4) Helping in Repetitive Jobs:
In our day-to-day work, we will be performing many repetitive works like sending a thanking mail, verifying certain documents for errors and many more things. Using artificial intelligence we can productively automate these mundane tasks and can even remove ""boring"" tasks for humans and free them up to be increasingly creative.
Example: In banks, we often see many verifications of documents to get a loan which is a repetitive task for the owner of the bank. Using AI Cognitive Automation the owner can speed up the process of verifying the documents by which both the customers and the owner will be benefited.
5) Digital Assistance:
Some of the highly advanced organizations use digital assistants to interact with users which saves the need for human resources. The digital assistants also used in many websites to provide things that users want. We can chat with them about what we are looking for. Some chatbots are designed in such a way that it's become hard to determine that we're chatting with a chatbot or a human being.
Example: We all know that organizations have a customer support team that needs to clarify the doubts and queries of the customers. Using AI the organizations can set up a Voice bot or Chatbot which can help customers with all their queries. We can see many organizations already started using them on their websites and mobile applications.
6) Faster Decisions:
Using AI alongside other technologies we can make machines take decisions faster than a human and carry out actions quicker. While taking a decision human will analyze many factors both emotionally and practically but AI-powered machine works on what it is programmed and delivers the results in a faster way.
Example: We all have played Chess games in Windows. It is nearly impossible to beat CPU in the hard mode because of the AI behind that game. It will take the best possible step in a very short time according to the algorithms used behind it.
7) Daily Applications:
Daily applications such as Apple's Siri, Window's Cortana, Google's OK Google are frequently used in our daily routine whether it is for searching a location, taking a selfie, making a phone call, replying to a mail and many more.
Example: Around 20 years ago, when we are planning to go somewhere we used to ask a person who already went there for the directions. But now all we have to do is say ""OK Google where is Visakhapatnam"". It will show you Visakhapatnam's location on google map and the best path between you and Visakhapatnam.
8) New Inventions:
AI is powering many inventions in almost every domain which will help humans solve the majority of complex problems.
Example: Recently doctors can predict breast cancer in the woman at earlier stages using advanced AI-based technologies.
As every bright side has a darker version in it. Artificial Intelligence also has some disadvantages. Let's see some of them
1) High Costs of Creation:
As AI is updating every day the hardware and software need to get updated with time to meet the latest requirements. Machines need repairing and maintenance which need plenty of costs. It' s creation requires huge costs as they are very complex machines.
2) Making Humans Lazy:
AI is making humans lazy with its applications automating the majority of the work. Humans tend to get addicted to these inventions which can cause a problem to future generations.
3) Unemployment:
As AI is replacing the majority of the repetitive tasks and other works with robots,human interference is becoming less which will cause a major problem in the employment standards. Every organization is looking to replace the minimum qualified individuals with AI robots which can do similar work with more efficiency.
4) No Emotions:
There is no doubt that machines are much better when it comes to working efficiently but they cannot replace the human connection that makes the team. Machines cannot develop a bond with humans which is an essential attribute when comes to Team Management.
5) Lacking Out of Box Thinking:
Machines can perform only those tasks which they are designed or programmed to do, anything out of that they tend to crash or give irrelevant outputs which could be a major backdrop.
SUMMARY:
These are some advantages and disadvantages of Artificial Intelligence. Every new invention or breakthrough will have both, but we as humans need to take care of that and use the positive sides of the invention to create a better world. Artificial intelligence has massive potential advantages. The key for humans will ensure the ""rise of the robots"" doesn't get out of hand. Some people also say that Artificial intelligence can destroy human civilization if it goes into the wrong hands. But still, none of the AI applications made at that scale that can destroy or enslave humanity.
",93
https://towardsdatascience.com/could-nim-replace-python-547145afcfd5?source=tag_archive---------6-----------------------,Could Nim Replace Python?,Why this fledgeling computer language might become the new scripting king,Emmett Boudreau,6,"For many years now, no programming language has been better suited for scripting than Python. Python is an interpreted language written in the late 1980s by Guido Van Rossum written in the language C. Van Rossum, like many other famous Computer Scientists, is from the Netherlands, where he wrote Python from within Centrum Wiskunde & Informatica, or in a rough English translation,
National Research Institute for Mathematics and Computer Science
Python has massive benefits of traditionally lower-level languages that populated computers at the time, such as C, FORTRAN, BASIC, C++, and Lisp. Firstly, Python's syntax is far more simple and easy to grasp. This makes Python perfectly usable by end-users to perform required tasks around their system. Additionally, though Python is widely considered "" slow"" by today's standards, in 1989 it was quite a feat to have a language that reads like English be even remotely as performance packing to its competitors.
Even more recently,
Python and the jobs that correspond with its popularity have overtaken "" the big boys"" with Python becoming more popular than both Java, as well as Javascript. Python's ecosystem is absolutely exceptional, not only in regards to programming in general but also very much in regard to the ever-expanding market of
Machine-Learning
With Python's growth in popularity, the flaws and problems with the languages have become more and more prevalent. Though Python is certainly a great language, it's speed, which I touched on earlier is its
"" Achilles Heel""
Though this isn't to say that Python is not still viable for machine-learning, there certainly are some performance benefits to be had with a change from the most popular statistical language on Earth. This is also not necessarily Python's fault, as the language has been developed for machine-learning only because of its sheer popularity. For most applications, Python works marvelously; but this can often change whenever many observations are loaded. For this reason, Scala is typically selected as the language for driving big-data inside of corporate America. Scala is not without its issues as well, although Scala certainly has a better ecosystem than the other two competitors,
R and Julia
Despite the similarities between both Nim and Python, Nim is significantly younger than Python, being rendered just 12 years ago in 2008. To this arises some significant advantages to Nim. Firstly, Nim is faster than Python while still being interpreted by the same language, C. Though Nim is technically run with an interpreter, it should be noted that Nim also has its own compiler. That being said, there are lots of cool features that make Nim a great potential replacement for Python that you might not have expected.
A common theme with Python is requiring Python in order to run Python, and this includes an application's dependencies. This is problematic because it means that Python applications to be packages in one way or another with said dependencies. On top of that, it's very likely that virtual environments will be frequented. While this isn't terrible, and to confess most statistical languages do exactly the same, Nim does this significantly better by packaging an executable with the included dependencies needed to run. This not only makes managing dependencies from system to system a breeze, but also makes deployment
EASIER than Py (see what I did there?)
These compiled executables are also compatible universally across the Unix-like systems, Linux, Mac, and Berkley Software Distribution, but also the Windows NT kernel. Compiled executables take care of dependency issues and make it incredibly easy to publish an application, or even deploy an API with a simple ""."" or ""source"" command.
Nim has a serious advantage to Python in that not only is Nim capable of being compiled in C, but also C++, and more excitingly:
Javascript
This means that not only does Nim have the potential to fill Python's role as the scripting language that runs the data-based back-ends of the web, but Nim can also be used as a front-end similarly to Javascript. This is a huge benefit over Python. While Python is certainly great for deploying endpoints, and often does the job fine, having single-language fluidity across the board certainly has its advantages!
Nim's code-base is primarily structured on the functional paradigm. This means that Nim can be a very expressive language, and furthermore can easily implement far more cool features than Python can. One of these features is one of my favorite features of all-time to be implemented into programming back in 1958 with the release of Lisp,
macros.
Macros and meta-programming have been around for nearly as long as computing itself, and can be very useful, especially on the grounds of machine-learning.
It's no secret that as scale goes up, using Python for everything can be very problematic. This is because many training algorithms utilize a recursive Cost or Loss function that is intensive to run for any language. There are lots of languages and ideas with the intent to counter-act this, such as Julia, Python In Python (that's a rabbit hole in and of itself), and more successfully: Cython.
With these solutions, however, come their own problems as well. Though Julia, which is, in fact, my favorite language, and likely the most apt to be a replacement for Python, does not have anywhere near the ecosystem that Python flaunts. Though there is a PyCall.jl, typically performance when using it dips below that of Python's, and in that case,
why not just use Python?
Python in Python is an interesting concept, but has yet to see great implementations, as the concept itself is quite complex. Even worse, Python in Python is much more difficult to implement than a solution like Julia or Nim. As for Cython, contrary to popular belief, Cython does not work universally, and relying on it probably isn't a good idea (been there, done that.)
Nim has the advantage of being faster than Python. For scripting, Nim's added speed could certainly change the way that system-maintenance and various scripts are run. Using Nim might not be as fast as Julia, or C, but with the simple similarity to both Python and Bash that it boasts, it could certainly be a lot easier.
Though Nim is certainly a really cool, and even useful language, I highly doubt that a "" Python takeover"" is imminent. Nim has a fraction of an ecosystem in comparison to Python, and furthermore would require a lot more work to get up to the grounds of Python, leaving it in somewhat of a limbo among the other programming languages. Though Nim certainly has features that outshine Python's, it's hard to argue with an established library of packages that everyone knows how to use.
I think Nim's story is one similar to Julia. Nim is a beautiful language that is both expressive, easy, and efficient, but without a stable back-end, I consider it unlikely that Nim will ever become top-dog in the scripting and more importantly to me: statistical world. But all of this isn't to say that Nim isn't a valuable language to learn. Nim is a super cool, easy to use and fast high-level and functional programming language. Anything that combines all of those words is most likely beneficial to your programming toolbox.
Though Nim might not be the world's next big thing, I've certainly been enjoying the time I've been spending with it. What's more exciting to me is the potential that Julia shows, as opposed to Nim, and with a little more development, I think we could certainly increase the speed of ML as we know it. In this regard, the analogy of a processor certainly fits: in years past there was no processor with 64-bit registries, no processor with a clock speed above 1 GhZ (a total slug by today's standards.) The future is exciting, and I simply can't wait to see what technology stack it will include.
",94
https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b?source=tag_archive---------5-----------------------,An End-to-End Project on Time Series Analysis and Forecasting with Python,Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics...,Susan Li,9,"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.
Time series are widely used for non-stationary data, like economic, weather, stock price, and retail sales in this post. We will demonstrate different approaches for forecasting retail sales time series. Let's get started!
We are using Superstore sales data that can be downloaded from here.
There are several categories in the Superstore sales data, we start from time series analysis and forecasting for furniture sales.
We have a good 4-year furniture sales data.
Timestamp('2014-01-06 00:00:00'), Timestamp('2017-12-30 00:00:00')
This step includes removing columns we do not need, check missing values, aggregate sales by date and so on.
Our current datetime data can be tricky to work with, therefore, we will use the averages daily sales value for that month instead, and we are using the start of each month as the timestamp.
Have a quick peek 2017 furniture sales data.
Some distinguishable patterns appear when we plot the data. The time-series has seasonality pattern, such as sales are always low at the beginning of the year and high at the end of the year. There is always an upward trend within any single year with a couple of low months in the mid of the year.
We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise.
The plot above clearly shows that the sales of furniture is unstable, along with its obvious seasonality.
We are going to apply one of the most commonly used method for time-series forecasting, known as ARIMA, which stands for Autoregressive Integrated Moving Average.
ARIMA models are denoted with the notation ARIMA(p, d, q). These three parameters account for seasonality, trend, and noise in data:
This step is parameter Selection for our furniture's sales ARIMA Time Series Model. Our goal here is to use a ""grid search"" to find the optimal set of parameters that yields the best performance for our model.
The above output suggests that SARIMAX(1, 1, 1)x(1, 1, 0, 12) yields the lowest AIC value of 297.78. Therefore we should consider this to be optimal option.
We should always run model diagnostics to investigate any unusual behavior.
It is not perfect, however, our model diagnostics suggests that the model residuals are near normally distributed.
To help us understand the accuracy of our forecasts, we compare predicted sales to real sales of the time series, and we set forecasts to start at 2017-01-01 to the end of the data.
The line plot is showing the observed values compared to the rolling forecast predictions. Overall, our forecasts align with the true values very well, showing an upward trend starts from the beginning of the year and captured the seasonality toward the end of the year.
The Mean Squared Error of our forecasts is 22993.58
The Root Mean Squared Error of our forecasts is 151.64
In statistics, the mean squared error (MSE) of an estimator measures the average of the squares of the errors  that is, the average squared difference between the estimated values and what is estimated. The MSE is a measure of the quality of an estimator  it is always non-negative, and the smaller the MSE, the closer we are to finding the line of best fit.
Root Mean Square Error (RMSE) tells us that our model was able to forecast the average daily furniture sales in the test set within 151.64 of the real sales. Our furniture daily sales range from around 400 to over 1200. In my opinion, this is a pretty good model so far.
Our model clearly captured furniture sales seasonality. As we forecast further out into the future, it is natural for us to become less confident in our values. This is reflected by the confidence intervals generated by our model, which grow larger as we move further out into the future.
The above time series analysis for furniture makes me curious about other categories, and how do they compare with each other over time. Therefore, we are going to compare time series of furniture and office supplier.
According to our data, there were way more number of sales from Office Supplies than from Furniture over the years.
((2121, 21), (6026, 21))
We are going to compare two categories' sales in the same time period. This means combine two data frames into one and plot these two categories' time series into one plot.
We observe that sales of furniture and office supplies shared a similar seasonal pattern. Early of the year is the off season for both of the two categories. It seems summer time is quiet for office supplies too. in addition, average daily sales for furniture are higher than those of office supplies in most of the months. It is understandable, as the value of furniture should be much higher than those of office supplies. Occasionally, office supplies passed furniture on average daily sales. Let's find out when was the first time office supplies' sales surpassed those of furniture's.
Office supplies first time produced higher sales than furniture is 2014-07-01.
It was July 2014!
Released by Facebook in 2017, forecasting tool Prophet is designed for analyzing time-series that display patterns on different time scales such as yearly, weekly and daily. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints. Therefore, we are using Prophet to get a model up and running.
We already have the forecasts for three years for these two categories into the future. We will now join them together to compare their future forecasts.
Now, we can use the Prophet Models to inspect different trends of these two categories in the data.
Good to see that the sales for both furniture and office supplies have been linearly increasing over time and will be keep growing, although office supplies' growth seems slightly stronger.
The worst month for furniture is April, the worst month for office supplies is February. The best month for furniture is December, and the best month for office supplies is October.
There are many time-series analysis we can explore from now on, such as forecast with uncertainty bounds, change point and anomaly detection, forecast time-series with external data source. We have only just started.
Source code can be found on Github. I look forward to hearing feedback or questions.
References:
A Guide to Time Series Forecasting with ARIMA in Python 3
A Guide to Time Series Forecasting with Prophet in Python 3
",95
https://towardsdatascience.com/data-scientists-will-be-extinct-in-10-years-a6e5dd77162b?source=tag_archive---------1-----------------------,Data Scientists Will be Extinct in 10 Years,"As advances in AI continue to progress in leaps and bounds, accessibility to data science at a base level has become increasingly democratized. Traditional entry barriers to the field such as a lack...",Mikhail Mew,4,"As advances in AI continue to progress in leaps and bounds, accessibility to data science at a base level has become increasingly democratized. Traditional entry barriers to the field such as a lack of data and computing power have been swept aside with a continuous supply of new data startups popping up(some offering access for as little as a cup of coffee a day) and all powerful cloud computing removing the need for expensive onsite hardware. Rounding out the trinity of prerequisites, is the skill and know-how to implement, which has arguably become the most ubiquitous aspect of data science. One does not need to look far to find online tutorials touting taglines like ""implement X model in seconds"" , ""apply Z method to your data in just a few lines of code"". In a digital world, instant gratification has become the name of the game. While improved accessibility is not detrimental on face value, beneath the dazzling array of software libraries and shiny new models, the true purpose of data science has become obscured and at times even forgotten. For it is not to run complex models for the sake of doing so, or to optimize an arbitrary performance metric, but to use as a tool to solve real world problems.
A simple but relatable example is the Iris data set. How many have used it to demonstrate an algorithm without sparing a thought for what a sepal is let alone why we measure its length? While these may seem like trivial considerations for the budding practitioner who might be more interested in adding a new model to their repertoire, it was less than trivial for Edgar Anderson, a botanist, who cataloged the attributes in question to understand variations in Iris flowers. Despite this being a contrived example it demonstrates a simple point; the mainstream has become more focused on ""doing"" data science rather than ""applying"" data science. However, this misalignment is not the cause for the decline of the data scientist but a symptom. To understand the origin of the problem we must step back and take a bird's eye view.
Data science has the curious distinction of being one of the few fields of study that leaves the practitioner without a domain. Pharmacy students become pharmacists, law students become lawyers, accounting students become accountants. Data science students must therefore become data scientists? But data scientists of what? The broad application of data science proves to be a double edged sword. On one side, it is a powerful toolbox that can be applied to any industry where data is generated and captured. On the other, the general applicability of these tools means that rarely will the user have true domain knowledge of said industries before the fact. Nevertheless, the problem was insignificant during the rise of data science as employers rushed to harness this nascent technology without fully understanding what it was and how it could be fully integrated into their company.
However, nearly a decade later, both businesses and the environment they operate in have evolved. They now strive for data science maturity with large entrenched teams benchmarked by established industry standards. The pressing hiring demand has shifted to problem solvers and critical thinkers who understand the business, the respective industry as well as its stakeholders. No longer will the ability navigate a couple of software packages or regurgitate a few lines of code suffice, nor will a data science practitioner be defined by the ability to code. This is evidenced by the increasing popularity of no code, autoML solutions such as Data Robot, Rapid Miner and Alteryx.
Data scientists will be extinct in 10 years (give or take), or at least the role title will be. Going forward, the skill set collectively known as data science will be borne by a new generation of data savvy business specialists and subject matter experts who are able to imbue analysis with their deep domain knowledge, irrespective of whether they can code or not. Their titles will reflect their expertise rather than the means by which they demonstrate it, be it compliance specialists, product managers or investment analysts. We don't need to look back far to find historic precedents. During the advent of the spreadsheet, data entry specialists were highly coveted, but nowadays, as Cole Nussbaumer Knaflic (the author of ""Storytelling With Data"") aptly observes, proficiency with Microsoft Office suite is a bare minimum. Before that, the ability to touch type with a typewriter was considered a specialist skill, however with the accessibility of personal computing it has also become assumed.
Lastly, for those considering a career in data science or commencing their studies, it may serve you well to constantly refer back to the Venn diagram that you will undoubtedly come across. It describes data science as an confluence of statistics, programming and domain knowledge. Despite each occupying an equal share of the intersecting area, some may warrant a higher weighting than others.
Disclaimer: Views are my own, based on my observations and experiences. It's ok if you don't agree, productive discussion is welcome.
",97
https://towardsdatascience.com/import-all-python-libraries-in-one-line-of-code-86e54f6f0108?source=tag_archive---------9-----------------------,Import all Python libraries in one line of code,Annoyed writing multiple import statements? Let PyForest do the work for you,Satyam Kumar,3,"Python is a primary language used by data scientists for data science projects, because of the presence of thousands of open-source libraries that can ease and perform a data scientist task. Over 235,000 Python packages can be imported through PyPl.
Multiple libraries and frameworks need to be imported to perform tasks in a data science case study. Every time a data scientist or analyst starts a new jupyter notebook or any other IDE, they need to import all the libraries as per their requirements. Sometimes writing multiple lines of the same import statement over and over again can be frustrating. Here pyforest libraries come into the rescue, and it does the work for you.
Pyforest is an open-sourced Python library that enables data scientists to feel the bliss of automated imports. While working on a data science case study one needs to import multiple packages or libraries like pandas, matplotlib, seaborn, NumPy, SkLearn, etc. Importing all these libraries every time can be boring and disrupt the natural flow of your work. Also, you are free from searching for the exact import statement, like from sklearn.ensemble import RandomForestClassifier .
Using pyforest one can overcome these problems. Pyforest enables you to use all your favorite libraries without importing them. Pyforest does the work for you by automatically importing the libraries you want to use for your case study.
Once you import the pyforest library in one line, now you can use all the python libraries like one usually does. Any of the libraries that you use is not imported, Pyforest imports it for you automatically. The libraries are only imported once you call them or create an object of it. If a library is not used or called, it won't be imported by pyforest.
Pyforest can be installed from Pypl using the following command:
After installing the library, you need to just import it in one line of code. Now you can use your favorite library like you usually do, without writing imports. In the below sample jupyter notebook, we have not imported pandas, seaborn and matplotlib library, still, we can use them by just importing the pyforest library.
This package aims to add all the libraries that account for more than 99% of the imports, including popular libraries such as pandas as pd, NumPy as np, matplotlob.pyplot as plt, seaborn as sns, and many more. In addition to these libraries, it also some helper modules such as os, tqdm, re, and many more.
If you want to see the list of libraries, use dir(pyforest).
To add your own import statements to the pyforest libraries list, type in your explicit import statement to a file present in your home directory ~/.pyforest/user_imports.py .
Pyforest automatically the python libraries as and when required. Pyforest comes up with some functions to know the status of libraries:
In this article, we have discussed the pyforest library, an automated importing library. Using this library can reduce the stress to import tons of necessary libraries, instead, it does automatically imports the requirements. This library can be helpful for a data scientist who constantly works with new jupyter notebooks to explore their data science case study, now they can be free from importing the libraries, hence accelerating their workflow.
[1] Pyforest documentation (Apr 17, 2020): https://pypi.org/project/pyforest/
Loved the article? Become a Medium member to continue learning without limits. I'll receive a small portion of your membership fee if you use the following link, with no extra cost to you.
satyam-kumar.medium.com
Thank You for Reading
",98
https://towardsdatascience.com/enchanted-random-forest-b08d418cb411?source=tag_archive---------2-----------------------,Enchanted Random Forest,A quick guide to Decision Trees and Random Forests.,Jose Marcial Portilla,8,"If you enjoy this article and wish to learn more about how to implement machine learning with Python, check out my online course!
This post will take you through a basic explanation of Decision Trees and Random Forests. Starting with simple analogies and slowly adding math along the way.
Let's start off with a quick story so we can get a feel for the framework of decision trees and ensemble methods. Throughout the story, the analogous machine learning terms are presented in parenthesis.
Imagine you are at the library and are trying to decide on a book to read. Luckily, your friend Frank is with you to help you decide. Frankie begins by asking you for a list of books you've read and whether or not you enjoyed them (a training set with labels). Then you pick a book from the shelf and Frank begins asking you questions to determine whether or not you'll enjoy the book. Frank asks ""yes"" or ""no"" questions, similar to the game 20 questions, such as ""Who is the author?"", or ""What year was the book written?"". Frank asks the more informative questions first (maximizing information gain) and then finally gives you his yes or no recommendation on the book at the end (classifying the test set).
Here, we can treat Frank as a decision tree.
However, Frank is only one person, and his questions sometimes hone in too much on particular books (overfitting). To get a better idea of whether or not you'll like the book from the shelf, you ask several friends through the same process. They each give a vote on the book, and you decide on the majority opinion (an ensemble classifier).
If you have a similar circle of friends, they may all have the exact same process of questions, so to avoid them all having the exact same answer, you'll want to give them each a different sample from your list of books. You decide to cut up your list and place it in a bag, then randomly draw from the bag, tell your friend whether or not you enjoyed that book, and then place that sample back in the bag. This means you'll be randomly drawing a sub sample from your original list with replacement (bootstrapping your original data). This gives some books more emphasis, if you drew a particular book several times for one friend, and some books less, possibly never drawn from the bag. Then each individual will give a unique recommendation on your book preferences.
Now your friends form a bootstrapped aggregated forest.
We still have one issue to resolve though. Imagine you enjoyed both Fahrenheit 451 and The Martian Chronicles by Ray Bradbury. There are several things linking these two books together, the same author, similar genre, and they were both written in the 1950s. One of your friends may come to the conclusion that you enjoyed both books because you enjoy books written in the 1950s, when in reality, Bradbury might be the only author from the 50s you like. So how do we fix this issue? We can fix this by introducing some randomness to the questions your friends get to ask. You force your friends to randomly choose the question they were going to ask (at each node of the decision tree, you randomly select the attribute to split on). It is important to note that previously, you were inducing randomness with your data by bootstrapping, now you are introducing it into your actual model-thus your large group of friends asking questions in random order is our analogy to the random forest.
Now that we have a conceptual understanding, let's start diving deeper into how we construct Decision Trees.
Let's start by looking at a simple example of a decision tree based off a few splits of data from passengers on the Titanic.
Here we can see this Decision Tree is splitting the passengers based on both categorical and continuous data to predict if they survived the crash. The first node splits the passengers based on gender. You can see that this simple decision tree decides that if the passenger was female they survived the crash. It also separates along continuous features by indicating a point to split on, such as age or number of siblings. So now an important question emerges.
How do we decide which features to split on?
In order to pick which feature to split on, we need a way of measuring how good the split is. This is where information gain and entropy come in.
We would like to choose questions that give a lot of information about the tree's prediction. For example, if there is a single yes/no question that accurately predicts the outputs 99% of the time, then that question allows us to ""gain"" a lot of information about our data. In order to measure how much information we gain, we introduce entropy.
The entropy is a measure of uncertainty associated with our data. We can intuitively think that if a data set had only one label (e.g. every passenger survived), then we have a low entropy. So we would like to split our data in a way that minimizes the entropy. The better the splits, the better our prediction will be. The equation for entropy is:
Here, p(x) is the percent of the group that belongs to a given class and H is the entropy.
If you have a collection of data points, the entropy will be large when they are evenly distributed across the classes and small when they are mostly the same class. Here's a graph to demonstrate what entropy looks like:
Intuitively this makes sense- imagine that a split on your tree equally divided your data with a probability of 50%. This would result in the highest possibly entropy because your data is evenly distributed across the classes. Conversely, as said before, entropy will be small when the data mostly belongs to the same class. In our plot, this is shown with a low entropy when the data has either close to a 0% or 100% chance of being in a class.
So we would like our decision tree to make splits that minimize entropy. We use information gain to determine the best split. Information gain is calculated by the following equation:
Here, S is the original set and D is the splitting of the set (a partition). Each V is a subset of S. All of the V's are disjoint and make up S. So the Information gain is just the original Entropy of the data before the split H(S), minus the sum of the weighted split entropy values.
So now that we know about decision trees and how we construct them, what are their pros and cons of decision trees?
For the pros, Decision Trees are easily interpretable and can handle missing values and outliers. They can also handle discrete and continuous data types, along with irrelevant features.
For the cons, Decision Trees can be very easy to overfit, and while they are computationally cheap for prediction, training the decision tree can be computationally expensive.
Now that we have gotten a grasp on Decision Trees, let's explore Random Forests.
Random Forests are an example of an ensemble method, in which we combine multiple machine learning algorithms to obtain better predictive performance. We'll run multiple models on the data and use the aggregate predictions, which will be better than a single model alone.
Random Forest is one of the most common ensemble methods, which consists of a collection of Decision Trees. Random Forest was developed by Leo Breimen, and I highly suggest you check out his webpage on them.
The idea behind a Random Forest is actually pretty simple: We repeatedly select data from the data set (with replacement) and build a Decision Tree with each new sample. It is important to note that since we are sampling with replacement, many data points will be repeated and many won't be included as well. This is important to keep in mind when we talk about measuring error of a Random Forest. Another important feature of the Random Forest is that each node of the Decision Tree is limited to only considering splits on random subsets of the features.
In the case of classification with Random Forests, we use each tree in our forest to get a prediction, then the label with the most votes becomes the predicted class for that data point. The usual parameters when building a forest (standard defaults used in the SciKit-Learn library) are 10 trees and only considering the square root of 'n' features, where n is the total number of features.
If we wanted to know how well our Random Forest performed, we could use a standard cross validation method of splitting the data into a training and testing set, then comparing the predictions to the actual values. However, as mentioned earlier, each tree doesn't get a chance to see all of the training data, so the unseen data can be used to cross validate each tree individually.
When selecting from the data set, about one third of the data is left out, (the mathematics behind this are discussed in further detail here). So essentially every data point can be tested with about 1/3 of the trees. After calculating the percent of these data points with a correct prediction we get the out-of-bag error.
For simple cases this out-of-bag error is sufficient in judging the performance of the Random Forest model.
Hopefully you've gained an intuitive understanding of the basics behind Decision Trees and Random Forests. There is still a lot more to cover (such as pruning, boosting, and much more) and hopefully I'll cover this and more in a future post. For more information check out the extensive wikipedia page on the topics covered here. Thanks for reading!
",100
https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850?source=tag_archive---------4-----------------------,Machine Learning in a Week,Getting into machine learning (ml) can seem like an unachievable task from the outside.,Per Harald Borgen,6,"Getting into machine learning (ml) can seem like an unachievable task from the outside.
And it definitely can be, if you attack it from the wrong end.
However, after dedicating one week to learning the basics of the subject, I found it to be much more accessible than I anticipated.
This article is intended to give others who're interested in getting into ml a roadmap of how to get started, drawing from the experiences I made in my intro week.
Before my machine learning week, I had been reading about the subject for a while, and had gone through half of Andrew Ng's course on Coursera and a few other theoretical courses. So I had a tiny bit of conceptual understanding of ml, though I was completely unable to transfer any of my knowledge into code. This is what I wanted to change.
I wanted to be able to solve problems with ml by the end of the week, even through this meant skipping a lot of fundamentals, and going for a top-down approach, instead of bottoms up.
After asking for advice on Hacker News, I came to the conclusion that Python's Scikit Learn-module was the best starting point. This module gives you a wealth of algorithms to choose from, reducing the actual machine learning to a few lines of code.
I started off the week by looking for video tutorials which involved Scikit Learn. I finally landed on Sentdex's tutorial on how to use ml for investing in stocks, which gave me the necessary knowledge to move on to the next step.
The good thing about the Sentdex tutorial is that the instructor takes you through all the steps of gathering the data. As you go along, you realize that fetching and cleaning up the data can be much more time consuming than doing the actually machine learning. So the ability to write scripts to scrape data from files or crawl the web are essential skills for aspiring machine learning geeks.
I have re-watched several of the videos later on, to help me when I've been stuck with problems, so I'd recommend you to do the same.
However, if you already know how to scrape data from websites, this tutorial might not be the perfect fit, as a lot of the videos evolve around data fetching. In that case, the Udacity's Intro to Machine Learning might be a better place to start.
Tuesday I wanted to see if I could use what I had learned to solve an actual problem. As another developer in my coding cooperative was working on Bank of England's data visualization competition, I teamed up with him to check out the datasets the bank has released. The most interesting data was their household surveys. This is an annual survey the bank perform on a few thousand households, regarding money related subjects.
The problem we decided to solve was the following:
Given a persons education level, age and income, can the computer predict its gender?
I played around with the dataset, spent a few hours cleaning up the data, and used the Scikit Learn map to find a suitable algorithm for the problem.
We ended up with a success ratio at around 63%, which isn't impressive at all. But the machine did at least manage to guess a little better than flipping a coin, which would have given a success rate at 50%.
Seeing results is like fuel to your motivation, so I'd recommend you doing this for yourself, once you have a basic grasp of how to use Scikit Learn.
It's a pivotal moment when you realize that you can start using ml to solve in real life problems.
After playing around with various Scikit Learn modules, I decided to try and write a linear regression algorithm from the ground up.
I wanted to do this, because I felt (and still feel) that I really don't understand what's happening on under the hood.
Luckily, the Coursera course goes into detail on how a few of the algorithms work, which came to great use at this point. More specifically, it describes the underlying concepts of using linear regression with gradient descent.
This has definitely been the most effective of learning technique, as it forces you to understand the steps that are going on 'under the hood'. I strongly recommend you to do this at some point.
I plan to rewrite my own implementations of more complex algorithms as I go along, but I prefer doing this after I've played around with the respective algorithms in Scikit Learn.
On Thursday, I started doing Kaggle's introductory tutorials. Kaggle is a platform for machine learning competitions, where you can submit solutions to problems released by companies or organizations .
I recommend you trying out Kaggle after having a little bit of a theoretical and practical understanding of machine learning. You'll need this in order to start using Kaggle. Otherwise, it will be more frustrating than rewarding.
The Bag of Words tutorial guides you through every steps you need to take in order to enter a submission to a competition, plus gives you a brief and exciting introduction into Natural Language Processing (NLP). I ended the tutorial with much higher interest in NLP than I had when entering it.
Friday, I continued working on the Kaggle tutorials, and also started Udacity's Intro to Machine Learning. I'm currently half ways through, and find it quite enjoyable.
It's a lot easier the Coursera course, as it doesn't go in depth in the algorithms. But it's also more practical, as it teaches you Scikit Learn, which is a whole lot easier to apply to the real world than writing algorithms from the ground up in Octave, as you do in the Coursera course.
Doing it for a week hasn't just been great fun, it has also helped my awareness of its usefulness of machine learning in society. The more I learn about it, the more I see which areas it can be used to solve problems.
If you're interested in getting into machine learning, I strongly recommend you setting off a few days or evenings and simply dive into it.
Choose a top down approach if you're not ready for the heavy stuff, and get into problem solving as quickly as possible.
Good luck!
Thanks for reading! My name is Per, I'm the co-founder of Scrimba, and I love helping people learn new skills. Follow me on Twitter and Instagram if you'd like to stay in touch.
A publication about improving your technical skills.
2.6K 
16
",101
https://towardsdatascience.com/50-statistics-interview-questions-and-answers-for-data-scientists-for-2021-24f886221271?source=tag_archive---------1-----------------------,50+ Statistics Interview Questions and Answers for Data Scientists for 2022,An updated resource to brush up your statistics knowledge for your interview!,Terence Shin,22,"You've probably heard me say this a million times, but a data scientist is really a modern term for a statistician and machine learning is a modern term for statistics.
And because statistics is so important, Nathan Rosidi, founder of StrataScratch, and I collaborated to write OVER 50 statistics interview questions and answers. You can check out his website here!
With that said, let's dive right into it!
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
A Z-test is a hypothesis test with a normal distribution that uses a z-statistic. A z-test is used when you know the population variance or if you don't know the population variance but have a large sample size.
A T-test is a hypothesis test with a t-distribution that uses a t-statistic. You would use a t-test when you don't know the population variance and have a small sample size.
You can see the image below as a reference to guide which test you should use:
The best way to describe the p-value in simple terms is with an example. In practice, if the p-value is less than the alpha, say of 0.05, then we're saying that there's a probability of less than 5% that the result could have happened by chance. Similarly, a p-value of 0.05 is the same as saying ""5% of the time, we would see this by chance.""
Cherry picking refers to the practice of only selecting data or information that supports one's desired conclusion.
P-hacking refers to when one manipulates his/her data collection or analysis until non-significant results become significant. This includes deciding mid-test to not collect anymore data.
Significance chasing refers to when a researcher reports insignificant results as if they're ""almost"" significant.
The assumption of normality is the the sampling distribution is normal and centers around the population parameter, according to the central limit theorem.
The central limit theorem is very powerful  it states that the distribution of sample means approximates a normal distribution.
To give an example, you would take a sample from a data set and calculate the mean of that sample. Once repeated multiple times, you would plot all your means and their frequencies onto a graph and see that a bell curve, also known as a normal distribution, has been created. The mean of this distribution will closely resemble that of the original data.
The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
The empirical rule states that if a dataset is normally distributed, 68% of the data will fall within one standard deviation, 95% of the data will fall within two standard deviations, and 99.7% of the data will fall within 3 standard deviations.
A permutation of n elements is any arrangement of those n elements in a definite order. There are n factorial (n!) ways to arrange n elements. Note the bold: order matters! The number of permutations of n things taken r-at-a-time is defined as the number of r-tuples that can be taken from n different elements and is equal to the following equation:
On the other hand, combinations refer to the number of ways to choose r out of n objects where order doesn't matter. The number of combinations of n things taken r-at-a-time is defined as the number of subsets with r elements of a set with n elements and is equal to the following equation:
If you want more technical interview questions like this, you can find more here!
Confidence intervals and hypothesis testing are both tools used for to make statistical inferences.
The confidence interval suggests a range of values for an unknown parameter and is then associated with a confidence level that the true parameter is within the suggested range of. Confidence intervals are often very important in medical research to provide researchers with a stronger basis for their estimations. A confidence interval can be shown as ""10 +/- 0.5"" or [9.5, 10.5] to give an example.
Hypothesis testing is the basis of any research question and often comes down to trying to prove something did not happen by chance. For example, you could try to prove when rolling a dye, one number was more likely to come up than the rest.
Observational data comes from observational studies which are when you observe certain variables and try to determine if there is any correlation.
Experimental data comes from experimental studies which are when you control certain variables and hold them constant to determine if there is any causality.
An example of experimental design is the following: split a group up into two. The control group lives their lives normally. The test group is told to drink a glass of wine every night for 30 days. Then research can be conducted to see how wine affects sleep.
Simple random sampling requires using randomly generated numbers to choose a sample. More specifically, it initially requires a sampling frame, a list or database of all members of a population. You can then randomly generate a number for each element, using Excel for example, and take the first n samples that you require.
Systematic sampling can be even easier to do, you simply take one element from your sample, skip a predefined amount (n) and then take your next element. Going back to our example, you could take every fourth name on the list.
Cluster sampling starts by dividing a population into groups, or clusters. What makes this different that stratified sampling is that each cluster must be representative of the population. Then, you randomly selecting entire clusters to sample. For example, if an elementary school had five different grade eight classes, cluster random sampling might be used and only one class would be chosen as a sample, for example.
Stratified random sampling starts off by dividing a population into groups with similar attributes. Then a random sample is taken from each group. This method is used to ensure that different segments in a population are equally represented. To give an example, imagine a survey is conducted at a school to determine overall satisfaction. It might make sense here to use stratified random sampling to equally represent the opinions of students in each department.
A type 1 error is when you incorrectly reject a true null hypothesis. It's also called a false positive.
A type 2 error is when you don't reject a false null hypothesis. It's also called a false negative.
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
The power of a test is the probability of rejecting the null hypothesis when it's false. It's also equal to 1 minus the beta.
To increase the power of the test, you can do two things:
The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.
Eg. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times.
The Pareto principle, also known as the 80/20 rule states that 80% of the effects come from 20% of the causes. Eg. 80% of sales come from 20% of customers.
A confounding variable, or a confounder, is a variable that influences both the dependent variable and the independent variable, causing a spurious association, a mathematical relationship in which two or more variables are associated but not causally related.
There are four major assumptions:
A model is heteroscedastic when the variance in errors is not consistent. Conversely, a model is homoscedastic when the variances in errors is consistent.
Interpolation is a prediction made using inputs that lie within the set of observed values. Extrapolation is when a prediction is made using an input that's outside the set of observed values.
Generally, interpolations are more accurate.
Selection bias is the phenomenon of selecting individuals, groups or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population.
Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group.
Types of selection bias include:
Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you adding bias in the sense that you're assuming that the data is not as spread out as it might actually be.
Mean imputation is the practice of replacing null values in a data set with the mean of the data.
Mean imputation is generally bad practice because it doesn't take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score that he actually should.
Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance.
Be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
Autocorrelation is when future outcomes depend on previous outcomes. When there is autocorrelation, the errors show a sequential pattern and the model is less accurate.
Potential biases include the following:
You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis.
Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true.
Last, you would set the level of the significance (alpha) and if the p-value is less than the alpha, you would reject the null  in other words, the result is statistically significant.
A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.
3 practical examples include the power law, the Pareto principle (more commonly known as the 80-20 rule), and product sales (i.e. best selling products vs others).
It's important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.
An outlier is a data point that differs significantly from other observations.
Depending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If the outlier is caused by a measurement error, it's important to remove them from the dataset. There are a couple of ways to identify outliers:
Z-score/standard deviations: if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if it's equal to +/- 3, then it's an outlier.Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score
Interquartile Range (IQR): IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q1-1.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.
Other methods include DBScan clustering, Isolation Forests, and Robust Random Cut Forests.
An inlier is a data observation that lies within the rest of the dataset and is unusual or an error. Since it lies in the dataset, it is typically harder to identify than an outlier and requires external data to identify them. Should you identify any inliers, you can simply remove them from the dataset to address them.
The Poisson distribution is a discrete distribution that gives the probability of the number of independent events occurring in a fixed time. An example of when you would use this is if you want to determine the likelihood of X patients coming into a hospital in a given hour.
The mean and variance are both equal to .
Design of experiments also known as DOE, it is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables).
There are a number of potential reasons for a spike in photo uploads:
The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause.
Since we looking at the number of events (# of infections) occurring within a given timeframe, this is a Poisson distribution question.
The probability of observing k events in an interval
Null (H0): 1 infection per person-daysAlternative (H1): >1 infection per person-days
k (actual) = 10 infectionslambda (theoretical) = (1/100)*1787p = 0.032372 or 3.2372%
Since p-value < alpha (assuming 5% level of significance), we reject the null and conclude that the hospital is below the standard.
Use the General Binomial Probability formula to answer this question:
p = 0.8n = 5k = 3,4,5
P(3 or more heads) = P(3 heads) + P(4 heads) + P(5 heads) = 0.94 or 94%
Using Excel...p =1-norm.dist(1200, 1020, 50, true)p= 0.000159
x = 3mean = 2.5*4 = 10
using Excel...
p = poisson.dist(3,10,true)p = 0.010336
Precision = Positive Predictive Value = PVPV = (0.001*0.997)/[(0.001*0.997)+((1-0.001)*(1-0.985))]PV = 0.0624 or 6.24%
See more about this equation here.
p-hat = 60/100 = 0.6z* = 1.96n = 100This gives us a confidence interval of [50.4,69.6]. Therefore, given a confidence interval of 95%, if you are okay with the worst scenario of tying then you can relax. Otherwise, you cannot relax until you got 61 out of 100 to claim yes.
Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy.
Using the General Addition Rule in probability:P(mother or father) = P(mother) + P(father)  P(mother and father)P(mother) = P(mother or father) + P(mother and father)  P(father)P(mother) = 0.17 + 0.06-0.12P(mother) = 0.11
Since 70 is one standard deviation below the mean, take the area of the Gaussian distribution to the left of one standard deviation.
= 2.3 + 13.6 = 15.9%
Given a confidence level of 95% and degrees of freedom equal to 8, the t-score = 2.306
Confidence interval = 1100 +/- 2.306*(30/3)Confidence interval = [1076.94, 1123.06]
Upper bound = mean + t-score*(standard deviation/sqrt(sample size))0 = -2 + 2.306*(s/3)2 = 2.306 * s / 3s = 2.601903Therefore the standard deviation would have to be at least approximately 2.60 for the upper bound of the 95% T confidence interval to touch 0.
See here for full tutorial on finding the Confidence Interval for Two Independent Samples.
Confidence Interval = mean +/- t-score * standard error (see above)
mean = new mean  old mean = 3-5 = -2
t-score = 2.101 given df=18 (20-2) and confidence interval of 95%
standard error = sqrt((0.62*9+0.682*9)/(10+10-2)) * sqrt(1/10+1/10)standard error = 0.352
confidence interval = [-2.75, -1.25]
Assuming we subtract in this order (New System  Old System):
confidence interval formula for two independent samples
mean = new mean  old mean = 4-6 = -2
z-score = 1.96 confidence interval of 95%
st. error = sqrt((0.52*99+22*99)/(100+100-2)) * sqrt(1/100+1/100)standard error = 0.205061lower bound = -2-1.96*0.205061 = -2.40192upper bound = -2+1.96*0.205061 = -1.59808
confidence interval = [-2.40192, -1.59808]
The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color. Let's walk through each step.
Let's say the first card you draw from each deck is a red Ace.
This means that in the deck with 12 reds and 12 blacks, there's now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to 11/(11+12) or 11/23.
In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to 23/(23+24) or 23/47.
Since 23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards.
When there are a number of outliers that positively or negatively skew the data.
There are 4 combinations of rolling a 4 (1+3, 3+1, 2+2):P(rolling a 4) = 3/36 = 1/12
There are combinations of rolling an 8 (2+6, 6+2, 3+5, 5+3, 4+4):P(rolling an 8) = 5/36
If the given distribution is a right-skewed distribution, then the mean should be greater than 30, while the mode remains to be less than 30.
You can tell that this question is related to Bayesian theory because of the last statement which essentially follows the structure, ""What is the probability A is true given B is true?"" Therefore we need to know the probability of it raining in London on a given day. Let's assume it's 25%.
P(A) = probability of it raining = 25%P(B) = probability of all 3 friends say that it's rainingP(A|B) probability that it's raining given they're telling that it is rainingP(B|A) probability that all 3 friends say that it's raining given it's raining = (2/3)3 = 8/27
Step 1: Solve for P(B)P(A|B) = P(B|A) * P(A) / P(B), can be rewritten asP(B) = P(B|A) * P(A) + P(B|not A) * P(not A)P(B) = (2/3)3 * 0.25 + (1/3)3 * 0.75 = 0.25*8/27 + 0.75*1/27
Step 2: Solve for P(A|B)P(A|B) = 0.25 * (8/27) / ( 0.25*8/27 + 0.75*1/27)P(A|B) = 8 / (8 + 3) = 8/11
Therefore, if all three friends say that it's raining, then there's an 8/11 chance that it's actually raining.
If you want more data science interview questions and answers to prep with, you can find more here!
If you made it to the end, congrats! I hope you found this useful in refreshing and patching up your statistics knowledge. I know there's a lot to remember, but the more often you use it, the less likely you'll lose it.
As always, I wish you the best in your data science endeavors. If you liked this article, I'd appreciate it if you gave me a follow! :)
If you enjoyed this be sure to subscribe here or to my exclusive newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
Not sure what to read next? I've picked another article for you:
towardsdatascience.com
and another one!
towardsdatascience.com
",102
https://towardsdatascience.com/top-10-data-science-projects-for-beginners-a2feb5e2153e?source=tag_archive---------5-----------------------,Top 10 Data Science Projects for Beginners,Strengthen your skills and build a portfolio that stands out,Natassha Selvaraj,8,"As an aspiring data scientist, you must have heard the advice ""do data science projects"" over a thousand times.
Not only are data science projects a great learning experience, they also help you stand out from the crowd of data science enthusiasts looking to break into the field.
In this article, I am going to walk you through the projects that are must-haves on your resume.
I will also provide you with sample datasets to experiment with for each project, along with associated tutorials that will help you complete the project.
Data collection and pre-processing is one of the most important skills to have as a data scientist.
In my data science job, most of my work involves data collection and cleaning in Python. After understanding the business requirement, we need to gain access to relevant data on the Internet.
This can be done with the use of APIs or web scrapers. Once that is done, the data needs to be cleaned and stored into data frames in a format that can be fed as input into a machine learning model.
This is the most time consuming aspect of a data scientist's job.
I suggest showcasing your skills in data collection and pre-processing by completing the following projects:
Tutorial: Zomato Web Scraping with BeautifulSoup
Language: Python
Scraping reviews from a food delivery website is an interesting and practical project to have on your resume.
Simply build a web scraper to collect all the review information from all the web pages of this site, and store it in a data frame.
If you want to take this project one step further, you can use the data collected to build a sentiment analysis model and classify which of these reviews are positive and which ones are negative.
The next time you are looking for something to eat, pick a restaurant that has reviews with the best overall sentiment.
Tutorial: Build a Web Scraper with Python in 8 Minutes
Language: Python
Want to find the best online course to take in 2021? It is difficult to scroll through hundreds of data science courses to find an affordable, yet highly rated course.
You can do this by scraping an online course website and storing all the results into a data frame.
Taking this project a step further, you can also create visualizations around variables like price and rating to find a course that is both affordable and of good quality.
You can also create a sentiment analysis model and come up with the overall sentiment surrounding each online course. You can then choose to do the course with the highest overall sentiment.
Create some projects where you collect data using an API or some other external tool. These skills will usually come in handy when you start working.
Most companies that rely on third-party data often purchase API access, and you will need to do the data collection with the help of these external tools.
A sample project you could do: Use the Twitter API to collect data related to a specific hashtag and store the data in a data frame.
After collecting and storing data, you will need to conduct an analysis of all the variables in your data frame.
You need to observe how each variable is distributed, and understand their relationship with each other. You must also be able to answer questions with the help of data available.
This is work you'd be doing very often as a data scientist, perhaps even more so than predictive modelling.
Here are some EDA project ideas:
Dataset: The Framingham Heart Study
Tutorial: The Framingham Heart Study: Decision Trees
Language: Python or R
This dataset comprises of predictors such as cholesterol, age, diabetes, and family history that are used to predict the onset of heart disease in a patient.
You can use Python or R to analyze the relationships present in this dataset, and come up with answers to questions such as:
Being able to answer these questions with the help of available data is a vital skill for a data scientist to have.
Not only will this project help strengthen your skill as an analyst, it will also showcases your ability to derive insight from large datasets.
Dataset: World Happiness Report
Tutorial: World Happiness Report EDA
Language: Python
The World Happiness Report tracks six factors to measure global happiness  life expectancy, economics, social support, absence of corruption, freedom, and generosity.
You can answer the following questions when performing an analysis on this dataset:
Again, this is a project that will help improve your skillset as an analyst. A trait I've seen in most successful data analysts is curiosity.
Data scientists and analysts are always looking for contributing factors.
They are always looking to find relationships between variables, and are constantly asking questions.
If you are an aspiring data scientist, doing projects like this will help you develop an analytical mind.
When you start working as a data scientist, your clients and stakeholders will usually be non-technical people.
You will need to break down your insight and present findings to a non-technical audience.
The best way to do this is in the form of visualizations.
Presenting an interactive dashboard will help you convey your insights a lot better, as graphs are easy to understand at a first glance.
Due to this, many companies list data visualization as a must-have skill for data science related positions.
Here are some projects that you can showcase on your portfolio to demonstrate your data visualization skills:
Dataset: Covid-19 Data Repository at Johns Hopkins University
Tutorial: Building Covid-19 Dashboard with Python and Tableau
Language: Python
You will first need to pre-process the dataset above using Python. Then, you can create an interactive Covid-19 dashboard using Tableau.
Tableau is one of the most in-demand data visualization tools, and is a pre-requisite to most entry level data science positions.
Building a dashboard using Tableau and showcasing it on your portfolio will help you stand out as it demonstrates your proficiency in using the tool.
Dataset: IMDb Top Rated Movies
Tutorial: Exploring IMDb Top 250 with Tableau
You can experiment with the IMDb dataset and create an interactive movie dashboard with Tableau.
As I mentioned above, showcasing Tableau dashboards that you have built can help your portfolio stand out.
Another great thing about Tableau is that you can upload your visualizations to Tableau Public, and share the link with anybody who wants to use your dashboard.
This means that potential employers can get to interact with your dashboard, which sparks interest. Once they are interested in your project and can actually play around with the end product, you are already a step closer to getting the job.
If you want to get started with Tableau, you can visit my tutorial here.
Finally, you will need to showcase projects that demonstrate your proficiency in machine learning.
I suggest doing both  supervised and unsupervised machine learning projects.
Dataset: Amazon Fine Food Reviews Dataset
Tutorial: A beginner's guide to sentiment analysis with Python
Language: Python
Sentiment analysis is a very important aspect of machine learning. It is used often by businesses to gauge the overall customer response to their products.
Customers usually talk about products on social media and customer feedback forums. This data can be collected and analyzed to gain an understanding of how different people respond to different marketing strategies.
Based on the sentiment analysis conducted, companies can position their products differently or change their target audience.
I suggest showcasing one sentiment analysis project on your portfolio, as almost all businesses have a social media presence and the need to gauge customer feedback.
Dataset: Life Expectancy Dataset
Tutorial: Life Expectancy Regression
Language: Python
In this project, you will be predicting a person's life expectancy based on variables such as education, number of infant deaths, alcohol consumption, and adult mortality.
The sentiment analysis project I listed above is a classification problem, which is why I'm adding a regression problem to the list.
It is important to showcase a variety of projects on your resume to show your expertise in different areas.
Dataset: Breast Cancer Dataset
Tutorial: Cluster analysis of breast cancer dataset
Language: Python
In this project, you will be using a K-means clustering algorithm to detect the presence of breast cancer based on target attributes.
K-means clustering is an unsupervised learning technique.
It is important to have clustering projects on your portfolio because most real world data is unlabelled.
Even massive datasets collected by companies usually don't have training labels. As a data scientist you might need to do the labelling yourself using unsupervised learning techniques.
You need to showcase projects that display a variety of skills  including data collection, analysis, visualization, and machine learning.
Online courses aren't sufficient for you to gain skills in all these areas. However, you can find tutorials for almost every kind of project you want to do.
All you need to have is basic knowledge of Python, and you will be able to follow along to these tutorials.
Once you get all the code to work and are able to follow along properly, you can replicate the solution and work on a variety of different projects on your own.
Remember, it is important to showcase projects on your portfolio if you are a beginner in the field of data science and don't have a degree or master's in the subject.
Portfolio projects are one of the best ways to display your skills to a potential employer, especially to land your first entry level job in the field.
",103
https://towardsdatascience.com/check-for-a-substring-in-a-pandas-dataframe-column-4b949f64852?source=tag_archive---------4-----------------------,Check For a Substring in a Pandas DataFrame Column,Looking for strings to cut down your dataset for analysis and machine learning,Byron Dolon,5,"The Pandas library is a comprehensive tool not only for crunching numbers but also for working with text data.
For many data analysis applications and machine learning exploration/pre-processing, you'll want to either filter out or extract information from text data. To do so, Pandas offers a wide range of in-built methods that you can use to add, remove, and edit text columns in your DataFrames.
In this piece, let's take a look specifically at searching for substrings in a DataFrame column. This may come in handy when you need to create a new category based on existing data (for example during feature engineering before training a machine learning model).
If you want to follow along, download the dataset here.
Now let's get started!
NOTE: we'll be using a lot of loc in this piece, so if you're unfamiliar with that method, check out the first article linked at the very bottom of this piece.
The contains method in Pandas allows you to search a column for a specific substring. The contains method returns boolean values for the Series with True for if the original Series value contains the substring and False if not. A basic application of contains should look like Series.str.contains(""substring""). However, we can immediately take this to the next level with two additions:
Applying these two should look like this:
Using the loc method allows us to get only the values in the DataFrame that contain the string ""pokemon"". We've simply used the contains method to acquire True and False values based on whether the ""Name"" column includes our substring and then returned only the True values.
In addition to just matching on a regular substring, we can also use contains to match on regular expressions. We'll use the exact same format as before, except this time let's use a bit of regex to only find the story-based Pokemon games (i.e. excluding Pokemon Pinball and the like).
Above, I just used some simple regex to find strings that matched the pattern of ""pokemon"" + ""one character or more"" + ""/"". The result of the new mask returned rows including ""Pokemon Red/Pokemon Blue"", ""Pokemon Gold/Pokemon Silver"", and more.
Next, let's do another quick example of using regex to find all Sports games with ""football"" or ""soccer"" in its name. First, we'll use a simple conditional statement to filter out all rows with the a genre of ""sports"":
You'll notice that above there was no real need to match on a substring or use regex, because we were simply selecting rows based on a category. However, when matching on the row name, we'll need to be searching different types of strings for a substring, which is where regex comes in handy. To do so, we'll do the following:
Now we've gotten a DataFrame with just the games that have a name including ""soccer"" or ""football"". We simply made use of the ""|"" regex ""or"" operator that allows you to match on a string that contains one or another substring.
So we've successfully gotten a DataFrame with only names that contain either ""football"" or ""soccer"", but we don't actually know which of those two strings it contains. If we wanted to know which of the two it contained, we could use the findall method on the name column and assign the returned values to a new column in the DataFrame.
The findall method returns matches of the pattern of regular expression you specify in each string of the Series you call it on. The format is largely the same as the contains method, except you'll need to import re to not match on string case.
You'll see at the end of the returned DataFrame a new column that contains either ""Soccer"" or ""Football"", depending on which of the two the videogame name contains. This can be helpful if you need to create new columns based on the existing columns and using values from those columns.
Finally, for a quick trick to exclude strings with just one additional operator on top of the basic contains method, let's try to get all the football and soccer games that don't include ""FIFA"" in the name.
As you can see, we've simply made use of the ~ operator that allows us to take all the False values of the mask inside the loc method.
And that's all!
Working with strings can be a little tricky, but the in-built Pandas methods are versatile and allow you to slice and dice your data in pretty much whatever way you need. The contains and findall methods allow you to do a lot, especially when you're able to write some regular expressions to really find specific substrings.
Good luck with your strings!
",104
https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=tag_archive---------4-----------------------,Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You...",Adam Geitgey,11,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in  , , Tieng Viet,  or .
Giant update: I've written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
Speech recognition is invading our lives. It's built into our phones, our game consoles and our smart watches. It's even automating our homes. For just $50, you can get an Amazon Echo Dot  a magic box that allows you to order pizza, get a weather report or even buy trash bags  just by speaking out loud:
The Echo Dot has been so popular this holiday season that Amazon can't seem to keep them in stock!
But speech recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments.
Andrew Ng has long predicted that as speech recognition goes from 95% accurate to 99% accurate, it will become a primary way that we interact with computers. The idea is that this 4% accuracy gap is the difference between annoyingly unreliable and incredibly useful. Thanks to Deep Learning, we're finally cresting that peak.
Let's learn how to do speech recognition with deep learning!
If you know how neural machine translation works, you might guess that we could simply feed sound recordings into a neural network and train it to produce text:
That's the holy grail of speech recognition with deep learning, but we aren't quite there yet (at least at the time that I wrote this  I bet that we will be in a couple of years).
The big problem is that speech varies in speed. One person might say ""hello!"" very quickly and another person might say ""heeeelllllllllllllooooo!"" very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text  ""hello!"" Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard.
To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let's see how it works!
The first step in speech recognition is obvious  we need to feed sound waves into a computer.
In Part 3, we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition:
But sound is transmitted as waves. How do we turn sound waves into numbers? Let's use this sound clip of me saying ""Hello"":
Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let's zoom in on one tiny part of the sound wave and take a look:
To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points:
This is called sampling. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That's basically all an uncompressed .wav audio file is.
""CD Quality"" audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech.
Lets sample our ""Hello"" sound wave 16,000 times per second. Here's the first 100 samples:
You might be thinking that sampling is only creating a rough approximation of the original sound wave because it's only taking occasional readings. There's gaps in between our readings so we must be losing data, right?
But thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples  as long as we sample at least twice as fast as the highest frequency we want to record.
I mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality. It doesn't.
</end rant>
We now have an array of numbers with each number representing the sound wave's amplitude at 1/16,000th of a second intervals.
We could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data.
Let's start by grouping our sampled audio into 20-millisecond-long chunks. Here's our first 20 milliseconds of audio (i.e., our first 320 samples):
Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time:
This recording is only 1/50th of a second long. But even this short recording is a complex mish-mash of different frequencies of sound. There's some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech.
To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it's component parts. We'll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a fingerprint of sorts for this audio snippet.
Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes C, E and G  all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea.
We do this using a mathematic operation called a Fourier transform. It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one.
The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip:
But this is a lot easier to see when you draw this as a chart:
If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk):
A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we'll actually feed into our neural network.
Now that we have our audio in a format that's easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the letter that corresponds the sound currently being spoken.
We'll use a recurrent neural network  that is, a neural network that has a memory that influences future predictions. That's because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said ""HEL"" so far, it's very likely we will say ""LO"" next to finish out the word ""Hello"". It's much less likely that we will say something unpronounceable next like ""XYZ"". So having that memory of previous predictions helps the neural network make more accurate predictions going forward.
After we run our entire audio clip through the neural network (one chunk at a time), we'll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here's what that mapping looks like for me saying ""Hello"":
Our neural net is predicting that one likely thing I said was ""HHHEE_LL_LLLOOO"". But it also thinks that it was possible that I said ""HHHUU_LL_LLLOOO"" or even ""AAAUU_LL_LLLOOO"".
We have some steps we follow to clean up this output. First, we'll replace any repeated characters a single character:
Then we'll remove any blanks:
That leaves us with three possible transcriptions  ""Hello"", ""Hullo"" and ""Aullo"". If you say them out loud, all of these sound similar to ""Hello"". Because it's predicting one character at a time, the neural network will come up with these very sounded-out transcriptions. For example if you say ""He would not go"", it might give one possible transcription as ""He wud net go"".
The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic.
Of our possible transcriptions ""Hello"", ""Hullo"" and ""Aullo"", obviously ""Hello"" will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we'll pick ""Hello"" as our final transcription instead of the others. Done!
You might be thinking ""But what if someone says 'Hullo'? It's a valid word. Maybe 'Hello' is the wrong transcription!""
Of course it is possible that someone actually said ""Hullo"" instead of ""Hello"". But a speech recognition system like this (trained on American English) will basically never produce ""Hullo"" as the transcription. It's just such an unlikely thing for a user to say compared to ""Hello"" that it will always think you are saying ""Hello"" no matter how much you emphasize the 'U' sound.
Try it out! If your phone is set to American English, try to get your phone's digital assistant to recognize the world ""Hullo."" You can't! It refuses! It will always understand it as ""Hello.""
Not recognizing ""Hullo"" is a reasonable behavior, but sometimes you'll find annoying cases where your phone just refuses to understand something valid you are saying. That's why these speech recognition models are always being retrained with more data to fix these edge cases.
One of the coolest things about machine learning is how simple it sometimes seems. You get a bunch of data, feed it into a machine learning algorithm, and then magically you have a world-class AI system running on your gaming laptop's video card... Right?
That sort of true in some cases, but not for speech. Recognizing speech is a hard problem. You have to overcome almost limitless challenges: bad quality microphones, background noise, reverb and echo, accent variations, and on and on. All of these issues need to be present in your training data to make sure the neural network can deal with them.
Here's another example: Did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise? Humans have no problem understanding you either way, but neural networks need to be trained to handle this special case. So you need training data with people yelling over noise!
To build a voice recognition system that performs on the level of Siri, Google Now!, or Alexa, you will need a lot of training data  far more data than you can likely get without hiring hundreds of people to record it for you. And since users have low tolerance for poor quality voice recognition systems, you can't skimp on this. No one wants a voice recognition system that works 80% of the time.
For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is gold. That's the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting Google Now! and Siri on every cell phone for free or selling $50 Alexa units that have no subscription fee is to get you to use them as much as possible. Every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms. That's the whole game!
Don't believe me? If you have an Android phone with Google Now!, click here to listen to actual recordings of yourself saying every dumb thing you've ever said into it:
So if you are looking for a start-up idea, I wouldn't recommend trying to build your own speech recognition system to compete with Google. Instead, figure out a way to get people to give you recordings of themselves talking for hours. The data can be your product instead.
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I'll only email you when I have something new and awesome to share. It's the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I'd love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun! Part 7!
",105
https://medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1?source=tag_archive---------6-----------------------,Everything You Need to Know About Artificial Neural Networks,"The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but...",Josh,9,"The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we're learning more about how to improve their systems. Everything is starting to align, and because of it we're seeing strides we've never thought possible until now. We have programs that can tell stories about pictures. We have cars that are driving themselves. We even have programs that create art. If you want to read more about advancements in 2015, read this article. Here at Josh.ai, with AI technology becoming the core of just about everything we do, we think it's important to understand some of the common terminology and to get a rough idea of how it all works.
A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). If you've read anything about them before, you'll have read that these ANNs are a very rough model of how the human brain is structured. Take note that there is a difference between artificial neural networks and neural networks. Though most people drop the artificial for the sake of brevity, the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work. Below is a diagram of actual neurons and synapses in the brain compared to artificial ones.
Fear not if the diagram doesn't come through very clearly. What's important to understand here is that in our ANNs we have these units of calculation called neurons. These artificial neurons are connected by synapses which are really just weighted values. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it ""travels."" The weighted result can sometimes be the output of your neural network, or as I'll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning.
Artificial neural networks are not a new concept. In fact, we didn't even always call them neural networks and they certainly don't look the same now as they did at their inception. Back during the 1960s we had what was called a perceptron. Perceptrons were made of McCulloch-Pitts neurons. We even had biased perceptrons, and ultimately people started creating multilayer perceptrons, which is synonymous with the general artificial neural network we hear about now.
But wait, if we've had neural networks since the 1960s, why are they just now getting huge? It's a long story, and I encourage you to listen to this podcast episode to listen to the ""fathers"" of modern ANNs talk about their perspective of the topic. To quickly summarize, there's a hand full of factors that kept ANNs from becoming more popular. We didn't have the computer processing power and we didn't have the data to train them. Using them was frowned upon due to them having a seemingly arbitrary ability to perform well. Each one of these factors is changing. Our computers are getting faster and more powerful, and with the internet, we have all kinds of data being shared for use.
You see, I mentioned above that the neurons and synapses perform calculations. The question on your mind should be: ""How do they learn what calculations to perform?"" Was I right? The answer is that we need to essentially ask them a large amount of questions, and provide them with answers. This is a field called supervised learning. With enough examples of question-answer pairs, the calculations and values stored at each neuron and synapse are slowly adjusted. Usually this is through a process called backpropagation.
Imagine you're walking down a sidewalk and you see a lamp post. You've never seen a lamp post before, so you walk right into it and say ""ouch."" The next time you see a lamp post you scoot a few inches to the side and keep walking. This time your shoulder hits the lamp post and again you say ""ouch."" The third time you see a lamp post, you move all the way over to ensure you don't hit the lamp post. Except now something terrible has happened  now you've walked directly into the path of a mailbox, and you've never seen a mailbox before. You walk into it and the whole process happens again. Obviously, this is an oversimplification, but it is effectively what backpropogation does. An artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given. When it is wrong, an error is calculated and the values at each neuron and synapse are propagated backwards through the ANN for the next time. This process takes a LOT of examples. For real world applications, the number of examples can be in the millions.
Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there's another question that should be on your mind. How do we know how many neurons we need to use? And why did you bold the word layers earlier? Layers are just sets of neurons. We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.
Layers themselves are just sets of neurons. In the early days of multilayer perceptrons, we originally thought that having just one input layer, one hidden layer, and one output layer was sufficient. It makes sense, right? Given some numbers, you just need one set of computations, and then you get an output. If your ANN wasn't calculating the correct value, you just added more neurons to the single hidden layer. Eventually, we learned that in doing this we were really just creating a linear mapping from each input to the output. In other words, we learned that a certain input would always map to a certain output. We had no flexibility and really could only handle inputs we'd seen before. This was by no means what we wanted.
Now introduce deep learning, which is when we have more than one hidden layer. This is one of the reasons we have better ANNs now, because we need hundreds of nodes with tens if not more layers. This leads to a massive amount of variables that we need to keep track of at a time. Advances in parallel programming also allow us to run even larger ANNs in batches. Our artificial neural networks are now getting so large that we can no longer run a single epoch, which is an iteration through the entire network, at once. We need to do everything in batches which are just subsets of the entire network, and once we complete an entire epoch, then we apply the backpropagation.
Along with now using deep learning, it's important to know that there are a multitude of different architectures of artificial neural networks. The typical ANN is setup in a way where each neuron is connected to every other neuron in the next layer. These are specifically called feed forward artificial neural networks (even though ANNs are generally all feed forward). We've learned that by connecting neurons to other neurons in certain patterns, we can get even better results in specific scenarios.
Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn't make decisions based on previous knowledge. A typical ANN had learned to make decisions based on context in training, but once it was making decisions for use, the decisions were made independent of each other.
When would we want something like this? Well, think about playing a game of Blackjack. If you were given a 4 and a 5 to start, you know that 2 low cards are out of the deck. Information like this could help you determine whether or not you should hit. RNNs are very useful in natural language processing since prior words or characters are useful in understanding the context of another word. There are plenty of different implementations, but the intention is always the same. We want to retain information. We can achieve this through having bi-directional RNNs, or we can implement a recurrent hidden layer that gets modified with each feedforward. If you want to learn more about RNNs, check out either this tutorial where you implement an RNN in Python or this blog post where uses for an RNN are more thoroughly explained.
An honorable mention goes to Memory Networks. The concept is that we need to retain more information than what an RNN or LSTM keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other.
Sam walks into the kitchen.Sam picks up an apple.Sam walks into the bedroom.Sam drops the apple.Q: Where is the apple.A: BedroomSample taken from this paper.
Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. However, the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized. This is done by noting a certain symmetry in how the neurons are connected, and so you can essentially ""re-use"" neurons to have identical copies without necessarily needing the same number of synapses. CNNs are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels. There's redundant information contained when you look at each individual pixel compared to its surrounding pixels, and you can actually compress some of this information thanks to their symmetrical properties. Sounds like the perfect situation for a CNN if you ask me. Christopher Olah has a great blog post about understanding CNNs as well as other types of ANNs which you can find here. Another great resource for understanding CNNs is this blog post.
The last ANN type that I'm going to talk about is the type called Reinforcement Learning. Reinforcement Learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward, which means that it in itself isn't an artificial neural network architecture. However, you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before. A great example and explanation can be found in this video, where YouTube user SethBling creates a reinforcement learning system that builds an artificial neural network architecture that plays a Mario game entirely on its own. Another successful example of reinforcement learning can be seen in this video where the company DeepMind was able to teach a program to master various Atari games.
Now you should have a basic understanding of what's going on with the state of the art work in artificial intelligence. Neural networks are powering just about everything we do, including language translation, animal recognition, picture captioning, text summarization and just about anything else you can think of. You're sure to hear more about them in the future so it's good that you understand them now!
This post was written by Aaron at Josh.ai. Previously, Aaron worked at Northrop Grumman before joining the Josh team where he works on natural language programming (NLP) and artificial intelligence (AI). Aaron is a skilled YoYo expert, loves video games and music, has been programming since middle school and recently turned 21.
Josh.ai is an AI agent for your home. If you're interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.
Like Josh on Facebook  http://facebook.com/joshdotai
Follow Josh on Twitter  http://twitter.com/joshdotai
Technology trends and New Invention?
586 
3
",106
https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba?source=tag_archive---------6-----------------------,Ways to Detect and Remove the Outliers,"While working on a Data Science project, what is it, that you look for? What is the most important part of the EDA phase? There are certain...",Natasha Sharma,10,"While working on a Data Science project, what is it, that you look for? What is the most important part of the EDA phase? There are certain things which, if are not done in the EDA phase, can affect further statistical/Machine Learning modelling. One of them is finding ""Outliers"". In this post we will try to understand what is an outlier? Why is it important to identify the outliers? What are the methods to outliers? Don't worry, we won't just go through the theory part but we will do some coding and plotting of the data too.
Wikipedia definition,
In statistics, an outlier is an observation point that is distant from other observations.
The above definition suggests that outlier is something which is separate/different from the crowd. A lot of motivation videos suggest to be different from the crowd, specially Malcolm Gladwell. In respect to statistics, is it also a good thing or not? we are going to find that through this post.
As we now know what is an outlier, but, are you also wondering how did an outlier introduce to the population?
The Data Science project starts with collection of data and that's when outliers first introduced to the population. Though, you will not know about the outliers at all in the collection phase. The outliers can be a result of a mistake during data collection or it can be just an indication of variance in your data.
Let's have a look at some examples. Suppose you have been asked to observe the performance of Indian cricket team i.e Run made by each player and collect the data.
As you can see from the above collected data that all other players scored 300+ except Player3 who scored 10. This figure can be just a typing mistake or it is showing the variance in your data and indicating that Player3 is performing very bad so, needs improvements.
Now that we know outliers can either be a mistake or just variance, how would you decide if they are important or not. Well, it is pretty simple if they are the result of a mistake, then we can ignore them, but if it is just a variance in the data we would need think a bit further. Before we try to understand whether to ignore the outliers or not, we need to know the ways to identify them.
Most of you might be thinking, Oh! I can just have a peak of data find the outliers just like we did in the previously mentioned cricket example. Let's think about a file with 500+ column and 10k+ rows, do you still think outlier can be found manually? To ease the discovery of outliers, we have plenty of methods in statistics, but we will only be discussing few of them. Mostly we will try to see visualization methods(easiest ones) rather mathematical.
So, Let's get start. We will be using Boston House Pricing Dataset which is included in the sklearn dataset API. We will load the dataset and separate out the features and targets.
Features/independent variable will be used to look for any outlier. Looking at the data above, it s seems, we only have numeric values i.e. we don't need to do any data formatting.(Sigh!)
There are two types of analysis we will follow to find the outliers- Uni-variate(one variable outlier analysis) and Multi-variate(two or more variable outlier analysis). Don't get confused right, when you will start coding and plotting the data, you will see yourself that how easy it was to detect the outlier. To keep things simple, we will start with the basic method of detecting outliers and slowly move on to the advance methods.
Box plot-
Wikipedia Definition,
In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.
Above definition suggests, that if there is an outlier it will plotted as point in boxplot but other population will be grouped together and display as boxes. Let's try and see it ourselves.
Above plot shows three points between 10 to 12, these are outliers as there are not included in the box of other observation i.e no where near the quartiles.
Here we analysed Uni-variate outlier i.e. we used DIS column only to check the outlier. But we can do multivariate outlier analysis too. Can we do the multivariate analysis with Box plot? Well it depends, if you have a categorical values then you can use that with any continuous variable and do multivariate outlier analysis. As we do not have categorical value in our Boston Housing dataset, we might need to forget about using box plot for multivariate outlier analysis.
Scatter plot-
Wikipedia Defintion
A scatter plot , is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.
As the definition suggests, the scatter plot is the collection of points that shows values for two variables. We can try and draw scatter plot for two variables from our housing dataset.
Looking at the plot above, we can most of data points are lying bottom left side but there are points which are far from the population like top right corner.
Z-Score-
Wikipedia Definition
The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.
The intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.
You must be wondering that, how does this help in identifying the outliers? Well, while calculating the Z-score we re-scale and center the data and look for data points which are too far from zero. These data points which are way too far from zero will be treated as the outliers. In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.
We will use Z-score function defined in scipy library to detect the outliers.
Looking the code and the output above, it is difficult to say which data point is an outlier. Let's try and define a threshold to identify an outlier.
This will give a result as below -
Don't be confused by the results. The first array contains the list of row numbers and second array respective column numbers, which mean z[55][1] have a Z-score higher than 3.
So, the data point  55th record on column ZN is an outlier.
IQR score -
Box plot use the IQR method to display data and outliers(shape of the data) but in order to be get a list of identified outlier, we will need to use the mathematical formula and retrieve the outlier data.
Wikipedia Definition
The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3  Q1.
In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.
It is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.
IQR is somewhat similar to Z-score in terms of finding the distribution of data and then keeping some threshold to identify the outlier.
Let's find out we can box plot uses IQR and how we can use it to find the list of outliers as we did using Z-score calculation. First we will calculate IQR,
Here we will get IQR for each column.
As we now have the IQR scores, it's time to get hold on outliers. The below code will give an output with some true and false values. The data point where we have False that means these values are valid whereas True indicates presence of an outlier.
Now that we know how to detect the outliers, it is important to understand if they needs to be removed or corrected. In the next section we will consider a few methods of removing the outliers and if required imputing new values.
During data analysis when you detect the outlier one of most difficult decision could be how one should deal with the outlier. Should they remove them or correct them? Before we talk about this, we will have a look at few methods of removing the outliers.
Z-Score
In the previous section, we saw how one can detect the outlier using Z-score but now we want to remove or filter the outliers and get the clean data. This can be done with just one line code as we have already calculated the Z-score.
So, above code removed around 90+ rows from the dataset i.e. outliers have been removed.
IQR Score -
Just like Z-score we can use previously calculated IQR score to filter out the outliers by keeping only valid values.
The above code will remove the outliers from the dataset.
There are multiple ways to detect and remove the outliers but the methods, we have used for this exercise, are widely used and easy to understand.
Whether an outlier should be removed or not. Every data analyst/data scientist might get these thoughts once in every problem they are working on. I have found some good explanations -
https://www.researchgate.net/post/When_is_it_justifiable_to_exclude_outlier_data_points_from_statistical_analyses
https://www.researchgate.net/post/Which_is_the_best_method_for_removing_outliers_in_a_data_set
https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/
To summarize their explanation- bad data, wrong calculation, these can be identified as Outliers and should be dropped but at the same time you might want to correct them too, as they change the level of data i.e. mean which cause issues when you model your data. For ex- 5 people get salary of 10K, 20K, 30K, 40K and 50K and suddenly one of the person start getting salary of 100K. Consider this situation as, you are the employer, the new salary update might be seen as biased and you might need to increase other employee's salary too, to keep the balance. So, there can be multiple reasons you want to understand and correct the outliers.
Throughout this exercise we saw how in data analysis phase one can encounter with some unusual data i.e outlier. We learned about techniques which can be used to detect and remove those outliers. But there was a question raised about assuring if it is okay to remove the outliers. To answer those questions we have found further readings(this links are mentioned in the previous section). Hope this post helped the readers in knowing Outliers.
Note- For this exercise, below tools and libaries were used.
",107
https://medium.com/hackernoon/how-to-crush-the-crypto-market-quit-your-job-move-to-paradise-and-do-whatever-you-want-the-rest-27a4a3cc2bb1?source=tag_archive---------2-----------------------,"How to Crush the Crypto Market, Quit Your Job, Move to Paradise and Do Whatever You Want the Rest of Your Life",Do you like making money at the push of a button?,Daniel Jeffries,25,"Do you like making money at the push of a button?
How about working from home in your underwear? Do you hate corporate meetings worse than the plague? Would you rather be traveling the world, having adventures and sleeping on a beach in Thailand than grinding your life away in an office for fifty years?
Then you might just be a trader, my friend.
But what kind of trader are you?
Are you looking to make steady, slow, conservative profits, year after year and protect yourself from big risks? Or are you looking to hit the home run trade, knock it out of the park and retire early to travel the world eating fine food and hiking in the rain forest?
Of course, there's no shame in going conservative. It's a lot easier. Just buy and hold some assets. If that's your game, stick to my easy button strategy, Mastering Shitcoins, the Poor Man's Guide to Getting Crypto Rich. In my opinion, that's the best way for someone who has a life and obligations and a family and who can't afford to stay welded to their computer screen all day. It's a simple plan and one I employ with part of my portfolio.
But there's another kind of trading strategy too. It holds the potential to deliver the kind of massive returns that most people can only dream about in their wildest fantasies.
Momentum trading.
This is where you're looking to hitch a ride on a rocket and crush the market. You don't just want slow and steady results, you want a grand slam and you want it fast. I'm not talking 20% returns. I'm talking 1000% or 2000% or 20,000% returns.
Impossible? Stupid to even try? Crazy?
Maybe.
The real question is, can it actually be done?
Turns out it can.
How do I know? Because I've gone out and found the traders who've done it.
I've sat at their feet and soaked up their wisdom. I've listened and learned. I've made money and lost money so you don't have to make all the mistakes. My Padawan training is complete.
Now I'm ready to show you what I've learned from these legendary masters. In this brand new series of articles, The Ultimate Guide to Crushing the Crypto Markets, I'm going to share all of their best strategies and secrets.
Some of these folks have paid trading dojos and private groups and training materials.
I'm giving you everything I've learned for free.
Why would anyone do that?
Because it's what I'm wired to do. I'm an author, a teacher and an open source idea factory.
Knowledge wants to be free.
And one of the best ways to find the wisdom of the ages is to study with the best of the best.
Studying the best in the world is a time honored strategy going back to Dale Carnegie and Napoleon Hill and other self help masters. But if I'm being honest, I've never really been a fan of self-help gurus. They always seem long on motivational pep talks and short on actual technique.
I want practical trading techniques, not a lot of fluff.
That's what I got.
So let's leap right in and learn how to lay siege to the market and storm the citadel of the 1%.
(Psst. If you're really impatient you can skip to the end of the article and see the charts of actual cryptos on the move. Actually if you're impatient, stop reading right now because the market is designed to steal money from impatient people.)
Let's roll!
First off though you're probably wondering who are these people?
My criteria was simple. I wanted to meet traders who started with between $1,000 and $45,000 and turned it into millions of dollars in under two years.
The second criteria was that they actually did it. Like Tim Ferris said in 4 Hour Work Week, no pretenders allowed.
And I didn't want people who just bought and held in the biggest parabolic boom in history. I wanted traders, people who made more than buy and holders.
Yes, that is possible.
Still you probably don't believe these people exist or that it is possible.
It's ridiculous right? Nobody can beat the market.
And yet I've studied with five different traders who've done just that, believe it or not.
Actually I don't ask you to believe anything because belief is the death of intelligence. All you need to do is look, listen with an open mind, learn and then decide for yourself. Every single one of these people, four guys and one gal, stressed the need to transcend your limiting belief systems and believe it can be done before you do anything else.
Jordan Belfort, the Wolf of Wall Street, said ""The only thing standing between you and your goal is the bullshit story you keep telling yourself as to why you can't achieve it.""
That's just the thing. Most people don't believe it's possible or they think the people who did it got lucky.
If you believe that you can or you can't, you're right. I used to think that was some hippy dippy crap, but it turns out it's literally true. If you don't think it's possible you can't even get off your ass and get started.
But this is not going to be some power of positive thinking new age nonsense puff piece. Positive thinking only gets you to the starting line. You can visualize a million dollars and a Lamborghini all you want but that's not going to get you there. Only hard work, discipline and study will. Making mistakes and learning from them will.
But first you have to think it's possible.
And yet even after reading that I've met and worked with five people who've done this, most people will write it off as impossible. To do that they have to actively deny evidence to fit what they already believe. That's how belief systems work. We're wired to think we're right even when we're wrong. It's crazy but it's true.
If I walk into a room of people and ask them who's a better than average driver, 90% of them will raise their hand. The other 10% won't raise their hand because they know I'm asking a trick question but secretly they believe it too. Everybody does.
Since it's impossible for everyone to be better than average there's only one conclusion we can come to here.
Most people are lying to themselves and to the world.
There's reality and there's what you think about the world. Usually they're not the same thing. When your beliefs meet reality, reality wins every time if they're not in sync.
You might think you can eat whatever crap you want and never work out but after eating burgers and fries for forty years you will learn your last and hardest lesson.
So that's step one from every single one of these legendary traders.
Get your mind right.
Learn to see yourself truthfully.
If you can't do that, you won't get anywhere. It's like those old long division problems you hated in school where you got step one wrong and the other fifteen steps were wrong automatically.
You can't start from the wrong premise and hope to come to the right conclusion.
How do you learn to see yourself more clearly?
Ask questions.
What are you good at? What do you suck at? When you fail, why did you fail? Be relentless in answering these questions. Did you over trade? Were you in a bad state of mind? Were you trading drunk? Did you use too much leverage? Were you seeing phantom patterns that don't exist? Do the patterns even work at all? If so which ones do and which ones don't?
Why do you just automatically assume that because you saw a bull flag on Twitter or in the bible of technical analysis it's true? Maybe bull flags are crap? Ever think of that? Who told you they were true? Accept nothing at face value until you test it for yourself. If a pattern continually fails you, assume it's wrong and move on until proven otherwise. Be flexible. Be adaptable.
If you want to get good at anything you have to learn to see your strengths and weaknesses accurately.
If you don't start with the truth you can't possibly get anything else right.
First the truth, then everything else.
Step two that every single one of these traders emphasized is that you have to learn this yourself.
If you're out there looking for a signals group so you can buy like a robot, you've already failed. You can join a group if you like but only if that group is there to teach you to fish not keep you dependent on the group.
What do I mean by that?
Give a man a fish and he eats for a day. Then he promptly starves to death waiting for the next free fish. Teach a man to fish and he eats for a lifetime.
You want to become the master yourself not sit at the feet of masters your whole life picking up their scraps.
This is an active process. You have to get in there and learn by doing.
You have to lose money, make money, lose it again. You have to ride the insane emotional waves as you win big and then lose it all. You have to think you're a God one day and the market is your personal ATM only to get taken out to the woodshed and beaten with a rubber hose. That's how this works. Nobody can teach you everything you need to know. You have to do it!
But that doesn't mean we can't learn the basics right here, right now. Even if we can't fully understand or appreciate or believe them yet, we can get them into our minds where they can take root and start to grow and flourish.
Sometimes we're not ready to understand something yet. They say the fool hears the truth a million times and never gets it but the wise man only needs to hear it a hundred thousand times before he gets it. Nobody gets it the first time or the second or the fiftieth.
But the key principles sit dormant in our unconscious and suddenly come back to us somewhere down the road when we're ready for them.
We're ready for them when we've done the work ourselves. Do the work!
Then one day you'll say ah! That's what he meant! I get it now.
Lastly, it's worth noting that not all of these folks are exclusively crypto traders. In fact, most of them trade all kinds of markets.
But every one of them will tell you that the best trading systems work across markets.
Trading small cap stocks on the Nasdaq or commodities like gold and oil is the same as trading Bitcoin and Litecoin. They just move at different speeds, have varying liquidity and efficiency and some of the ways to value the fundamentals very wildly.
The markets change their external manifestations but human psychology never changes and the markets are driven by very human hopes and dreams and fears.
Now that we've started to get our minds right, let's get to the good stuff, the stuff you've all been waiting for since line one: the killer trading strategy.
This lesson was one of my favorites and it's dramatically changed my trading.
In all markets, in all times, there is one trade to rule them all.
The parabolic swing trade.
That's when a small, virtually unknown stock or cryptocoin takes off on a wild ride to glory. It goes up 10X or 20X or more. It starts at 20 cents or $5 and climbs to $30 or $40 or $50. In crypto it gets even crazier. A coin that was trading at $1 a few years ago might now be trading at $100s or $1000s of dollars. The crypto market is unlike any other in terms of how far and how fast it moves. But again, the principals are the very same.
I'm not going to lie. This strategy can and does result in financial ruin for most traders. High risk. High reward.
When government officials say Bitcoin is ""highly speculative"" through gritted teeth, they don't mean that as a compliment. They mean to strike fear into the heart of traders everywhere. Stay away. Don't touch. Danger!
But highly speculative is where the big money is, no question about it.
Every trader who has ever cashed in big on the markets was a gun slinging speculator. While the sheep of the tribe think that's a bad thing, it's the early risk takers who grow markets when nobody else is willing to touch them. Speculators make the market boom, so that it reaches an equilibrium and serves the masses later. That's how it works and that's how it's always worked.
For every ten thousand people who turn back when they see the warning sign at the mouth of the dark forest, there's an Indiana Jones who straps on his whip and his leather jacket and strikes out into the unknown. He's looking for the big score, the ultimate trade. He's hunting buried treasure!
But the promise of glory also brings the promise of crushing defeat.
This is a death defying trade. It makes and breaks great fortunes. It's a massive mountain that rises into the stratosphere and it's littered with the bones of broken traders.
So how do you catch this crazy, wild trade?
First off you have to know a little secret.
Every market and asset is a bubble at some point.
Every stock, bond, coin, commodity or market goes through boom to bust, over and over and over.
It's just a question of how fast it happens.
Most people don't see it because it happens every ten or fifteen years in mature markets. In less liquid and early stage markets like cryptocurrency it happens much faster.
But it happens.
Every. Single. Time.
You don't know how or when it will happen but it will happen, no matter what the market or the year. The markets in the 1800s are no different from the markets now. The gold market is the same as the stock market and the bond market. The markets before, during and after the dot com bubble are the exact same. They just move faster or slower.
The key to breaking through in your trading is to understand the market cycle.
You also have to understand that the market cycle is a large fractal of each individual asset which goes through this very same cycle at different times. Sure there are the big market movements, boom to bust, but nobody can time those really. Trying to beat The Market, with a capital M, is a fool's game. The bust will come when it comes.
But you can capture the cycle of individual assets consistently. That's what these master traders do.
Now you've probably seen the cycle before, usually as a warning to stay away or posted by people laughing at bubbles. Like all simple truths, you ignored it and stepped right over it thinking it has to be more complex or that you already have it all figured out.
But what you find again and again, if you take the time to understand and master any skill is that simple is better. You also realize that what you thought was too simple to be the truth is actually all there is to it.
The truth is that which cannot be simpler.
The people who achieved great success in life mastered a few fundamental and universal truths better than anyone else.
So what does the market cycle look like?
Here you go.
I know. I know. You've seen it a million times. You got it.
But stick with me for a moment here. You've seen this a million times but have you ever really seen it?
Be honest. Probably not.
Now stop reading for a minute. Look at this graphic. Really look at it. Burn it into your brain. Then come back. The article can wait. Take as much time as you need.
Back?
Cool. Now your first thought is that this graphic only applies to bubbles and tulip crazed assets like gold and Bitcoin, right?
Wrong.
This cycle applies to all assets, at all times, in all markets. The cycle is eternal.
This is the universal market cycle.
No asset is exempt.
Every asset will follow this pattern at some point. The mountain might be spread out over many, many years, which is why most people miss it. They're zoomed in too far.
Zoom out!
Look at the market over time and you'll see this pattern again and again and again.
Now I know you've been taught that the market is random chaos. The efficient market hypothesis says that all market information is perfectly distributed and priced into an asset and eventually the market beats everyone. This is utter and complete nonsense. The only people who believe it are academics and people who have never risked a single cent in their whole life.
Information is absolutely not evenly distributed. It's asymmetrical. And just because everyone gets the same information eventually doesn't mean they know what to do with it. Most people simply can't process that information correctly and make good decisions. They can't separate the signal from the noise.
In other words, the average person hasn't gotten a whole lot smarter.
That's why I can give out this information to whoever wants to read it. I don't need to keep strategies a secret because most folks won't believe it or have any idea what to do with it.
Remember the legendary Turtle Traders experiment? That's when some great market masters wondered if trading was nature or nurture. In other words could they teach people the rules and turn them into trading superstars who made millions?
Here are the complete rules they used, free of charge, written by one of the original turtles. (Quick note: I had the wrong link in there earlier. Fixed now.)
He gave them away.
Why?
Because most folks won't use them, won't know how to apply them or think the rules are too old and the markets have already moved past them.
And even if they understand these rules, they won't follow them in times of great stress. The market loves to play with your emotions like a cat playing with a string. One of the greatest moments in turtle trader history is a guy who lost thirty two trades in a row on coffee and gave up, swearing the system would never work. He walked away and never returned.
The very next coffee trade skyrocketed.
The market is ""global psychological warfare,"" as legendary trader Jesse C. Stine writes in the single greatest book ever written on badass momentum trading, Insider Buy Superstocks: The Super Laws of How I Turned $46K into $6.8 Million (14,972%) in 28 Months. Stop reading and go buy it right away. It will change your trading game overnight. Get it and come on back. I'll wait.
I know. I know. The title just screams scam. BS. Crap. Nonsense.
But it's not. Despite it's super click bait title, it's filled with tremendous wisdom and practical steps from someone who actually did it. This is not a pretender. This is someone who delivered and decided to share his knowledge with the world.
That is incredibly rare.
Most people have a breakthrough realization and they just drop out of society after they get it. Once you figure out a great truth there's nothing to do but execute on it. Why bother posting in any forum anymore? Why bother asking for anyone else's advice? There is nothing more to learn from those places. So the master just fades away, trusting nothing but his own internal compass now.
And besides if you write down a great truth, it doesn't matter because truth must be earned by each individual. It can't be bought or cheated or even learned without personal dedication and hard work.
So why bother? Not only will most people ignore you, even worse some will turn actively hostile to you. They'll hate you for your knowledge. The people suffering the worst, the ones who live their whole life in fear and who live with a constant undercurrent of self loathing will lash out at you, call you a liar and a fool and a scammer.
And that's why most people don't even bother to teach. They just master Kung Fu and disappear to the temple to practice in simple joy for the rest of their life.
But some people even go beyond that and become great teachers because there's a special kind of joy in sharing knowledge that is unmatched by anything else in life and makes it all worth the suffering.
That's why Stine shared his knowledge and I will too.
So let's look at some real crypto charts to see the market cycle in action.
Here is Bitcoin after its huge parabolic run up the mountain starting in January of 2017 and taking off big time in the summer before starting to crash out in Dec.
Do you see the mountain?
Do you see us coming down the mountain?
Go ahead and zoom out on Bitcoin to the 2 hour or 4 hour chart and lay that market cycle graph right on top of it. What do you see? Ignore all my old lines. Those are for me and mean nothing to you. Just look at the Bitcoin price movement alone.
Looks the same right?
Of course, not every single asset follows that chart precisely. Don't get fundamentalist on it. Bitcoin can come down and decide to boom up again half way through, but just be prepared if it doesn't.
How do you know when a trend has changed? When the trend is changed.
Perhaps the greatest trader of all time, Ed Seykota, who you can read about in the great book, Market Wizards, is a man who's returned 40% to 60% every year since the 1980's with his own system. Yet when people go to hear him talk, they think he's kind of an idiot. That's because Buddha like gurus speak very directly and simply. Behind that simplicity is great power. It's just people can't hear it because they're looking for something more complex.
Someone asked him ""How do I know the trend is up?""
He said ""When the trend is up.""
Most people hear that and think he's trolling them or he's a sarcastic jerk.
He's not. He just gave you the real answer.
Did you miss it?
You only know the trend is up when it is consistently moving up.
Two points on a graph are not a trend. Three points are only the start of a trend.
The most basic truths in life sound absurd or ridiculous. But they're still the truth.
People spend a lot of time trying to guess when the trend will change. They react to the first green candle in a downtrend or the first red candle in an up trend. They keep guessing. Ed does not guess. He follows the trend as far as it will go and then he waits for confirmation that the trend has changed.
Confirmation only comes over time. That means you won't catch the exact bottom or sell at the exact top.
So how do you know when the exact top is in or the exact bottom?
You don't. Not until it happens. And at that point you adjust.
It sounds simple but it's ridiculously hard in practice because you're trading against your emotions and your most basic instincts for self preservation which are screaming get out now! It will all come crashing down! The end is near!
Humans are fear driven animals. We constantly imagine terrible tragedies in order to try to react ahead of them. But the problem is that you spend your whole life reacting to imaginary events instead of the one real one. That is the path to suffering and pain.
Tragedies will come eventually. Deal with them then and not a second before that time.
Here's a riddle. Is the bottom for Bitcoin $14,000? How about $11,000? How about $8,000? $4,000? $400?
Answer: Nobody knows.
Anyone who says they do is fooling themselves and you.
We can draw support lines and those are very useful. When a price bounces off that support we get a sense that the direction is changing or continuing. A good support trend indicator is the Exponential Moving Average 200, 50 and 10 or EMA 200, 50, 10 on the 2 hour chart in crypto. In more established markets, the EMA 10 on the weekly chart is about all you need.
But remember that just because something bounces off our predicted support does not mean the trend is really reversed. The price will often bounce and test that support again and again. The trend can go sideways until it decides to go down or up again.
See what the great game is giving you. Play the board.
The trend is down until it's not. It's going sideways until it's not. It's going up until it's not.
Always bet with the trend. When your trades start to go sour, pause and stay out. Wait for the next trend to confirm and then go with that trend.
If the price action is bouncing around in a sideways channel then sell at the top of that channel after it starts to sink down after bouncing off the top line. If it hits the bottom, buy. Let it approach the top of the channel and see if it breaks out and bursts to the sky. If it does, hold. If not, sell it again and assume the sideways trend is going to continue.
Take a look at this graphic of the ultimate pattern to trade for assets.
Every time I show this graphic to people they start asking me about when the breakout from the bottom basing channel is going to come. I have no idea. Tomorrow. Next week. Next month. Next year.
But how do you know, they ask? You don't.
You just buy when it actually breaks out.
Now from there the price can do several things. It can crash back down into the channel. That's where you sell with your stop loss. It's a failed break out. Or it can keep going, so you stay with it. At some point it will start to pull back, but be sure to zoom out and see if it's still moving in its uptrend, even if it's pulling back. Stop looking at five minute charts and thirty minutes charts. Waste of time.
Now let's take a look at Zcash.
Notice how it's been through a massive run up only to crash down and then finally stabilize with a long base.
Right now, its linear regression channel, which you can see in red and blue, is pointing up. You can find the linear regression indicator on Trading View. It automatically draws based on the scatter plot of closing candle prices. Linear regression finds the best line through a trend, whether that is up, down or sideways.
At some point the coin will start to rally out of its strong base and start another journey up the mountain.
Now let's take a look at Ripple starting in on its big run.
It's previous mountain move is off the chart, but it did the same thing as the other two. Just zoom out if you want to see it. It went way up, then crashed down and then slowly slid and formed a strong base before breaking out on high volume above its 200 bar exponential moving average on the 2 hour charts.
I've discovered that cryptos move a lot faster than more liquid and efficient markets like the Nasdaq or New York Stock Exchange. Whereas a daily or weekly chart will serve you much better trading those markets, the short time frame of two to four hours works better for catching the runs in crypto.
Don't take that as gospel though. Go ahead and see for yourself. If you find a better time line, go with that one. A good time line has fewer moves through it not more. Fewer. Less trades are better. Stop over trading.
Notice that XRP has gone up three legs here on its parabolic run. I bought after the second run up and pullback because it was a double confirmed up trend on high volume. It burst out above its EMA 200 and ran. Then it pulled back, staying above that line before taking off again. I first noticed it breaking out of its base at a high ""angle of attack"" as Stine calls it. That is my red line that goes up at a 45 degree angle along the top of the trend.
Usually there are five waves to these runs. While other traders were calling the rally over, I decided to buy because I could see the parabolic run pattern forming and only two steps in its clearly defined ladder.
Other traders were calling the top because they're trading their belief systems and psychology and refusing to see the chart as it was actually playing out. That's because most folks in crypto don't like the centralized nature of Ripple.
I don't either. I hope Ripple fails long term in favor of a more decentralized asset. That said, when it comes to trading, I don't have the luxury of wasting time with my moral belief system.
I will happily take money from Ripple and then move it into assets I care about long term. Ripple will fail or succeed no matter what I do. The market will ultimately decide if it's worth keeping around.
But if you let your beliefs blind you to what is right in front of you, you will fail.
The market doesn't care about your opinions or what you like and don't like. It does what it wants, when it wants, and this chart was signalling one thing: XRP was ready to really break out. So while others were shorting I went long and captured the third wave in its big new run.
I then sold for profits as it started to pull back and shorted it to max profits. When it stabilized I bought back in for the next wave.
Understanding the truth will set you free.
Your beliefs are a prison system you've created for yourself.
But here's the thing. The door is not locked. It never was and you can escape your limiting ideas about reality any time you want to open that door.
So do it. Open that door.
Observe. Don't imagine. See the chart as it actually is and then you can begin to make good decisions.
Use trial and error to test your observations. If your observations are wrong, discard them ruthlessly and move on to the next observation.
The market cycle is universal. Capturing the parabolic run is the holy grail of trading in any market. It's where the big money is and where people get rich playing the market.
Go ahead and look at almost any chart and you will find this pattern on a long enough time line. When it comes to new stocks, aka new companies, or new cryptos there isn't enough time for it to form correctly. So basically you will just see a flat trading baseline.
From there you can only go and look at the project fundamentals. Read the white paper. Follow the team. Get on their Slack channels. Get a sense of whether they will succeed or fail over time. Then you can decide if you want to take a chance and invest in them for an eventual run at glory.
But go look at any asset or coin that's been around a bit and you'll see The Pattern. Doesn't matter if it's Zcash or Amazon or Facebook or Netflix or gold or oil or sugar.
Don't get caught up in looking for the pattern to manifest perfectly every time. This is not some rigid pattern that always happens precisely. Sometimes a stock or coin gets really choppy, thrashing up and down or moves like a wild snake but eventually, slowly but surely it will form the mountain run and collapse.
The key is to know how to buy at the bottom and get out at the top.
That's the part buy and holders always miss. They get to laugh at traders on the way up because the trader didn't get to catch the entire movement. They got out a little early and miss the crazy explosion at the very end but traders get the last laugh as the market or asset starts to slide because they sell and the holders keep holding as 40% or 85% of their gains vaporize.
Master the art of selling!
If you don't know when to sell what you're buying and holding what's the point?
Once you understand this and recognize it as true, only then can you start to dominate the market consistently.
And then maybe, just maybe, you might be able to quit your job, move to paradise and do whatever you want for the rest of your life.
Happy trading.
############################################
DISCLAIMER: Be a big boy or girl and make your own decisions about where to put your hard earned money. I am not a financial adviser and this is not financial advice and if I really need to tell you this then it's best to keep your money under a mattress anyway because when you lose it you'll only blame other people for your mistakes rather than yourself.
############################################
############################################
If you love the crypto space as much as I do, come on over and join DecStack, the Virtual Co-Working Spot for CryptoCurrency and Decentralized App Projects, where you can rub elbows with multiple projects. It's totally free forever. Just come on in and socialize, work together, share code and ideas. Make your ideas better through feedback. Find new friends. Meet your new family.
############################################
############################################
A bit about me: I'm an author, engineer and serial entrepreneur. During the last two decades, I've covered a broad range of tech from Linux to virtualization and containers.
You can also check out the Cicada open source project based on ideas from the book that outlines how to make that tech a reality right now and you can get in on the alpha.
############################################
############################################
#BlackLivesMatter
32K 
75
",108
https://medium.com/@allenfarrington/bitcoin-is-venice-8414dda42070?source=tag_archive---------5-----------------------,Bitcoin Is Venice,Rhapsody on a Theme of Nakamoto,allen farrington,35,"Rhapsody on a Theme of Nakamoto
NOT FINANCIAL ADVICE. DO YOUR OWN RESEARCH.
Quentin Skinner's monumental overview of the development of early modern political philosophy, The Foundations of Modern Political Thought, begins with the following lines,
""As early as the middle of the twelfth century the German historian Otto of Freising recognised that a new and remarkable form of social and political organisation had arisen in Northern Italy. One peculiarity he noted was that Italian society had apparently ceased to be feudal in character.""
While Skinner's concern is political philosophy and not economic history, it is easy enough to identify that these social changes were made possible by a nascent form of capitalism. As the great medievalist Henri Pirenne commented on the period and region in his Medieval Cities,
""Lombardy, where from Venice on the east and Pisa and Genoa on the west all the commercial movements of the Mediterranean flowed and were blended into one, flourished with an extraordinary exuberance. On the wonderful plain cities bloomed with the same vigor as the harvests. The fertility of the soil made possible for them an unlimited expansion, and at the same time the ease of obtaining markets favored both the importation of raw materials and the exportation of manufactured products. There, commerce gave rise to industry, and as it developed, Bergamo, Cremona, Lodi, Verona, and all the old towns, all the old Roman municipia, took on new life, far more vigorous than that which had animated them in antiquity.""
Pirenne added that the rise of these cities, which was predicated on commercial and industrial expansion,
""strongly stimulated social progress. It made no less a contribution in spreading throughout the world a new conception of labor. Before this it had been serf; now it became free, and the consequences of this fact, to which we shall return, were incalculable. Let it be added, finally, that the economic revival of which the twelfth century saw the flowering revealed the power of capital, and enough will have been said to show that possibly no period in all history had a more profound effect upon humanity.""
And wouldn't you know it, but feudalism seems to be making a comeback. Joel Kotkin introduces his pithy tract, The Coming of Neo-Feudalism, anticipating this re-emergence:
""Of course it will look different this time around: we won't see knights in shining armor, or vassals doing homage to their lords, or a powerful Catholic Church enforcing the reigning orthodoxy. What we are seeing is a new form of aristocracy developing in the United States and beyond, as wealth in our postindustrial economy tends to be ever more concentrated in fewer hands. Societies are becoming more stratified, with decreasing chances of upward mobility for most of the population. A class of thought leaders and opinion makers, which I call the ""clerisy,"" provide intellectual support for the emerging hierarchy. As avenues for upward mobility are diminishing, the model of liberal capitalism is losing appeal around the globe, and new doctrines are arising in its place, including ones that lend support to a kind of neo-feudalism.""
Not all, but certainly some, of these afflictions can readily be attributed to the normalized strip mining of capital in the pursuit of ever more leveraged ""growth"" I outlined in Part Two of this series, The Capital Strip Mine. Those who do not own hard assets are increasingly tending to drown in debt from which they will realistically never escape, unable to save except by speculation, and unable to afford the inflation in the essential costs of living that does not officially exist. What amounts to an ""official"" message is the likes of Christine Lagarde (then president of the IMF and now of the ECB) musing that, ""we should be happier to have a job than to have our savings protected,"" and the World Economic Forum projecting that, by 2030, ""you will own nothing, but you will be happy."" You will use things that somebody owns, mind you. But that somebody will not be you.
If we were to believe that these people actually mean what they say, and that the strip mining of capital is not going to stop  indeed, that it cannot stop  we might be as similarly inclined as Otto of Freising to look for any sprouts of civilization that manage to advance beyond our rebooted feudalism. There may end up being a variety of reasons that different groups avoid this state. I think that, for some, the reason will be Bitcoin.
What does that mean? I am sure it seems hyperbolic to most, if not outright ludicrous, but it's actually quite prosaic. It means that those social units that voluntarily choose to liquidate their positions in self-referentially mispriced toxic loans in favor of a global, digital, sound, open-source, programmable money will be in a position to accumulate long-term oriented capital at a disproportionate rate to those who do not. They will have a superior economic foundation from which to build healthy social and political institutions, which will contrast to those left behind as medieval Venice did to the remnants of the Western Empire. This could be true at any and every scale. It could be an individual, a family, a friend group, a neighborhood, a company, a city, an industry, a country, or the entire world. We will have to wait and see.
Of course, it could be nobody. It could fail altogether. I say this primarily to guard against accusations of blind faith, speculative mania, and fundamental unseriousness. But I don't say it to feign intellectual sophistication with post-hoc unfalsifiable fence-sitting. As if this wasn't entirely clear already, I am very happy indeed to be on the record as saying it is overwhelmingly likely Bitcoin will succeed. And so, while there are good reasons it might fail, ""it's dumb,"" and, ""I don't like it"" are not among them. In order to sensibly articulate the reasons why it might fail you have to understand it in the first place. Most do not. As I explained in Part One of this series, Wittgenstein's Money, most do not even know what it is they are looking at. Nor are they likely to any time soon because they don't want to see it. As philosopher of science Norwood Russell Hanson might say, their perception is theory-laden. Also, their theories are wrong. Oops.
And so, in the spirit of such colorful outrage as ""it's a Ponzi scheme!"", ""it's a waste of energy!"", and, ""it's backed by nothing!"", I will round off this trilogy with my own set of colorfully outrageous metaphors to try to help people understand what is actually happening right now, and why things seem exactly like they should.
So outrageous they might just be accurate ...
Bitcoin is Ariadne
Anyone who accumulated large amounts of wealth while remaining independent of military-political command structures faced the problem of safeguarding what he had gained. Unless a merchant could count on the protection of some formidable man of power, there was nothing to restrain local potentates from seizing his property any time his goods came within reach. To gain effective protection was likely to be costly  so costly as to inhibit large-scale accumulation of private capital.
- William McNeill
Bitcoin is often framed as ""competing"" with fiat currency. This is true in a sense but I fear there is a rhetorical danger of invoking the wrong kind of ""competition"". It is not a fight, for example. There is no conflict. Bitcoin is not trying to damage or sabotage its opponents, because it isn't trying anything and it knows no opponents. It has no awareness whatsoever of who might oppose it or why. It is simply an alternative; an exit valve; an opt-out. It is competing only insofar as it is proving to be a far superior alternative. It is not a sword for Theseus to fight the Minotaur, but a thread to follow to exit the labyrinth. Bitcoin is Ariadne.
There will be tremendous value in normalizing this rhetoric amidst the likely growing chorus of opposition desperate to smear Bitcoin as inherently nefarious, or hostile, even. Opponents must be forced to explain what is wrong with people interacting freely, and why true goodness can only follow from coercion, in their understanding. Should those who have found a way out of the unbearable labyrinth of capital strip mining not take it? What do they owe the Minotaur?
Does anybody really believe that, having fully understood the choice they face, any individual would choose to save in a self-referentially mispriced toxic loan rather than a provably sound digital bearer asset? Or, more simply still, that they will think it makes less sense to hold money that is a pure asset than money that is literally defined as a liability? Why not opt into a financial system that is built on trustless verifiability rather than unverifiable trust?
Threats of violence, perhaps? After all, the only way to ""seize"" properly secured bitcoin is through torture. In, The Pursuit of Power, historian William McNeill interprets efforts in early modern Europe to industrialize and standardize weaponry and military drill as having the effect that, ""the magnitude and controllability of organized violence per tax dollar went up  spectacularly."" It seems reasonable to suggest the potential for a similarly spectacular fall in such returns of late, as German prosecutors and Reuters alike recently discovered:
It is worth working through the optics of any decision to engage with Bitcoin in a truly hostile manner, because it is certainly coming. McNeill reminds us that, even some seven-hundred-or-so years ago, ""the breakdown of established patterns of conduct always appears deplorable to a majority of those who witness it."" By no means do I have a utopian outlook on this subject  rather, it is something of an intellectual rite of passage to accept the nonzero utility of dystopian paranoia. Bitcoin will be banned, many times, in many places. But a ban is an open admission of practical and moral failure and is arguably the best advertisement of all. A ban is the Berlin Wall; fragments of any ban will one day become souvenirs of the folly and cruelty of repression. Bitcoin doesn't force anybody to stay. They come, and then they stay, because they want to  because it is both practically and morally superior.
As with East and West Berlin, it is also worth working through the likely ripples on society at large of the foundational difference of valuing and enshrining voluntary interaction. Yes, Bitcoin has different mechanics  I will get to this further down  but from these mechanics follow different behaviors; from these behaviors, different cultures; from these cultures ... who knows?
I don't profess to know, but I can offer some ideas. First, we are wholly unprepared for the societal implications of making most wealth and much capital entirely mobile. We have been edging in this direction for decades as software has eaten the world, as Marc Andreessen, professional baller, famously put it. As I articulated here, Andreessen's text remains probably the most important treatise on finance written this century, and yet many in finance have not read it, and many who have think it is not about finance but about tech. It is only about tech insofar as by ""tech"" you mean ""software"", by ""software"" you mean ""everything"", and by ""everything"" you mean ""finance"". So, you are right, but in at least three different wrong ways. Maybe more.
My preferred philosophical abstraction of Andreessen's argument would be something like the following: software is productive capital for which the raw ingredients are coherent human thoughts. This has recreated the independent skilled-laborer-cum-entrepreneur as a class of economic agent whose capacity for capital creation is human, not financial. This class has arguably been minimized in the economic landscape at large since the Industrial Revolution morphed the Venetian blueprint of capitalism to its vastly more complex successor stage dependent on organizing and directing labor around immobile capital. Such agents have vast bargaining power over financial capitalists, which they tend to exercise currently by demanding equity. But note, the equity stake grounds capital creation in the existing financial system. This power was only ever forward-looking  such workers could bargain over and make mobile wealth they were yet to create.
But Bitcoin has severed the final link. Vastly more capital is, in theory, now mobile as it no longer needs to be moored to a given financial system. By ""everything"" we need not mean ""finance"", but can actually mean ""everything"", particularly since a silver lining of the nightmare of lockdowns seems to have been normalizing knowledge work from just about wherever the workers want rather than a handful of unliveable metropoles. Kotkin laments that,
""Rather than a base for upward mobility, the great cities have largely become magnets for those who are already well-to-do. Few working-class or middle-class families can now afford to move to places like Paris, London, Tokyo, New York, San Francisco. Many former residents, like Chicago's black middle class, have left to make their future elsewhere. Many who still work in those cities are forced into intolerably long commutes. As the middle class dwindles, it leaves behind a marginal urban population who depend on the city for a livelihood but often can barely get by.""
But likely no longer. And of course, wherever this high-skilled, newly mobile capital-cum-labor recongregates, all other forms of work will be viable as well  this need not be understood at surface level as an elitist prediction, but rather as baby steps towards feasible localism, at long last.
Physical capital still matters, clearly. So does cultural capital. These are so obvious as to be weird to need to point out. But those in a position to extract protection rents on physical capital, likely with the allure of cultural capital, will need to adjust to this new reality. Sticks are out, carrots are in. What are you gonna do about it? Build a wall? Good luck with that.
In the remarkable essay, Economic Consequences of Organized Violence, historian Frederic Lane emphasizes the importance of sovereign competition in using and controlling violence in an era of more mobile capital than we are used to today:
""If all the tribute was used for conspicuous consumption, a term which seems particularly appropriate for the court of a prince of the ancien regime, growth was slowed by lack of investment. Merchants who gained protection rents from international trade and colonization, although not entirely inconspicuous in their consumption, probably had a lower propensity to consume. If so lower profits for governments and higher profits for trading enterprises meant more capital accumulation and more growth.""
McNeill similarly observes that in the wake of the eleventh-century upsurge of Venetian and Genoese private commercial activity in the Mediterranean,
""Rulers of old-fashioned command societies were simply unable to dominate behavior as thoroughly as in earlier times. Peddlers and merchants made themselves useful to rulers and subjects alike and could now safeguard themselves against taxation and robbery by finding refuge in one or another port of call along the caravan route and seaways, where local rulers had learned not to overtax the trade upon which their income and power had come to depend.""
We may well be heading back to such a dynamic, with the wilds of the Internet the spiritual successor to the high seas.
But what does finance look like in such a society? It certainly doesn't follow from the above that finance disappears. Surely it just changes - but to what? I think there are two helpful strands of answers. The first is programmable, which, by its nature is unpredictable except by its potential. Analogies to the early web are cliche, but with perfectly good reason. With open access and a programmable interface, who knows what will be invented? Who knows how quickly inventions will be iterated and combined? The second is Islamic.
Bitcoin is Halal
""An individual can be arrested for 'manufacturing' money in his own home but the commercial banking system is given the full protection of law in doing what amounts to the same thing. There is no justice in this ... There are those who say that we must develop an Islamic alternative to modern commercial banking. But why must we do so? The Islamic alternative to the cigarette industry is no cigarette industry, and were we to remain true to our principles we might realize that the Islamic alternative to commercial banking is no commercial banking.""
- Tarek El Diwany
That Bitcoin can potentially be considered concordant with the teachings of Islamic finance is an insight I owe to conversation with Saifedean Ammous. What we mean by this is roughly as follows: as Bitcoin is a digital bearer asset and not a debt instrument, its natural state of safe custody is outside financial institutions. Also, without the ability to mint new Bitcoin as and when politically convenient, deposit insurance is impossible and loan origination requires the prior provision of liquid capital. Hence prospective intermediaries would not be able to guarantee investor protection from loss arising from the debtor's activities.
In combination, it becomes comparatively much less likely that contributors of capital will accept a fixed upside and unlimited downside, particularly given the near-certainty of deflation rather than the contemporary norm of inflation that must be forcefully chased with low-volatility capital appreciation. Would-be depositors will either save unilaterally or demand the shared upside of equity, for the most part. Supply will contract and swathes of demand for leverage will be priced out of whatever market is left.
What remains of interpersonal, non-programmatic banking is likely to look very similar to the proscriptions of Islamic finance, with a dramatically reduced role for debt, and a focus on risk-sharing rather than risk-transfer. Bitcoin is halal.
To be clear, the reasoning that arrives at this outcome is quite different. In Islam, interest on monetary debt (riba, ) is unlawful (haram, ) on ethical grounds whereas I suggest it is unlikely to emerge to any great extent in a Bitcoin standard on purely economic grounds given the risks will (finally!) be properly priced, hence can be legitimately shared rather than dishonestly transferred. Even still, the resulting behavior has a clear ethical resemblance to the cultural norms in the Bitcoin community, centered first and foremost on low time preference. Consider the Islamist scholar Abul Ala Mawdudi's (  ) exhortation that,
""it is incumbent on every member of the Muslim community to live within his means. He is forbidden to let his expenditure exceed his income, thus compelling him to stretch his hands out to others in order to sustain his extravagance, use unfair means to grab the wealth of others or become indebted to others to help finance his unending needs and, by consuming his resources in clearing his debt, eventually join the ranks of the destitute.""
And even if we contest the underlying ethical objection, more similarities in attitude at a higher level are readily forthcoming. We might think, for example, that even in a Bitcoin standard there will still be some willing supply and demand for interest-bearing, at-risk capital; why then should consensual exchange be prevented? The Islamic response hinges on the haram feature of what is deemed ""speculation""; given the inherent uncertainty of entrepreneurship, the commitment to pay interest on monetary debt finance is unavoidably fraudulent given the debtor does not and cannot really know she can meet her obligations. Only pari passu equity stakes are fair and honest, and furthermore, introduce no disharmony of incentives or outlooks between the capital providers. Once more, we may differ in our understanding of the ethics of such contracts, but Mawdudi's reckoning of the consequences of the proliferation of such ""speculation"" has a clear appeal:
""Due to the absence of a reasonable and healthy relationship of participatory cooperation between the capitalist [creditor] and the entrepreneur [debtor], the global economy suffers tremendously and faces alternative highs and lows that adversely affect the world's economic health. The capitalist's stranglehold had helped to boost the spirit of speculation and minting of money through interest. this has naturally poisoned the bilateral relationship between capital and enterprise, and the raising and lowering of interest rates are now done in such a way as to keep the entire world's economic health always in risk.""
By simply substituting out ""all debt"" for ""money as debt"" and similarly tracing out the consequences, Bitcoiners would likely agree entirely. Economist and noted Islamic Finance scholar Mohammad Siddiqi dryly makes this connection in, A Vision for the Future of Islamic Economics, noting that, ""almost all money in circulation is interest bearing debt transferring wealth from fund users to the owners of capital. It is not technically necessary for society's means of payment to play this role."" In, The Problem with Interest, Tarek El Diwany makes a strikingly similar point to my own in The Capital Strip Mine, all the more remarkably from an entirely different basis, that, ""polluted rivers, festering rubbish tips and resource-depleted seas may be just the first installment of the price that is paid for entering in to a race with compound interest.""
To a limited extent, believe it or not, the IMF seems to agree as well. In a 2010 working paper titled, The Effects of the Global Crisis on Islamic and Conventional Banking: A Comparative Study, Maher Hasan and Jemma Dridi conclude that Islamic banks' asset-based, rather than debt-based, operations, ""make their activities more closely related to the real economy and tend to reduce their contribution to excesses and bubbles."" The paper also includes the unintentionally hilarious deadpan explanation that,
""the profit/loss-sharing nature of investment deposits provides Islamic banks with an additional buffer. However, this feature was not tested in the crisis given that most banks remained profitable. In addition, in the context of the crisis and given the loose monetary stance in most countries, this feature is likely to put Islamic banks' profitability at a disadvantage compared to conventional banks.""
The closest analogs to ""loans"" as we might think of them are qard al-hasan ( ), a straightforward interest-free, benevolent loan, and sukuk (), a kind of pooled, fixed-term equity investment  curiously more like the Venetian colleganza than any common contemporary instrument. Collateral is uncommon in either case, but where it is taken there is a final intriguing comparison to be made: the collateral must be transferred to the possession of the creditor for the term, hence in the case of default, no repossession occurs. As with everything in Islamic finance, the ultimate rationale is simply fairness and justice; those who default on a mortgage are not kicked out of their home because the home is in possession and use by the debtor and could not have been halal collateral to begin with. The connection to Bitcoin is the likely incentive in a healthily deflationary environment not to recourse to hard assets to store value over the long-term, combined with an increased background incentive in financial transactions for all parties to access and retain sound money:
One need not personally subscribe to all, or any, tenets of Islamic finance to appreciate a broader point than its admittedly tenuous connections to Bitcoin. The study of Islamic economics and finance is intriguing as it is, to my mind, the only systematized, contemporary, and successful alternative to what we might call ""Western"" financial precepts, now, of course, near-globalized and seemingly omnipresent in commerce. Note, for example, the IMF describes ""Islamic Banks"" as opposed to ""Conventional Banks"" (and I stress ""successful"" above, by the way, to properly distinguish and contrast Islamic economics to socialism, of which, the reader might be interested to know, Mawdudi's Mankind's Economic Problems and their Islamic Solutions provides possibly the most concise and effective refutation I have ever read, fusing Mises and Havel in a two-page polemic).
A common difficulty with the conceptual challenge posed by Bitcoin is that this omnipresence makes it difficult to think rigorously in terms outside the framework of mainstream Western finance at all. Western finance is water. But perspective can be achieved, and precepts can be challenged. Mawdudi, Siddiqi, and El Diwany challenged them, Nakamoto challenged them, and the reader owes it to herself to challenge them too. Elevating morality above perceived efficiency makes for a profound starting point.
Bitcoin is Gravity
""An amount of money lent to a government, and the interest amount charged, is assumed to be risk-free because it is in turn assumed that a government can tax, borrow, or print further amounts of money to pay its debt. These three options are indeed available to a modern government, but one must not ignore the fact that the government has no access to risk-free rates of return when investing the borrowed money. The above mentioned options are in fact nothing more than means of passing on the bill to others when the fact of a non-risk-free physical system eventually reasserts itself.""
- Tarek El Diwany
To date, it has been very difficult to conceptualize what value, exactly, has peacefully opted out of fiat and into Bitcoin. Pricing only happens at the margin, and marginal fiat exchanged for bitcoin is just a bank liability that the bank relabels. Bitcoin exchanged at the margin likely engenders lower time preference, as discussed above, hence lower consumption of garbage, but which is conceivable only counterfactually.
This will change when people start selling not just their fiat, and not just their time, but when they start liquidating real assets. Gold will probably be the first victim, for readily understandable reasons as Bitcoin is an upgrade in almost every respect. But gold is not systemically important. This shift will be noticeable but not otherwise impactful. When the accumulation drive hits short-term credit, real estate, and passive equity, that is when the party will really start.
These three are artificially large asset classes given they are de jure productive, hence cash generative and priced on yield, yet de facto speculative savings instruments given long-term saving with fiat is impossible. But more vitally still, they are systemically important. Their prices, in aggregate, affect capital formation. The TLDR of Part Two, The Capital Strip Mine, is that these prices are wrong, and hence the capital is being strip-mined as quickly as it is being formed. Reversing this is the long-term hope, but anticipating the short-term mechanics of this reversal is another matter entirely.
The key insight is that if these assets were actually priced on yield, the kind of flows I anticipate would have no impact on long-term holders beyond minor disappointment. But because they are not, any substantial outflow can easily become a self-fulfilling prophecy. Corporations use short-term credit as ersatz cash with an inflation-hedging yield, however minuscule these days. But this is not an ""investment"". There is no upside, but just-sure-enough downside protection. If the downside protection disappears then the entire proposition evaporates  and note this could easily happen without substantial selling but simply a neglect to continue buying, given the whole point of short-term credit is that it continuously rolls over. What would likely happen next is central banks stepping in to ""support"" these markets with asset purchases, which, of course, is the best imaginable endorsement of Bitcoin's utility.
Behind all of this is the seemingly straightforward question of bitcoin's ""fair value"". There will always be a hesitancy to shift savings from something as well understood and naturally priced as short-term credit to bitcoin on the basis of being entirely unsure of how to compare bitcoin's price to its ""fundamentals"". What will gradually be realized is that, with Bitcoin, the traditional relationship is inverted. I do not think it is quite accurate to say that bitcoin's price is its fundamentals, but certainly its price is a largely reflexive function of its fundamentals: as the price goes up, the fundamentals go up (and we must be mindful also that as the price goes down, the fundamentals go down. Sustained attack that drives the price down for long enough is by far the biggest risk). Bitcoin was weakest when smallest, but less so the more time passes. Bitcoin is a black hole sucking unsustainably artificial value beyond its event horizon. As it grows, so does its pull. Bitcoin is gravity.
The ""store of value realization"" argument for its gravitational pull is by far the most obvious, the least creative, and is only scratching the surface of its likely continued evolution. Consider the implications of deepening liquidity, which, note, is subtly different from ""price"" alone. This is a necessary precondition for increasingly large purchases in the first place. MicroStrategy could not have done what it did a year earlier. Apple and Berkshire (dare I say?) still cannot do what they likely one day will.
But market deepening has far more interesting implications. It enables Strike, for example, Zap's soon-to-be-widely-copied, soon-to-be annihilation of FX markets:
Strike combines ever-tightening spreads on fiat liquidity pools with Lightning's instant settlement and relative programmability to offer unmatchable FX transfers. There are several astonishing features here that it is worth making absolutely sure we understand.
First, this service cannot be matched within the fiat settlement infrastructure. I don't mean that it is difficult; I mean that it is impossible. Interbank payments with the same currency and within the same jurisdiction can be more or less free and instant, and in many places are, since all this amounts to is relabelling a bank liability or, at worst, a net flow between mutual counterparty banks that can be batched and properly settled at enormous scale, hence offered to the end-users at low or zero cost. But across currencies, jurisdictions, or both, this is impossible  fundamentally because fiat is a debt instrument. What we might think of as a simple ""payment"" in this context is really more like a credit relay. Each party needs to trust the next party in the chain, and price not only the operational expense but this perceived risk, before passing it on, given the actual claim will be settled much, much later. And payment streaming? Fuhgeddaboudit. Not in your wildest dreams. With Strike, none of this is relevant. Lightning has no lower bound on value and settles instantly, and that's the end of that.
This service does not expose the user to the price of bitcoin at all. And yet, the fact of its existence and usage deepens the markets, which directly contributes to bitcoin's fundamentals, hence price, increasing. And if heavy users of this system one day decide they'd prefer to keep the transfers they receive in sound, open-source, programmable money, well, that makes the process even simpler still ...
It doesn't even end there. The Lightning infrastructure is still young and small and it needs staked value to grow. What better way to put bitcoin-denominated capital to work than seeking a return on competitive liquidity and routing? As bitcoin's fiat value grows, so do the incentives to contribute to scaling Lightning, which increases the efficiency of fiat payments routed via Strike-and-others over Lightning, which increases the depth of the fiat markets for bitcoin. And the more Lightning scales, the more the prospect of payment streaming opens up opportunities for better funding decentralized infrastructure  for example, incentivizing running Tor exit nodes, the storage and routing building blocks sought by the likes of Sci-Hub, accessible and portable economies in gaming, off-platform content monetization and advertising-free content-driven apps, but also things literally nobody has yet imagined. All of which increases Lightning's utility, which increases Bitcoin's utility. The more falls into this orbit, the bigger the orbit gets. The Jevons paradox in the metaverse!
Deeper markets also indirectly legitimize lending fiat against bitcoin reserves. While nominally tailored to allowing synthetic institutional leverage, normalizing this service will reduce the incentive for anybody to ever sell, in inverse proportion to how widely accepted bitcoin is for regular payments at a given time. It will be Pierre Rochard's speculative attack, but without even requiring awareness or intention. It will just be the sensible thing to do. If or when miners are able to access this service to pay for electricity, even partially, marginal supply will evaporate. It will also make increasingly viable credit card rewards programs, salary allocations, and, once again, things literally nobody has yet imagined, which might read as negligible in nominal terms, but are more about buying mindshare than fiat. Small buys lead to big buys.
Properly sophisticated, grown-up, Ivy League MBA, CFA-accredited readers might liken this entire line of reasoning, and my enthusiasm for it, to GameStop, the finance hilarity du jour, in the sense of uppity retail thinking they are sticking it to the man but really just blowing their savings on a practical joke from which Citadel will be the ultimate winner (not my reading, to be clear, but a common one. Mine is that these people knew exactly what they were doing and that you can prove it if you are willing to just look). I would encourage such readers to think more seriously about the game theory involved in all of this, particularly if gold and then short-term government credit fall into Bitcoin's orbit.
You might think this leads precisely nowhere at all, but the central banks of Venezuela, Iran, North Korea, and Singapore would disagree with you, that we know of so far. Central bank accumulation will become the defining macroeconomic issue of the decade, and advocacy for accumulation by the tech-savvy one of the defining political issues. Entirely mobile, unseizable capital will be attracted to, and will compound physically, wherever it is most welcomed, as will the human capital that likely comes with it. Countries with geopolitical rivals who decide to ban Bitcoin will be cutting off their nose to spite their face. When China starts to pay Russia for natural gas first in dollar stablecoins on Bitcoin, then in bitcoin, don't say I didn't tell you to think about it a little more than not at all.
Come to think of it, even GameStop can be non-ironically tied to this discussion. It has turned out a decent amount of faux-populist rancor was misplaced and the real culprit for screwing the little guy wasn't shady backroom deals between Citadel, Sequoia, the SEC, and the Fed, but rather the limitations of equity clearing and settlement given the mechanics of counterparty risk. Try to imagine, if only for a moment, tokenized equity certificates pegged to a secure digital bearer asset, with no counterparties, that settle in T+right now. Can you imagine that? Citadel might be the short-term winner in all this, but the long term is all about tokenized equity on sidechains.
Bitcoin is Logos
""To imagine a language is to imagine a form of life.""
- Ludwig Wittgenstein
LinkedIn founder Reid Hoffman infamously said on the Tim Ferris podcast that Bitcoin is like a Wittgenstein language game, with no elaboration whatsoever! Were I to pick up the baton, I would refer to Philosophical Investigations and the dictum that, ""the meaning of a word is its use in the language."" In other words (no pun intended), Hoffman is characterizing Bitcoin as understandable only by interpreting the actions of the participants as essentially consisting of communication with other participants, hence expressing in a codified grammar what they mean and relying on this grammar to understand what others mean.
I think it is worth invoking Norwood Russell Hanson once again to appreciate the position of a functionally illiterate outsider in the presence of such a language game; an adherent of the (admittedly satirical) semantic theory of money. If perception is theory-laden, and if our theory invalidates the possibility of a new money monetizing from scratch, and that money takes the form of a language we don't speak, and we won't learn this language because we think it can't exist ... we are just about guaranteed not to understand it.
That Bitcoin can be thought of as a language game clarifies why it is inherently peaceful. Money is an information system that records and updates who has done work that is valued by others, such that credit can be universalized and socially scaled. Frederic Lane and Reinhold Mueller note in Money and Banking in Medieval and Renaissance Venice that, ""both 'medium of exchange' and 'standard of value' are sufficiently ambiguous to make 'moneyness' a matter of degree,"" and that, ""conceptually and historically the two are separable.""
This recording and updating is a technical problem, and candidate technical solutions ought to be evaluated not on how and how well they fit sufficiently ambiguous definitions, but on how and how well they work; be they Rai stones, gold ducats, or dollar-denominated bank liabilities; physical or digital; abstract or instantiated; debt or pure asset. The solution provided by Bitcoin is in some sense the purest yet conceived in that it captures this information as speech  we only use software to check the grammar.
As was made clear by Ross Stevens at SaylorCon, Ammous' framing of ""salability across space"" and ""salability across time"" is now firmly in the lexicon. It is worth teasing out further in the context of understanding Bitcoin as language, and money as an information system. The essence of temporal salability is soundnesss; the essence of spatial salability is portability. Prior to Bitcoin, the two were in inescapable tension; economic development induces a market demand for money to be increasingly purely informational, as commerce itself becomes more complex than movement of specie can efficiently support. But information is, by its nature, not scarce at all, and so retaining some semblance of scarcity in informational money, hence temporal salability, requires trust in a centralized source of truth.
Note this does not imply ""fiat"", but rather ""fiduciary"", from the Latin fiducia, for trust. With relatively sound specie held in reserve by relatively prudent bankers, ""bank money""  payment purely by debit and credit of accounts with a bankin thirteenth-century Venice was relatively trustworthy, and in fact dominant and thriving. Not to mention in Genoa, Florence, Barcelona, and Bruges, interoperable via bills of exchange; all a little less temporally salable, perhaps, but vastly more spatially salable. But of course, Bitcoin resolves the underlying tension entirely. It is digital (that is, informational) scarcity. We get the portability of email with no trust, just verification.
A Bitcoin transaction is a global speech act that means, roughly, I am provably entitled to this portion, x, of the money supply, and am now transferring it to somebody else, in a language that everybody remembers forever and which can't be used to lie. This is why efforts to ban Bitcoin, while they will certainly be attempted, will also almost certainly fail. Bitcoin is the ultimate samizdat. Bitcoin is logos.
This is also a source of great optimism given prevalent legal and social commitments to protecting politically undesirable speech. Naive as I am sure it will seem to some, I think one of the most important US Supreme Court cases of the next 20 years will be the ruling that the right to broadcast Bitcoin transactions is guaranteed under the First Amendment. Prior to this, while the legality is still up in the air, I fully expect a sitting congressperson to invoke congressional privilege and ""broadcast"" a transaction by dictating its hexadecimal representation on the floor of the House or Senate.
This will probably be followed by tweeting one, working one into a public deposition, embedding one in a flag  which will end up on t-shirts and lapel pins, as well as fully physically to be waved around as a staple at protests. Don't Tread on My Node! You either make countably infinite sets of numbers, letters, and colors illegal  whatever that even means  or you accept that Bitcoin is going to happen.
Bitcoin is Techne
""The Venetians were not thinkers: they were doers. Empiricists par excellence, they mistrusted abstract theories.""
- John Julius Norwich
Recording and updating the speech act of transferring value is a technical problem, to which Bitcoin is a technical solution. It is not an idea about how things ought to work. It is a real thing that does work. Although the observation may seem flippant, the distinction is enormously important.
Nic Carter put it to Frances Coppola recently that in order to make an apple pie from scratch, you have to first invent the universe, by which he was conveying that if you want to create a robust, fast-settling, online payments system with finality, you need to create Bitcoin. No alternative has ever actually worked. Given the problems Bitcoin solves are very clearly not just academic but are core to human civilization, actually working is rather important. Bitcoin is techne.
I think this captures what is likely the biggest hurdle for most newcomers who do actually make an effort to understand the details, because, on the face of it, Bitcoin is completely absurd as an engineering construct. Miners do WHAT?!? Coins are stored HOW?!? etc.  we've all had these conversations. Even the mathematically inclined who enjoy toying with the cryptographic primitives may very reasonably think, should they lack the wider context, this is completely ridiculous because everything is fine. But with the proper context, we can of course say, this is exactly as ridiculous as it needs to be because everything is not fine.
And yet there is a particular brand of skeptic who professes to admire some or most of Bitcoin's design, but can't bring themselves to get fully on board because of some pet problem Bitcoin seems not to fully solve, or some pet issue with the way in which Bitcoin solves the problems it clearly does solve. Coppola is very much in this camp. I would argue Peter Schiff and Mike Green are too. All are interesting and serious people who seem to have the right attitude on many issues Bitcoin touches, with the mysterious exception of Bitcoin itself. On Bitcoin, all adopt variations of this pedantic quibbling that is superficially sophisticated but really the most meretricious and unserious position of all because nothing remotely practical is offered instead. They deal with reality not as it is but as they would like it to be. Their ""solutions"" are clean, slick, and are never going to happen. Bitcoin deals with reality as it actually is. It is ugly. And it works.
This entire line of argument will likely never go away, given it was refuted in its entirety as early as May 2011 in the now legendary Bitcoin is Worse is Better, by Gwern:
""The sacrifice Bitcoin makes to achieve decentralization is  however practical  a profoundly ugly one. Early reactions to Bitcoin by even friendly cryptographers & digital currency enthusiasts were almost uniformly extremely negative, and emphasized the (perceived) inefficiency & (relative to most cryptography) weak security guarantees. Critics let 'perfect be the enemy of better' and did not perceive Bitcoin's potential. However, in an example of 'Worse is Better', the ugly inefficient prototype of Bitcoin successfully created a secure decentralized digital currency, which can wait indefinitely for success, and this was enough to eventually lead to adoption, improvement, and growth into a secure global digital currency.""
Embracing this engineering ethic will immunize the curious to such inane posturetalk as Nassim Taleb's unpredictable outbursts on ""complex systems"", ""volatility"", or ""scale transformations,"" or Eric Weinstein claiming that, ""we need to get rid of the blockchain so that it's a locally enforced conservation law that replaces space-time with a system of computer nodes."" If you want to embed Bitcoin in a gauge theory, Eric, go right ahead. I look forward to reading the BIP. But please actually do it. Don't burble Chomsky sentences in the metalanguage and expect to be taken seriously. Of colorless green ideas a decentralized currency is not made.
Bitcoin is Venice
""Of the various centres in which republican ideas continued to be discussed and celebrated throughout the later Renaissance, the one with the most enduring commitment to the traditional values of independence and self-government was Venice. While the rest of Italy succumbed to the rule of the signori, the Venetians never relinquished their traditional liberties.""
- Quentin Skinner
I tend to find Bitcoin analogies that aren't transparently rhetorical to inevitably have some fatal flaw that ultimately makes them more confusing than they are helpful. And yet, Venice has an enigmatic appeal that I cannot bring myself to categorize as entirely fanciful. As a social and political order emerging from feudalism by an embrace of trade and capital formation, it is certainly instructive. But there seems to me to be more. Clearly, Bitcoin is not a city, but it is a system, and a symbol, in a way that transcends its instantiation as code, much as Venice transcended its islands and lagoon.
Some comparisons are cute and easy. Venice was far, far easier to defend than to attack, to the point that attack was essentially futile. Its governance model was bewilderingly opaque and constitutionally resistant to seizure. If seizure nonetheless became a realistic threat, an immune response seemed to be triggered that innovated around the danger. Was the legendary putting down of Bajamonte Tiepolo's insurrection, tipped not by the Commune's security forces but by an old woman throwing a stone from a window, a primitive user activated soft fork? Sure, why not. And of course, what of emerging from a dark age characterized first and foremost by monetary debasement? Pirenne's observation certainly suggests a precedent,
""If it is admitted, as it must be admitted, that the reappearance of gold coinage, with the florins of Florence and the ducats of Venice in the thirteenth century characterized the economic renaissance of Europe, the inverse is also true: the abandoning of gold coinage in the eighth century was the manifestation of a profound decline.""
What of Venice's relative egalitarianism? John Julius Norwich writes in A History of Venice that Venice was famed, ""for a system of justice which gave impartial protection to rich and poor, aristocrat and artisan, Venetian and foreigner; for, in theory, at any rate and for the most part in practice too, every man living beneath the banner of St Mark was equal in the sight of the law."" Pirenne notes this attitude was fundamentally rooted in the necessities of commerce and hence extended beyond the city's own jurisdiction and internal affairs: ""No scruple had any weight with the Venetians. Their religion was a religion of businessmen. It mattered little to them that the Moslems were the enemies of Christ, if business with them was profitable."" Similarly, that Bitcoin is apolitical or ""money for enemies"" is well memeified at this point, but I was particularly struck by Terry Crewes' rather more visceral anecdote to this effect:
What of Venice's continual flouting of the proclamations of the Church, the literal counterpart to Kotkin's modern clerisy of elite tastemakers and thinkers of right thoughts, who already have and no doubt will continue to take, ""a defiant and hostile attitude toward the commercial revival which must, from the very first, have seemed to it a thing of shame and a cause of anxiety,"" as Pirenne put it?
But I think the most striking comparison of all is the synthesis of disparate ideas into a financial cornerstone. Very little of commercial note was invented in medieval and Renaissance Venice  double-entry bookkeeping was likely borrowed from Genoa, having originally been imported to Italy from the Levant; the numeral system with which it is most useful is famously Indian, relayed in Arabic via Persia, the Levant, and the Maghreb; most other contributions to business administration were probably imported from Arabia and Constantinople; and the material industrial advances of the time predominantly originated in China. But Venice combined them all to perfection. Most of the outlines of modern finance were arguably present in Venice by the early fifteenth century at the latest, with very little truly invented since, rather than further combined, standardized, scaled, or modernized. To my mind, only central banking and options have been both material and entirely novel.
Of course, technology, industry, and society have advanced immeasurably since, and yet we still live by Venetian financial customs and have no idea why. Even the word ""bank,"" in the financial sense, originates in Venice, from the banca or ""benches"" of moneychangers by the Rialto Bridge, with Lane and Mueller pointing out that, ""true banking had developed, it is now generally agreed, not from moneylending or pawnbroking, but from the manual exchange of coins."" Modern banking is the legacy of a problem that technology has since solved.
The following explication of Venetian financial infrastructure around the fourteenth century from Lane's Venice, A Maritime Republic is remarkable in that I think it is a perfectly solid foundation for understanding the role played, today, by credit card networks and bank settlement schemes alike:
""The main function of a Venetian banker was not making loans but making payments on behalf of his clients. Even if a merchant had plenty of coins in his treasure chest, it was a bothersome and dangerous business to get them out every time he made a purchase, making sure each coin was genuine and in good condition. Nor did he want to go through a similar process each time he made a sale. He was happy to receive payment by being given credit on the books of a well-known banker. He could use that credit to pay for his next purchase. These credits were not transferred through writing checks, as is done today, but depended on the person who was making a payment appearing in person before the banker who sat behind a bench under the portico of a church at Rialto, with his big journal spread out in front of him. The payor orally instructed the banker to make a transfer to the account of the person being paid. The banker wrote as directed in his book, which was an official notarial record, so that there was no need of receipts. There were normally four or five such bankers with booths on the campo next to the Rialto bridge. Everyone of any consequence in business had an account so that he could make and receive payments through the banks. They were called banche di scritta or del giro because their main function was to write transfers and thus to rotate (girare) credits from one account to another at the command of the merchants.""
If we add bills of exchange, credit creation, and floating the state debt, all natural extensions of the utility of these ledgers  and of course if we subtract a hard reserve asset, available on-demand, that was deposited in the first place  there is not much left to account for. And note as well the seedlings of why the trust-minimizing natively digital, computational, and decentralized features of the Bitcoin for settlement and Lightning for payments dramatically improve on this setup. Bitcoin may be magic Internet money, but more importantly, it is money for the Internet. Prior to 2009, you could send any information you want to anybody, anywhere in the world, instantly ... except the most important information of all: value. Now we are all caught up.
It is often commented that Bitcoin is really more an ingenious combination of prior advances in applied cryptography than an invention in its own right. I am quite partial to the romantic idea that Bitcoin was discovered rather than invented. It is a foundation to scale the next great phase of economic progress. Bitcoin is Venice.
Bitcoin Is
""Our history forbids us to be surprised that an orthodoxy of thought should become narrow, rigid, mercenary, morally corrupt, and vengeful against dissenters. This has happened over and over again. It might be thought the maturity of orthodoxy; it is what finally happens to a mind once it has consented to be orthodox. But one may be permitted a little amusement, if not surprise, that this should have befallen a modern science, which was set up, as it never tires of advertising, to pursue truth, not protect it ... If change is to come, then, it will have to come from outside. It will have to come from the margins.""
- Wendell Berry
Colorfully outrageous metaphors finally aside, the most remarkable fact of all remains that Bitcoin even exists. Bitcoin is. This is undeniable, although the reasons why can be ignored  our monetary system is optimized to strip-mine capital  and its ascent can be misunderstood  the mainstream understanding of money invalidates it by definition rather than observation.
Ludwig Wittgenstein once asked a friend, ""tell me, why do people say it is more natural to think that the sun rotates around the earth than that the earth is rotating?"" The friend said, ""well, obviously, because it just seems like the sun is going around the earth."" Wittgenstein replied, ""well, what would it seem like if it did seem like the earth were rotating?""
If it seemed like a global, digital, sound, open source, programmable money was monetizing from absolute zero, it would seem a lot like this.
follow me on Twitter @allenf32
Thanks to Giacomo Zucco, Saifedean Ammous, Obaid Khan, and Robert Natzler for edits and contributions.
",109
https://themakingofamillionaire.com/how-i-built-a-net-worth-of-500-000-before-age-30-502200443171?source=tag_archive---------3-----------------------,"How I Built a Net Worth of $500,000 Before Age 30","No, I did not start a dropshipping store, join a network marketing company, or receive an inheritance or other financial assistance. I did...",Lauren Como,9,"I can't stress this enough! We all logically know that if we make $60,000 a year and spend $65,000, we will be in trouble-yet, so many Americans do this. So much of this (in my humble opinion) has to do with mindset.
We save and invest very little because we are afraid of missing out. We worry about how others perceive us. We feel that personal finance is a foreign language, and developing a financial plan is too complex. I hope to address how to solve the above issues in this article, in addition to encouraging you to take more risks to achieve a higher income.
When I graduated college in 2013, I relocated to San Jose, California. If you aren't familiar with real estate prices in the Bay Area, they are insane. I was lucky enough to find a roommate through a mutual friend that I lived with in college, who was also joining the world of high tech in Silicon Valley. We signed a 17-month lease and split the rent for a 2 bedroom/2 bathroom apartment for a total of $2800.
When I moved to Silicon Valley, I only brought the belongings that I could fit in my car to avoid the costs of shipping heavy furniture, boxes, etc. I didn't have much money, and my employer's relocation bonus was not going to be available until after I started my job. I ended up using my savings and opening an Amex card to get myself into CA (which I paid off as soon as I earned my first paycheck).
When I arrived, I was able to find a deal on Craigslist for a new mattress that ran me a couple of hundred bucks. The seller had a truck and was willing to deliver the mattress for free. That was the only piece of furniture that I purchased-the mattress that I threw on the ground.
My greatest realization from living on a mattress on the ground and minimal belongings is that I wasn't any less happy. Most of what we buy is to impress other people-often people that we don't even like. There are seven days in a week; I can't wear 50 shirts and 30 pairs of shoes throughout the week.
My best memories from that period in my life were from activities that were either free or very inexpensive. Walking through San Francisco with my friends and exploring cost me absolutely nothing. Hiking and exploring the beautiful Northern California landscape is free. Sitting on the beach at Half Moon Bay and listening to the waves crashing along the shore is also free. The bottom line: you do not need to spend much to find fulfillment and joy in life.
My priorities out of college were paying my rent, building an emergency fund, and taking advantage of my employer's retirement match. After building an emergency fund, I moved on to aggressively investing. Below are the steps that I took to accumulate my $500k net worth.
Key Takeaways: Live under your means, pursue fulfillment instead of materialism, and develop an aggressive and diversified financial plan to build wealth as soon as possible.
Once my 17-month lease was coming to an end, I realized that with California taxes + a $1400 a month rent payment, I needed to do more in the savings department. I went on Craigslist and was able to find a room for rent in a condo in Milpitas, California.
The condo was a few miles from my old office, and the rent was $900 a month with all utilities included-major score! The downside: the condo was next to a prison, but I survived!
I found a gentleman with a truck through a mutual friend who offered to move my mattress and a few boxes for $60. I moved into the condo with bright orange walls, carpet that should have been replaced five years ago, and appliances that needed a serious upgrade. I threw the mattress on the ground (yet again) and decided it was best for my dating life to invest in a cheap dresser to avoid having my undergarments on a hanger in the closet like a weirdo.
I spent 5.5 years living in the orange room with the mattress on the ground and shag-ish carpet. I have zero regrets. Once again, this goes back to having the self-awareness and realization that you will not be happier by throwing away much of your paycheck on an expensive apartment to impress people that you don't even like.
Key Takeaways: You'll most likely feel much better moving into a less expensive apartment, condo, or house. Oh, and consider getting a rug if your residence has shag carpet.
Fast forward to around March of 2019. I was turning 28, and I realized that I didn't want to enter my 30's paying high taxes and living with a roommate. I had always had this idea that in my 30's I'd relocate to a place like Texas. Most of my family lives in Arizona, and Texas is a short flight away. Texas has no state income taxes and relatively affordable housing, low gas prices, etc.
In April 2019, my employer reached out a month later and asked if I'd be interested in relocating to either Austin or Raleigh, North Carolina. My employer was having a difficult time retaining talent due to the high cost of living in San Jose, California, and wanted to offer a better solution.
I jumped at the opportunity; I flew down to Austin over the summer, spent six days, and decided that Texas was going to be my new home. I spent one of the six days looking for an apartment, which was relatively easy to secure. My new apartment cost me a total of $1,100 for just under 1,000 square feet!
I was elated; my new home was in one of the safest neighborhoods in Austin with a beautiful view of the Greenbelt and a quick drive to the lake. The lease on my condo was month-to-month, so I was able to make a fast exit and move within 6 weeks.
The cheapest solution I found was using Upack (I had a great experience and highly recommend them). You essentially load your belongings in a trailer, and they drive the trailer to your new home. I drove my car down to Texas on a three-day road trip and found great deals on Airbnb for rooms as low as $29 a night. My entire move cost me less than $2k.
At the time of writing this article, I have been a resident of Texas for a little over sixteen months. This is one of the best life decisions I have ever made. The cost savings is insane.
My portfolio almost doubled in the 16 months that I have lived in Texas. In addition to lowering my cost of living by moving out of state, I took another risk-I left my comfortable salaried job to pursue sales three months after moving. I hoped to do something that I enjoyed, in addition to earning a higher income through making a commission on any deals I close.
Key Takeaways: Don't be afraid to move to decrease your cost of living and provide yourself and/or your family a better quality of life. Something else to note: the people in Texas is very friendly. Southern hospitality is real, and when you are around good people, you tend to feel happier and do better work in your career. Also, moving to a state with great BBQ and live music is a plus!
I always knew that I wanted to go into sales, and it took me a LONG time to make the jump. I had no experience, so I was rejected very frequently. Long story short, I finally found a sales manager that was willing to give me a chance. If you are in this situation (wanting to make a career move without experience), I highly recommend offering to work for free to gain experience in addition to being very persistent in your job search.
The result: I found more meaningful work, made more money, and lowered my cost of living. I would say that all three outcomes are a huge win. I can't stress this enough; it is very hard to do good work for an extended period in a job that you dislike. When you find meaning in your work, you are much more likely to want to work more and build additional skills to be more marketable to employers and customers.
Key Takeaways: Take the risk and leave the job you don't enjoy to pursue a better opportunity. Don't spend time worrying about how others will perceive you. Much of what we fear will never come to fruition. When I made my announcements that I was leaving Silicon Valley and moving into sales, I was met with some resistance.
None of the resistance mattered to me because I realized that those individuals were projecting their insecurities and fears onto me. Again, the worst that happened to me was I found enjoyable work, made more money, lowered my cost of living, and met great people.
Closing Thoughts: With the right mindset of spending less than you earn, making short-term sacrifices to improve the timeline to financial freedom, cultivating a diverse financial plan, and taking risks to increase your income, you are well on your way to reaching a net worth of $500,000 (or more)!
For information on the latest investing opportunities, career building tips, and help with building your own portfolio, you can subscribe to my free newsletter here.
This article is for informational and entertainment purposes only. It should not be considered Financial or Legal Advice. Not all information will be accurate. Consult a financial professional before making any significant financial decisions.
",110
https://medium.com/conversations-with-tyler/tyler-cowen-nassim-nicholas-taleb-skin-in-the-game-black-swan-104620da8a57?source=tag_archive---------6-----------------------,Nassim Nicholas Taleb on Self-Education and Doing the Math (Ep. 41  Live at Mercatus),The best thing to do on an airplane? Twitter fight!,Mercatus Center,32,"Though what Nassim Nicholas Taleb was really after was a discussion with Bryan (read that here), the philosopher, mathematician, and author most recently of Skin in the Game also generously agreed to a conversation with Tyler.
They discuss the ancient Phoenicians and the Greco-Roman heritage of Lebanon, philology, genetics, the blockchain, driverless cars, the advantages of Twitter fights, defining religion, fancy food vs. Auntie Anne's pretzels, autodidactism, The Desert of the Tartar, why Taleb refused to give a book tour, inverse role models, why math isn't just a young man's game, and more.
Listen to the full conversation
Read the full transcript
This transcript has been edited for clarity. If you notice an error, please send us an email.
COWEN: We're very honored today to have with us the great Nassim Nicholas Taleb.
I'm reminded of the words of the Hall of Famer Ernie Banks from Chicago, who used to always say, ""Let's play two,"" when there was the possibility of a doubleheader. Nassim Nicholas Taleb has been gracious enough to agree to this dual event. First, he and I will converse, and then he will talk with Bryan Caplan.
Just to be clear, as always, this is the conversation with Nassim Nicholas Taleb I want to have, not the one that you want to have.
[laughter]
Historically, a very basic question: on page 7 of The Black Swan, you mention the 1975 Lebanese Civil War as having been a black swan of sorts.
If we think back to the earlier history of the region  the growing role of the PLO in the country; the end of the Eisenhower Doctrine, which meant maybe the US would not intervene; the prior conflict in 1958; ongoing differences in birth rates with more Muslims being born  wasn't it, in a sense, actually fairly predictable and not a black swan?
How do you see the history of your own country in this way?
NASSIM NICHOLAS TALEB: No, it's going to surprise everybody. It might have been predictable, but not on that scale for several reasons. The first one is that nobody understood the effect of modern weapons because previous conflicts were more local, and more confined. You didn't have artillery, and then things died down quickly.
Then, you have another thing  that these idiots brought in the PLO. After 1973, as a way that Lebanon wanted to stay neutral with Israel  not neutral, but engaged, to say, ""OK, you guys can fight from here, but we're not going to fight Israel.""
Then, they didn't realize that these guys were going to is try to take over the place. So the imbalance came not from within. It came from a huge number of armed Palestinians in Lebanon that disrupted the balance and caused immediately, as a reaction, the Christians to go and get gone.
COWEN: What's the role of the Phoenicians in your thought?
TALEB: There was a recent thing circulating that Phoenicia didn't exist, which is not true, I think. Of course they didn't call themselves Phoenicians, but Canaanites. Just like Greeks didn't exist. That was some kind of confederation, Arcadian confederations, and not the small, little city-states with their hinterland.
We know that, genetically, the Phoenicians seem to have come from Anatolia largely, and Anatolia is the same place the Greeks came from. So there is a connection that was just discovered a year ago. Maybe in five years from now, we'll have a clearer view, rewriting history based on ancient DNA.
But that would not explain the people. Why was it that Phoenicians had some kind of great working relation with the Greeks? It looks like they were similar, maybe the same. They had the same grandparents, to put it this way. Also, there's some mysterious thing. Why is it that the only people in Athens who were not metics, foreigners. A metic is basically an H1B. You can't vote.
[laughter]
TALEB: The only ones who were not metics were the people from Sidon. That's why I like to call these people Greco-Phoenicians, the modern version of the Phoenician, Greek Phoenicians. Of course, now we know from DNA that the DNA of the past 3700 years hasn't changed much, that the local population is a pretty ancient one.
COWEN: I was talking with Eric Weinstein recently, and I suggested the following to him. The way I read your work, on one hand, it was trying to explain what happened to Lebanon, and so it's a very regional concern. But you are also trying to put forward a vision of what an alternative Lebanese history would look like, a Phoenician one.
That positive program was the underdiscussed part of understanding your work. You're trying to lay out how Lebanon could be seen or could be a Phoenician culture in the future.
TALEB: It is an eastern Mediterranean culture, and it is very easy to convince people it's an eastern Mediterranean culture in Lebanon, despite the textbooks writing the opposite. Let me tell you really what happened.
In 1860, the Christians of Lebanon did not write in Arabic. They wrote in a character called garshoune. The American University in Beirut came in and tried to recruit Muslims, and it didn't work. At the time it was called Syrian Protestant College. They couldn't convert Muslims, so they decided to convert Christians.
They translated the first translation of the Bible, the whole thing, into Arabic, which was done by the surgeon called Van Dyke. An attempt had been done years earlier and failed. So this Arabization that started in the 1860s in Lebanon, when the Turks were around, is, in fact, a distortion of history, a perception of habit.
It's trying to bring that part of the world away from the Ottoman Empire, which was a Balkan eastern Mediterranean world, in which the place had spent 500 years, and before that, a thousand years under Greco-Romans  trying to move it closer to Arabia. People bought the narrative because the Christians wanted to be non-Muslim, yet have the same rights.
So they invented themselves some genetic story that they came from Yemen, they came from a tribe called the Banu Ghassan. The Christians invented themselves some kind of story, ""Yeah, the Phoenicians were here, but the Arabs kicked them out, and we, the Christians, come from . . ."" and it's totally genetically bogus that the population had to move.
In fact, there is the same people. I see that you brought a book about Charles Corm. Charles Corm was the opposite narrative, the Western narrative that we are Greco-Roman. We had the School of Beirut, the religious school in Beirut that for 500 years made or unmade laws, the only place where laws were made in the Roman Empire.
We are Greco-Romans by culture. Let's go back to the Greco-Roman world, away from 150 years of Arabization. If you look at Lebanon, you realize that 150 years of Arabizing people had no effect.
COWEN: You're Greek Orthodox by background. If I were to ask, how did the Maronites fit into your schema? There's a long-standing tension between these groups. It seems to me often that the Greek Orthodox would actually side with the Muslims, wanting a more stable Lebanon, suspicious of the Crusades, suspicious of the Maronites themselves. What's your take on this?
TALEB: I'm glad you asked me because finally someone really understood the problem... When a lot of people, a lot of factions in Constantinople in the 1400s, the Church was already in the area surrounding Constantinople for 250 years before that, and it was just a different country before, different countries. There was a battle, but it was not . . .
Initially, the Turks came in as the Byzantine Empire, and they were very upset that the Russian one called themselves caesars, ""What do you mean caesar? We are . . . the sultan is the continuation of the Byzantine Empire."" There was that tension East-West.
And this manifests itself through religion. Let me explain it, what I saw. The coastal Levant was monstrously Arabized except for Beirut, which was Latinized because of the language of Beirut. And the people were horrified to see that they use Latin because there was a school of law, but the rest was Greek. The countryside spoke Aramaic.
If you see the religion schisms, they map to Aramaic speakers versus Greek speakers. The Maronites were one of the lines of the first schism. Earlier, of course, you have the Nestorians as a schism. You look at the line of how people have the schism in Asian Minor, and you see that these schisms were entirely driven by ethnicity, like the English Protestant versus Irish Catholic.
If these English were Catholic, I bet you then the Irish would have been Protestant, definitely.
[laughter]
That's like what happened along these lines. Of course, the Byzantines were very suspicious of the West, and they saw Islam as a . . . Effectively, it was not an unrealistic thought because if you look at what happened, there's 500 years under the Ottomans. Syriac Orthodox, Armenian Orthodox, Greece State Orthodox, Romanian Orthodox  these parts are Orthodox.
The idea, when you look at the world, the division East/West and Christianity/non-Christianity  at that time there were three blocks. There were the French, those Westerners. There were the East Meds. And then there were the Easterners  Arabs, Persians  and that was how it was seen.
Of course, the people identify along these lines, and then progressively, when you had the heresy, people would go to Rome because ""the enemy of my enemy."" So the Maronites  and Gibbon tells this story beautifully  how they came down to Tripoli to do a deal with the Franks, with a group of crusaders, to do a deal with Rome because they wanted to move away from that.
The tension is not a tension that is modern. It is a tension  the one between the Maronites and the Greek Orthodox  is a tension that has been there forever.
One more comment, since we're talking about it. First, I didn't know you know this stuff. I'm honored that you're interested in the same thing I'm interested in.
If you look at what happened also in the Levant, people fail to understand a few things, that you have Nestorians, basically the Syriacs, the Orthodox, and non-Chalcedonian. If you look at these people, the Syriac speakers, who absolutely hated the Greeks, just like the Copts hated the Byzantines.
People forget that before the Arabs conquered the Levant . . . Actually they didn't really conquer the Levant. They just went through the Levant, and it's not interesting. There's olive oil and nothing else.
[laughter]
You had two generations of Persians in Lebanon. The Persians came, and they brought with them Nestorians. Nestorians are similar to Maronites  different heresy  and they brought with them Zoroastrians, and then they left. The Persians were kicked out by the Byzantines.
Those who stayed were Zoroastrians and Nestorians. By some mysterious thing, you find Zoroastrianism, as a Shiite Islam, born in Lebanon and converting the Persians in the same place where these guys were brought in.
The Shiites were in Lebanon, though it seems to me that the population of Zoroastrians who came, because the Byzantine hated Zoroastrians and non-Chalcedonians, and they stayed, or at least the religion came in and there's a big connection between Zoroastrianism and Shiite Islam.
COWEN: Let me give you a purely subjective impression I have reading your latest book, Skin in the Game. There's this long-standing tension with the Maronites, and you also have a very interesting discussion of why, in terms of Christology, Christ must in some way be at least part a man for God to have skin in the game.
[laughter]
I read that, almost as your part of intellectual reconciliation, that the new element for your big vision of what Lebanon could have been and maybe will be, that you finally found a way of integrating the Maronites into your basic story, that you yourself are willing to accept and indeed embrace.
TALEB: OK yeah, mea culpa.
[laughter]
Two things about the Maronites: The first one is I was convinced until about seven years ago that the Maronites were migrants who came to Lebanon from the East because the fact of it, their language, their liturgical language is not Aramaic, but Syriac, which is Eastern Aramaic, and that brand, that of course came.
But it turned out that, in fact, it's only the religion. The DNA is as local as it can be. The Maronites are the descendants of the Phoenicians. So are the Shiites of Lebanon. We Greek Orthodox and the Sunnis have a little more Greek or stuff like that. When I realized that, then I realized that they were a part of us so it was already something.
Second point, let's talk about skin in the game. To connect to this, as I was watching Donald Trump debate the other guys wearing suits . . .
[laughter]
I knew he was going to win. I was certain he was going to win.
I couldn't figure out why I was so convinced he was going to win, at least at that stage of the primary. And it turned out, I figured it out and said, ""Yes, people love . . ."", then I had heard the day before the news that were effectively not that correct, that he lost more than a billion dollars of his own money.
I thought about it. I said, ""OK, is there anything more human than showing a scar, to say, 'At least I'm real, and you guys are just like pencil-pushers,' or whatever?"" I thought about it, and I said that his skin in the game, exhibiting risk-taking, is effectively what elevates you over . . . Traditionally, you exhibit a scar or have some kind of scar or some kind of sign of devastation coming from war.
Effectively, it didn't hurt or harm him. I was a practitioner coming from trading, the trader that lost a lot of money, at least it's real. It's not a bad thing. So I realize immediately that, and then I thought about it.
I had trouble figuring out why is it that the Christian religion, we have that Trinity, which makes absolutely no sense to Muslims and makes no sense to anybody who's not Christian. It makes so much sense to you if you are Christian.
Why is it that it makes so much sense to have a Trinity? Then the Christ, is he God? Well, he's sort of like God, but he's not God. Then you figure it out. If I go to the circus and I have a fellow walking on a tightrope with a parachute, I ask for my money back.
[laughter]
That's not how it works. The Christ was sufficiently man to have skin in the game and sufficiently God to be God, so it worked. For that, we had to concoct a story that appears to be absurd, but is necessary. It kept coming back to the Trinity, no matter what they tried.
COWEN: Let me try comparing you to another best-selling Lebanese author. Are you the anti-Kahlil Gibran?
[laughter]
COWEN: You both have books of sayings. You're both, in a sense, offering sermons from exile. You both moved to America, but he was a Maronite. His influences are maybe Baha'i and the Sufis. You're, in a sense, doing everything in reverse and rebelling against him. Is that true?
TALEB: Not quite because when you grow up in Lebanon, you don't have a lot of . . . We think that Gibran's kind of New-Age stuff for Americans.
[laughter]
TALEB: Plus, the other thing is he comes from an area right above my village, like 30 miles away, 20, 30 kilometers away, and they invaded us two or three times.
[laughter]
The last Byzantine outpost, and they'd come and invade us. So maybe you want to do some psychology on that.
[laughter]
But Gibran has never been in high standing in my . . . A lot of people are in high standing from the area, but not Gibran.
COWEN: Let me try a question a reader emailed to me, and I quote, ""What advice could you give to the timid and unconfident? Does one seeking conventional employability and respect out of lack of imagination or lack of confidence deserve only contempt? How does one begin to learn meaningfully if you're not awesome?""
[laughter]
TALEB: It's the exact opposite. The point is that we are imperfect. And the way you can function best is accepting we're imperfect. It's why we have theology. You want perfection, you can find it in theosis and find a lot of things.
Incidentally, to go back to the idea of being orthodox, theosis is a way for us humans to rise above our condition as human, and it's given to us openness, this equal opportunity for anyone. If you consider that we are imperfect, and the way you can arise, this sense of honor, by doing duties or self-sacrifice, then you have a lot of risk in the game.
It's taking risks for the sake of becoming more human. Like Christ. He took risks and he suffered. Of course, it was a bad outcome, but you don't have to go that far. That was the idea.
I didn't talk about theosis. I just mentioned it in one footnote. It's like we understand that we're not in here to eat mozzarella and go to Tuscany. We're not in here to accumulate money. We're in here mostly to sacrifice, to do something. The way you do it is by taking risks.
Some people take risks and some people labor in the fields. You have the option of doing either one or the other. But my point is you should never have someone rise in society if he or she is not taking risks for the sake of others, period. That's one rule.
We understand that we're not in here to eat mozzarella and go to Tuscany. We're not in here to accumulate money. We're in here mostly to sacrifice, to do something. The way you do it is by taking risks.
The second one, you should never be a public intellectual if any statement you make doesn't entail risk-taking. In other words, you should never have rewards without any risk. That's the thing. Then you can accept inequality if the person who's unequal is taking risks because that would make things rotate.
For example, if someone becomes a billionaire, it's fine. It's unequal, but they've got to keep taking risks.
You cannot be locked into a frozen upper-class condition, and then use your situation to use the government to prop you up there. And that's the good thing about America is the rotation that you have.
COWEN: When it comes to childbearing decisions, do men have enough skin in the game?
[laughter]
TALEB: I don't know if we can divide things so narrowly because men have low life expectancy and have had, in history, low life expectancy. I don't know about today in Washington, DC, it's not reversed. Of course, we have more criminals among men  one ratio is 10 to 1  and in jail, incarceration, so you have a high rate, maybe not in the bearing of the child, but in doing something else less dangerous.
COWEN: Let me ask you another question about religion. Is volatility, including exchange rates, is it a problem?
[laughter]
TALEB: When it comes to religion, I wrote in here we don't know what the eff we're talking about when we talk about religion. People start comparing religion, and the thing is ill-defined. Some religions are religions. Some religions are just bodies of laws. Judaism and Islam are not religions like Christianity is a religion, the exact opposite. Let me explain.
The foundation of Judaism was law, but it was minuscule, it was for a tribe. It was law: ""you should not go and do this or that."" Then in Islam, the same word, din , in Arabic means law in Hebrew, but not in Syriac, which is a Semitic language used by Christians, where they use two different words, one nomos, law, and one, din, for religion. Why is it so?
Islam and Judaism are laws. It's law  there's no distinction between holy and profane  whereas Christianity is not law. Why isn't it law? A simple reason  you remember the Christ said what is for Caesar and is not for Caesar? It's because the Romans had the laws. You're not going to bring the law because they already have the law, and very sophisticated law at that, the Romans.
With Christianity was born the separation of church and state. It's secular, so it's effectively a secular religion that says that when you go home, you do whatever you want. Of course, Christianity, they got to have theocracies, a few, but it was all cosmetic.
For example, when you have the codes, whether Theodosius or Justinian Code  you take Justinian's code, you look at it. You see, just cosmetically, he said you were blessed by the grace of God , et cetera  two pages.
TALEB: The rest is intact, the Roman law. When you talk about religion, when people are talking about Salafi Islam  it's not a religion in the sense that Mormon Christianity is a religion. It is a body of laws. It's a legal system. It's a political system. It's a legal system.
So people are very confused when they talk about religion. They're comparing things that are not the same. Effectively, when I say that I'm Christian, it's very different from saying I am something else.
The same weakness that I see sometimes describe ethnicity. Being Greek Orthodox is more ethnicity than something else, or being Serbian versus Croatian. Sometimes religion becomes an identity, sometimes law, sometimes very universal.
And sometimes you have pagan tendencies hidden under some kind of Taqiya that you see in the north, you have the monastic religions. Comparing religions naively is silly, it's heuristic and leads to things like saying, ""Well, he has a right to exercise his faith.""
Some faiths should not have the right to be exercised, like Salafis or extreme jihadism because they're not religions. They're a legal system. They're like a political party that wants to ban all other political parties. If you go with that, you're repeating certain mistakes.
COWEN: How has the Aramaic language influenced or helped structure your thought?
TALEB: Zero. I picked it up later. It's always interesting. I'm doing some philology, and I discovered two or three things that are not skin in the game. I discovered there are two kinds of people  hopefully we'll talk about it again with Bryan  people who come from practice and people who come from theory.
This planet doesn't have a third category of people who have both. So you have linguists who absolutely don't know what the fu  sorry, what the hell they're talking about when they talk about religion. I noticed in the description of categorization of languages, linguists are not fluent in the languages they're categorizing, and it makes a huge difference.
I think that Levantine is close to Aramaic, but linguists don't accept it because they have their metrics that don't work. If you use a statistical method, then you realize that it's Aramaic. Or, if you use practice, you realize that.
COWEN: I'd like to toss out the names of just a few countries or regions. You tell me whether or not you think they're antifragile. Singapore?
TALEB: Singapore has size going for it. You see that we're talking about a city-state.
COWEN: Isn't it a vulnerable city-state that relies on our protection, and we don't always care?
TALEB: Who's gonna invade it? One thing I've learned from history, particularly the Phoenicians. The Phoenicians don't really have an army or an empire. At some point they had some army, but you might say it's not economically viable. Why? When you come to invade them, unless you're Nebuchadnezzar, and supposedly the history books say that he was very nasty, but then fact-checking take place. The genetics don't actually show what really may have happened.
A guy comes in, very bloodthirsty, comes to you, and you tell him, ""Listen, what do you want? You kill us all, you get nothing. Land is not interesting. What are you going to get? We'll give you 5 percent. What do you want, 5 percent of something or 100 percent of nothing?""
That's how the Phoenicians operated. Someone would come in. They had a hiccup with Alexander, one pound higher than a hiccup with Alexander.
[laughter]
TALEB: They had an ego problem on both sides, but other than that, it worked very well as a system.
COWEN: The Seleucids did conquer the Phoenicians, right ?
TALEB: The Phoenicians? No, the Seleucids came in, they said, ""OK."" The system, at the time, was patronage. You come in, you're a vassal state.
You guys here, you don't understand. I live in New York City, so I have two options. One, pay the state  with all of this now, it's going to go 50-some percent taxes  and you almost get nothing. Or, you can go to mafia now and give them 2 percent, and you get protection.
[laughter]
You get all the things that you want done for 2 percent. That's exactly what happened. Think about the defense budget if it were run by the mafia.
[laughter]
The guy would come in, and the system at the time was the system of  when you say ""conquer,"" the imperial methods everywhere, including the Ottomans, before them the Romans, before them the Seleucids and the Ptolemies. The Ptolemies had more integration.
The whole technique was, you come in . . . And remember that government role, the GDP was, at the turn of the century in France, 5 percent, OK, last century. So having been, you're not part of anything, you're just paying taxes to someone you'll never see  that was the thing. The integration usually was through commerce, not through military conquest.
The idea of Singapore, someone invaded  let's say Malaysia decides to take over Singapore. What are they going to do with that? They've got nothing. It's much better for you to go to Singapore, tell them, ""We want 2 percent."" Or ""We want 10 percent."" And then they will break it down to 3 percent.
COWEN: Putin of Russia, fragile or antifragile?
TALEB: I did a study on countries when we wanted to look at . . . There is metrics in this town [Washington, D.C.]of predicting stability of countries based on past stability, which to me is absurd because the metric would tell you that Saudi Arabia, when you're right at the black swan actually, I came here, I told him. I said I took seven countries, same government for 40 years, 40 governments in 40 years. Which country is more stable?
Two sets of countries on the right, one on the left. They said the one that has the same government, same families actually  people here in this town, at the Wilson Center.
I said that these guys have things backward. The countries on the right were Syria and Saudi Arabia. Now, we're, so far, waiting for a second shoe to drop. The other one was Italy. Last time I checked, Italy was still standing, and the mozzarella was excellent.
[laughter]
The idea of countries that have too much stability become weaker, particularly if it's propped up, sort of like companies. You see companies that go bust, you get companies that have zero volatility, compressed volatility.
Many of them really should have a little volatility, but some of them would be sitting on dynamite.
COWEN: You're smoothing to cover things up, right?
TALEB: Or if, for example, someone has a very stable salary, he or she is employed as an antifragile by day. But then you're laid off, at 65, after 30 years of employment, and basically can't do anything, whereas someone who has a little volatile income is more adaptive.
When I looked at countries, we looked at countries that are in the Eastern bloc. These countries had effectively two properties. One, they took a lot of heat during that period. Post-Soviet phase was very, very, very rough.
All of them  Armenia, Georgia, Russia, Romania  all of them, in fact, improved through that phase. Currently, it makes me believe that they could take another economic crisis, any crisis, and survive. They have proved the ability to survive. I'm not sure France can manage tomorrow the same disruption.
All of them  Armenia, Georgia, Russia, Romania  all of them, in fact, improved through the [Post-Soviet] phase. Currently, it makes me believe that they could take another economic crisis, any crisis, and survive. They have proved the ability to survive. I'm not sure France can manage tomorrow the same disruption.
COWEN: Erdogan's Turkey, fragile or antifragile?
TALEB: I'm going to Turkey in two weeks, so I'd rather answer when I come back.
[laughter]
OK, but from my feeling that what you see, the Turks are very happy because they got washing machine, they got stuff. They attribute it to Erdogan, so Erdogan is associated with growth, not so much with religion.
COWEN: PLO, fragile or antifragile?
TALEB: PLO?
COWEN: Palestinian Liberation Organization.
TALEB: Yeah, they were antifragile. Also, when they were in Lebanon, they had nothing to lose, which is why they have no skin in the game. That's really, that was the idea of a group that always has nothing to lose, so they can get into civil war.
In Lebanon today, every time someone sneezes in downtown Beirut, people are afraid of a civil war. But they don't realize that everyone is up to here in real estate. Nobody has any interest in waging war. It's not like, say, 1975, or 1973.
COWEN: What's it like housing six Syrian refugees?
TALEB: It is very nice. They're Sunnis. My mother hates Assad, and they love Assad, so the fights between my mother and the Sunnis . . .
[laughter]
One of them went to Syria to vote for Assad and couldn't come back, just to tell you how devoted he is. So what I do is, I talk to him to know what's going on, he calls his brothers, his siblings.
He's violently Sunni. Why? He said that he's religious Sunni. He said because at least for him, Assad represents stability. The fact that for a lot of Syrians, you got to look at Syria from Syria, not Syria from Saudi Arabian-financed lobbies here, OK? You get a different story.
People want stability. I don't know. One lesson I learned from Syria, that he's complaining. He said, ""You know, cellphones, I go here in Lebanon. This guy's selling for $200, this guy, $220, $240."" He said, ""Syria's perfect. Same price everywhere.""
[laughter]
Let me tell you, the Ba'aths have indoctrinated people to the point of maybe no return. People understand that Assad is not a god, but I bet you a lot of Iraqis would like Saddam to come back after what they saw.
The idea that they all have regarded as saying  if you were on the ground, you don't have this theoretical thing. ""This guy is an asshole."" OK, fine. You've got to realize what scissors. You got to look at both sides of the scissors.
That when you have civil war you have two groups fighting, so you take the least asshole becomes someone good in your eyes, but you're only analyzing one portion.
Assad, his father blew up my house. My grandfather was a member of parliament, and voted for pro-Israeli candidate Gemayel, and he came in and blew up our house. So I have a hatred for Assad's family, but at the same time I just realize I have a bigger hatred for the jihadis and for the clients of Obama.
This is how we can analyze it, comparatively, not naively like one-sided.
COWEN: I have a series of quick questions to ask you about the topic of wisdom. I'll just shoot these out. Feel free to pass if you don't want to address them. First, what is the biggest mistake people make when they go to the gym?
TALEB: They exercise too much.
COWEN: What's the best way to find and consume dark chocolate?
TALEB: What chocolate?
COWEN: Dark chocolate.
TALEB: One thing is that you're 10 years too late with me on that.
[laughter]
TALEB: Somehow I lost the taste for chocolate. But the best way to find and consume is not those labels, the fancy things made in Brooklyn.
[laughter]
TALEB: I'm past chocolate. I'm sorry, but . . .
COWEN: If he were here today, what would you ask Umberto Eco?
TALEB: What he would think of Trump.
COWEN: What can we learn from Sufism?
TALEB: Sufism?
COWEN: Yes.
TALEB: How to have a branch of Islam, effectively peaceful, allows drinking. I don't know if you know that.
[laughter]
TALEB: So that's very convenient.
[laughter]
The problem, that it was destroyed by Saudi Arabian funding. What can we learn from them is how your religion can be destroyed without anybody noticing. The number of Sufis take Tripoli in Lebanon, it was Sufi. Why did it become non-Sufi? Because of funding from Saudi Arabia  you indoctrinate two generations, and that's it.
COWEN: How do you find the right mentors in life?
TALEB: I don't know, but I know how to find inverse mentors.
COWEN: How do you do that?
TALEB: People  you know they're doing something wrong, and you figure out what makes them do something wrong. There's a fellow I worked with, and I knew that he was a complete failure but a nice person. When he would do something wrong, he was always caught into details. I realized that there's only one set of details. You cannot get into more than one set of detail. So that's one thing I learned.
Also, I find inverse role models, people you don't want to be like when you grow up.
[laughter]
You pick someone and you go with it. You have an instinct to know what you don't want to look like. Look at what they've done, what they do, and then you counter-imitate. You do a reverse imitation, and it works.
[laughter]
COWEN: What's the best thing to do on an airplane?
TALEB: Twitter fight.
[laughter]
I tried, but the problem is nobody fights with me anymore.
[laughter]
I tried to fight with a fellow who was . . . You have the problem with the pseudo expert . . . My worst are microeconomists because you can macro-bullshit more easily than they micro-bullshit.
There's no attachment, there's no feedback, so I tried to get into some fights with macro people. I tried to get in a fight with an Indian fellow who's repeating that story that we're refusing expertise at all. Remember that cartoon? They're imitating that cartoon in The New Yorker that shows people with the sign that they don't need the expertise of the pilot.
You cannot compare a macroeconomist to a pilot. There are two classes of experts. Belly dancers are experts at belly dancing. The people who steal radios from cars are experts at stealing radios from cars. Dentists are experts at dentistry. I'm not sure macroeconomists know anything about anything.
Because there's no feedback, so we don't know. Maybe they know. Policymakers or people in the State Department, I'm not sure they know anything because there's no feedback. We definitely know that a carpenter is an expert at carpentry, you see?
I tried to have a Twitter fight with a fellow because usually Twitter fights, number one, makes you look forward to sitting down and opening your computer.
And it kills time like nothing. But no one will engage me any more.
I tried to have a Twitter fight with a fellow because usually Twitter fights, number one, makes you look forward when sitting down and opening your computer.
And it kills time like nothing. But no one will engage me any more.
[laughter]
COWEN: Could we put our trust in cryptocurrencies?
TALEB: I have no idea, but one thing for sure that's working with cryptocurrencies is that it's forcing those fucks here in Federal Reserve to understand that they don't have a monopoly over our lives. So that's one thing.
And they are scared of it, believe me. That's one. Sorry if I used the Latin.
[laughter]
COWEN: Does the blockchain worry you? Once it's written, you cannot rewrite it or appeal to an external authority. That's that. Or if enough miners in northern China somewhere decided to collude and hit 51 percent, it seems in some ways a quite nonrobust system. Maybe under small probabilities, but not . . .
TALEB: The system's been robust, typically. We have had the equivalent of blockchain with letter credit. We're practicing letter credit at two levels, the first one, the recent one, since commerce in 500 years.
Of course, over time, you find you developed. So long as you don't have a lot of transactions on one single model. Then you figure out if it doesn't work, that someone will bring something with a counter  a trick that will have other flaws.
So there will be barriers. If you take earlier, the blockchain would resemble Phoenician model trading: ""I'll call your mother if you don't do the merchandise.""
The whole idea, the minute the whole concept of a currency that's triggered . . . In other words, we have now Visa payments that are triggered by receipt of merchandise, or coupled with the receipt of merchandise, is a brilliant idea. This is why, at some point, we will discover the flaws of blockchain. There are plenty of flaws, of course, but we don't know all of them.
COWEN: Self-driving cars, are they fragile or antifragile?
TALEB: I have a self-driving car. I mean semi self-driving, the Tesla thing. I drive it. The problem is, they don't know how self-driving cars will react to other self-driving cars. The sum, it's like flocks of birds.
It behaves differently from the sum of birds, like markets behave differently from individuals. When you have flocks of drones, typically, it's one computer running all these drones. If one computer ran all the self-driving cars, that may work, but then someone could take over the world that way. ""I own the world.""
[laughter]
I don't know. I have no idea. I think it's a great idea to have self-driving cars if someone in the car is responsible if there's an accident. Someone I can sue or someone I can blame if something . . . someone who is in the vehicle. Not saying your self-computer, not some anonymous person.
COWEN: Should we someday just go all collectively and turn our back on social media?
TALEB: No, because social media is Lindy. Let me tell you . . . Lindy means that there are things that are robust in time, like some basins that are robust in time. You realize that for example, this cup of coffee . . . OK, I'm not going to go back to Phoenicians  .
[laughter]
COWEN: You can, You can.
TALEB: No, no. OK, the book is 500 years old under this form, and maybe several thousand years old under a different form. So Lindy means that they're robust in time, and they come back. Until they disappear, they tend to come back.
Now, it so happens that at no point in history, except during the postwar period, did people receive news without being conveyors of news. That nuclear family, where people  pop, mom, 2.2 kids, one dog  are watching TV, receiving information and not transmitting.
The solitude of big city blocks  that was the idea. Well, it's gone because traditionally, you get the news and you purvey the news. So you're a recipient and a purveyor, with a little bit of alteration in the process.
There, we get back to the social media. I knew very quickly to learn to identify that there was a false alert yesterday in Saudi Arabia, as if there was a coup or something. You can figure out that there are some people you can trust, others you can't trust. Those you can't trust, you quickly identify them  those who cry wolf all the time.
Social media is bringing us back to a naturalistic environment because, say, in Athens, what was the newsroom?
The newsroom was the barbershop  you go in, you give information, and you take information. Or a fish market  you go in, you get fish, you get information and give information. And funerals, where you go in and chat, fake like you're crying, and then you're getting all the gossip.
[laughter]
We're gossip machines. Social media is great in that respect. I love it. I don't know if you were told. I refused to have a book tour. I refused to give media interviews. You asked at least. I gave one by accident.
[laughter]
Send my book to newspapers. Random House said, ""What?!"" I said, ""OK, fine, other authors. It's OK."" They agreed, although they cheated, I think, by sending some people the book.
[laughter]
Social media is bringing us back to a naturalistic environment because, say, in Athens, what was the newsroom?
The newsroom was the barbershop  you go in, you give information, and you take information. Or a fish market  you go in, you get fish, you get information and give information. And funerals, where you go in and chat, fake like you're crying, and then you're getting all the gossip.
We're gossip machines. Social media is great in that respect. I love it.
So there was no book review in US, my book, on social media. And it opened number 12 on best-seller list. It tells you that you don't need the New York Times to exist as an author, that Twitter is sufficient, Twitter and some Facebook. That's it.
COWEN: For our final segment, I have a few questions on what I call the Nassim Nicholas Taleb production function. You've written a few times that you've described yourself as an ascetic, in some ways. How did you become an ascetic early in your life?
TALEB: Ascetic?
COWEN: A-s-c-e-t-i-c.
TALEB: You mean like a tomato or . . . ?
[laughter]
COWEN: Yes.
[laughter]
TALEB: I'm not that ascetic. It depends on what you're ascetic about, but I discovered . . . Let me give you an anecdote that's in the book.
One day, I went with someone to dinner. I wanted go to a taverna, and, ""Oh, no. We've got to go to a better restaurant."" So I ended up having a meal, which you have to realize the look of a meal in a three-star Michelin restaurant. We sit down. There's farming people, microscopic organs. It's work, we're doing work. You're concentrating, afraid, biting too much, and you get all this sophistication for nothing.
[laughter]
Then I realize that as people get rich, they get controlled by the preferences, they're controlled by the outside.
It was $200 a person. I said, ""OK, I'd rather pay $200 for a pizza and would pay $6.95 for the same meal except that by social pressure."" This is how we use controlling preferences. It's the skin in the game. You discover that your preferences are . . . People are happier in small quarters. You have neighbors around you and narrow streets.
I'd rather eat with someone else a sandwich, provided it's good bread  not this old bread  than eat at a fancy restaurant. It's the same thing I discovered little by little. Even from a hedonic standpoint, sophistication is actually a burden.
Aside from that, there is something also that, from the beginning, you realize that hedonism  that pursuit of pleasure for pleasure's sake  there's something about it that gives me anxiety. On the other hand, doing something productive  not productive in the sense of virtual signaling, but something that fits a sense of honor  you feel good.
COWEN: What books influenced you early in life, say before you were 15 years old?
TALEB: The book I kept rereading was The Desert of the Tartar.
COWEN: Even before you were 15?
TALEB: Before 15, and I reread it many times. I'd say, before 15, I read Dostoyevsky and I read The Idiot. There's a scene that maybe I was 14 when I read it. Prince Myshkin was giving this story. Actually, it was autobiographical for Dostoyevsky.
He said he was going to be put to death. As they woke him up and were taking him to the execution place, he decided to live the last few minutes of his life with intensity. He devoured life, it was so pleasurable, and promised himself, if he survives, to enjoy every minute of life the same way.
And he survived. In fact, it was a simulacrum of an execution, and Dostoyevsky . . . effectively that says the guy survived. The lesson was he no longer did that. It was about the preferences of the moment. He couldn't carry on later. He forgot about the episode. That marked me from Dostoyevsky when I was a kid, and then became obsessed with Dostoyevsky.
COWEN: What was your favorite part of the Bible as a boy?
TALEB: I'm going to be honest. The Bible didn't play into a lot of this. It's too complex. There's too many names in the Bible, so it wouldn't do too many things.
COWEN: Final question. What is it that you do in your moments of solitude?
TALEB: Math.
COWEN: Math? That pleases you? Or that's a form of work that relieves anxiety?
TALEB: No, I only develop anxiety when I go to fancy restaurants.
[laughter]
TALEB: . . . when I'm not doing something right.
The math is  I like it. People tell me that, as people age, they like math less and math is a young man's game. Yes, it's more and more enjoyable. I do math.
Mostly, there's a Twitter math that I'm part of. So there's always something to solve. Usually, really, that's totally unpredictable. It takes between one minute and one day to solve one problem. You don't know. Then this thing comes in, and then I stop.
COWEN: Based on your own upbringing as a boy, if you were giving advice to someone raising a child through the age of 18, what would be the takeaway you would offer from your own life experience up to that age?
TALEB: Get a degree from school, but become an autodidact. Don't waste time trying to get an A because you're not going  we're gonna talk about it with Bryan  you're not going to remember all that shit. You always remember what you try to read by yourself.
Read as much as you can, and try to get the lowest possible passing grades you can at school.
I remember the stuff I read by myself, that I was driven. I don't remember stuff that was given to me at school. It's an allocation of time.
I discovered that I wanted to be a writer as a kid. I realized to have an edge as a writer, you can't really know what people know. You've got to know a lot of stuff that they don't know.
I started reading books voraciously, and also read books that, with some instinct, that would be helpful 20 years from now. Therefore, it's not the latest nonfiction best seller.
So I read a lot of stuff. And I think that I would recommend doing the same. Read as much as you can, and try to get the lowest possible passing grades you can at school. Don't study stuff like history because it's going to be revised.
[laughter]
Geography, history, all these. For instance, chemistry or stuff like that. Math is, I think, probably the only thing you can pick up at school that's useful.
COWEN: Nassim Nicholas Taleb, thank you very much.
[applause]
The conversation continues! Click here to read Taleb's discussion with Bryan Caplan about his book The Case Against Education.
A podcast in which esteemed economist Tyler Cowen engages...
2.8K 
12
",111
https://medium.com/publicmint/public-mint-polkastarter-ido-launching-23rd-of-february-whitelist-now-open-bc4821948f07?source=tag_archive---------6-----------------------,"Public Mint Polkastarter IDO: Launching 23rd of February, Whitelist Now Open!",We're pleased to announce that the Public Mint Polkastarter IDO will be held on Tuesday the 23rd of February! The whitelist is now open...,Public Mint,3,"We're pleased to announce that the Public Mint Polkastarter IDO will be held on Tuesday the 23rd of February! The whitelist is now open and will close at 5pm GMT on Friday the 19th of February.
We will be raising a total of $250K USD via the Polkastater IDO with more details on the POLS/non-POLS pools to be released in the coming days.
Please note that anyone looking to participate in the IDO is required to whitelist, both POLS holders and non-POLS token holders will need to complete the whitelist tasks to qualify for the lottery draw.
Public Mint Polkastarter IDO Whitelist Lottery Details
The Public Mint IDO Whitelist will close at 5pm GMT on Friday the 19th of February. In order to qualify for the Public Mint Polkastarter whitelist lottery, entrants must -
All entries in the lottery will be checked for valid ERC20 addresses, Telegram handles and Twitter accounts. Bot entries will be removed prior to a randomised draw of qualified users who will then be whitelisted for the IDO. Entries that have not completed all of the above steps will be disqualified.
Whitelist lottery winners will be contacted via email and all whitelist lottery winners will need to pass the Public Mint KYC process in order to be able to participate in the IDO.
Public Mint Polkastarter IDO Token Sale Details
POLS holders have been allocated 1 500 000 MINT tokens. Non-POLS holders have been allocated 1 000 000 MINT tokens.
Public Mint Polkastarter IDO TLDR
Full token metrics are shown below and can also be found in the Public Mint Lite paper.
About Public Mint
Public Mint bridges the worlds of traditional fiat with the innovative world of crypto, offering a complete platform for synthetic fiat, which is fully collateralized, regulated and FDIC insured.
Public Mint offers a fiat-native blockchain, APIs and web components, open and ready for anyone to build fiat-native applications and accept credit cards, ACH, wire transfers and more. No bank accounts needed.
Our upcoming MINT Global Earn program brings direct fiat liquidity to CeFi and DeFi, exposing these opportunities to the world at large via earnings-bearing synthetic fiat currencies.
Public Mint Resources
",112
https://entrepreneurshandbook.co/this-is-how-i-made-40k-in-passive-income-by-age-26-e57b651bc483?source=tag_archive---------3-----------------------,This is How I Made $40k In Passive Income By Age 26,3 hours of effort over 3 years,Amardeep Parmar,6,"I'm talking here about real passive income, not the kind where you spend years writing a book. There's one caveat though and you need money to make money.
I started investing part of my income every month at age 23. Three years later, I had made $40k in profit tax-free and could put down a deposit on my first house. All with less than an hour of effort per year. $13k per hour of work doesn't sound bad, does it?
It's not sexy but I relied on getting a professional job and investing my excess income. Many in my position don't do this and sacrifice future financial freedom. You can take the profits to start up your own business with less reliance on outside help. Self-funding the initial stages gives you more credibility when asking others for more money.
My Economics bachelors and central bank experience made me confident to invest responsibly. Yet the steps I took weren't complex and here I break down what I did.
NOTE: Lucky factors went my way with exchange rates, freak performance, and government bonus schemes amongst others. Do not read this and think similar performance can be produced reliably in the future. This is a high-level overview and I do not go into blow-by-blow detail.
One of the biggest mistakes I see is people thinking they are exceptional. Investment funds have whole teams of hyperqualified people and complex algorithms. Yet 85.1% of active funds have failed to beat the S&P 500 in the last 10 years. How can you honestly believe you can win?
I bought index and active funds from the major economies rather than individual stocks. This takes the decision making out of my hands. As I'm from the UK, I invested through an ISA (the equivalent of a superpowered Roth IRA) to earn tax-free.
I spread myself out geographically with stocks in the USA, UK, mainland Europe, and Asia. My risk was dramatically reduced as I owned shares in thousands of companies. By using index funds, my fees were far lower than buying individual stocks. When I wanted exposure and index funds were unavailable, I found funds by managers with long histories.
Yes, you might miss out on the next Amazon but you'll also miss out on the thousands of failed unicorns. Theranos and Jawbone both seemed certain to be big and each received over $500m in funding. Both are bankrupt.
Every month, the same amount left my account automatically. I never considered this as spending money so it never factored into my buying decisions. I could start the account with significant savings from 1.5 years of working that were sitting in a low-interest current account.
There are all kinds of apps to encourage people to invest their savings. One of the tricks I dislike is rounding up purchases to send to the pot. You buy a cookie for 20 cents and 80 cents goes straight into your fund. This takes control away from you and leaves your input reliant on chance events. The return is already based on chance so why make it even more uncertain!
Some portray compounding as a type of sorcery. Yet 7% return per year for ten years on ten dollars is $9.67 profit. On a thousand dollars it is $967. Don't make the excuse of something is better than nothing when you can put away more. It takes time to build a portfolio to the point where it can make a difference in your life. I had a massive advantage by living with my parents.
If you truly want passive income, you need to examine your spending habits too and decide if anything is a luxury you are happy to be without.
I could invest more than I did but I always kept some in reserve. If anything happened to me, I could cope with losing half the value of my investments. The amount you're willing to risk can change over time and change your plans in line with this.
The worst crashes in the S&P history have taken the value to around half but they have always bounced back. We still didn't fall below this even when news of the pandemic hit or when the financial crisis of 2008 struck. You can be confident a developed country's stock market won't completely self-destruct. Only a massive event could do this and then you'd have bigger problems!
Individual stocks can go to zero but it is harder for a fund to do so. You must feel comfortable with the unlikely worst-case scenario for peace of mind. There's always a chance of great losses and you can't blame anyone else if you lose more than you can handle. It is possible to lose everything!
Let's not pretend it isn't a privilege to invest. Not only must you cover your expenses but also your debts. I was fortunate to have student loan debt with an interest of less than 2%. As long as I believed I could beat this rate, it made sense to invest extra money rather than paying off debts early.
Yet I know others are not as lucky. The average stock market return in the long-run has been 7% for the S&P. If the interest on your debt is higher than this, pay it off first! You have to decide your willingness to take the risk if your interest is less than this. I cannot tell you how much. I took a risk by investing in emerging economies and those paid off.
For entrepreneurs, when starting a business you should believe you can beat this rate in the long run. At the time, I didn't have a business idea I thought would be a better path. You should be confident in forecasting significantly more than this to make the extra effort worth it.
There's a secret of investing many people seem to forget. Looking at the numbers doesn't magically make them increase. Interfering too much will backfire.
I thought about taking my money out several times when it looked like the peak. I thought about adding more whenever it looked like the bottom. Every time I was wrong. I would have lost wealth if I had acted. Trying to perfectly time the market will leave you anxious and constantly checking the news. Not to mention the lost income by needing to pay fees for every trade.
My investments ran on autopilot so I didn't waste thousands of hours checking something I couldn't change. This is dollar-cost averaging, where you put in the same amount every period in the same area. When the price is low, you buy more and when the price is high, you buy less!
Investing in the way I did gave me much greater financial freedom. I did it while working a 9-5 and fresh out of university. The hardest part is working to get the money to invest but once you have this, it's about making the strategy as easy as possible. These are the steps I took and can help you too.
Thank you for reading and have a wonderful day! Remember this is my story and you must examine the risks for yourself. I have intentionally not given the exact funds because they may not perform the same in the future.
Any actions taken are completely at your own risk, this should not be considered financial or legal advice. I am not a financial advisor. Please consult a financial professional before making major financial decisions.
How to succeed in entrepreneurship
11.9K 
74
11.9K claps
11.9K 
",113
https://medium.com/@creamdotfinance/introducing-the-iron-bank-bab9417c9a?source=tag_archive---------7-----------------------,Introducing The Iron Bank,Part II: The Road to Cream v2,C.R.E.A.M.,5,"We've been hard at work since our launch in August to become one of the most innovative platforms in DeFi. Our last post looked back at 2020, shared more info on our merger with Yearn, and explained our sharpened focus on our core lending protocol with the wind-down of CreamY Swap and Cream Swap.
We also teased the latest in milky CREAM v2 goodness  the Iron Bank, which will be the main focus of this post.
The foundation of CREAM v2 is the Iron Bank. If the DeFi ecosystem were Game of Thrones' mythical Westeros, CREAM v2's Iron Bank would be the king maker for other protocols.
The Iron Bank is CREAM's paradigm-shifting protocol-to-protocol lending platform and liquidity backstop for the entire DeFi ecosystem. Existing money markets like Cream v1 are peer-to-peer. In traditional finance, the peer-to-peer lending market size is around $70 billion in loans outstanding. That is a pittance when compared to the size of all US corporate debt which at year end 2020, soared past $10 trillion.
We believe that DeFi will one day absorb all global financial markets. As protocols replace enterprises, the new face of corporate credit will be protocol-to-protocol lending, which could one day far oustrip DeFi peer-to-peer lending just as corporate lending markets far surpass peer-to-peer markets. This is why we launched the Iron Bank: we believe protocol-to-protocol lending will one day be a multi-trillion dollar market.
Consolidating cross-protocol liquidity will lead to even stronger composability in DeFi. The critical innovation at the heart of the Iron Bank is zero-collateral lending  protocol-to-protocol loans will use a credit system that is not currently possible with existing peer-to-peer lending solutions, all of which are overcollateralized. We believe that the capital efficiency made possible by protocol-level loans will accelerate our industry's growth.
Once a protocol has been whitelisted, and a credit limit set, it can begin to borrow funds from CREAM v2 directly. This is essentially the same function that determines whether a CREAM borrower who has posted collateral to CREAM has enough collateral to allow them to borrow more. Only now, the crucial difference is that protocols won't have to waste their own liquidity in order to receive the funds that they need.
For now, we are focusing on the existing set of partners and are not open to additional reviews. CREAM will set credit limits on each whitelisted protocol to better manage risk.
The available pool of assets that protocols can borrow from CREAM v2 is currently limited to wETH, DAI, and y3Crv. You can find these and future CREAM v2 assets on Yearn Finance's lending portal. We will also soon add USDT, USDC, sUSD, mUSD, DUSD, LINK, YFI, SNX, WBTC.
While CREAM v2 will remain limited to blue chip assets, new assets will continue to be listed on CREAM.Finance for collateralized peer-to-peer lending.
Yearn Vaults
As the much anticipated Yearn v2 is rolled out, strategists will be able to borrow assets from the Iron Bank. The Iron Bank will enable Yearn yVaults to develop leveraged yield-farming strategies and cross-asset strategies. Users will be able to deposit DAI and borrow an equivalent dollar amount of ETH via CREAM then enter SushiSwap liquidity pools utilizing Alpha Homora's leveraged yield-farming product. Users will ultimately be able to obtain 90x leverage on stablecoins or 80x leverage on ETH to farm SUSHI, CRV, ALPHA. If the user uses Yearn yVaults they will also potentially be eligible to earn PICKLE.
Additionally, Yearn is launching an abstraction to leverage every yearn strategy via the Iron Bank.
Alpha Finance Lab's Alpha Homora V2
2021 is already off to a massive start as last week, one of our first partners, Alpha Homora, announced that our teams have been working closely to bring the first undercollateralized protocol-to-protocol lending product.
This is the first time that a contract (Alpha Homora V2) can borrow from a lending protocol (CREAM V2) in an undercollateralized way. Alpha Homora V2 will borrow liquidity from CREAM V2 and offer that liquidity as leverage to its users. CREAM's users will benefit from higher lending interest rates that accompany an increase in demand to borrow assets. In the meantime, both Alpha Homora and its users will benefit from deeper liquidity for leveraged yield farming.
The CREAM protocol reserve pool earns fees via the reserve factor as more borrowers enter the CREAM market to borrow. With fully collateralized loans, these fees are constrained by the amount of assets that are deposited in the protocol, the collateral factor, and the asset cap.
Allowing whitelisted borrowers to tap into CREAM's liquidity will increase potential earnings for the protocol with more capital efficiency, leading to increased revenue for CREAM holders.
We're very excited to continue to onboard strong DeFi protocols to the Iron Bank. We are in talks with a number of leading names in the industry to expand our offering.
In the meantime, we are currently upgrading our financial health monitoring capabilities and re-evaluating our token model.
Special thanks to Andre Cronje, Sam Priestley, and Carlos Sessa from the Yearn core team for their insights and development in erecting the Iron Bank. Also, thanks to Tascha Punyaneramitdee and the rest of the Alpha Homora team for their support and partnership.
The Iron Bank: http://v1.yearn.finance/lending Join us on Discord, follow us on Twitter, or visit us at cream.finance.
Crypto Rules Everything Around Me, C.R.E.A.M.
",114
https://medium.datadriveninvestor.com/i-used-acorns-robinhood-and-stash-for-2-years-this-is-what-i-learned-and-earned-21baf91dda0e?source=tag_archive---------3-----------------------,"I used Acorns, Robinhood, and Stash for 2 years. This is what I learned and earned.","For the last 2 years, I have been using Stash, Acorns, and Robinhood all for investing. I plan to continue using all three as a part of my...",Alex White,9,"For the last 2 years, I have been using Stash, Acorns, and Robinhood all for investing. I plan to continue using all three as a part of my savings strategy. But I have found each to have pros and cons that may work better for some people than others! I don't have a lot of money to invest, but I do what I can. These tools have been a great learning experience and really have helped me not only save money but get into a better mindset when it comes to money and saving.
This reviews each platform weighs the pros and cons and explains why I abandoned one in favor of the other two. At the bottom, you can see a side by side comparison of the three platforms, including how well the return on my own investment worked out on each platform.
Acorns was the first app I started using. I loved the idea of ""Roundups"", which is where when you make purchases on a debit card which is linked to your Acorns account, it will invest it for you! Instead of saving spare change in a jar, it goes into an account that bears interest!
In addition to roundups, you can also deposit or withdraw at any time, and schedule regular automatic deposits. Once I got going on the platform, the novelty of the roundups wore off a bit. It is just as well to set up an automatic deposit. But for anyone who wants to save, but doesn't always have much money in their checking accounts, the roundups are great.
Acorns invests your money into a specific kind of fund. Specifically, your investment is distributed over six Exchange Traded Funds through their partner Vanguard. Using these six funds, your investment is distributed in over 7,000 stocks. So you can be sure your investment is diversified and robust.
The app itself is beautiful. It is very intuitive to use, I can't recall a single time of not being able to find what I needed.
Bottom line: A fantastic app, and a good return on investment. Best for beginners who want a low effort way to save some money. Two years after writing this article, Acorns has become my favorite of the three.
Note: Acorns now offers two new services, one for saving for retirement called Acorns Later, and another called Acorns Spend, which is a debit card that is integrated with their other services. Thes his review does not cover this, but only their core offering, which is designed for more mid-term savings.
Get $5 when you use my invite link: https://acorns.com/invite/QMMYSU
Robinhood is different from the other two, as it does not deal with diverse portfolios, but with individual stocks (and more recently, cryptocurrencies).
This means a higher learning curve, higher risk, and higher potential reward. I learned a lot about investing through my use of Robinhood, and personally, I enjoy the extra attention trading single stocks needs. I mostly just buy tech company stock, because that is the field I work in and know best. I also have bought a couple of stocks based on tips from a trusted resource.
This is the only platform I've made any real money using, but its also the only one that where there is hope for more than a single digit return. Along with that comes much greater risk. I picked some good ones and did great, but I just as easily could have lost most of my investment.
One thing that is great about this service is that there it is freemium, so there are no fees beyond what the regulators require!
The app itself is beautiful, polished and intuitive.
Bottom line: This is a great app if you want the excitement, risk, and potential reward of trading single stocks. It's the largest learning curve, so would be best for people who want to be involved, not set something up and forget it.
Join Robinhood and we'll both get a stock like Apple, Ford, or Facebook for free. Sign up with my link: invite.robinhood.com/katherw403
EDIT: Unfortunately, in the last year and a half, I have had a huge problem with Stash, and can no longer recommend their service. Here's what happened.
Over a year ago I closed the bank account I had connected to Stash, and opened another one. Since then I have not been able to access a single dollar of my money. I have tried opening TWO bank accounts for the primary purpose of using it to connect to Stash so I could retrieve funds. Both produced cryptic errors like ""We're sorry, this routing number is not accepted.""
When I complained to Stash customer service, I got passed around from person to person for months with no one able to, or much interested in, helping me. All I wanted was my money that I entrusted in them, or at the very least an explanation of why my bank accounts were not being accepted. I wanted to know the criteria by which they choose to accept or not accept banks so that I would not be opening a THIRD bank account and have that not work either!
I got NONE of these.
Additionally, my repeated requests to be connected with a manager were ignored. I'm not sure if that is even legal. I was generally treated like a complainer and a problem, rather than like a client who had invested thousands of dollars into their app and entrusted them with my money.
As of this writing, there have been dozens of emails over the course of more than a year, and I still do not have access to my money. This has undoubtedly cost me several hundred dollars, as I would have cashed out a few months ago when the economy was strong and I could have really used the cash.
This whole thing has left me feeling like my cash is being held hostage by a corporation that couldn't care less about the impact this is having on my life.
DO NOT USE STASH.
I'm removing my promotion code from here, even though that means that I might miss out on some money from people using it. I absolutely cannot in good conscience suggest that anyone use this service.
In contrast, Robinhood and Acorns' respective customer services have been excellent, and I've had no problems accessing my funds, even though I was in the same situation with those apps where I had to change banks.
I'll leave the old review below so people can still learn how it works in comparison to the other apps.
Edit: I did eventually get my money out and no longer use this service.
Here's my original review of Stash:
Stash is similar to Acorns, in that it allows you to easily invest in nicely diversified stock portfolios and ETF's curated by the company. The biggest difference is rather than having only one offering, as with Acorns, the user can choose from a growing number of customized portfolios, which are grouped by industry and causes.
I absolutely love this flexibility. You can choose custom portfolios based on what industries you feel might be poised for huge gains, like ""Internet Titans"" or ""Data Defenders"", and invest in companies that champion causes that you care about like ""Combat Carbon"" and ""Equality Works"". And some that do both, like ""Corporate Cannabis.""
I believe on voting with my money. When I realized by investing through Acorns I was supporting some companies that I ideologically don't wish to support, and that through Stash I can actually invest in things I believe in, I was wooed pretty hard towards the latter.
This is a trade-off. The Acorns app is simpler, as it invests everything in their own portfolio. Stash takes a little more time and energy to decide what you want to invest in. For this reason, it is possible to get a worse return based on bad picks, which is something I experienced! But due to the extra effort, I also learned a lot more about investing by using this app than I did with using Acorns, where the learning curve was pretty non-existent.
Yes, I lost money in the last 2 years. However, to be fair, Stash is designed for ""buy and hold"" over a longer time. Market volatility should even out long-term and even the poorest of choices in portfolios should still give you an overall net gain.
The app itself is quite good. It is not as polished and beautiful as Acorns, but being able to pick your portfolio adds quite a bit of complexity! There's a lot going on in this app, and they do a very good job of organizing it.
Bottom line: A great app that gives you the freedom to maximize your investment  or minimize it by making poor choices as I did! Best for people who want to learn about investing without too much risk involved, or for people who want to invest in companies that do right by causes they are passionate about.
Note: Stash has also gotten into the retirement savings business, and into more traditional banking, including a debit card that is in the works. This review only includes their initial offering, their investment service.
Reminder: I wrote the above review before they held my money hostage, and refused to answer my questions or put me in touch with a manager. Dealing with this company has possibly the worst experience I've ever had as a customer of anyone, and I used to be a Comcast customer, so that's saying a lot. Stay away from this company.
Acorns: Round ups, deposits, auto recurring deposits
Stash: Deposits, auto recurring deposits
Robinhood: Deposit money from your bank account, and use that balance to buy stocks.
Acorns: ETF's consisting of over 7k stocks
Stash: Pick and choose your own custom portfolios
Robinhood: Individual stocks
Acorns: $5, which is free with this link: https://acorns.com/invite/QMMYSU
Stash: $5
Robinhood: $0, Sign up with my link and get a free stock:
invite.robinhood.com/katherw403
Market: 3.66%
Cash: -5.4%
Acorns: 2.32% 
Stash: -5.51% <- I did worse than mattress money! 
Robinhood: 1,900.1%!!! 
Acorns: 10/10
Stash: 8/10
Robinhood: 10/10
Acorns: ""Found money"" feature where when you shop at certain retailers, those retailers will give you a discount and the difference gets invested in your Acorns account.
Stash: Same deal as above, just with a different name and different partners.
Robinhood: Some perks available to premium members. Also, Robinhood allows investment in Crypto, which is pretty cool. (Now is either the best time in a while to buy, or a terrible time depending on your perspective.)
Robinhood is the best for those who want to learn, risk more, and hope for a larger return. It is my personal favorite, and I think it would be my favorite even if I didn't do so well on it. I just really enjoy learning and trading single stocks.
Acorns is best for people who just want to set something up to save money and not think too hard about it.
Stash lies somewhere in between. All of these are great apps and great tools for saving money. If I had to delete one of them today, I'd delete Stash, but I'd be sad because I love to vote with my dollar, and they make that very easy. But at the end of the day, it's the worst performer, and these are tools for saving money, not giving it away!
Disclaimer: I do benefit if you click these links and sign up. But otherwise, I am in no way affiliated with these brands. I don't personally ever promote anything I don't personally use and benefit from.
Start investing with Acorns today! Get $5 when you use my invite link: https://acorns.com/invite/QMMYSU
Join Robinhood and we'll both get a stock like Apple, Ford, or Facebook for free. Sign up with my link: invite.robinhood.com/katherw403
Hey! My name is Alex White and I write about self-transformation and strange and interesting topics! Occasionally I review products and services I've used and loved.
My newest post reviews the cheapest and best checking account I've ever had  especially as someone who likes to travel. Check it out here!
empowerment through data, knowledge, and expertise.
705 
12
705 claps
705 
",115
https://keepingstock.net/explaining-blockchain-how-proof-of-work-enables-trustless-consensus-2abed27f0845?source=tag_archive---------4-----------------------,Explaining blockchain  how proof of work enables trustless consensus,"Blockchain technology, of which Bitcoin is an example, can be quite hard to understand. Mainly this is because core concepts tend to get...",Aleksandr Bulkin,12,"Blockchain technology, of which Bitcoin is an example, can be quite hard to understand. Mainly this is because core concepts tend to get lost among the complexity of non-essential details. This article tries to fill the gap between general audience literature that is entirely uninformative to computer professionals and highly specialized literature that is informative, but often overwhelming. It is written for people with some technology background but without in-depth familiarity with this particular field.
The subject of this article is technology of distributed trustless consensus, for this is the one area in which blockchain systems, like Bitcoin, are indeed a major breakthrough. When it comes to other goals, such as distributed data storage, anonymity, transaction verifiability, data obfuscation, shared ledgers, micropayments, high throughput, digital contracts, and so on, cryptographic blockchain systems are, essentially, incidental. Solutions to these problems are well known outside of the blockchain space and, consequently, I will not focus on them here.
The main innovation that Satoshi Nakamoto introduced in his article is using so-called proof of work (POW) to create distributed trustless consensus and solve the double-spend problem. POW is not a new idea, but the way Satoshi combined this and other existing concepts  cryptographic signatures, merkle chains, and P2P networks  into a viable distributed consensus system, of which cryptocurrency is the first and basic application, was quite innovative.
Proof of work is a requirement that expensive computations, also called mining for reasons which later will become clear, be performed in order to facilitate transactions on the blockchain. To understand the link between computational difficulty and trustless consensus within a network implementing a distributed cryptocurrency system is a serious mental feat. With this writing I hope to help those who are attempting it.
Because distributed trustless consensus is the primary innovation of blockchain technology, we start by understanding what it is. As the term suggests, there are three parts to the puzzle: (1) consensus, (2) distributed, and (3) trustless. Having explained the three terms, I will synthesize them by illustrating the problem that arises when you put all three together. In the process we will see how a distributed ledger based on POW solves this problem.
Consider what happens when people talk to each other. If I say ""please pass the salt"", I know that what I mean to convey is the desire to receive the salt shaker. When you hear those words you also understand that this is their meaning. However there is more to this. Implicitly, you understand that I understand that this is the meaning and, furthermore I understand that you understand that I understand this. Imagine that I am in a country where English is not commonly spoken. I will most likely not say these words, not because they suddenly stop meaning what I know they mean, but because I no longer have assurance of our mutual consensus around the meaning  I no longer know that you know what I mean.
The state of consensus around meaning of language, therefore, is defined as a state where we both say X to mean Y, we both know that we say X to mean Y, we both know that we both know this and so on, ad infinitum. In other words, it requires what people call common knowledge.
With money the picture is very similar, but consensus must include not just the transacting parties, but the rest of the social group where a particular currency is used. For me to receive money in payment for services or valuable goods, not only must I recognize what's presented to me as money, but I must also know with sufficient assurance that everyone else will also recognize it as such.
The requirement that money supply be scarce is crucially important. If people accepted frogs in exchange for useful things  we would be collecting frogs. If people took slips of paper with a hand-drawn image of a giraffe we would all be drawing giraffes. The reason why choosing a scarce resource as currency is important is that overall this provides a motivating factor in the larger social system to which we have all become accustomed. A non-scarce money will not motivate people to render services or produce goods. So hand-drawn giraffes as money won't work. This understanding for us is intuitive and places the restriction on what can be used as money.
Consequently, the consensus around money (denoted Cm) can be simplistically described as follows.
I will accept some token or process in payment for valuable goods or services if:
I, additionally, believe that
3. ... everyone else adheres to (1), (2) and (3)
This is simplistic and is by far not a rigorous account, but for the goal of explaining digital finance it is sufficient. Notice that (3) implies an existing common knowledge of what money means, while (2) provides a requirement for there to be common knowledge around this particular token in fact being money according to the consensus laid out in (3). Logicians usually have a field day with these kinds of self-referential statements, but we shall not get distracted by marveling as they do. We shall just plow on.
We must notice an important implication to which we will return later. It doesn't really matter what money is, what specific token is used, as long as Cm is obtained for it. Cm doesn't tell us what valid money is, but it does tell us some things about what it isn't: hand drawn pictures of giraffes it isn't. Let's remember this for now and move on to the next topic.
Digital money can solve Cm(1)  the requirement that it be in limited supply  by establishing a central entity that controls the supply. This is straightforward: a database on some server holds the information about who holds what amount of money and when transactions are performed it ensures that a person doesn't overdraw his or her account. This is how digital banking works today.
Leaving trust issues aside until the next section let's consider the difficulty in making this process distributed. The distributed system must ensure consistency, that is when a transfer is made using the information supplied by one of the nodes it must be correctly reflected in all other nodes. This problem is hard but we know how to solve it. We either make things slow, by waiting for the information to propagate across the network (known as strict consistency), or slightly unreliable by confirming the transaction immediately but reserving the right to cancel it if it encounters a conflict elsewhere on the network (eventual consistency).
The third and final problem is trust. Trust is implicitly present in digital finance in many different ways. There is trust that the entity that holds your account information doesn't go about randomly subtracting value from it, the trust of safety. There is trust that the entity who ensures circulation doesn't go about randomly assigning money to itself out of nowhere  trust of issuance. Finally, there is trust that the system in fact ensures consistency of information, that is it performs its main function  trust of correctness.
What does it mean, then, to say that a system is trustless? Obviously, we have to trust a system which we use to facilitate value exchange. When we talk about trustless systems, we mean that our ability to trust it does not depend on the intentions of any particular party, which could be arbitrarily malicious. This is a curious proposition, for how can we possibly hope to transact through a bad intermediary in a way we can rely on?
Turns out, as the reader is well aware, trustless systems are not new. For example, email can not be trusted to protect your content for unintended eavesdropping. Yet, this presents little problem to those familiar with modern cryptography. An encrypted message can be safely sent through an untrusted channel, making this a trustless system.
Similarly, cryptography answers the needs of trust of safety, because we can require transactions to have cryptographic signatures. If everyone in the systems is able to verify cryptographic signatures and refuses to accept payment without one  the trust of safety is achieved, for it makes it impossible for anyone to transact on your funds.
It is the other two kinds of trust that present a serious problem. But before we can look at that closer, we have to synthesize what we know so far.
A digital transaction is some event or process whose effect is to transfer money in the sense we laid out in Cm. Putting together our understanding of consensus around money with our idea of trust, we can now list the requirements we have placed on such process so far (we will refer to them as Tx).
In order for our digital monetary system to work, the recipient of a transaction must be able to confirm that:
A user  human being or an automated device, say, a vending machine  uses some algorithm that ensures that Tx (1)-(4) are met. These statements expressed in a natural language describe a combined state between the user and the algorithm. For example, an algorithm may include a verification of a cryptographic signature used in a transaction. A combination of the fact that the user knows this check to be implemented, correctly, within the algorithm and the fact that the check succeeds creates a combined state of knowledge about the validity of the signature. Similarly, Tx(1)-(4) can only be true in a sense of being a combination of the user's knowledge of the details of the algorithm and the context in which it operates, together with the results of the algorithm's execution.
If it is commonly known that the same algorithm (or, more generally, protocol) is used by everyone who engages in financial transactions then it implies assurances regarding what others' view of the transaction is  namely the views mandated by the protocol. Consequently, whenever the description of Tx speaks of assurances about ""others'"" views, this is on condition that everyone uses the same protocol.
This condition, of course, makes sense. Using the same protocol as everyone else is, naturally, a prerequisite to ensuring Cm(2)  one's ability to spend money later. Consequently, circumventing the system to some malicious end must be done within the constraints of what this common protocol allows.
So far we have not said anything about exactly what a transaction is, except to posit a common understanding of its effects. In a centralized system, transactions are simply changes to a central database. Because centralized systems involve trust, Tx(1)-(4) are confirmed through a combination of what the central server reports and the common belief, based on trust, that the central server reports accurate and consistent information.
In a trustless system, however, things are quite a bit more complex. For starters, one can not rely on a report by any single entity. Cryptocurrency developers take an even stronger view  that one can not rely on a report by any number of entities. The reason for this is simple: we assume that there is little to no cost to creating any number of colluding malicious entities, which means that faced with conflicting information deciding whose report to believe may be impossible. (I don't find this premise to be unquestionably true, but that's what blockchain is based on, so there)
The assumption that one can't trust anyone's reports leads to the conclusion that one has to be able to confirm things for oneself. This gives rise to the notion that when presented with a transaction, the recipient must ensure by personally reviewing the entire history of transactions the fact that the sender is in fact in possession of the funds and that the way this came to be is valid. This, in turn, leads us down the path of shared ledgers where the main premise is that the entire history of all transactions is completely open and public.
Here is where things start to get interesting. Didn't I just say that you can't believe reports anyone makes? And if so, what about the server from which I am downloading the ledger? Can't it present me with a doctored picture?
The answer has two parts.
First is that it is much harder to produce a totally self-consistent ledger than a report related to a state of a single account. A self-consistent ledger requires, for instance, that transactions be cryptographically signed by the account holders, making a ledger in which you deduct someone else's money simply impossible.
But producing multiple self-consistent ledgers is still not very hard. For example, you can easily create a ledger that only differs in the last transaction in the account from which you hold the cryptographic key.
So the second part of the solution has to answer the question of what does one do when faced with entirely self-consistent but different ledgers? In the crypto parlance this is referred to as the double-spend problem. It is so called because of the possibility that someone malicious can present two different ledger states to two different users, or even worse  present the entire network with a ledger state and then present the entire network with a different state but one that will be ultimately accepted, negating the first one.
The solution that was originally proposed in Satoshi's article addresses this problem. To explain it, we first posit an assumption that the Internet is eventually connected that is every state of the public ledger will eventually be observed by everyone. In other words, hiding is impossible for a prolonged period of time.
Next, we point out that the choice between conflicting states is not one where one state is a-priori good and another is a-priori bad, since we posited that both states are totally self-consistent. The difference between a good state and a bad state is that of consensus only, that is if we can all agree on which one is good and simply ignore the other one, we will have solved the problem.
There is slightly more to this, though. Namely, by Cm(2) we require that consensus established in the network persist in the future. If our common way of choosing a good state out of several conflicting ones doesn't ensure this, then someone can present the network with a state of a ledger that shows them to be a sender of some transaction, receive the benefits from the recipient and then present the network with another, ""better"" state, that negates the earlier transaction.
So what we want is a situation where once we see a state of a ledger and accept it as good, that we must all believe that it will be impossible to create a better one later. And this is where proof of work comes in. Proof of work adds artificial computational difficulty to the ledger. The common protocol requires that of any two conflicting states we observe on the network we select the one that was hardest to generate. We also require that there is a common and high level of hardness to recording transactions in the ledger.
Mining is a process of generating proof of work. The way it works is that all miners compete to find a number that, when added to the block of transactions, causes this block to hash to a code with certain rare properties. Based on the cryptographic features of the hash function used in this process, finding such a rare number is hard, but verifying its validity when it is found is easy. For the ledger to be considered self-consistent we require that every block contain such rare number whose hardness we control based on the size of the participating network (a value expressed as a hash-rate).
This ensures that once we observe a valid state of the ledger, transactions that have certain age can not be negated, because producing a longer ledger than the one we see requires the malicious entity to have computing power that can compete with the entire existing network. Consequently, these transactions have lasting consensus regarding their validity.
One final note related to mining is to understand how the limited money supply comes into existence in the first place  how the currency is issued. Bitcoin protocol assigns a preset amount of newly issued currency for every block to the miner who first assembled it with the correct proof of work. This, along with transaction fees, incentivizes miners to perform the work that is required to create consensus for the entire network, keeping the network sufficiently large that circumventing it becomes very expensive. This is why mining is so called  because it can be paralleled to the process of ""digging"" for new bitcoins.
When presented with a new cryptocurrency solution, the reader is now empowered to ask herself, based on the above explanation, how it is that the solution achieves distributed trustless consensus. Similarly, when considering using a system based on POW, the reader is in position to understand that POW is only useful when trustless consensus is required, otherwise it is an expensive and needless waste of resources.
There now exist solutions which do not use blockchains, but still strive to achieve distributed trustless consensus, Iota being one example. Furthermore, there are so-called consortium blockchains, which apply some of the ideas developed in decentralized cryptofinance to facilitate transactions between mainstream financial entities. While it is not clear what exact approach they will use, it is clear from the above explanation that POW would be a feature entirely unnecessary in such systems, since they don't have to function in a trustless fashion.
With all the hype around cryptotechnology I hope that this article could be helpful in navigating the complexity and diversity of this space. If you have questions, comments or suggestions please feel free to join me on Slack at slack.coinfund.io.
the cashflow stories that matter.
2.6K 
9
Thanks to Jake Brukhman. 
2.6K claps
2.6K 
",116
https://medium.com/@blakeross/wealthfront-silicon-valley-tech-at-wall-street-prices-fdd2e5f54905?source=tag_archive---------1-----------------------,Wealthfront:Silicon Valley Tech at Wall Street Prices,"Somewhere in the Bay Area, a developer teeters on the edge of insanity.",Blake Ross,13,"Somewhere in the Bay Area, a developer teeters on the edge of insanity.
For the last 60 hours, he's been A/B testing one word against another (""Get"" or ""Buy!"" or ""Install""?), one screenshot against the next. Delirious, he starts to A/B test reality  is that penguin really making paninis?  but all will be worth it in the end if he can manage to realize the impossible American dream: Convince you to pay one dollar, one time to install his life's work on your iPhone.
Should he beat the odds, he'll then take your 70 cents and plow them into another app on his phone. This app will not only charge him every day for the rest of his life, but it also increased his fee last year by over a dollar a month.
What the hell is going on here?
That's long been the promise on Wealthfront's homepage: A $100,000 account for less than $20 a month.
Wealthfront loves to paint itself as the anti-Wall Street, but it exploits the same achilles heel as its Manhattan cousins: Many people don't have an intuitive grasp of the magic of compound interest, and so they certainly haven't internalized the tyranny of compound fees.
Then let us be clear: A 30-year old who invests $100,000 in his retirement with Wealthfront ""for less than a night at the movies"" will likely pay the company over $100,000 in fees by his 75th birthday.
If he sets aside additional money from his salary over that time period  say, $15,000 per year  the fees could come to half a million or more. Still less than a night at the movies, assuming you also had to produce the movie.
This is all a great deal for Wealthfront. If they hook enough young professionals early, the company gets to invest their money for the rest of their lives, skimming a larger and larger portion off the top as it compounds. Heck, I like this model enough that I consider investing in Wealthfront, Inc. at least twice a year.
But it's not such a good deal for you. There are other options available that would enable you to stop working years earlier, and that fact gets buried in the self-righteous rhetoric that continues to flow from the company. As with Wall Street firms, this rhetoric must keep flowing to paper over the same infallible truth that has bedeviled meatspace financial advisors for decades.
Here it is: If you open a retirement account, and you invest some of your paycheck each month into a Vanguard Target Retirement Fund, and you just...leave it...you just leave it right there until retirement...
...you don't do anything when the folks on CNBC announce that the sky is falling; you don't do anything when Cousin Eddy calls from a secure underground bunker in the badlands and says that the fed is printing money and it's time to liquidate and ammo up; you don't think it's a sign that your parrot said ""fuhgeddaboutit"" but you thought she said ""get a nugget"" and surely that must mean a gold nugget? and you looked online and noticed that the price of shiny yellow metal was crashing and wait your parrot is also yellow and I'll be damned if that isn't a sign to buy...
... no, if you just leave it there to compound over decades...
...then you will probably make more money than if you hired the guy from Edward Jones, and even that Senior Executive Double-Stuf Vice Managing Director from Goldman Sachs. You will probably make more money than if you outsourced your investments to the kindly rabbi who performed your bris because my gosh how nice is he for offering to do my finances and I'm really starting to see him as a sort of father figure. And you will probably make more money than if you used Wealthfront.
The financial industry has spent decades and billions persuading people of the opposite  that investing is difficult, that it requires sophisticated certifications and products, that you better not go it alone, that it takes a lot of time. It's not true.
Don't take my word for it; take Warren Buffett's.
Buffett recommends Vanguard because once you tune out the rhetoric from both coasts, one unassailable fact remains: As a non-profit that is owned by you and other shareholders, Vanguard is the only company in the financial industry that is not trying to make a huge profit off of you. Everyone else is talking their book. EVERYONE.
That's a pretty harsh reality for competitors, but it's nothing a little creative marketing can't mollify. Wealthfront's CEO took pleasure in mocking Betterment's spin today, but let's take a look at what Wealthfront itself is putting out there.
Of all the tall tales in Wealthfront's marketing, I imagine this is the one that spawned the most email threads marked ""A/C priv."" It's part of the company's new effort to convince you that its service is cheaper than a product that is already run at cost:
The cost of our Direct Indexing service is actually lower than the Vanguard ETF it replaces. Vanguard currently charges an annual 0.05% management fee for VTI. For $500,000 accounts, our equivalent fee for Direct Indexing is only 0.02%. And for accounts above $1 million, our fee is even lower at 0.014%.
That's because we do not charge a management fee for the roughly 80% of our Direct Indexing position that's comprised of individual stocks. The cost of that service (including all commissions) is included in our annual 0.25% advisory fee.
Here's even more good news: When it became illegal to sell foie gras in California, many generous restauranteurs decided to just give it away*!
(* But the cracker it was served on cost $20.)
Seriously, if someone at Wealthfront can justify the Hollywood accounting here with a straight face, I will breed them a panini penguin.
Because they do charge a management fee on your entire Direct Index balance. It says it right there: 0.25%. It appears that Wealthfront would prefer to call it an ""advisory fee"" here even though it relents with ""management fee"" on its homepage. Personally I like to call it a Safe Rides Fee. Whatever you call it, if you want to own the entirety of the public U.S. market, Wealthfront alone is going to take $2,500 out of your $1 million Direct Index to Vanguard VTI's $500 on year 1. Sixty years later, this ""next evolution of index investing"" ends up considerably more expensive than the last one.
On and on it goes:
By allowing a Wealthfront investor to hold the individual securities that comprise an index in her own account on a commission-free basis, Direct Indexing effectively eliminates any Index fund or ETF management fees on the associated position, which reduces overall portfolio cost.
and:
Owning an index fund or ETF comes with a fee. Although the fee may be small, it still acts as barrier to fully replicating the performance of a passive index investment.
and worst of all:
We believe our Direct Indexing service meaningfully addresses the two remaining shortcomings with modern index investing  the cost of the Index Fund and ETF management fees and the missed tax-savings from the inability to pass on tax losses. For this reason, we view Direct Indexing as the next evolution of index investing.
The divide between an ""index fund fee,"" which you pay to Vanguard to manage a collection of securities that track an index, and Wealthfront's fee, which you pay to Wealthfront to manage a collection of securities that track an index, is a distinction without a difference. And it's the oldest Wall Street trick in the book  the financial advisor who treats his esteemed clients to FREE! luxury box seats at the big game (*** when you pay him $10,000 a year).
Bottom line: Existing index funds trounce Wealthfront when it comes to the annual expense of owning the U.S. equity market. Please don't pee on our leg and tell us it's raining.
It's a wonder that with so many brilliant folks in Wall Street and in Silicon Valley, there is so much confusion over the definition of ""free.""
Schwab, for instance, thinks its new Wealthfront competitor is ""free"" even though you pay for the service by keeping a mandatory portion of your portfolio in a Schwab Haha-Interest-That's-A-Good-One Checking account.
Wealthfront also likes to take Schwab to task for commission-free ETFs that aren't, while touting its own ""commission-free"" service instead.
A quick recap:
Free (/fre/): The miniature spoonful of frozen yogurt you asked for at the checkout counter so you could ""test"" that crazy new flavor, Vanilla.
Not Free (/nat fre/): The bacon-wrapped date samples that are ""given"" to you once you show your $55-a-year membership card at the entrance to Costco.
Wealthfront trades are certifiably the latter. If they're going to play this game, I wish they would at least do it the Silicon Valley way  with creepily overtargeted ads.
To the extent the SEC allows, Wealthfront argues that the benefits of its automated tax loss harvesting service can outweigh its fees. In short, they argue that there's free money on the table.
This kind of claim crops up often in the investing world, so here's a flowchart I use whenever I'm presented with a golden opportunity:
First, let's get a few pesky facts out of the way:
Having said this, let me be clear: Wealthfront and I strongly agree that tax loss harvesting can improve aftertax returns. But as with so many Wall Street firms, and contrary to its own holier-than-thou marketing, Wealthfront wildly overstates the benefits. There is simply no evidence, nor any theoretical reason, to believe that a portfolio managed with Wealthfront will outperform a simple Vanguard portfolio bought and held for retirement, once you account for Wealthfront's fees.
The key gimmick that undermines Wealthfront's marketing paper is that it tallies (and later even compounds) the entirety of every potential harvestable tax loss as if it's all money in your pocket:
Tax Alpha is used to directly measure the tax benefit generated by proactively selling stocks with capital losses within a certain short period of time (say a single tax year)...We view Tax Alpha as an easy to compute and understandable metric to compute the performance of Direct Indexing.
Wealthfront claims that it ""could"" generate an ""incremental annual after-tax return of 2.46% over just owning the S&P 500."" This is an astonishing number. If software could reliably generate 2.5% of investing alpha at one-tenth the cost, I would move my entire portfolio to Wealthfront tomorrow. Heck, Harvard would probably move its endowment.
But again and again, Wealthfront tries without blinking to draw a straight line between tax alpha and cold hard cash. For example, they offer a chart titled ""After-tax Price Return of VTI vs. Direct Indexing"" that appears to show that if you had merely flipped on Direct Indexing in 2000, you would have earned ~2% compared to losing 9% with Vanguard's ancient index fund technology! They appear to reach these numbers simply by adding the maximum possible tax alpha to VTI's return.
All these cases neglect to mention that you will probably only see the maximal gain if you are maximally messing up already, by needlessly churning your account to generate capital gains. As Vanguard's founder advises: Don't just do something; stand there.
So where else might you find these gains that Wealthfront will magically offset? Well, if you look at the fine, fine print on that paper (yes, even finer than Betterment's): ""The net tax benefit over the period includes the liquidation of positions transferred in and sold to invest the client account in the Wealthfront portfolio.""
In other words, Wealthfront will now partially offset the gains that you were only ever forced to realize by switching to the service in the first place. Hooray!
There are plenty of other holes in Wealthfront's claims, which are well covered here (and still not substantively addressed by the company to date).
In short: When evaluating the benefits of tax loss harvesting, presumptively calculating and compounding anything beyond the government's capped annual income deduction is like saying that you just made $400 million because you decided not to buy a 747  okay, sure, but good luck trading that in for beer.
Calm down, capitalists: Wealthfront should absolutely charge as much as people are willing to fork over. In fact, I hope they convince some folks to pay a million dollars a year, so that more informed clients like you can get it for free. (That's how Wall Street works. It's why you're receiving 2% cash back on your credit card while your neighbor pays 12% on his. But it's also why your advisor has a yacht and you don't.)
I'm not asking why Wealthfront helps itself to such margins, which is obvious and perfectly normal, but rather why the market bears it. After all, it's hard to think of many other service industries that work like this. You would probably find it unreasonable if your piano teacher charged you more and more each week just because you have more money than you did when you first started rounding the Circle of Fifths.
Or, to use an example closer to home: TurboTax doesn't get to take a cut of every dollar you make until they put you in the ground, even though, like Wealthfront, they have a direct hand in growing your money. Navigating the Pease deduction cap, the Minimum Tax Foreign Tax Credit on Exclusion Items (""MTFTCE"" for ""short""), and the year-to-year tango in the rest of the U.S. tax code, is arguably far more intricate than passive index investing. But Intuit has to make do with fifty bucks a year  and even that is a miracle in this day and age. TurboTax doesn't even get a bonus for maximizing your refund, but they owe you if they don't!
When it comes time to market to those who work at Google and at Facebook and at Twitter, Wealthfront all but demands that we evaluate it as a technology company than a stuffy banking firm. Yet when we judge Wealthfront on its own terms, it looks virtually unprecedented.
It's not just that Wealthfront charges users for its software, which is rare. It's not just that Wealthfront charges users a recurring subscription fee, which is even rarer. It's also that, on average, Wealthfront increases its subscription fee every day.
And Wealthfront enjoys other invaluable perks over most consumer tech companies: It does all this automatically, without having to notify users or seek proactive permission (jealous, Amazon?). It does so without having to give Apple or Visa a large cut. The train rolls on even when the customer changes credit cards, or addresses. It doesn't have to send you a bill, or add a line item to your Amex statement, or otherwise ask you if it's cool to charge you $9 for a service that cost you $8 this time last year.
Wealthfront also boasts high switching costs. Sure, there's no account transfer fee  you can take your balls and go home at any time. But with Direct Indexing, you'll be walking away with thousands of individual stocks. Do you plan to manage all those by hand? Or liquidate and take the tax hit? Once you go robo, you never go back.
The market tolerates this pricing because Wealthfront has pulled the oldest trick on Wall Street: Cherry-picking the benchmark. They've anchored us all to something irrelevant, then blown it away before our eyes. How could anyone complain about a 0.25% fee when the Wall Street average is 1%?
But for all its bleating about Valley-style tech disruption, Wealthfront is still just milking the pricing precedent that the Street established decades ago:
The more money you earn, the more money we earn.
By the time we all knew Uber was better than taking cabs, it was already showing us why it's better than owning cars. But Wealthfront is still just telling us that they suck less than Bank of America. If you insist on wrapping yourself in the cuddly blanket of Silicon Valley, then you must also take the swing.
Here's what that means to me: The standout technology companies here take repetitive, mundane, ridiculous chores that no human should ever have to do  rebalancing retirement funds across 9,000 companies, or calculating the generation-skipping gift tax  and write code once to do them for us a million times over.
This technology helps wring out real, quantifiable savings; the code doesn't care if that net worth integer is 1,000 or 1,000,000. These cost benefits are so obvious and undeniable that the sales team doesn't even need tricks to close the deal; they just need to write down the truth.
Those savings are then passed on to the consumer, even as the founder walks off  yes, just like Wall Street  with a huge barrel of money. So much is created in the nuclear transition from unscalable human effort to unrelenting CPU that both the house and the gambler can win big.
This might surprise you, then: I'm certain that Wealthfront, or something like it, is the future. There is tremendous alpha to be realized through automation  if you've ever paid a bill one day earlier than you had to, or left a penny uninvested one day longer than required, then you know this must be true. It is inevitable that computers will optimize our cashflow, our investments, and our taxes to a degree that will make the status quo look downright laughable to our grandkids. The right solution in this space will mean people can retire earlier and spend more of their time doing what they love. I desperately want to see it exist so I can recommend it to my mother, and my brother  and use it myself.
But for now, Wealthfront is just another pretender to the throne, tilting at Schwab windmills and Fidelity bogeymen even as it tacitly joins them in guarding Wall Street's greatest secret: It doesn't have to be this way. You don't have to work harder and harder into your gray years, paying more and more of your paycheck to an advisor who is doing the same amount of work as the day you two shook hands.
So, to borrow a page from Wealthfront's chief:
We can do better than this. We have to be better than this. Stop charging proportional fees for advice.
The world doesn't need another Wall Street.
(If you like tech snark, you can also check out my post on Uber.)
",117
https://towardsdatascience.com/crypto-trading-bots-a-helpful-guide-for-beginners-60decb40e434?source=tag_archive---------3-----------------------,Crypto Trading Bots  A helpful guide for beginners [2020],Everything you need to know before you start your journey,Janny Kul,9,"Hey, I'm Janny.
I've been a trader at Citi & Merrill Lynch for 7 years and recently I started applying my algorithmic trading knowledge to the cryptocurrency space. I'm here to share what I've learnt in the hope that it helps you too.
Cryptocurrencies as an asset class are volatile, very volatile. Now, as a buy and hold (or hodl ) investor, volatility isn't what you want. Imagine the pain of losing 20% of your entire hard-earned money in one day or even worse 23% in 15 hours  ouch!
However, for a trader, volatility is great.
Being able to invest at lower prices when the market is having a tantrum and offloading risk when the market is in euphoria is literally the life-blood of a market maker at an Investment Bank.
If we could apply the same principles to create algorithms and then automate the whole set up, well that would be just fab now wouldn't it?
This is exactly what I've been working on for the past 1.5 years and I'm finally close to having something I can share with the world.
Enough about me though, let's jump right in.
Cryptocurrency trading bots are computer programs that automagically buy and sell various cryptocurrencies at the right time with the goal of generating a profit.
That's literally it.
It's important to note here that not every bot is profitable, in fact, most aren't.
It is trivial to make a working bot, less so to have a profitable one. @pdesgrippes
So,
Ideally the bots actually generate a profit and ideally that profit is greater in risk-adjusted terms than had you have just bought the same coins and held them throughout.
When I say risk-adjusted, what I mean is that your positive gains relative to the negative gains you've suffered whilst being invested is better. To demonstrate this have a think about the following,
Which would you rather have:
Hopefully you picked the first one.
See,
The first example is consistent. And when something is consistent it becomes a lot less risky.
In fact, you should still prefer the consistent returns of the first example even if the second one ended up at 1000% over the course of the year.
Hint, the key is in compounding: 1.01250  1 = 1103%. This is really outside the scope of this article though.
The point here is that given the option of consistent (strong) returns and a rollercoaster ride, you should almost always pick the consistent option even if the rollercoaster ride may land you with higher returns in future.
High returns isn't enough, you want high risk-adjusted returns.
And this is really what makes crypto trading bots such an interesting proposition. If we can find a way to capture most of the upside of cryptocurrencies yet without the regular gut-punches, this would make a much more attractive investment proposition than what the hodlers have to offer.
Most sophisticated trading bots work with 3 moving parts:
[signal generator] -> [risk allocation] -> [execution]
This is where we make predictions. There will be some data that goes into the signal generator and a buy or sell signal pops out of the other side.
If you see any bot's that use ""technical indicators"" then it's probably a good idea to try not to make eye contact and just back away slowly. 
This takes the buy or sell signal then decides how much to buy. As in, should we allocate our entire capital to this trade or just a portion? Should we buy all in one go or should we average in?
So now we know the direction, we know how much we want to buy or sell, next is the part that actually executes the trade.
See, if you have a lot to buy in one go (say you have to buy $10,000,000 in total for 500 clients) then you probably don't want to do this all in one trade as you're unlikely to get a favourable price.
You ideally want to dribble your order into the market.
If you have the exact same bot as 1000 other people and you're all running in disjoint instances (i.e. they don't communicate with each other) then this is really going to give you unfavourable pricing.
All three parts, signal, risk, and execution, need their own distinct algorithms and optimisation processes applied. If you have a bot that fudges through any of these parts or worse, disregards them altogether, it won't hold you in good stead for profitability.
You see, there are many benefits to running bots and it's all down to their skill set being vastly different to that of a human. Bots are consistent and monotonous. To be profitable you need consistency and quite frankly have to do everything thats highly counterintuitive to human nature.
Like, running towards the fire.
However it's worth bearing in mind, a bot, any bot, is only ever going to be as good as the human creating it. The old adage is true, garbage in, garbage out.
There are ways in which humans can outperform and that's mainly with subjective thinking.
When a particular piece of information doesn't have a specific outcome attached and lateral or second-degree thinking is needed to understand the implication then you're better off with a human.
I wouldn't really worry about this too much because whenever a bot reaches a state that's fairly subjective it can choose to not invest at all.
Without going too technical (I can cover this in another post if anyone wants) there are really only ever two types of algorithm used. They're disguised in different ways and called different things but this is really all there is to it.
Prices are up and we think they'll continue to go up = Buy (or vice versa). Statistically, most momentum strategies don't win particularly often but when they do win their gains are fairly large. Win:lose rates of around 55% with gain:loss of around 70% are fairly common with profitable algo's.
Prices are up but we think they're due a pull-back = Sell (or vice versa). Conversely, most mean reversion strategies win more often than they lose however the gain to loss ratio is smaller. Win:lose rates of around 70% with gain:loss of around 55% are also pretty common with profitable algo's.
Yup, it's that simple.
A commonly overlooked factor that greatly impacts your profitability is your fees.
See,
Your transaction costs (paid to the exchange) and trading costs (bid offer) can have a drastic effect on how much your bot makes.
Exchange fees can vary whether a bot is a liquidity taker (passive) or a liquidity maker (active).
The best algorithms will manage their active to passive trade ratio and also trade across multiple exchanges dynamically choosing an exchange based on the optimal transaction costs.
Bot's that only eke out a small statistical edge can have this completely swallowed up in fees if it's the aggressor on every trade.
Now, admittedly I haven't gone out and tested a bunch of bots yet however we can turn to our good friends over on reddit and other blogs to see what the consensus is from those that have used bots available on the market.
All commercial bots I've tried lost money compared to buy and hold, no matter what settings were tried.  intertron
If bots worked everyone would use them. This does not discount the private bots used by BlackRock and other massive trading firms. We will never have access to the data and teams of devs that they do.  vibrate
It is safe to say that the best bots are the ones you never hear about and will never be offered.  Kai Sedgwick via Coin Trellis
Its a good way to lose all your money....if people here don't know what they are doing in the market I wouldn't expect them to be able to [...] set [a bot] up so it is useful.  Person51389
This is unfortunately where the reality of the current trading bots (available to the public) kicks in.
Algorithmic trading can be extremely profitable.
However,
The problem really is that there's just a huge level of disconnect between the knowledge base of professional investors vs the hackers that build these sorts of algorithms and then make them available to the public.
The fact that so many public-facing forums still tout technical analysis as a viable investing tool gives you a small glimpse into the extent of the problem.
And don't even get me started about overfitting.
The lack of profitability of existing solutions is what motivated me to start creating a platform for the crypto community in the first place.
Before you buy/trial/invest in any bot, ask yourself these 3 questions:
Unfortunately, choosing a trading bot to go with isn't as trivial as answering these three questions.
In my opinion, everything ultimately comes down to people.
Forget about the marketing malarkey surrounding the company and ask yourself, would you trust this person with your money?
We're creating a ""set and forget"" crypto trading bot platform which is ideal for beginners (although applicable to experienced folk too) with the basic premise that our bots are already fully optimised to run out of the box.
We also take full responsibility for the profitability of our clients as opposed to leaving you out there all by yourself.
All you'll need to get started is:
There's an early bird offer on there too for the first members given we haven't launched yet.
credium.io
I always reply to comments so if you have questions just drop them below or email me: firstname at credium dot io.
Disclaimer. Financial markets are inherently risky. Unlike stocks which have intrinsic value and are correlated with the global economy, cryptocurrencies unfortunately aren't productive assets and they have no intrinsic value. Therefore you should be very conservative if you plan on getting involved with cryptocurrencies; the risks are pretty huge. The entire space could in theory be made illegal by governments which would cause cryptocurrencies to lose a lot of their value. This post isn't intended as investment advice so please do your own research before investing in anything.
",118
https://medium.com/@dan.jeffries/why-people-still-dont-get-cryptocurrency-2c0607857c63?source=tag_archive---------6-----------------------,Why People Still Don't Get Cryptocurrency,"After more than a decade of blistering growth, most people still don't get cryptocurrency.",Daniel Jeffries,21,"After more than a decade of blistering growth, most people still don't get cryptocurrency.
Bitcoin is the best performing asset of the last decade by a huge margin and yet everyone from the mainstream media, to most nation states, to prominent traders, struggle to see what it means for the future. India is threatening to ban cryptocurrencies in move that will prove disastrously foolish if they really do it. Prominent gold bug, Peter Schiff, continues to watch the price of gold sputter while Bitcoin races by him and makes his son rich instead.
Over the past ten years, journalists and talking heads everywhere have declared Bitcoin dead over 400 times and it just keeps proving them wrong. It's a Ponzi scheme. Fatally flawed. A grift. Not real money. Worthless. All of that as Bitcoin has surged from pennies to S60,000 a coin.
Even as artists rake in record value for their art with NFTs, it's dismissed as nothing but a passing fad that will never take hold. That's despite big franchises like the NBA adopting NFTs.
If you're a fan of cryptocurrency, it's easy to fall into the trap of thinking all these folks are just idiots who refuse to see something obvious but that would be a tremendous mistake. Many of these folks are incredibly smart and successful. So why can't they see the value of Bitcoin and cryptocurrency? How are they missing something that's continued to defy expectations for more than a decade?
To understand why, we just need to take a little dive into the history of money and how human beings form beliefs.
Let's get one thing out of the way first:
Tomorrow's children won't even remember paper and metal money.
The generations that come after Gen Z will grow up with programmable, electronic money from the moment they're born and the only place they'll encounter paper and metal money will be in history class.
The reason is simple. Today's money is analog. It's dumb money. It's made of dead trees and metal. It's inert. Even the electronic version of it you zip around on Paypal and via wire transfers is just an electronic record in a database. It's not really electronic and programmable. If you put a dollar bill on the table, you can't make it execute a smart contract. All you can do is save it or exchange it for goods and services.
Don't get me wrong, paper and metal money are amazing pieces of technology. You probably don't think of them as technology but that's exactly what they are. It required minting coins in a way that were hard to copy so that people couldn't make counterfit money out of thin air, which is a simpler form of the advanced printing methods we use today in federal reserves on highly specialized machines. That's analog tech.
Metal and paper money concentrated value into a single universal form that everyone agreed to and it swiftly replaced bartering, which was highly inefficient. It saved people from having to figure out how many cows to trade for grain.
Exactly how many cows are worth how much grain? Bartering was inexact and hard to deal with because the guy you traded the cows to needed a place to store those cows. Now he was rich in cows, but what if someone didn't want cows? How would he buy stones to build a house if the stone mason didn't need more cows?
Metal and paper money solved the barters' dilemma. Now the cow owner could sell them for coins and trade those coins directly for whatever he wanted. The grain salesperson could trade the grain for money and go get that stone to build a new house without needing to form a bilateral trade negotiation between the cow guy and the stone guy.
Analog money was a fantastic, universal value storage technology. Just look at the kind of coin and you know how much it's worth because everyone else agrees to accept that coin too. It changed the world and the way we run our civilizations.
But the time of analog money is passing and passing fast.
Digital technology is infinitely more flexible and it will let us take money in a thousand new directions we're only beginning to imagine now.
Tomorrow's money is smart money.
Just as digital cameras have replaced their analog counterparts, so too will digital, programmable money replace plain old paper money. Kodak once owned 90% of the market for film and stood for a hundred years but it didn't see digital cameras coming in time and it chose to see them as a flash in the pan. For a time they were right.
Digital cameras were slow and clunky. They couldn't achieve real time shutter speed. They didn't have the fidelity of traditional film.
Right up until engineers solved all those problems.
And within five years Kodak went from 100 years of dominance to bankrupt. That's how fast innovation can crush those who stay willfully blind to the future.
But if you're looking down on Kodak for missing something so obvious, take a look at your own predictions and I'm betting you didn't see most of today's innovations coming. When I started working in the Internet in the late 90s everyone told me I was insane and the Internet was a waste of time. It was only for nerds and fools. It was slow and clunky and ugly. They said the same when I went to work for a Linux company. Now the Internet is as ubiquitous as air and Linux powers every single device that matters to you in the world.
The thing is most folks just aren't wired to see the future. They're firmly fixated on the now. It makes sense from an evolutionary perspective. We spent a few million years running around in the forest, hunting and gathering. A focus on finding food and not getting eaten by tigers made a lot of sense if we wanted to survive. Thinking about tomorrow's tigers didn't help anyone. It was the province of the Shaman and medicine men of the tribe to look to the future and imagine where it was going but the rest of the tribe was firmly focused on not dying a horrible death.
We only started living in cities about 8,000-10,000 years ago. Civilization literally means ""living in cities."" That's not a hell of a lot of time for our brains to develop to a new way of seeing the world. And almost none of us did. Most people haven't adapted to seeing beyond the here and now.
I've spent my whole life gazing into the misty haze of the future and over time I've come to realize one critical difference between how I see the world and how almost everyone else sees it. I look at broad patterns but most folks are zoomed in, looking at much shorter patterns. I look at abstractions of abstractions over decades, hundreds of years and even millennia. Because people are so zoomed in, they tend to see iterations of technology while I'm looking at categories of technology. Home video recording is a category. Betamax, VHS, DVD, Bluray and streaming are all iterations of that category.
The problem is most people mistake the current iteration of the technology for the category.
That's a massive mistake that makes it almost impossible to see the future clearly.
People look at something like Betamax and they project its current characteristics indefinitely into the future. They see something that's ugly and grainy and that can't record two hours so it can't record a whole movie. They extrapolate those limitations and limited characteristics to the technology as a whole and believe it will never work.
What they don't realize is that engineers are always chipping away at those problems and that it's only a matter of time before they get solved.
The same problem is happening today with digital money. People look at the current iterations of cryptocurrency and they see big energy waste, slow transaction times, volatile prices and they imagine those problems will go on forever. It leads to utterly short sighted backlashes against the technology and fear driven narratives. But the same things are already happening in crypto as happen in every other technology. The early iterations are starting to give way to new iterations.
Take something like Crypto Kitties. It pushed the limits of the Ethereum blockchain to the limits a few years ago. It was a collector game that let people breed and trade cute digital kittens and it really took off. But it ran up against the limits of Ethereum's sluggish transactions and limited scale. Most people dismissed it but they mistook the iteration for the category.
The category is ""Collectibles."" The global collectibles market, which is everything from rare toys to signed sports cards, is roughly $370 billion dollars.
The Crypto Kitties creators realized they needed to go beyond Ethereum. They needed a new kind of consensus and faster, smarter blockchain, so they built Flow. That's already a sign of engineers learning from the past and at a much faster pace. Flow has near instantaneous settlement, rapid scaling, and upgradeable smart contracts. Upgradeable smart contracts are a must if we want the ecosystem to really evolve over time. If a smart contract has a bug, it needs a way to get patched and patched fast before it become immutable.
The Flow blockchain now boasts big businesses running on top of it, like the NBA, who knows a little something about the collectibles market, because sports trading cards have dominated that arena for decades.
Artist Ben Mauro, who's art I've loved for years, recently sold his first series of trading cards on Flow for $2 million dollars, a life changing amount of money for an artist. Ben was a successful artists by any standard before that NFT sale. He's worked on movies like Elysium and games like Call of Duty Black Ops. But while those companies made 100s millions off his work, he just made a living. NFTs and by extension, blockchains and cryptocurrency, hold the power flip the script and let artists profit directly from their own work.
I admit to being a little skeptical of NFTs until now. I made the same mistake as everyone else. Sometimes I still revert to my ""lizard brain"" which is focused on the immediate too. I saw potential long term promise in the tech but I couldn't quite see how it would work in the long run. In other words, I was looking at the iteration of the technology and not the category.
But the blinders are off.
Look into the future just a little bit and it's not hard to see digitally unlockable cards that only you own, real ownership contracts that hold up in a court of law and subscriptions to artists that unlock special rewards like super discounts on upcoming cards and collectibles and systems that keep paying artists no matter how many copies of a jpeg are made.
All that and more is coming in future iterations of digital collectibles and it's just the start of a wider revolution in money.
Nobody likes a middle man except the middle man.
We all hate scalpers because we can't get a PS5 for a reasonable price as bots buy up the stock in seconds so scalpers can flip it on Ebay for double or triple the price. The scalpers love it but nobody else loves it.
Banks are middle men too. So are governments.
The biggest problem with today's dumb money is we have to trust centralized middle men to keep that money and move it around world. But tomorrow's children won't need as many of those middle men. Today, when you buy a house, your money goes into escrow, a holding account by a central authority. Once the contract gets signed by both parties and enough time passes the money gets released to the seller.
Tomorrow we won't need an escrow intermediary. A small program will simply self-escrow the money and it will release when both sides digitally sign the contract. That's what the DeFi craze is all about. Tomorrow you'll have an easy way to loan money directly to other people you've never seen or met and earn interest on it instead of the banks. And you'll borrow money that way too. You won't go to a bank and beg them to give you money, you'll get it from a pool of small, crowdsourced investors, as those investors earn real interest on their money instead of the measly interest they get today in their savings account in a bank.
But that's tomorrow. Today is different.
Today paper and metal nation-state issued money dominates the world.
Even though Bitcoin surged to over a trillion dollars in market cap over the last few years, it's still just a drop in the bucket. You can forgive people for not seeing its true potential. There are single companies bigger than Bitcoin's market cap, like Apple, who's market cap is roughly double the worth of all Bitcoin in existence. They're not the only company bigger than Bitcoin. Many of the world's oil companies, not to mention brick and mortar behemoth Walmart, beat the world's biggest crypto by comparison.
Check out this chart below and you'll see that global property and derivatives dwarf cryptocurrency too, as does the M2 money supply, which includes all the money in the world in terms of currency, savings and deposits, which currently stands at 95 trillion dollars.
But in the coming decade those numbers will start to flip because the M2 money supply will be nothing but electronic money, a mixture of private cryptocurrency and central bank digital currencies. Cash will cease to exist. It's already happening across the world, in cutting edge tech countries like China, where even the homeless get paid with QR codes.
As M2 increasingly becomes synonymous with crypto, the world will look very different from the world today. It will be filled with micro-transactions for everything from your favorite web pages, to newsletters, to buying bread at the local sourdough bakery. You'll love the bakery so much that you'll buy a fractional share in it on a peer to peer exchange and you'll add it to an ever shifting portfolio of assets that deliver unprecedented passive income to regular people everywhere. You won't use a card, you'll use a QR code, cutting out the VISA and MasterCard machine driven middle man.
Make no mistake, a massive sea change is coming to money. And that's one of the biggest reasons people still don't get crypto.
Tomorrow's world looks almost nothing like today and that makes it so much harder to see.
To understand why you just need to understand a little about Rum.
The biggest reason that people can't see programmable money coming down the pike is because they've never seen a different kind of money in their entire lives.
Money has always been the same. It's issued by nation states. That's what they understand.
Decentralized, programmable money, issued by nobody but the protocol itself, is a completely different pattern. It's a millennia spanning pattern in money, one that transcends the current human life space. It's the shift from one kind of money to another. It's happened multiple times in the past but not in the lifetimes of anyone alive today.
And for most people, seeing beyond the scope of their own life is next to impossible.
The idea that you could have private money or money that doesn't need a central authority to issue it is totally outside their understanding. It's as if an alien ship landed with radically different starship and weapons that don't obey the laws of physics as we know them. They're shooting lasers that don't go straight, but that home in on people, bending around corners at will.
Truly long term patterns are incredibly difficult for people to process and understand. We're short term pattern matching masters. We abstract away a short term pattern and that's how we learn.
Usually that works really well for us. It keeps us focused on the immediate threats and the joys of life in the here and now. It limits the amount of brain activity to understand something we've never seen before now. If we can match that new thing to a pattern we've already seen we don't need to waste a lot of energy trying to understand it. Thinking is hard and it requires a lot of energy. If we didn't have shortcuts to thinking we'd crumple on the ground in a ball and have to eat around the clock like Gorillas, who spend half their waking hours chewing raw food.
When we get cut by a knife just one time, we abstract the concept of sharpness. We don't need to get cut by a sharp rock to know that will hurt us too. We don't need to get pricked by a needle to know we should avoid it. We already know it because we're abstraction machines.
But that short term pattern matching can betray us badly. When something truly new comes along, our higher brain needs to kick in and do some heavy processing to understand it. Most people don't bother to do the work. If it's not an immediate threat or benefit to their lives than it doesn't really matter all that much to them so they rely on their lizard brain, which is their lower brain. That's the part of the brain where our quick firing heuristics and pattern matching work. When we cross the street because someone is weaving and shouting wildly, we don't need to do a lot of processing. Our lizard brain is at work and that pattern of ""dangerous and unstable person spotting"" is usually good enough to keep us alive.
The pattern that everyone alive today has seen over their entire lifespan is ""nations states issue dumb money"" so it's hard to see beyond it. In the modern world the nation state reigns supreme. Everyone lives under the banner of a clear and stable border with a flag and a patriotic song.
But as Yuval Harari writes in Lessons for the 21st Century, this wasn't the case for most of history. Borders were fluid and likely to change overnight as one hideous conqueror got slaughtered by another one who took over and moved the chains. Kingdoms and borders often shifted multiple times in the course of a person's natural life span. One day you could wake up under the jurisdiction of one tribal power only to find yourself under the power of an expanding empire the next.
If you lived during the horrific Waring States period of Japan, your rulers changed dozens of times as powerful warlords fought for territory through a brutal 125 years of civil war, more than four generations of entire human lives.
Money was a lot different in those days too. It was often more decentralized because you couldn't trust the people in power to last. People mostly used commodities as trading instruments, like salt, or silk, or spirits.
A quick trip back through the history of money is all you need to understand that money is not a fixed idea. It's ever evolving. It's been decentralized and issued by central authorities. It's been powered by the people or powered by the might of empires. Here are just a few things that have acted as money in the course of human history:
The intrepid traders braving the ancient Silk Roads to bring goods from near and far, used silk because it was a highly concentrated form of wealth. The idea that money is intrinsic to the medium that houses the store of value is just not true though. If people feel something is valuable, it is valuable. It's a simple as that. Silk may have been rare, and hard to make, but it has no intrinsic value other than people thought it was beautiful and precious.
Gold bugs will tell you that gold is the only real money and that it has utility value. There is no value in gold except we say there's value in it. It was shiny and pretty and people gravitated to shiny and pretty long before it ever got used a passable semiconductor and before semiconductors even existed.
We saw gold as money because it was semi-rare and it looked good around a princesses' neck when turned into jewelry. Its utility value is practically nil. As precious metals go, it's not particularly hard or much of a conductor versus other precious metals. Diamond and graphene outpace it by an order of magnitude when it comes to strength. So does carbon fiber.
If you want real utility, look at something like Lithium, which is used in electronics and batteries and medicine but we don't use that for money.
Gold has value because we say it has value. It's a simple as that and that's the truth of every money that's ever existed.
Anything that can act as a store of value can act as money. Read The History of the World in Six Glasses and you'll see why Rum was used as money and dominated the horrific and disgusting historical slave trade in Africa and the Americas.
Rum was a super concentrated form of wealth. Water usually carried disease for the vast majority of human history so everyone from the very young to the very old drank beer, wine or spirits. It was usually watered down so they weren't hammered all day or just got a little buzzed. Having a highly concentrated alcohol meant you could dilute it for a much longer time, which made it extremely long lasting. In other words, just like drugs today, you could cut it, which meant you could make it last longer and string out the divisibility of its value.
American slave traders paid African slave traders in their favorite form of concentrated wealth, Rum, because it was long lasting, cheap to produce, and acted as a store of value like nothing else at the time. In fact, it wasn't the US dollar that dominated international trade in the early years of the American rise to power.
It was Rum.
There's one last reason people can't see the rise of programmable money coming.
Fear.
New things are terrifying to people. They're disruptive. They don't want to change. Change could mean they're now on the wrong side of the winning equation. People don't want to be left behind.
Banks see crypto as an existential threat and they should. Banks will not go away but they'll need to radically adapt their business models or die a fiery death. Most won't make it. They'll stay deliberately blind, beholden to their current business model. Many will go the way of Kodak. A few will be smart enough to survive though, pinning their hopes on tomorrow instead of today. Others will pile up patents and try to sue their way to profitability, but that's not a business model and they'll die too and they should because patent trolls are parasites on the modern world.
Governments fear new money most of all.
The nation state masters of the world today own the power of the Gods: The exclusive right to mint and distribute money. That makes them the masters of economics and international trade and it gives them dominion over everyone's lives. Control the money and you control the world.
They don't want to give that power up and they won't do it easily.
Authoritarian and authoritarian leaning countries are even more worried. They're worried about crypto's ability to disrupt their power and absolute control over the minds of their citizens.
Good.
Authoritarian governments are never for people. They're governments that seized power using the name and rhetoric of the people, while choking the life out of people with their hideous, selfish, insane policies. Crypto that they don't control can allow their people to bust out of that bondage. Imagine if a truly scalable, widely accepted international cryptocurrency existed, that held its value steadily, a stablecoin of near perfect balance that they could spend anywhere. In Venezuela, as insane authoritarians came to power and dramatically destabilized markets and money itself, giving way to hyperinflation, the people could have simply switched their money to a stable, international money instead and waited out the regime or escaped its brutal power when they had a change to slip over the border.
Today people are largely trapped when insane people take over. Tomorrow they'll have a shot at escaping the insanity if they can take their money and go.
Authoritarian attacks on crypto will blow back on them. They'll ban it like India is trying to do now. That play book has worked for them for centuries but it won't work out well for them now. What they hope is they'll be able to exploit the power of blockchains by banning private crypto, but they won't be able to do it because they'll have no one in the country capable of developing it or understanding it.
You see, a blockchain is nothing without the coin to power it. The coin makes it go. Without the coin it's just a distributed database and a shitty distributed database at that. If people have no incentive tied in the coin they won't work with the technology and it will be nothing but an inert technology like any other.
As the years go by and other countries embrace blockchains and private currency, they'll soak up all the talent while authoritarian governments will have nothing but also-ran talent struggling to build working blockchain tech that nobody wants or trusts.
We only have one planet and the power of individual nation states has reached its zenith.
Just as the power of the Samurai eventually fell away to a civil government, so too will the modern form of the nation state that rules over all our lives now change in the coming centuries. Their power won't disappear over night or even in our lifetimes, but it will slowly and irrevocably decline and give way to a more international order of independent territories, whether the populists want it to or not. The populists are on the wrong side of history, as they always are and their rage and fear can't hold back the future.
Nobody can stand against the tide of time.
But maybe you're still struggling to understand crypto? Well, you really just need to understand one thing:
Crytpo is smart money.
It's programmable. Changeable. Intelligent.
Today's money is stupid. Tomorrow's kids will not understand stupid money.
What do you mean I can't send it across the world instantly? What do you mean I can't self-escrow the money when I buy something and only release it when I get the thing?
Today's money will be like looking at an old flip phone that can't take pictures, chat, text and browse the web. What's the point?
Crypto is programmable money. We can bend it, warp it, shift it, and make it dance. More than anything that's the real draw of crypto.
All the various technological hurdles will get solved with time. Scaling? Ease of use? Private and public transactions? Governance? All of it will get solved in the next decade or two. All of it is getting solved right now, as new and better consensus systems rise and developers learn from the blockchains of yesterday.
Today we see the same three tired, boring stories in the mainstream media:
Those narratives are dying off.
The energy debate is still there but it will disappear as we switch to proof of stake and other consensus mechanisms.
It's used for bad stuff is mostly dead except for when the government wants to jam more regulation down our throats at the last second. Giant hedge funds and institutions like Tesla and Twitter are already buying. Coinbase is going public and valued at 68-100 billion dollars.
Crypto is increasingly mainstream.
The very idea of money is changing too. It used to be that only governments could do it but a look back to history tells us how foolish that view really is and it will fall away too.
The only constant in life is change. And all our yesterdays have lighted fools the way to dusty death.
The foolish narratives of the short sighted are dying. Slowly but surely they're dying.
In twenty five years most folks won't even remember dumb money printed on paper and minted as coins. There won't be any stories about cryptocurrency versus ""real"" money.
People won't even know the word cryptocurrency.
They'll just call it money.
###########################################
I'm an author, engineer, pro-blogger, podcaster, public speaker. My upcoming book, Mastering Depression and Living the Life You Were Meant to Live tells the story of how I battled the dark forces of existence and still found a way to live a big, bold and beautiful life.
###########################################
Early links to every article, podcast and private talk. You read it and hear first before anyone else!
A monthly virtual meet up and Q&A with me. Ask me anything and I'll answer.
###########################################
",119
https://medium.com/@matt_11659/matt-barrie-australias-economy-is-a-house-of-cards-6877adb3fb2f?source=tag_archive---------2-----------------------,Australia's Economy is a House of Cards,Co-authored with Craig Tindale.,Matt Barrie,64,"Co-authored with Craig Tindale.
I recently watched the federal treasurer, Scott Morrison, proudly proclaim that Australia was in ""surprisingly good shape"". Indeed, Australia has just snatched the world record from the Netherlands, achieving its 104th quarter of growth without a recession, making this achievement the longest streak for any OECD country since 1970.
Australian GDP growth has been trending down for over forty yearsSource: Trading Economics, ABS
I was pretty shocked at the complacency, because after twenty six years of economic expansion, the country has very little to show for it.
For over a quarter of a century our economy mostly grew because of dumb luck. Luck because our country is relatively large and abundant in natural resources, resources that have been in huge demand from a close neighbour.
That neighbour is China.
Out of all OECD nations, Australia is the most dependent on China by a huge margin, according to the IMF. Over one third of all merchandise exports from this country go to China- where 'merchandise exports' includes all physical products, including the things we dig out of the ground.
Source: Austrade, IMF Director of Trade Statistics
Outside of the OECD, Australia ranks just after the Democratic Republic of the Congo, Gambia and the Lao People's Democratic Republic and just before the Central African Republic, Iran and Liberia. Does anything sound a bit funny about that?
Source: Austrade, IMF Director of Trade Statistics
As a whole, the Australian economy has grown through a property bubble inflating on top of a mining bubble, built on top of a commodities bubble, driven by a China bubble.
Unfortunately for Australia, that ""lucky"" free ride is just about to end.
Societe Generale's China economist Wei Yao said recently, ""Chinese banks are looking down the barrel of a staggering $1.7 trillion  worth of losses"". Hyaman Capital's Kyle Bass calls China a ""$34 trillion experiment"" which is ""exploding"", where Chinese bank losses ""could exceed 400% of the U.S. banking losses incurred during the subprime crisis"".
A hard landing for China is a catastrophic landing for Australia, with horrific consequences to this country's delusions of economic grandeur.
Delusions which are all unfolding right now as this quadruple leveraged bubble unwinds. What makes this especially dangerous is that it is unwinding in what increasingly looks like a global recession- perhaps even depression, in an environment where the U.S. Federal Reserve (1.25%), Bank of Canada (1.0%) and Bank of England (0.25%) interest rates are pretty much zero, and the European Central Bank (0.0%), Bank of Japan (-0.10%), and Central Banks of Sweden (-0.50%) and Switzerland (-0.75%) are at zero or negative interest rates.
Summary of Current Interest Rates from Central Banks (16th October 2017). Source: Global-rates.com
As a quick refresher of how we got here, after the Global Financial Crisis, and consequent recession hit in 2007 thanks to delinquencies on subprime mortgages, the U.S. Federal Reserve began cutting the short-term interest rate, known as the 'Federal Funds Rate' (or the rate at which depository institutions trade balances held at Federal Reserve Banks with each other overnight), from 5.25% to 0%, the lowest rate in history.
When that didn't work to curb rising unemployment and stop growth stagnating, central banks across the globe started printing money which they used to buy up financial securities in an effort to drive up prices. This process was called ""quantitative easing"" (""QE""), to confuse the average person in the street into thinking it wasn't anything more than conjuring trillions of dollars out of thin air and using that money to buy things in an effort to drive their prices up.
Systematic buying of treasuries and mortgage bonds by central banks caused the face value of on those bonds to increase, and since bond yields fall as their prices rise, this buying had the effect of also driving long-term interest rates down to near zero.
Both short and long term rates were driven to near zero by interest rate policy and QE. Source: Bloomberg, CME Group
In theory making money cheap to borrow stimulates investment in the economy; it encourages households and companies to borrow, employ more people and spend more money. An alternative theory for QE is that it encourages buying hard assets by making people freak out that the value of the currency they are holding is being counterfeited into oblivion.
In reality, the ability to borrow cheap money was mainly used by companies to buy back their own shares, and combined with QE being used to buy stock index funds (otherwise known as exchange traded funds or ""ETFs""), this propelled stock markets to hit record high after record high even though this wasn't justified the underlying corporate performance.
Almost all flows into the equity market have been in the form of buybacks. Source: BofA Merrill Lynch Global Investment Strategy, S&P Global, EPFR Global, Convexity Maven
In literally a ""WTF Chart of the Day"" on September 11, 2017, it was reported that the central bank of Japan now holds 75% of all ETFs. No, not 'owns units in three out of four ETFs'  the Bank of Japan now owns three quarters of all assets by market value in all Japanese exchange traded funds.
In today's world Hugo Chavez wouldn't need to nationalise assets, he could have just printed money and bought them on the open market.
Bank of Japan now owns 75% of all Japanese ETFs. Source: Zerohedge
Europe and Asia were dragged into the crisis, as major European and Asian banks were found holding billions in toxic debt linked to U.S. subprime mortgages (more than 1 million U.S. homeowners faced foreclosure). One by one, nations began entering recession and repeated attempts to slash interest rates by central banks, along with bailouts of the banks and various stimulus packages could not stymie the unfolding crisis. After several failed attempts at instituting austerity measures across a number of European nations with mounting public debt, the European Central Bank began its own QE program that continues today and should remain in place well into 2018.
In China, QE was used to buy government bonds which were used to finance infrastructure projects such as overpriced apartment blocks, the construction of which has underpinned China's ""miracle"" economy. Since nobody in China could actually afford these apartments, QE was lent to local government agencies to buy these empty flats. Of course this then led to a tsunami of Chinese hot money fleeing the country and blowing real estate bubbles from Vancouver to Auckland as it sought more affordable property in cities whose air, food and water didn't kill you.
QE was only intended as a temporary emergency measure, but now a decade into printing and the central banks of the United States, Europe, Japan and China have now collectively purchased over US$19 trillion of assets. Despite the lowest interest rates in 5,000 years, the global economic growth in response to this money printing has continued to be anaemic. Instead, this stimulus has served to blow asset bubbles everywhere.
Total assets held by major central banks. Source: Haver Analytics, Yardeni Research
This money printing has lasted so long that the US economic cycle is imminently due for another downturn- the average length of each economic cycle in the U.S. is roughly 6 years. By the time the next crisis hits, there will be very few levers left for central banks to pull without getting into some really funny business.
It wasn't until September 2017 that the U.S. Federal Reserve finally announced an end to the current program, with a plan to begin selling-off and reducing its own US$4.5 trillion portfolio beginning in October 2017.
How these central banks plan to sell these US$19 trillion in assets someday without completely blowing up the world economy is anyone's guess. That's about the same in value as trying to sell every single share in every single company listed on the stock markets of Australia, London, Shanghai, New Zealand, Hong Kong, Germany, Japan and Singapore. I would think a primary school student would be able to tell you that this is all going to end up going horribly wrong.
To put into perspective how perverted things are right now, in September 2017, Austria issued a 100 year euro denominated bond which yields a pathetic 2.1% per annum. That's for one hundred years. The buyers of these bonds, who, on the balance of probability, were most likely in high school or university during the global financial crisis, think that earning a miniscule 2.1% per annum every year over 100 years is a better investment than well anything else that they could invest in- stocks, real estate, you name it, for one hundred years. They are also betting that inflation won't be higher than 2.1% on average for one hundred years, because otherwise they would lose money. This is even though in 20 years time they'll be holding a bond with 80 years left to go to be paid out in a currency that may no longer exist. The only way the value of these bonds will go up is if the world continues to fall apart, causing the European Central Bank to cut its interest rate further and keep it lower for 100 years. Since the ECB refinancing rate is currently zero percent, that would mean that if you wanted to borrow money from the European Central Bank, it would literally have to pay you for the pleasure of borrowing money from it. The other important thing to remember is that on maturity, everyone that bought that bond in September will be dead.
So if one naively were looking at markets, particularly the commodity and resource driven markets that traditionally drive the Australian economy, you might well have been tricked into thinking that the world was back in good times again as many have rallied over the last year or so.
The initial rally in commodities at the beginning of 2016 was caused by a bet that more economic stimulus and industrial reform in China would lead to a spike in demand for commodities used in construction. That bet rapidly turned into full blown mania as Chinese investors, starved of opportunity and restricted by government clamp downs in equities, piled into commodities markets.
This saw, in April of 2016, enough cotton trading in a single day to make a pair of jeans for everyone on the planet, and enough soybeans for 56 billion servings of tofu, according to Bloomberg in a report entitled ""The World's Most Extreme Speculative Mania Unravels in China"".
Market turnover on the three Chinese exchanges jumped from a daily average of about $78 billion in February to a peak of $261 billion on April 22, 2016  exceeding the GDP of Ireland. By comparison, Nasdaq's daily turnover peaked in early 2000 at $150 billion.
While volume exploded, open interest didn't. New contracts were not being created, volume instead was churning as the hot potato passed between speculators, most commonly in the night session, as consumers traded after work. So much so that sometimes analysts wondered whether the price of iron ore is set by the market tensions between iron ore miners and steel producers, or by Chinese taxi drivers trading on apps.
Average futures contract holding times for various commodities. Source: Bloomberg
In April 2016, the average holding period for steel rebar and iron ore contracts was less than 3 hours. The Chief Executive of the London Metal Exchange, said ""Why should steel rebar be one of the world's most actively-traded futures contracts? I don't think most people who trade it know what it is"".
Steel, of course, is made from iron ore, Australia's biggest export, and frequently the country's main driver of a trade surplus and GDP growth.
Australia is the largest exporter of iron ore in the world, with a 29% global share in 2015-16 and 786Mt exported, and at $48 billion we're responsible for over half of all global iron ore exports by value. Around 81% of our iron ore exports go to China.
Unfortunately, in 2017, China isn't as desperate anymore for iron ore, where close to 50% of Chinese steel demand comes from property development, which is under stress as house prices temper and credit tightens.
In May 2017, stockpiles at Chinese ports were at an all time high, with enough to build 13,000 Eiffel Towers. Last January, China pledged ""supply-side reforms"" for its steel and coal sectors to reduce excessive production capacity. In 2016, capacity was cut by 6 percent for steel and and 8 percent for coal.
In the first half of 2017 alone, a further 120 million tonnes of low-grade steel capacity was ordered to close because of pollution. This represents 11 percent of the country's steel capacity and 15 percent of annual output. While this will more heavily impact Chinese-mined ore than generally higher-grade Australian ore, Chinese demand for iron ore is nevertheless waning.
Over the last six years, the price of iron ore has fallen 60%.
Iron ore fines 62% Fe CFR Futures. Source: Investing.com
While the price of iron ore briefly rallied after the U.S. election in anticipation of increasingly less likely Trumponomics, DBS Bank expects that global demand for steel will remain stagnant for at least the next 10-15 years. The bank forecasts that prices are likely to be rangebound based on estimates that Chinese steel demand and production have peaked and are declining, that there are no economies to buffer this slowdown in China, and that major steel consuming industries are also facing overcapacity issues or are expected to see lower growth.
Australia's second biggest export is coal, being the largest exporter in the world supplying about 38% of the world's demand. Production has been on a tear, with exports increasing from 261Mt in 2008 to 388Mt in 2016.
Australian Coal Exports by Type 1990-2035 (IEA Core Scenario). Source: International Energy Agency, Minerals Council of Australia
While exports increased by 49% over that time period, the value of those exports has collapsed 38%, from $54.7 billion to $34 billion.
The only bright side for Australian coal in 2017 was that, unexpectedly, Cyclone Debbie wiped out several railroads and forced the closure of ports and mining operations, which has caused a temporary spike in coal prices.
Australian Thermal Coal Prices. (12,000- btu/pound, <1% sulfur, 14% ash, FOB Newcastle/Port Kembla, US$ / metric ton). Source: IMF, Quandl
Australian Premium Coking Coal FOB $/tonne. Source: Mining.com
There are two main types of coal- thermal coal, which is burnt as fuel, and coking coal, which is used in the manufacture of steel. The prospects for coking coal are obviously tied to the prospects of the steel market, which are not particularly good.
Thermal coal, on the other hand, is substantially on the nose, and while usage is still climbing in non-OECD nations, it is already in terminal decline in OECD nations. Recently, in April 2017, the United Kingdom experienced its first day without burning coal for electricity since the industrial revolution in the 1800s.
World Coal Consumption by Region 1980-2040 (forecast). Source: US Energy Information Administration
Australia's main export markets for coal are Japan and China, two markets in which the use of coal is forecast to decline through 2040.
Australia's top export market for coal is Japan, and the unfortunate news is that the ramp up in coal exports here is a short lived adaptation after power companies idled their nuclear reactors in the wake of the Fukushima disaster. Between a zombie economy and fertility levels far below the replacement rate, Japan's population is shrinking and thus naturally net electricity generation has also been declining in Japan since 2010.
Japan net electricity generation by fuel 2009-15. Source: US Energy Information Administration
Coal consumption in China has dropped three years in a row, and in January 2017, 100 coal fired power plants were cancelled. China has announced that it is spending a whopping $360 billion on renewables through 2020, and this year is implementing the world's biggest cap-and-trade carbon market to curb emissions.
Blind to the reality of this situation, Australia is ramping up coal production while China commits to ending coal imports in the very near future in what can only be described as a last-ditch ""dig it up now, or never"" situation.
Major Export Markets for Australian Coal (2014). Source: Wikipedia
Coal Consumption in China, the US and India 1990-2040. Source: US Energy Information Administration
Coal exports rely on substantial investment by investors who build significant infrastructure, like ports and rail, the cost of which is shared among users according to volume. If a coal company defaults then the remaining coal companies pay extra to collectively cover the loss. A single failure can significantly increase the cost to the other users and can in turn cause pressure on the remaining partners. As this happens, their bonds get downgraded causing balance sheet erosion that ultimately can impact project viability.
Moodys recently downgraded the ratings of several Australian coal ports- including Adani's Abbot Point- after U.S. coal miner Peabody Energy, which ships through these ports, defaulted on several of its bonds.
Despite all of this, some in government can't get their head around why the Big Four banks and major investment banks including, Citigroup, JPMorgan, Goldman Sachs, Deutsche Bank, Royal Bank of Scotland, HSBC and Barclays are not keen to fund the gargantuan Carmichael coal project in Queensland's Galilee Basin.
The now former deputy Prime Minister of Australia, Barnaby Joyce, a New Zealand-Australian politician who served unconstitutionally as the Deputy Prime Minister of Australia, wants Australian taxpayers to be the lenders of last resort to Adani, an Indian miner, for $900 million to build a rail line from their proposed Carmichael Thermal Coal Mine to the port at Abbot Point, where it would be shipped to India. Adani is looking for a handout because, unsurprisingly, the banks knocked them back because the project was too risky and the public backlash against the project has been overwhelming. If it does go ahead, it is likely to be a rail line to nowhere, because by the time it opens, there is a chance that the project will be unviable.
Unless the government steps in, it's increasingly more likely that the project will go the way of the Wiggins Island coal export terminal, the fraught development originally conceived by Glencore and seven other project partners in 2008, at the literal top of the market for coal. Since conception, three of the project's original proponents  Caledon Coal, Bandanna Energy and Cockatoo Coal  have gone into administration. Only one of the project's three stages has been completed, at twice the estimated cost. The five remaining take-or-pay owners have been left with more than US$4 billion in debt to repay and hope is fading on any any chance of refinancing before it all falls due.
What makes the Adani project so absurd is that India has recently cancelled more than 500 gigawatts of planned coal projects and the Indian government has said, however realistic that may be, that it intends to phase out thermal coal imports- precisely the type of coal Carmichael produces- entirely by 2020.
It's even more perplexing when you consider that 2016 was the year that solar became cheaper than coal, with some countries generating electricity from sunshine for less than 3 cents per kilowatt-hour (which is half the average global cost of coal power) and by October 2017, wind power is now cheaper than coal in India.
Furthermore, global policy to limit the rise in temperatures by 2% could result in a 40% drop in the trade of thermal coal, which would cut Australia's exports of such by 35%, according to a study by Wood Mackenzie. In 2014, thermal coal was 51% of our coal exports by volume, and this is precisely the type of coal that will be mined by Adani at Carmichael.
Given that Baarnaby's service was ruled invalid, one can only hope that his actions regarding Government funding for the Adani project might also be invalidated and we can put this flawed project to bed.
Recent events have given manifest life to Mark Carney's landmark 2015 speech in which Carney, the Governor of the Bank of England, warned that if the world is to limit global warming to below 2 degrees, then the estimates for how much carbon the world can burn makes between 66% and 80% of global oil, gas and coal reserves unusable.
In an essay last year, David Fickling wrote ""More than half the assets in the global coal industry are now held by companies that are either in bankruptcy proceedings or don't earn enough money to pay their interest bills, according to data compiled by Bloomberg. In the U.S., only three of 12 large coal miners traded on public markets escape that ignominious club, separate data show"".
So while our politicians gaze wistfully in parliaments at a lump of coal, undoubtedly the days are clearly numbered for our second largest export.
Losing coal as an export will blow a $34 billion dollar per annum hole in the current account, and there's been no foresight by successive governments to find or encourage modern industries to supplant it.
Australian Treasurer Scott Morrison gazes wistfully at a lump of coal. Source: AAP, Lukas Coch
What is more shocking is that despite the gargantuan amount of money that China has been pumping into the system since 2014, Australia's entire mining industry- which is completely dependent on China- has struggled to make any money at all.
Across the entire industry revenue has dropped significantly while costs have continued to rise.
China credit impulse leads its manufacturing index (which in turn fuels commodities). Source: PIMCO
According to the Australian Bureau of Statistics, in 2015-16 the entire Australian mining industry which includes coal, oil & gas, iron ore, the mining of metallic & non-metallic minerals and exploration and support services made a grand total of $179 billion in revenue with $171 billion of costs, generating an operating profit before tax of $7 billion which representing a wafer thin 3.9% margin on an operating basis. In the year before it made a 8.4% margin.
Collectively, the entire Australian mining industry (ex-services) would be loss making in 2016-17 if revenue continued to drop and costs stayed the same. Yes, the entire Australian mining industry.
Collectively, the entire Australian mining industry (ex-services) would be loss making in 2016-17 if revenue continued to drop and costs stayed the same. Source: Australian Bureau of Statistics
Our ""economic miracle"" of 104 quarters of GDP growth without a recession today doesn't come from digging rocks out of the ground, shipping the by-products of dead fossils and selling stuff we grow any more. Mining, which used to be 19% of GDP, is now 6.8% and falling. Mining has fallen to the sixth largest industry in the country. Even combined with agriculture the total is now only 10% of GDP.
Operating profit before tax by Australian Industry- the entire small and medium mining industry collectively has been loss making from 2014-16 on an operating basis. Source: Australian Bureau of Statistics
Mineral production in regional Western Australia, where 99% of Australia's iron ore is mined, contributed only 6.5 percent to Australia's GDP growth in 2016.
To make matters worse, in 2017 there has been a sharp downturn in Chinese credit impulse (rate of change), which is combined with a negative, and falling global credit impulse. According to PIMCO's Gene Fried ""the question now is not if China slows, but rather how fast"". This will cause even more problems for Australia's flagging resources sector.
China's contribution to the global credit impulse (market GDP weighted). Source: PIMCO
The ""economic miracle"" of GDP growth is also certainly not from manufacturing, which has collapsed in the last decade from 10.8% to 6.6% of Gross Value Add, and has grown by... negative 275,000 jobs since the 1990s.
Industry share of Gross Value Add 2005-6 versus 2015-6. Source: Australian Bureau of Statistics
This is even before the exit of Australia's last two remaining car manufacturers, Toyota and Holden, who both shut up shop in 2017. Ford closed last year.
Australian Manufacturing Employment and Hours Worked. Source: AI Group
In the 1970s, Australia was ranked 10th in the world for motor vehicle manufacturing. No other industry has replaced it. Today, the entire output of manufacturing as a share of GDP in Australia is half of the levels where they called it ""hollowed out"" in the U.S. and U.K.
In Australia in 2017, manufacturing as a share of GDP is on par with a financial haven like Luxembourg. Australia doesn't make anything anymore.
Manufacturing value add (% of GDP) for Australia. Source: World Bank & OECD
With an economy that is 68% services, as I believe John Hewson put it, the entire country is basically sitting around serving each other cups of coffee or, as the Chief Scientist of Australia would prefer, smashed avocado.
David Llewellyn-Smith recently wrote that this is unsurprising as ""the Australian economy is now structurally uncompetitive as capital inflows persistently keep its currency too high, usually chasing land prices that ensure input costs are amazingly inflated as well.
Wider tradables sectors have been hit hard as well and Australian exports are now a lousy 20% of GDP despite the largest mining boom in history.
The other major economic casualty has been multifactor productivity (the measure of economic performance that compares the amount of goods and services produced to the amount of combined inputs used to produce those goods and services). It has been virtually zero for fifteen years as capital has been consistently and massively mis-allocated into unproductive assets. To grow at all today, the nation now runs chronic twin deficits with the current account (value of imports to exports) at -2.7% and a budget deficit of -2.4% of GDP.""
The Reserve Bank of Australia has cut interest rates by 325 basis points since the end of 2011, in order to stimulate the economy, but I can't for the life of me see how that will affect the fundamental problem of gyrating commodity prices where we are a price taker, not a price maker, into an oversupplied market in China.
This leads me to my next question- where has this growth come from?
Successive Australian governments have achieved economic growth by blowing a property bubble on a scale like no other.
A bubble that has lasted for 55 years and seen prices increase 6556% since 1961, making this the longest running property bubble in the world (on average, ""upswings"" last 13 years).
In 2016, 67% of Australia's GDP growth came from the cities of Sydney and Melbourne where both State and Federal governments have done everything they can to fuel a runaway housing market. The small area from the Sydney CBD to Macquarie Park is in the middle of an apartment building frenzy, alone contributing 24% of the country's entire GDP growth for 2016, according to SGS Economics & Planning.
According to the Rider Levett Bucknall Crane Index, in Q4 2017 between Sydney, Melbourne and Brisbane, there are now 586 cranes in operation, with a total of 685 across all capital cities, 80% of which are focused on building apartments. There are 350 cranes in Sydney alone.
Crane Activity  Australia by Key Cities & Sector. Source: RLB
By comparison, there are currently 28 cranes in New York, 24 in San Francisco and 40 in Los Angeles. There are more cranes in Sydney than Los Angeles (40), Washington DC (29), New York (28), Chicago (26), San Francisco (24), Portland (22), Denver (21), Boston (14) and Honolulu (13) combined. Rider Levett Bucknall counts less than 175 cranes working on residential buildings across the 14 major North American markets that it tracked in 3Q17, which is half of the number of cranes in Sydney alone.
According to UBS, around one third of these cranes in Australian cities are in postcodes with 'restricted lending', because the inhabitants have bad credit ratings.
This can only be described as completely ""insane"".
That was the exact word used by Jonathan Tepper, one of the world's top experts in housing bubbles, to describe ""one of the biggest housing bubbles in history"". ""Australia"", he added, ""is the only country we know of where middle-class houses are auctioned like paintings"".
An Auctioneer yells out bids in the middle class suburb of Cammeray. Source: Reuters
Our Federal government has worked really hard to get us to this point.
Many other parts of the world can thank the Global Financial Crisis for popping their real estate bubbles. From 2000 to 2008, driven in part by the First Home Buyer Grant, Australian house prices had already doubled. Rather than let the GFC take the heat out of the market, the Australian Government doubled the bonus. Treasury notes recorded at the time say that it wasn't launched to make housing more affordable, but to prevent the collapse of the housing market.
Treasury Executive Minutes. Source: Treasury, The First Home Owner's Boost
Already at the time of the GFC, Australian households were at 160% debt to net disposable income, 30% more indebted than American households at their peak, but then things really went crazy.
The government decided to further fuel the fire by ""streamlining"" the administrative requirements for the Foreign Investment Review Board so that temporary residents could purchase real estate in Australia without having to report or gain approval.
It may be a stretch, but one could possibly argue that this move was cunningly calculated, as what could possibly be wrong in selling overpriced Australian houses to the Chinese?
I am not sure who is getting the last laugh here, because as we subsequently found out, many of those Chinese borrowed the money to buy these houses from Australian banks, using fake statements of foreign income. Indeed, according to the AFR, this was not sophisticated documentation  Australian banks were being tricked with photoshopped bank statements that can be bought online for as little as $20.
UBS estimates that $500 billion worth of ""not completely factually accurate"" mortgages now sit on major bank balance sheets. How much of that will go sour is anyone's guess.
Llewellyn-Smith writes, ""Five prime ministers in [seven] years have come and gone as standards of living fall in part owing to massive immigration inappropriate to economic circumstances and other property-friendly policies. The most recent national election boiled down to a virtual referendum on real estate taxation subsidies. The victor, the conservative Coalition party, betrayed every market principle it possesses by mounting an extreme fear campaign against the Labor party's proposal to remove negative gearing. This tax policy allows more than one million Australians to engage in a negative carry into property in the hope of capital gains. In a nation of just 24 million, 1.3 million Australians lose an average of $9,000 per annum on this strategy thanks to the tax break.""
The astronomical rise in house prices certainly isn't supported by employment data. Wage growth is at a record low of just 1.9% year on year in 2Q17, the lowest figure since 1988. The average Australian weekly income has gone up $27 to $1,009 since 2008, that's about $3 a year.
Private sector wage price index (annual percentage). Source: SMH, Australian Bureau of Statistics
Household income growth has collapsed since 2008 from over 11% to just 3% in 2015, 2016 and 2017. This is one sixth the rate that houses went up in Sydney in the last year.
Employment growth is at an anaemic 1% year on year in 4Q16, and the unemployment rate has been trending up over the last decade to 5.6%.
Unemployment rate and Employment growth. Source: ABS, RBA, UBS
Foreign buying driving up housing prices has been a major factor in Australian housing affordability, or rather unaffordability.
Urban planners say that a median house price to household income ratio of 3.0 or under is ""affordable"", 3.1 to 4.0 is ""moderately unaffordable"", 4.1 to 5.0 is ""seriously unaffordable"" and 5.1 or over ""severely unaffordable"".
Demographia International Housing Affordability Survey. Source: Demographia
At the end of July 2017, according to Domain Group, the median house price in Sydney was $1,178,417 and the Australian Bureau of Statistics has the latest average pre-tax wage at $80,277.60 and average household income of $91,000 for this city. This makes the median house price to household income ratio for Sydney 13x, or over 2.6 times the threshold of ""severely unaffordable"". Melbourne is 9.6x.
Sydney House values by Suburb. Source: Core Logic
This is before tax, and before any basic expenses. The average person takes home $61,034.60 per annum, and so to buy the average house they would have to save for 19.3 years- but only if they decided to forgo the basics such as, eating. This is neglecting any interest costs if one were to borrow the money, which at current rates would approximately double the total purchase cost and blow out the time to repay to around 40 years.
Ex-deputy Prime Minister Barnaby Joyce recently said to ABC Radio, ""Houses will always be incredibly expensive if you can see the Opera House and the Sydney Harbour Bridge, just accept that. What people have got to realise is that houses are much cheaper in Tamworth, houses are much cheaper in Armidale, houses are much cheaper in Toowoomba"". Fairfax, the owner of Domain, or more accurately, Domain, the owner of Fairfax, also agrees that ""There is no housing bubble, unless you are in Sydney or Melbourne"".
Now probably unbeknownst to Barnaby, who might be more familiar with the New Zealand housing market, in the Demographia International Housing Affordability survey for 2017 Tamworth ranked as the 78th most unaffordable housing marketing in the world. No, you're not mistaken, this is Tamworth, New South Wales, a regional centre of 42,000 best known as the ""Country Music Capital of Australia"" and for the 'Big Golden Guitar'.
According the Australian Bureau of Statistics, the average income in Tamworth is $42,900, the average household income $61,204 but the average house price is $375,000, giving a price to household income ratio of 6.1x, making housing in Tamworth less affordable than Tokyo, Singapore, Dublin or Chicago.
If you used the current Homesales.com.au data, which has the average house price at $394,212, or 6.6x, Tamworth would be in the top 40 most unaffordable housing markets in the world. Yes, Tamworth. Yes, in the world. Unfortunately for Barnaby, Armidale and Toowoomba don't fare much better.
Tamworth, which at current prices would be in the top 40 most unaffordable housing markets tracked by Demographia in the world. Really? Source: GP Synergy
Out of a total of 406 housing markets tracked globally by Demographia, eight (or 40%) of the twenty least affordable housing markets in the world were in Australia, including in addition to Sydney and Melbourne such exotic places as Wingcaribbee, Tweed Heads, the Sunshine Coast, Port Macquarie, the Gold Coast, and Wollongong. Looking at all regional Australian housing markets, they found 33 of 54 markets ""severely unaffordable"".
The 20 most unaffordable housing markets in the world. Source: Demographia, 13th Annual Demographic International Housing Affordability Survey:2017
If you borrowed the whole amount to buy a house in Sydney, with a Commonwealth Bank Standard Variable Rate Home Loan currently showing a 5.36% comparison rate (as of 7th October 2017), your repayments would be $6,486 a month, every month, for 30 years. The monthly post tax income for the average wage in Sydney ($80,277.60) is only $5,081.80 a month.
Commonwealth Bank Standard Variable Rate Home Loan for the average house. Source: CBA as of 7th October 2017
In fact, on this average Sydney salary of $80,277.60, the Commonwealth Bank's ""How much can I borrow?"" calculator will only lend you $463,000, and this amount has been dropping in the last year I have been looking at it. So good luck to the average person buying anything anywhere near Sydney.
Federal MP Michael Sukkar, Assistant Minister to the Treasurer, says surprisingly that getting a ""highly paid job"" is the ""first step"" to owning a home. Perhaps Mr Sukkar is talking about his job, which pays a base salary of $199,040 a year. On this salary, the Commonwealth Bank would allow you to just borrow enough- $1,282,000 to be precise- to buy the average home, but only provided that you have no expenses on a regular basis, such as food. So the Assistant Minister to the Treasurer can't really afford to buy the average house, unless he tells a porky on his loan application form.
The average Australian is much more likely to be employed as a tradesperson, school teacher, postman or policeman. According to the NSW Police Force's recruitment website, the average starting salary for a Probationary Constable is $65,000 which rises to $73,651 over five years. On these salaries the Commonwealth Bank will lend you between $375,200 and $419,200 (again provided you don't eat), which won't let you buy a house really anywhere.
Unsurprisingly, the CEOs of the Big Four banks in Australia think that these prices are ""justified by the fundamentals"". More likely because the Big Four, who issue over 80% of residential mortgages in the country, are more exposed as a percentage of loans than any other banks in the world, over double that of the U.S. and triple that of the U.K., and remarkably quadruple that of Hong Kong, which is the least affordable place in the world for real estate. Today, over 60% of the Australian banks' loan books are residential mortgages. Houston, we have a problem.
Residential Mortgages as a percentage of total loans. Source: IMF (2015)
It's actually worse in regional areas where Bendigo Bank and the Bank of Queensland are holding huge portfolios of mortgages between 700 to 900% of their market capitalisation, because there's no other meaningful businesses to lend to.
Australian banks' mortgage exposure as a percentage of market capitalisation. Source: Roger Montgomery, Company data
I'm not sure how the fundamentals can possibly be justified when the average person in Sydney can't actually afford to buy the average house in Sydney, no matter how many decades they try to push the loan out.
Mortgage Stress Trends to Oct 2017. Source: Digital Finance Analytics
Indeed Digital Finance Analytics estimated in a October 2017 report that 910,000 households are now estimated to be in mortgage stress where net income does not covering ongoing costs. This has skyrocketed up 50% in less than a year and now represents 29.2% of all households with mortgages in Australia (or 9% of all households). Things are about to get real.
Probability of default in 30, 90 days across Australian demographics in October 2017. Source: Digital Finance Analytics
It's well known that high levels of household debt are negative for economic growth, in fact economists have found a strong link between high levels of household debt and economic crises.
This is not good debt, this is bad debt. It's not debt being used by businesses to fund capital purchases and increase productivity. This is not debt that is being used to produce, it is debt being used to consume. If debt is being used to produce, there is a means to repay the loan. If a business borrows money to buy some equipment that increases the productivity of their workers, then the increased productivity leads to increased profits, which can be used to service the debt, and the borrower is better off. The lender is also better off, because they also get interest on their loan. This is a smart use of debt. Consumer debt generates very little income for the consumer themselves. If consumers borrow to buy a new TV or go on a holiday, that doesn't create any cash flow. To repay the debt, the consumer generally has to consume less in the future. Further, it is well known that consumption is correlated to demographics, young people buy things to grow their families and old people consolidate, downsize and consume less over time. As the aging demographic wave unfolds across the next decade there will be significantly less consumers and significantly more savers. This is worsened as the new generations will carry the debt burden of student loans, further reducing consumption.
Parody of Sydney real estate, or is it?
So why are governments so keen to inflate housing prices?
The government loves Australians buying up houses, particularly new apartments, because in the short term it stimulates growth  in fact it's the only thing really stimulating GDP growth.
Australia has around $2 trillion in unconsolidated household debt relative to $1.6 trillion in GDP, making this country in recent quarters the most indebted on this ratio in the world. According to Treasurer Scott Morrison 80% of all household debt is residential mortgage debt. This is up from 47% in 1990.
Australia Household Debt to GDP. Source: Bank for International Settlements, Macro Business
Australia's household debt servicing ratio (DSR) ties with Norway as the second worst in the world. Despite record low interest rates, Australians are forking out more of their income to pay off interest than when we had record mortgage rates back in 1989-90 which are over double what they are now.
Everyone's too busy watching Netflix and cash strapped paying off their mortgage to have much in the way of any discretionary spending. No wonder retail is collapsing in Australia.
Governments fan the flame of this rising unsustainable debt fuelled growth as both a source of tax revenue and as false proof to voters of their policies resulting in economic success. Rather than modernising the economy, they have us on a debt fuelled housing binge, a binge we can't afford.
We are well past overtime, we are into injury time. We're about to have our Minsky moment: ""a sudden major collapse of asset values which is part of the credit cycle.""
Such moments occur because long periods of prosperity and rising valuations of investments lead to increasing speculation using borrowed money. The spiraling debt incurred in financing speculative investments leads to cash flow problems for investors. The cash generated by their assets is no longer sufficient to pay off the debt they took on to acquire them. Losses on such speculative assets prompt lenders to call in their loans. This is likely to lead to a collapse of asset values. Meanwhile, the over-indebted investors are forced to sell even their less-speculative positions to make good on their loans. However, at this point no counterparty can be found to bid at the high asking prices previously quoted. This starts a major sell-off, leading to a sudden and precipitous collapse in market-clearing asset prices, a sharp drop in market liquidity, and a severe demand for cash.
The Minsky Cycle. Source: Economic Sociology and Political Economy
The Governor of the People's Bank of China recently warned that extreme credit creation, asset speculation and property bubbles could pose a ""systemic financial risk"" in China. Zhou Xiaochuan said ""If there is too much pro-cyclical stimulus in an economy, fluctuations will be hugely amplified. Too much exuberance when things are going well causes tensions to build up. That could lead to a sharp correction, and eventually lead to a so-called Minsky Moment. That's what we must really guard against"". A Minsky moment in China would be an extreme event for the parasite on the vein of Chinese credit stimulus- the Australian economy.
Today 42% of all mortgages in Australia are interest only, because since the average person can't afford to actually pay for the average house- they only pay off the interest. They're hoping that value of their house will continue to rise and the only way they can profit is if they find some other mug to buy it at a higher price. In the case of Westpac, 50% of their entire residential mortgage book is interest only loans.
Percentage of interest only loans by bank. Source: JCP Investment Partners, AFR
And a staggering 64% of all investor loans are interest only.
Share of new loan approvals for Australian banks. Source: APRA, RBA, UBS
This is rapidly approaching ponzi financing.
This is the final stage of an asset bubble before it pops.
Today residential property as an asset class is four times larger than the sharemarket. It's illiquid, and the $1.5 trillion of leverage is roughly equivalent in size to the entire market capitalisation of the ASX 200. Any time there is illiquidity and leverage, there is a recipe for disaster- when prices move south, equity is rapidly wiped out precipitating panic selling into a freefall market with no bids to hit.
The risks of illiquidity and leverage in the residential property market flow through the entire financial system because they are directly linked; today in Australia the Big Four banks plus Macquarie are roughly 30% of the ASX200 index weighting. Every month, 9.5% of the entire Australian wage bill goes into superannuation, where 14% directly goes into property and 23% into Australian equities- of which 30% of the main equity benchmark is the banks.
ASX200 by market capitalisation, Big 4 banks top and Macquarie on the left (arrows). Source: IRESS
You don't read objective reporting on property in the Australian media, which Llewelyn-Smith from Macro Business calls ""a duopoly between a conservative Murdoch press and liberal Fairfax press. But both are loss-making old media empires whose only major growth profit centres are the nation's two largest real estate portals, realestate.com.au and Domain. Neither report real estate with any objective other than the further inflation of prices. In the event that the Australian bubble were to pop then Australians will certainly be the last to know and the propaganda is so thick that they may never find out until they actually try to sell.""
Take, for example, this recent headline from the Fairfax owned Sydney Morning Herald on March 1st 2017, ""Meet Daniel Walsh, the 26-year-old train driver with $3 million worth of property"". It appeared in the property section, which for Fairfax today sits on the homepage of their masthead publications, such as the Sydney Morning Herald, immediately below the top headlines for the day and above State News, Global Politics, Business, Entertainment, Technology and the Arts. The article holds up 26 year old Daniel, who services five million dollars worth of property with a train driver's salary and $2,000 a week of positive cash flow.
This is what the Australian press more commonly holds up as a role model to young people. Not a young engineer who has developed a revolutionary new product or breakthrough, but an over leveraged train driver with a property portfolio on mostly borrowed money where a 1% move in interest rates will wipe out the entirety of this cash flow.
Yet this young train driver isn't an isolated case, there are literally hoards of these young folk parlaying one property debt onto another in the mistaken belief that property prices only ever go up. Jennifer Duke, an ""audience-driven reporter, with a background in real estate and finance"" from Domain, also promotes Robert, a 20 year old, who had managed to accumulate three properties in two years using an initial $60,000 gift from his mum. Jeremy, a 24 year old accountant, has 8 properties with a loan to value ratio of 70%, Edward, a 24 year old customer service representative, has 6 properties despite a debt level of 69% and a salary under $50,000, and Taku, the Uber driver, has 8 properties, with plans for 10 covered by a net equity position of only $1 million by November 2017.
How a train driver can service five million dollars of property on $2,000 a week of positive cash flow comes through the magic of cross-collateralised residential mortgages, where Australian banks allow the unrealised capital gain of one property to secure financing to purchase another property. This unrealised capital gain substitutes for what normally would be a cash deposit. This house of cards is described by LF Economics as a ""classic mortgage ponzi finance model"". When the housing market moves south, this unrealised capital gain will rapidly become a loss, and the whole portfolio will become undone. The similarities to underestimation of the probability of default correlation in Collateralised Debt Obligations (CDOs), which led to the Global Financial Crisis, are striking.
Fairfax's pre-IPO real estate website Domain runs these stories every week across the capital city main mastheads enticing young people into property flipping as a get rich quick scheme. All of them are young, with low incomes, leveraging one property purchase on to another.
At Fairfax  whose latest half year 2017 financial results had Domain Group EBITDA at $57.3 million and the entire Australian Metro Media which includes Australia's premier mastheads Australian Financial Review, Sydney Morning Herald, the Age, Digital Ventures, Life and Events EBITDA at $27.7 million  property is clearly the most important section of all.
In between holding up this 26 year old train driving property tycoon as something to aspire to, Jennifer has penned other noteworthy articles, such as ""No surprise the young support lock-out laws"" which parroted incredulous propaganda claiming that young people supported laws designed to shut down places where young people go  Sydney's major entertainment districts.
As if the Australian economy needed further headwinds, the developer-enamoured evangelical right have crucified NSW's night time economy. Reactionary puritans and opportunists alike seized on some unfortunate incidents involving violence to simply close the economy at night. NSW State Government, City of Sydney, Casinos, NSW Police, public health nannies, property-crazy media and, of course, property developers had the collective interest to manufacture and blow up a fake health & safety issue to create lockout laws  and then instituted broad night time economic terraforming policies designed to herd patrons to large casinos so they could become permanent monopoly owners of the night time economy in Sydney and Brisbane, while conveniently damaging the balance sheets of small businesses located in competing entertainment areas, so the property could be demolished and turned into apartment blocks.
Property watching at Fairfax has become a fetish. Almost on a daily basis Lucy Macken, Domain's Prestige Property Reporter, publishes a gossip column of who bought what house, complete with the full address and photos of the exterior and interior and any financial information she can glean about them. I know of one person whose house was robbed  completely cleaned out  shortly after Macken published their full address. Perhaps that was a coincidence, but I am utterly amazed that Fairfax senior management allows this column to exist given the risks it poses to the people whose houses and private details are splashed across its pages.
Fairfax, to be fair, is not without its fair share of great journalists, albeit a species rapidly becoming extinct, who are very well aware of what is really going on. Elizabeth Farrelly writes, ""Just when you thought the government couldn't get any madder or badder in its overarching Mission Destroy Sydney  when it seemed to have flogged every floggable asset, breached every democratic principle, whittled every beloved park, disempowered every significant municipality and betrayed every promise of decency, implicit or explicit  it now wants to remove council planning powers. The excuse, naturally, is 'probity'. Somehow we're meant to believe that locally elected people are inherently more corrupt than those elected at state level, and that this puts local decision-making into the greedy mitts of Big Developers"".
However, despite the picture Domain would like to paint, young people with jobs aren't responsible for driving house prices up, in fact their ownership is at an all time low.
In 2015-16 there were 40,149 residential real estate applications from foreigners valued at over $72 billion in the latest data by FIRB. This is up 244% by count and 320% by value from just three years before.
To put this 40,149 in comparison, in the latest 12 months to the end of April 2017, according to the Australian Bureau of Statistics, a total of 57,446 new residential dwellings were approved in Greater Sydney, and 56,576 in Greater Melbourne.
Even more shocking, in the month of January 2017, the number of first home buyers in the whole of New South Wales was 1,029  the lowest level since mortgage rates peaked in the 1990s. Half of those first home buyers rely upon their parents for equity.
The 114,022 new residential dwellings in Sydney and Melbourne in 2015-16 should also be put in comparison to a net annual gain of 182,165 overseas immigrants to Australia of which around 75% go to New South Wales or Victoria.
This brings me onto Australia's third largest export which is $22 billion in ""education-related travel services"". Ask the average person in the street, and they would have no idea what that is and, at least in some part, it is an $18.8 billion dollar immigration industry dressed up as ""education"". You now know what all these tinpot ""english"", ""IT"" and ""business colleges"" that have popped up downtown are about. They're not about providing quality education, they are about gaming the immigration system.
In 2014, 163,542 international students commenced English language programmes in Australia, almost doubling in the last 10 years. This is through the booming ELICOS (English Language Intensive Courses for Overseas Students) sector, the first step for further education and permanent residency.
This whole process doesn't seem too hard when you take a look at what is on offer. While the federal government recently removed around 200 occupations from the Skilled Occupations List, including such gems as Amusement Centre Manager (149111), Betting Agency Manager (142113), Goat Farmer (121315), Dog or Horse Racing Official (452318), Pottery or Ceramic Artist (211412) and Parole Officer (411714)  you can still immigrate to Australia as a Naturopath (252213), Baker (351111), Cook (351411), Librarian (224611) or Dietician (251111).
Believe it or not, up until recently we were also importing Migration Agents (224913). You can't make this up. I simply do not understand why we are importing people to work in relatively unskilled jobs such as kitchen hands in pubs or cooks in suburban curry houses.
At its peak in October 2016, before the summer holidays, there were 486,780 student visa holders in the country, or 1 in 50 people in the country held a student visa. The grant rate in 4Q16 for such student visa applications was 92.3%. The number one country for student visa applications by far was, you guessed it, China.
Number of Student Visa Applications by Country 2015-16. Source: Department of Immigration and Border Protection
While some of these students are studying technical degrees that are vitally needed to power the future of the economy, a cynic would say that the majority of this program is designed as a crutch to prop up housing prices and government revenue from taxation in a flagging economy. After all, it doesn't look that hard to borrow 90% of a property's value from Australian lenders on a 457 visa. Quoting directly from one mortgage lender, ""you're likely to be approved if you have at least a year on your visa, most of your savings already in Australia and you have a stable job in sought after profession""  presumably as sought after as an Amusement Centre Manager. How much the banks will be left to carry when the market turns and these students flee the burden of negative equity is anyone's guess.
In a submission to a senate economics committee by Lindsay David from LF Economics, ""We found 21 Australian lending institutions where there is evidence of people's loan application forms being fudged"".
The ultimate cost to the Australian taxpayer is yet to be known. However the situation got so bad that the RBA had to tell the Big Four banks to cease and desist from all foreign mortgage lending without identified Australian sources of income.
Ken Sayer, Chief Executive of non-bank Mortgage House said ""It is much bigger than everyone is making it out to be. The numbers could be astronomical"".
So we are building all these dwellings, but they are not for new Australian home owners. The Westpac-Melbourne Institute has overall consumer sentiment for housing at a 40 year low of 10.5%.
Instead we are building these dwellings to be the new Swiss Bank account for foreign investors.
Share of consumers saying 'wisest place for saving' is real estate. Source: ABS, RBA, Westpac, Melbourne Institute, UBS
Foreign investment can be great as long as it flows into the right sectors. Around $32 billion invested in real estate was from Chinese investors in 2015-16, making it the largest investment in an industry sector by a country by far. By comparison in the same year, China invested only $1.6 billion in our mining industry. Last year, twenty times more more money flowed into real estate from China than into our entire mineral exploration and development industry. Almost none of it flows into our technology sector.
Approvals by country of investor by industry sector in 2015-6. Source: FIRB
The total number of FIRB approvals from China was 30,611. By comparison. The United States had 481 approvals.
Foreign investment across all countries into real estate as a whole was the largest sector for foreign investment approval at $112 billion, accounting for around 50% of all FIRB approvals by value and 97% by count across all sectors  agriculture, forestry, manufacturing, tourism  you name it in 2015-16.
In fact it doesn't seem that hard to get FIRB approval in Australia, for really anything at all. Of the 41,450 applications by foreigners to buy something in 2015-16, five were rejected. In the year before, out of 37,953 applications zero were rejected. Out of the 116,234 applications from 2012 to 2016, a total of eight were rejected.
Applications for FIRB consideration, approved versus rejected 2012-13 to 2015-6. Source: FIRB
According to Credit Suisse, foreigners are acquiring 25 percent of newly completed housing supply in NSW, worth a total of $39 billion.
Demand for Property from Foreign Buyers in NSW (% of total, unstacked). Source: NAB, SBS
In some circumstances, the numbers however could be much higher. Lend Lease, the Australian construction goliath with over $15 billion in revenue in 2016, stated in that year's annual report that over 40% of Lend Lease's apartment sales were to foreigners.
I wouldn't have a problem with this if it weren't for the fact that this is all a byproduct of central bank madness, not true supply and demand, and people vital for running the economy can't afford to live here any more.
What is also remarkable about all of this is that technically, the Chinese are not allowed to send large sums of money overseas. Citizens of China can normally only convert US$50,000 a year in foreign currency and have long been barred from buying property overseas, but those rules have not been enforced. They've only started cracking down on this now.
Despite this, up until now, Australian property developers and the Australian Government have been more than happy to accommodate Chinese money laundering.
After the crackdown in capital controls, Lend Lease says there has been a big upswing with between 30 to 40% of foreign purchases now being cash settled. Other developers are reporting that some Chinese buyers are paying 100% cash. The laundering of Chinese cash into property isn't unique to Australia, it's just that Transparency International names Australia, in their March 2017 report as the worst money laundering property market in the world.
Australia is not alone, Chinese ""hot money"" is blowing gigantic property bubbles in many other safe havens around the world.
But combined with our lack of future proof industries and exports, our economy is complete stuffed. And it's only going to get worse unless we make a major transformation of the Australian economy.
We can't rely on property to provide for our future. In 1880, Melbourne was the richest city in the world, until it had a property crash in 1891 where house prices halved causing Australia's real GDP to crash by 10 per cent in 1892 and 7 per cent the year after. The depression of the 1890s caused by this crash was substantially deeper and more prolonged than the great depression of the 1930s. Macro Business points out that if you bought a house at the top of the market in 1890s, it took seventy years for you to break even again.
Australia CQ Real Housing Price Index 1890-2016. Source: LF Economics, Macro Business
Instead of relying on a property bubble as pretense that our economy is strong, we need serious structural change to the composition of GDP that's substantially more sophisticated in terms of the industries that contribute to it.
Australia's GDP of $1.6 trillion is 69% services. Our ""economic miracle"" of GDP growth comes from digging rocks out of the ground, shipping the by-products of dead fossils, and stuff we grow. Mining, which used to be 19%, is now 7% and falling. Combined, the three industries now contribute just 12% of GDP thanks to the global collapse in commodities prices.
If you look at businesses as a whole, Company tax hasn't moved from $68 billion in the last three years  our companies are not making more profits. This country is sick.
Indeed if you look at the budget, about the only thing going up in terms of revenue for the federal government are taxes on you having a good time- taxes on beer, wine, spirits, luxury cars, cigarettes and the like. It would probably shock the average person on the street to discover that the government collects more tax from cigarettes ($9.8 billion) than it collects from tax on superannuation ($6.8 billion), over double what it collects from Fringe Benefits Tax ($4.4 billion) and over thirteen times more tax than it does from our oil fields ($741 million).
Turnbull is increasing the tax on cigarettes by 12.5% a year for the next four years. In the latest federal budget, the government forecasts that by 2020 that it will collect $15.2 billion from taxes on tobacco per annum. This is four times the amount that the government collects from the entire coal industry per annum.
Just compare these numbers: $15 billion is over double what the government projects it will collect from petrol excise in that year ($7.15b), 21 times what it will collect from luxury car tax ($720m), 27 times what it will collect from taxes on imported cars ($560m) and 89 times what it will collect from customs duty on textile and footwear imports ($170m).
As a sign of how addicted to taxing you the government has become, look at the myriad of taxes on cars  high import duties, stamp duty and a luxury car tax  these were designed to protect a car manufacturing industry which doesn't exist anymore. Yet the government is still increasing them. We closed the last factory this year. These taxes are not only blatant cash grabs but serve to stifle the deployment of electric cars, which have hit a dead end in Australia. Likewise, the taxes on textile and footwear imports were originally designed to protect our textiles, an industry that has now collapsed and that lost 30% of its manufacturing workers this year.
If you look through federal budget forecasts, taxes on cigarettes is the only thing practically floating the federal government's finances other than wishful thinking in forward projections. Which is, of course, some other future administration's problem.
How they think they can raise $15 billion in taxes per year on cigarettes  a product that costs a cent per stick to make and will retail for almost $2 a stick in 2020  without creating a thriving black market, another Pablo Escobar and throwing hundreds, perhaps thousands of people in jail, who will decide unwisely to participate in that black market, astounds me. But that's how the government decides to plug the hole in its accounts instead of cutting spending.
Of course like so many things this all gets sold to you, the general population, under the banner of ""health and safety""- and it's easy to sell because all you need to do is parade out a few patronising doctors. The truth is that it's really just for the health and safety of the government budget, because the economy is really, really sick.
If the government wants to fix the budget, I would have thought the most practical way to do it would be to find ways to grow the economy. You'll never wean the government off wasteful spending no matter who is in power. The politicians, after all, need to keep that up in order to buy votes through profligate policies such as welfare for the middle class.
But instead of thinking of intelligent ways to grow the economy, the focus is purely on finding more ways to tax you. Just think of all the times over the last couple of years, all the random thought bubbles, that various politicians have proposed raising taxes on superannuation, high earners, banks, property, tripling fines for cyclists, tripling fines for companies, the GST to 15% or 20%, the GST on low value imports, the GST on digital goods, stamp duty, alcohol, sugar, red meat, it's endless.
They are even proposing banning the $100 note, so that when the RBA drives interest rates negative, you won't be able to withdraw your hard earned funds in cash so easily. You'll either have to spend it or have the rude shock of the bank taking money out of your account each month rather than earning interest.
Here's a crazy idea: the dominant government revenue line is income tax. Income tax is generated from wages. Education has always been the lubricant of upward mobility, so perhaps if we find ways to encourage our citizens to study in the right areas  for example science & engineering  then maybe they might get better jobs or create better jobs and ultimately earn higher wages and pay more tax.
Instead the government proposed the biggest cuts to university funding in 20 years with a new ""efficiency dividend"" cutting funding by $1.2 billion, increasing student fees by 7.5 percent and slashing the HECS repayment threshold from $55,874 to $42,000. These changes would make one year of postgraduate study in Electrical Engineering at the University of New South Wales cost about $34,000.
We should be encouraging more people into engineering, not discouraging them by making their degrees ridiculously expensive. In my books, the expected net present value of future income tax receipts alone from that person pursuing a career in technology would far outweigh the short sighted sugar hit from making such a degree more costly  let alone the expected net present value of wealth creation if that person decides to start a company. The technology industry is inherently entrepreneurial, because technology companies create new products and services.
Speaking of companies, how about as a country we start having a good think about what sorts of industries we want to have a meaningful contribution to GDP in the coming decades?
For a start, we need to elaborately transform the commodities we produce into higher end, higher margin products. Manufacturing contributes 5% to GDP. In the last ten years, we have lost 100,000 jobs in manufacturing. Part of the problem is that the manufacturing we do has largely become commoditised while our labour force remains one of the most expensive in the world. This cost is further exacerbated by our trade unions  in the case of the car industry, the government had to subsidise the cost of union work practices, which ultimately failed to keep the industry alive. So if our people are going to cost a lot, we better be manufacturing high end products or using advanced manufacturing techniques otherwise other countries will do it cheaper and naturally it's all going to leave.
Last year, for example, 30.3% of all manufacturing jobs in the textile, leather, clothing & footwear industries were lost in this country. Yes, a third. People still need clothes, but you don't need expensive Australians to make them, you can make them anywhere.
That's why we need to seriously talk about technology, because technology is the great wealth and productivity multiplier.
However the thinking at the top of government is all wrong.
I recently heard a speech by the Chief Scientist of Australia where he held up a smashed avocado on toast as a prime example of Australian innovation. Yes, smashed avocado on toast. I am not sure which Australian company has the patent on smashed avocados on toast  it's too surreal to even think about.
Australian Innovation according to the Chief Scientist of Australia. Source: ChiefScientist.gov.au
In the same speech, he said that an Australian iron ore mine is every bit as innovative as a semiconductor fabrication plant. My mind was seriously blown.
You can throw as much automation, AI and robotics at an iron ore mine as technologically possible, but it doesn't change the fact that mines are, and always will be wasting assets that output a commodity for which we are a price taker, not a price maker, into what is currently an oversupplied global market. An iron ore mine, not matter how advanced, is not a long term scalable productivity multiplier; it is a resource to be extracted with finite supply. Once it's gone, the robots will be dormant.
A semiconductor fabrication plant on the other hand, makes automation of the mine possible. It powers the robotics, the AI and the software  not just for the iron ore mine, but factories and businesses all over the world. It's the real productivity and wealth multiplier. It's a long term sustainable, competitive advantage. Smart and efficient resource extraction is just an application of this technology.
That's why we shouldn't get confused about what is a technology company, because there is no other industry that can create such immense wealth, with such capital efficiency and long term benefit to the world, as the technology industry.
Today, the largest public company in the world, Apple, is a technology company. Apple's market capitalisation of $810 billion is bigger than the entire US retail market sector. Its revenue of over $215 billion generates over US$2 million dollars per employee per year. And that's just the company directly. Think of all the business, jobs, wealth creation and benefits to society that have come indirectly from using the company's computers, mobile devices, software, services and products.
The largest four companies by market capitalisation globally as of the end of Q2 2017 globally were Apple, Alphabet, Microsoft and Amazon. Facebook is eight. Together, these five companies generate over half a trillion dollars in revenue per annum. That's equivalent to about half of Australia's entire GDP. And many of these companies are still growing revenue at rates of 30% or more per annum.
These are exactly the sorts of companies that we need to be building.
With our population of 24 million and labour force of 12 million, there's no other industry that can deliver long term productivity and wealth multipliers like technology. Today Australia's economy is in the stone age. Literally.
By comparison, Australia's top 10 companies are a bank, a bank, a bank, a mine, a bank, a biotechnology company (yay!), a conglomerate of mines and supermarkets, a monopoly telephone company, a supermarket and a bank.
We live in a monumental time in history where technology is remapping and reshaping industry after industry  as Marc Andreessen said ""Software is eating the world!""  many people would be well aware we are in a technology gold rush.
And they would be also well aware that Australia is completely missing out.
Most worrying to me, the number of students studying information technology in Australia has fallen by between 40 and 60% in the last decade depending on whose numbers you look at. Likewise, enrollments in other hard sciences and STEM subjects such as maths, physics and chemistry are falling too. Enrolments in engineering have been rising, but way too slowly.
This is all while we have had a 40% increase in new undergraduate students as a whole.
Women once made up 25 percent of students commencing a technology degree, they are now closer to 10 percent.
All this in the middle of a historic boom in technology. This situation is an absolute crisis. If there is one thing, and one thing only that you do to fix this industry, it's get more people into it. To me, the most important thing Australia absolutely has to do is build a world class science & technology curriculum in our K-12 system so that more kids go on to do engineering.
In terms of maths & science, the secondary school system has declined so far now that the top 10% of 15-year olds are on par with the 40-50% band of of students in Singapore, South Korea and Taiwan.
For technology, we lump a couple of horrendous subjects about technology in with woodwork and home economics. In 2017, I am not sure why teaching kids to make a wooden photo frame or bake a cake are considered by the department of education as being on par with software engineering. Yes there is a little bit of change coming, but it's mostly lip service.
Meanwhile, in Estonia, 100% of publicly educated students will learn how to code starting at age 7 or 8 in first grade, and continue all the way to age 16 in their final year of school.
At my company, Freelancer.com, we'll hire as many good software developers as we can get. We're lucky to get one good applicant per day. On the contrary, when I put up a job for an Office Manager, I received 350 applicants in 2 days.
But unfortunately the curriculum in high school continues to slide, and it pays lip service to technology and while kids would love to design mobile apps, build self-driving cars or design the next Facebook, they come out of high school not knowing that you can actually do this as a career.
I've come to the conclusion that it's actually all too hard to fix  and I came to this conclusion a while ago as I was writing some suggestions for the incoming Prime Minister on technology policy. I had a good think about why we are fundamentally held back in Australia from major structural change to our economy to drive innovation.
I kept coming back to the same points.
The problems we face in terraforming Australia to be innovative are systemic, and there is something seriously wrong with how we govern this country.
There are problems throughout the system, from how we choose the Prime Minister, how we govern ourselves, how we make decisions, all the way through.
For a start, we are chronically over governed in this country. This country has 24 million people. It is not a lot. By comparison my website has about 26 million registered users. However this country of 24 million people is governed at the State and Federal level by 17 parliaments with 840 members of parliament. My company has a board of three and a management team of a dozen.
Half of those parliaments are supposed to be representatives directly elected by the people. Frankly, you could probably replace them all with an iPhone app. If you really wanted to know what the people thought about an issue, technology allows you to poll everyone, everywhere, instantly. You'd also get the results basically for free. I've always said that if Mark Zuckerberg put a vote button inside Facebook, he'd win a Nobel Peace Prize. Instead we waste a colossal $122 million on a non-binding plebiscite to ask a yes/no question on same sex marriage that shouldn't need to be asked in the first place, because those that it affects would almost certainly want it, and those that it doesn't affect should really butt out and let others live their lives as they want to.
Instead these 840 MPs spend all day jeering at each other and thinking up new legislation to churn out.
In 1991, the late and great Kerry Packer said ""I mean since I grew up as a boy, I would imagine, that through the parliaments of Australia since I was 18 or 19 years of age till now, there must be 10,000 new laws been passed, and I don't really think it's that much better place, and I would like to make a suggestion to you which I think would be far more useful. If you want to pass a new law, why don't you only do it when you've repealed an old one. I mean this idea of just passing legislation, legislation, every time someone blinks is a nonsense. Nobody knows it, nobody understands it, you've got to be a lawyer, they've got books up to here. Purely and simply just to do the things we used to do. And every time you pass a law, you take somebody's privileges away from them.""
Last year the Commonwealth parliament alone spewed out 6,482 pages of legislation, adding to over 100,000 pages already enacted. That's not even looking at State Governments.
In Australia, the average person in the street might think that the way that you get into the Prime Minister's office is by being elected by the people. Since 1966, this has only been true about a third of the time.
In fact, of the 15 Prime Ministers since Menzies, only five have come into the office via being elected by the people from opposition. Yes, only five since 1966. They were Gough Whitlam in 1972, Bob Hawke in 1983, John Howard in 1996, Kevin Rudd in 2007 and Tony Abbott in 2013.
The typical way to get into the Prime Minister's office in Australia is not by being voted in, but by stabbing the incumbent in your own party in the back. Or in the case of Malcolm Fraser, getting the Governor General to do your dirty work for you. That's how 60% of our Prime Ministers have gotten into office since we stopped using pounds Sterling as currency. It's crazy.
In the technology industry we had high hopes for number fifteen but it looks like we might be onto our sixteenth very shortly.
I say it looks like we might be onto number 16 shortly as the Australian government is currently in the grips of a major political crisis. A crisis for the absurd reason that a large number of our politicians do not know they were a dual citizen of another country (or worse, they tried to hide it)! In Australia this is not allowed under section 44 of the Constitution. On almost a daily basis, members of parliament across the political spectrum have been found to be dual citizens of other countries. This has happened to such an extent that the coalition government has now lost its majority and is teetering at the brink of collapse.
The level of incompetence from these politicians that spend all day dreaming up rules about how we all should live our lives and standards to that our businesses must submit to is astounding, not to mention their parties. I would have thought that the first page of the ""So you want to be a politician?"" checklist that each party handed out to bright young recruits would have said ""Have you stolen any money? Are you a drug addict? Have you fiddled with any kids? Are you a citizen of another nation? Then the career of a politician probably isn't for you!"".
It's not like this hasn't happened before, either.
Now how the sixteenth Prime Minister will pick their team is completely crazy. The problem is section 64 of the Constitution. This is the part that says that federal Ministers  members of the executive  must sit in Parliament. This is nuts.
Not so long ago the former Minister of Trade for Indonesia, Tom Lembong, visited my company. Tom's entire career has been in private equity and banking. He'd never been in politics before- Jokowi simply asked him to be Minister of Trade. Similarly the Minister for Communications, Rudiantara, spent his entire career running telecommunications companies. In Indonesia they vote for the President & Vice President, and then separately for the legislature. The President can pick his own team for the executive. This is how you get good people in government, because you can pick people with real world domain expertise to run a portfolio. In Australia we end up with lawyers, evangelicals or career politicians. People who don't have a clue about their portfolio. Imagine trying to run a company, but instead of of being able to pick the best engineer to be Vice President of Engineering, you have to pick it from a pool of lawyers, crazy people or card carrying political hacks. How can we have a science, technology and engineering focused agenda, which the country critically needs, when this is how cabinet gets chosen?
Then we have the problems that are a result of regulatory duplication, confusion and duplication of responsibilities or the mindless populism of absurd policies of the State Governments. Here I think we have some of the biggest problems.
I ended up doing Electrical Engineering completely by accident. I went to one of the best private schools in the country. When I graduated, at careers day, nobody talked about engineering. In fact, nobody even mentioned the word engineering throughout my entire schooling. I honestly thought it had something to do with driving a train.
I was disheartened to go back to that same school, Sydney Grammar, to talk at careers day. The students still thought that engineering had something to do with driving a train.
This is completely nuts, when I told the students that by working in engineering you get to design satellites, self driving cars, virtual reality helmets, design rockets like those SpaceX will one day send to Mars or build the next Facebook, many in the room got excited. Just they didn't have a clue how to head towards a career in engineering because it wasn't mentioned once to them in thirteen years of schooling. It's not just my old school, almost all the schools are like this.
So how do you fix K-12 education in this country so that we can drive innovation in the future? It's the remit of the bureaucracy of the State Governments.
Trying to get them to all agree to modernise the economy is an exercise in futility. Since taking power, the NSW Government has sold 384 Department of Education properties. That is despite leaked Department of Education documents that report NSW is facing an influx of 15,000 school students a year, and will require $10.8 billion in funding for 7,500 new classrooms and buildings over just 15 years.
If you look at their profit & loss statements you'll see the bizarre way in which State Governments think.
The biggest revenue generator for NSW is payroll tax. In NSW companies pay $8.4 billion dollars as a result of this idiotic tax which is basically a penalty imposed on you for hiring a lot of people. $8.4 billion that could be better used employing more people. If I hire a lot of people, I should get a discount, not a penalty.
The second is stamp duty & land tax. NSW collects $7.8 billion of stamp duty. This is a tax that simply makes it expensive to transact. The stamp duty on an average house in Sydney is $42,000, or about 70% of the average NSW citizens' post tax annual income. The average person has to work for most of year just to be able to transact in the housing market. The illiquidity this tax causes will be one of the biggest pain points behind a housing crash.
The State Government then tries to build a road between all these apartments, and because property and construction costs are too high, Westconnex, a 33 kilometer road, will cost between $20 and $40 billion. Trump's wall, which is 1600 km long is costed at around $15 billion.
When the NSW government proposes to build a 14 kilometer tunnel to Manly, it's costed at $14 billion dollars. That's $1 million dollars per metre just to build. At $14 billion, that's about the same price Gotthard tunnel cost, which is the deepest and longest tunnel in the world which goes for 57 kilometers under the Swiss Alps, 2.3 km below the surface of the mountains above and through 73 different kinds of rock at temperatures of up to 46 degrees. Yet a tunnel to Manly costs New South Wales the same price.
This is the absurdity of how State Governments think and operate.
Something is clearly very wrong.
New South Wales also collects $2.4 billion in fees for access to roads, and fines for actually using them. Fines which are erratically enforced through the strategic placement of cameras in areas of maximal revenue, random busts on jaywalkers, through to the ridiculous 350% increase in fines on cyclists for not wearing a helmet, when all the public health policy globally says it's better to have your citizens ride bikes and get healthy.
It's so absurd that in NSW a kid riding home on his bike without a helmet now gets fined more ($319) than the speeding driver doing almost 80kms/hr in a 60 zone that ran over him ($269).
Of course, this gets sold to you again under the banner of ""health and safety"". But that's all a load of crap. The only health and safety it's ensuring is the health and safety of government finances.
This is why I wouldn't hold your breath for the deployment of electric cars in Australia. State governments will get a rude shock when all of a sudden car ownership collapses and there are no more fines from speeding, red light cameras or poor driving, let alone a crash in fees from parking meters and parking levies. State governments simply won't let it happen. They'll also find an excuse to still stop and search your car even though driving under the influence won't be an adequate excuse anymore.
Why is this important? Well if you are trying to attract young smart people to come back to Australia to join the technology industry, it's a bit hard when the hashtag #nannystate is trending on Twitter.
After that, all you are left with of any size are gambling and betting taxes. In NSW this is $2.1 billion. The NSW Government is so addicted to gambling revenue that it has shut down most of Sydney's nightlife in order to boost this line item by funneling people into the casino or pokies rooms, which has the added benefit that they can turn those entertainment areas into apartment blocks for more stamp duty & land tax.
Again, of course, the general public has all been taken for fools because once more it has been sold to you under the guise of ""health and safety"". It's a bit hard to enact structural change in the economy by building a technology industry when every second twenty year old wants to leave because you've turned the place into a derelict bumpkin country town.
A little while ago I was sent an essay by Paul Graham of YCombinator, the greatest technology incubator in the world entitled ""How to make Pittsburgh into a Startup Hub"". The main thesis of this essay was to make it somewhere that 25-29 year olds want to live  build restaurants, cafes, bars and clubs- places that young people want to be.
About young people he said:
I've seen how powerful it is for a city to have those people. Five years ago they shifted the center of gravity of Silicon Valley from the peninsula to San Francisco. Google and Facebook are on the peninsula, but the next generation of big winners are all in SF. The reason the center of gravity shifted was the talent war, for programmers especially. Most 25 to 29 year olds want to live in the city, not down in the boring suburbs. So whether they like it or not, founders know they have to be in the city. I know multiple founders who would have preferred to live down in the Valley proper, but who made themselves move to SF because they knew otherwise they'd lose the talent war.
He then went on to say:
It seems like a city has to be very socially liberal to be a startup hub, and it's pretty clear why. A city has to tolerate strangeness to be a home for startups, because startups are so strange. And you can't choose to allow just the forms of strangeness that will turn into big startups, because they're all intermingled. You have to tolerate all strangeness.
Sydney will never be a technology hub if all the young people want to flee overseas.
You're kidding yourself if you think they are going to come back one day. In the last 18 years that I have been running technology companies in Australia, out of the scores that have left I'd estimate that less than 10 percent come back. They are at the time of their lives where when they go overseas they usually meet a boy or a girl and eventually settle down.
Not so long ago the topic of Innovation was discussed on ABC's Q&A.
Stephen Merity asked: ""I'm an Australian programmer working on machine learning and artificial intelligence in San Francisco after studying at Harvard. I want to return to Australia but I fear it won't ever be the right choice. Research and educational funding has been slashed, the FTTP NBN has been abolished, and my most competent engineer friends have been left with the choice of leaving home for opportunities or stunting themselves by staying in Australia. Even if all that was fixed, it's not enough to just prevent brain drain, we need to attract the world's best talent to Australia. Does the Liberal government truly believe their lacklustre policies can start fixing this divide?""
The response from Labor's Ed Husic was ""Okay. So on the issue of the brain drain, you can take it two ways. Obviously you can, as Stephen was saying, there is some negative factors that drove him away and I've had a father email me of a son who said ""I had to leave because I didn't have opportunities, I had to go elsewhere to pursue"", in terms of his science career, you know, pursue opportunity elsewhere. I actually also see the positive in that, you know, a lot of the start-ups, a lot of people that are moving overseas are pursuing opportunity to grow and they're going to gain experience and potentially come back and replenish our pool. The key for us is if people are leaving, what's being done to backfill the places? What's being done to replenish the talent pool?""
This is like a business saying well we have no customer retention because our product is crap, so let's go find some new customers.
I taught Stephen Merity here at the University of Sydney. He also worked for me at Freelancer. He's one of the top graduates in computer science that this University and country has ever produced. He's never coming back.
What about trying to attract more senior people to Sydney?
I'll tell you what my experience was like trying to attract senior technology talent from Silicon Valley.
I called the top recruiter for engineering in Silicon Valley not so long ago for Vice President role. We are talking a top role, very highly paid. The recruiter that placed the role would earn a hefty six figure commission. This recruiter had placed VPs at Twitter, Uber, Pinterest.
The call with their principal lasted less than a minute ""Look, as much as I would like to help you, the answer is no. We just turned down [another billion dollar Australian technology company] for a similar role. We tried placing a split role, half time in Australia and half time in the US. Nobody wanted that. We've tried in the past looking, nobody from Silicon Valley wants to come to Australia for any role. We used to think maybe someone would move for a lifestyle thing, but they don't want to do that anymore.
""It's not just that they are being paid well, it's that it's a backwater and they consider it as two moves  they have to move once to get over there but more importantly when they finish they have to move back and it's hard from them to break back in being out of the action.
""I'm really sorry but we won't even look at taking a placement for Australia"".
We have serious problems in this country. And I think they are about to become very serious. We are on the wrong trajectory.
I'll leave you now with one final thought.
Harvard University created something called the Economic Complexity Index. This measure ranks countries based upon their economic diversity- how many different products a country can produce- and economic ubiquity- how many countries are able to make those products.
Where does Australia rank on the global scale?
Worse than Mauritius, Macedonia, Oman, Moldova, Vietnam, Egypt and Botswana.
Worse than Georgia, Kuwait, Colombia, Saudi Arabia, Lebanon and El Salvador.
Sitting embarrassingly and awkwardly between Kazakhstan and Jamaica, and worse than the Dominican Republic at 74 and Guatemala at 75,
Australia ranks off the deep end of the scale at 77th place.
Australia's ranking in the Harvard Economic Complexity Index 1995-2015. Source: Harvard
77th and falling. After Tajikistan, Australia had the fourth highest loss in Economic Complexity over the last decade, falling 18 places.
Australia keeps good company in the Harvard Economic Complexity Index at position #77. Source: Harvard
Thirty years ago, a time when our Economic Complexity ranked substantially higher, these words rocked the nation:
""We took the view in the 1970s  it's the old cargo cult mentality of Australia that she'll be right. This is the lucky country, we can dig up another mound of rock and someone will buy it from us, or we can sell a bit of wheat and bit of wool and we will just sort of muddle through ... In the 1970s ... we became a third world economy selling raw materials and food and we let the sophisticated industrial side fall apart ... If in the final analysis Australia is so undisciplined, so disinterested in its salvation and its economic well being, that it doesn't deal with these fundamental problems ... Then you are gone. You are a banana republic.""
Looks like Paul Keating was right.
The national conversation needs to change, now.
",120
https://keepingstock.net/my-first-two-months-trading-stocks-with-robinhood-9580af92532c?source=tag_archive---------0-----------------------,My First Two Months Trading Stocks with Robinhood,How I made a few bucks on Wall Street with minimal effort and very little understanding of how the actual stock market works.,Jeff MacDonald,8,"UPDATE 10.22.21:6 years & 1M+ views later. Back in 2015 I posted about my experience trading on Robinhood, I never knew this article would take off like this. Thanks again for reading.
Hello there Receive a free stock when you sign up!
Hello there Receive a free stock when you sign up!
To get a Robinhood account you'll sign-up through the app After you've signed up, you'll need to connect Robinhood to a bank account in which you'll fund your brokerage account with. I initially put in $100 just to test it out. Once you make the first transfer it will take about three days for your money to appear in the app ready to trade. Trading in the app is as easy as liking someone's post on Facebook. You can try doing some research inside that app about a particular stock, but I recommend using a mixture of tools to find out who to invest in.
Overall, the process is made as simple as it could be considering you now are a stock trader, with all the power to make and lose fortunes.
Unfortunately it appears that Stockflare.com shutdown, so I'm currently looking for a replacement, let me know what you use by leaving a comment! 4.23.20
Once your account is funded, the homework starts. I spent the first three days, while I waited for my money to clear, researching what stocks I wanted to buy. The site I found most helpful was stockflare.com. At Stockflare you can easily see performance of stocks based on their 5-star system:
You can sort stocks by these different variables, as well as filter what sectors and trading styles. After some research my first buys were for Kroger ($KR) and Petroleo Brasileiro ($PBR). Kroger did very well and I still hold it; Petroleo, not so much. After my first go at it , I decided to diversify my research a bit. I started reading CNBC and keeping an eye on other market trends. I added Stocktwits.com to my research stack as well. Stocktwits allows you to see what other hobby traders are talking about with stocks. You can enter any company and see the sentiment of the group and see how it's changed over time. The community is very active and you can get into some great conversations through the site. [UPDATE 11.23.15:Robinhood has partnered with StockTwits to add the ability to trade right from within StockTwits. Read more about it here.]
So far, I've made $12 bucks from my investments, or a 4.9% return. [UPDATE 03.12.15: In my 3rd month of trading. I got it as high as 8.26%. My secret: Not trading. I'm just playing the long game.] Not good, but not bad for my first shot out of the gates. I've made some missteps along the way, looking at you Box.com ($BOX), that have eaten into my overall earnings. This is way more fun, though, than just sitting and staring at your money in a savings account.
You will not become Gordon Gekko using this app. At least not right away. I learned a few things about the stock market while using Robinhood; some not so pretty things. First off, the big guys are the big guys because they have the money to stay on top. When Shake Shack's ($SHAK) IPO started a buzz during January, I became really excited about the possibility of getting in early on a stock I knew would succeed. I did my research and saw that trading on SHAK would start at around $25 a share. The morning of their IPO Shake Shack traded in Robinhood at nearly $50 a share. That's because the big guys at the hedge funds bought up early shares that drove the price then dumped it onto the little guys at double what they got it for. That's how it works, and unless you have millions of bucks, you won't beat the system.
The other downside is you can't trade a stock and immediately use your profits to buy up another. They sit unusable as ""unsettled funds"" for as many as three business days. Here's how Robinhood addresses it:
Unsettled Funds: Cash from the sale of stock that the buyer has yet transferred to the seller. This transfer is part of the settlement process, and may take up to 3 business days.
Last I checked, they were actively working to make this time faster.
BIG UPDATE 2.23.16:On the one year anniversary of this article, Robinhood announced Instant. Which removes the three day waiting period and allows you to reinvest immediately after selling your stock. You can also invest up to $1000 in stock immediately after depositing from you bank. Goodbye 5 day wait!
Before you begin trading with Robinhood, I recommend reading their FAQ to make sure you understand how it works.
11.1.17  Robinhood has announced a new desktop interface that brings its app more inline with other trading sites like E*TRADE. While a desktop experience will make it easier for people to trade while they research, it also brings a whole slew of new updates to the platform. These changes start to show Robinhood's larger strategy to become the one-stop-shop for your trading needs.
Most of the new features launching in 2018 will focus on the more social aspects of the platform, such as ""People Also Bought"" and ""Price Paid"", which will help highlight trades other users are making on the site. Features like ""Analyst Rating"" and ""News and Fundamentals"" start to move into CNBC, StockTwits, and Stockflare's territories. I look forward to getting into the beta to see more details, but until then check out the screenshots below and the official announcement on Robinhood's blog.
1.25.18  Robinhood is going to get into the crypto game, and they are going to make it easier than Coinbase, and without the fee! Normally Coinbase charges 1.5 to 4 percent fees in the US. With this update in February users will be able to trade Bitcoin and Ethereum, as well as track 14 other cryptocurrencies. Trading will start in waves with California, Massachusetts, Missouri, Montana, and New Hampshire. More states added soon after that. Included is a screenshot, I love the ""Don't Sleep"" mantra.
It's actually pretty brilliant, like Superman 3 penny scheme brilliant. They make interest off the uninvested money left in people's Robinhood accounts. I assume every brokerage does this since it's the basis for how banks make money, but I thought it was smart.
You can read more here about their plan to offer margin accounts and other plans to generate revenue.
Robinhood is not the first brokerage to do zero-commission trades. Others have tried and failed after not finding enough revenue, but Robinhood appears to be on track to succeed. They've made trading stocks as easy as using Instagram or Twitter. That's not a good thing, according to the pros. In a September 2014 Marketplace.com published an article named, The Trouble with Robinhood's No Commission Trading App, that described how mobile trading is leading to bad investment strategies. When buying or selling a stock is made so convenient, investors tend to make rushed, poor decisions rather than waiting to do their homework.
As for me, well, it's something short of an addiction. It's so easy to keep that app open and to check it throughout the work day. I find myself at lunch cheering on the little green highlights in the app as my stocks rise a few pennies. I'm not a serious trader, and I think I'll probably wait to invest any serious cash into Robinhood, but it's been a great learning experience. Never before have I been so educated on the economy and up-to-date on business news. That for me, has been the biggest win so far.
Robinhood may not be the best option in the long term to stock market investing, but it does bring the barrier of entry to an all time low. A new generation of investors who are use to simplicity and on-the-go tools, now, have a pretty slick app for investing. Robinhood's biggest legacy might be how it influenced an industry that is big, bloated, and not easily changed to become fast and mobile.
Hello there Receive a free stock when you sign up!
Thank you so much for reading. Please follow me on Twitter and Medium to hear more about my perspectives on technology and advertising.
the cashflow stories that matter.
1.5K 
24
1.5K claps
1.5K 
",121
https://entrepreneurshandbook.co/this-is-what-it-takes-to-go-from-0-to-1-million-in-less-than-one-year-7ac31bc39eef?source=tag_archive---------2-----------------------,What It Takes to Go from $0 to $1 Million in Less Than One Year,"There's no ""secret sauce""  it just requires going through these same boring steps",Stephen Moore,6,"I came across an individual who figured out how to start a successful business from zero multiple times.
No resources, no capital, no investors.
His name is Michael Sherman, a Long Island native. He launched his latest business LetterDash in July 2018. One year later, he says his business is on track to pull in over $100,000 in revenue in July 2019.
Previously, he started a handful of other businesses, including Qualified Impressions, which reached $2M per year before decline, Penalty Be Gone, $500,000 before decline, and Great Agencies, $100,000 per year.
I was intrigued to know what his process looks like: what, exactly, does he do to build these companies?
So I reached out, and picked his brain over a chat. Here's what he said.
When I ask Michael to define what he does, he says that he's not sure what to call his ""profession.""
Before becoming what he is today, he gave the 9-5 a shot  about 10 times.
His resume of jobs is varied. In no particular order, he has done door-to-door sales selling vacuums, worked at Home Depot and McDonalds, became a licensed investment banker, tried his hand at computer tech, and even became a licensed bartender.
His education is no different. Michael transferred in and out of a total of five different universities before finally graduating, and that was all after dropping out of high school.
Nothing gave him the excitement or passion he craved. So he took a different path  entrepreneurship  and never looked back.
Each business Michael built achieved a healthy profit  and he did that with nothing more than his brain, a laptop, and a couple of Google Ads coupons.
His latest company, LetterDash, an on-demand legal letter sending service, just turned one year old. Michael shared some of the financial figures from the year so far 
But Michael has gained far more than just money from LetterDash. The experience has been full of learning, and below he shares the lessons you can learn from his journey.
When he started LetterDash, he didn't need or want to raise capital. He explained that raising capital only adds layers of complications and pressure.
""These pressures come in many forms  pressure to grow fast, pressure to hire, pressure to put up numbers.""
And this pressure can lead to bad decisions, he warns.
In the end, he started the business with a bank account of zero, no framework, and no plan.
There was nothing but an idea. He says that all you need in the early days is the commitment to grow from there.
Michael learned this lesson the hard way. He invested heavily in building software applications for previous companies and all of the businesses failed miserably.
He had learned that the software produced was never the problem. The problem was that he had focused solely on his vision and on how it would be executed, before figuring out who the software was for.
In the case of LetterDash, he says that the software they would have developed would have automated the interactions between the client, LetterDash, and the attorney.
But now that the company is up and running, he sees that it would have been completely useless, and was able to dedicate resources elsewhere.
His advice?
""Get customers first, build your software later. It's cool to say you have software, it's much cooler to say you have revenue from paying customers.""
Getting to a million is never easy. It might be easy for the Elon Musks of the world, those with deep connections and vast resources, who can spend millions in advertising and infrastructure.
For all the Michael Shermans out there, it's easier said than done.
He said his approach to building LetterDash was boring and straightforward.
""We spent about 15 minutes of keyword research using the Google Ads Keyword Tool. There was enough search volume being reported to warrant the green light and test the idea. We came up with a business name, found a cheap .co domain and threw up a very basic, yet credible looking website.""
This is a point worth emphasizing. The purpose of the website is to generate leads and validate demand. If both of these are ticked, there is scope to build bigger down the line.
""We spent about $100 on web development. We found a Google Ads coupon, I believe it was $100 free for spending $25, we put together a barebones ad campaign and started sending traffic to the site. Twenty-four hours later, we received our first client request. A week later, we had a dozen requests.""
This was the indicator he was looking for, and he was now ready to hire some attorneys and begin scaling.
""There wasn't a single action or moment that led to success or the $1M in revenue. I'd say there might have literally been a million little tweaks and ideas tested along the way.""
Michael says that as they received customer feedback, they began to analyze the success of the letters being sent out by one attorney to another. As they went through responses to the attorneys' letters, they continuously tested newideas.
""We're constantly tweaking and optimizing the letterheads, the shipping method, the packaging, the presentation, even the language used by the attorneys in the letters.""
That same customer feedback led to the formulation of new products and ancillary services that significantly boosted company revenue, without having to worry about new user acquisition costs.
""Listening to the customers, what they want and what they need after the primary service was delivered has been key.""
Customers are everything. Their feedback and word of mouth are critical to expanding your client base, and in being able to improve and expand your service.
With LetterDash, customer service has played a major role in the growth of the company.
""The many 18 hour workdays and time we spent focusing on each tiny thing to make sure the customer was blown away by the service have all been a major factor in our explosive growth and ability to reach that $1M milestone,"" says Michael.
The biggest lesson here is to focus on customers' feedback and to use it to improve your service. You must always be focused on making sure every customer is a satisfied one, even when in reality that likely cannot be achieved.
You might have noticed that the advice Michael offers is nothing unique. You've likely seen a lot of it before. And that's exactly the point. As he said himself, it's boring and straightforward.
But this means it's replicable. It's a process that anyone can follow to achieve similar successes.
The key takeaway here for entrepreneurs is this: Don't focus or obsess on anything other than validating demand for your product or solution. In the end, nothing else matters if you don't have a market.
If you want to read more of my work, or the work of the many, many great writers on this platform, consider becoming a member. Your membership fee directly supports the writers you read.
How to succeed in entrepreneurship
22K 
63
Thanks to Michael Thompson, Nico Ryan, and George J. Ziogas. 
22K claps
22K 
",122
https://survivingtomorrow.org/tesla-is-dead-and-elon-musk-probably-knows-it-2858c86589d0?source=tag_archive---------3-----------------------,Tesla Is Dead (And Elon Musk Knows It),"The $600+ billion company is a game-changer, but it won't exist in 50 years",Jared A. Brock,8,"I will never forget the first time I drove a Tesla Model X. My producer rented one when we met up with a movie star to record narration for a film I was directing. ""This better not be tacked onto the film budget,"" I griped.
He grinned and tossed me the Tesla-shaped key. ""It's your birthday present.""
I dropped the body to its most ground-hugging setting, set the acceleration to Ludicrous Mode, and roared out of the airport. It was one of the most exhilarating rides of my entire life  almost as fun as the time I drove 150MPH with no plates and no insurance on a toll road as an idiot teenager.
Driving a Tesla X is a pure pleasure, but it doesn't mean Tesla Inc. will survive.
In fact, forces are aligning that could easily wipe Tesla off the map. Here are seven reasons why Tesla probably won't exist fifty years from now:
As professor Scott Galloway recently pointed out, if you subtract Tesla's Bitcoin ponzi profits and emissions credits, Tesla actually loses money:
""Tesla posts an accounting profit, but in its most recent quarter, it was emissions credits (a regulatory program that rewards auto companies for making electric rather than gas vehicles) and  wait for it  $101 million in bitcoin trading profits that morphed earnings from a miss to a beat. What Tesla did not do last quarter was produce a single one of its two premium cars, the Model S or the Model X.""
Losing money doesn't seem to worry speculators during peaks of irrational exuberance, but when the rubber meets the road and the stock bubble pops and corporate credit constricts, real investors will want no part in money-burning businesses.
And it won't take a full market meltdown for Tesla to become a money-losing entity: If the global crypto ponzi bubble pops due to more countries banning or regulating it, or regulators do away with emissions credits, Tesla once again becomes a money-bleeding company.
One thing you've got to appreciate about Elon Musk is that he's voraciously curious and wants to solve some of humanity's biggest challenges.
But that's not who you want as CEO of a publicly-traded company.
One of the reasons you don't see most Fortune 500 CEOs on Joe Rogan and SNL and, you know, running five other companies, is because they're heads-down focused on running one company. When he ran Disney, Bob Iger woke up at 4:15 AM every day. Apple's Tim Cook gets up at 3:45 AM and reads 800 emails. Elon Musk also puts in absurd hours  I personally question if sleep deprivation is what rational shareholders are looking for in any CEO  but in Elon's case, it's spread across too many projects to be sustainable for decades to come.
Have you ever heard of Dan Schulman?
Me neither.
He's a former AMEX guy, now the CEO of Paypal.
Elon is brilliant at getting out early and pivoting hard.
He did it with Zip2, and then Paypal, and now he's putting out feelers to do it with Tesla:
SpaceX.SolarCity.Hyperloop.The Boring Company.Neuralink.BTC and DOGE. (Side note: Elon knows he's the king memer and could easily add $100 billion to his net worth by launching his own altcoin.)
It's only a matter of time before one of these side hustles takes off and he steps down as Tesla's CEO, if only because...
Elon once again put Tesla in the crosshairs when he started manipulating the cryptocurrency markets.
Never forget how close he came to getting banned from leading a publicly-traded company by the SEC.
If he keeps up these sorts of shenanigans  and he needs to in order to keep the stock price pumped  it's only a matter of time before government regulators and progressive politicians renew their efforts to rein him in.
Speaking of lawsuits: There are already rumblings that his SNL Asperger's announcement should have been disclosed to investors  when the stock tanks, expect to see this admission somewhere in the shareholder lawsuit, whether it's fair grounds or not.
Cue the angry comments from hodlers. (But please note that I automatically delete comments if the poster doesn't disclose their TSLA holdings.)
As a sound investment, $TSLA stock is one of the worst picks in the world. As a fun gamble/speculation, it's one of the best. But, just like Bitcoin, small investors are going to lose hundreds of billions of dollars when the price bubble pops.
Because let's face it: Tesla is a story stock.
Don't believe me? Just look at who's been buying shares:
Tesla stock is clearly being pumped by unsophisticated investors who haven't done their due diligence regarding the company's actual long-term worth.
The end result: When thousands of Tesla speculators lose their life savings, many will turn their backs on the company, if not become actively hostile.
First, we need some context. The price-to-earnings (P/E) ratio is considered the benchmark number for comparing one company's stock price to another. The ratio is based on the current stock price divided by the trailing 12-month earnings per share. If a stock price is $10/share, and the P/E ratio is 10, it means that company is earning $1 per share. If you buy a $10 share with a P/E of 20, it'll roughly take you 20 years to break even.
Tesla's P/E ratio is currently over 600.
That's $0.99 worth of earnings for every $625 invested. Would you buy a business with an ROI of 0.001584%? Would you acquire a company that will take 600+ years to break even?
Cue the irrational exuberancers: ""But Tesla's future potential is huge!""
No, it's not, not compared to its current price. To fall in line with the S&P's historical averages and provide a reasonable rate of real return, Tesla would need to 40X its earnings. To provide a 10% annual return, it would need to 63X its earnings. Well over $2 trillion in annual revenue... 4+X more revenue than the largest revenue-earning company on earth. Not gonna happen.
Objectively, Tesla is wildly overpriced even compared to the overall market bubble. It's a double bubble  the overall market bubble + the Musk fanboy story stock bubble. Tesla may very well be 13Xs better than the average S&P company right now, but that just means Tesla's price bubble is that much more inflated once you scrub out all the irrational exuberance.
Tesla's market cap is currently over $600 billion. If it traded at the same P/E as Amazon  arguably one of the strongest companies on earth  Tesla's market cap drops to $60 billion. If you compare Tesla to Apple, which is a fair comparison and a far more rational P/E, it means that in reality, Tesla is probably only worth a measly $20 billion.
To put things in perspective, Tesla's market cap is currently higher than Mercedes, BMW, GM, Ferrari, and Ford, plus all the major airlines... combined.
But does Tesla have more customers, wider distribution, better engineers, deeper pockets, and more political connections than the rest of the auto and airline industries?
Absolutely not.
All his major competitors have deeper capital pools, wider distribution networks, and far more customers. Musk has nowhere near the political power. And the innovation gap is closing rapidly. That's why Elon is constantly seeking new capital and pulling out all the stops to keep pumping the stock, even going so far as to manipulate people's psychology through stock splits.
Elon Musk has unquestionably (and rightly) created a Thucydides Trap in the automotive industry, but is Tesla really the Athens that can best Sparta?
The question is almost irrelevant because another company is about to out-Athens Tesla and stuff Elon in his own Thucydides trap:
When Apple releases an electric car  and you can bet your bottom dollar it will  we can safely assume it will rival Tesla for looks and coolness and will likely beat it on price, too.
Follow the money with me...
To be clear, Tesla is an amazing company at a $20 billion valuation, and if Elon can't keep the $TLSA stock price inflated indefinitely, an acquisition is inevitable. Never mind the bite in Apple's logo... someone could chomp Tesla whole.
I adore Tesla. Like Russia and HBO, it punches way above its weight.
I also like Elon, minus his market manipulation. He's an extremely important person in the carmaking space. I'll say it loudly: Elon Musk is the best thing to happen to the auto industry since Henry Ford. As a maverick agitator, he awoke the slumbering giants who'd happily relied on fossil fuel combustion for more than a century. We're better for having him.
But, in the same way that Paypal will continue to lose ground to companies like Wise and Stripe, expect Tesla to lose ground to Volkswagen and Apple and whatever innovators come next. If things play out the way I predict regarding an eventual acquisition, fifty years from now Tesla probably won't even exist.
In the meantime, don't buy into the stock hype and endanger your family's future.
Just rent a Model X for a weekend and enjoy the ride.
Dear Space-Obsessed Billionaires: Please Stop
Bitcoiners Are Desperate For One Last Pump So They Can Dump
The World Is On Fire And The Fix Is Embarrassingly Simple
It's Time To Calculate Your Personal Inflation Rate
Follow Jared on Medium + subscribe to Surviving Tomorrow.
Will you survive tomorrow? Joins 1000s who get the free newsletter+podcast: jaredabrock.substack.com
8K 
115
8K claps
8K 
",123
https://blog.producthunt.com/ubers-credit-card-is-bankrupting-restaurants-and-it-s-all-your-fault-af76ea9ca46d?source=tag_archive---------7-----------------------,Uber's Credit Card Is Bankrupting Restaurants... and It's All Your Fault,"Uber is replacing local restaurants with big data, and they're coming sooner than you think.",Nick Abouzeid,10,"Listen to this story
--:--
--:--
To the excitement of credit card addicts everywhere, Uber announced its new Uber Credit Card last month. Needless to say, I was pretty pumped.
At first glance, this seemed like a natural progression of Uber's core business. Nearly every airline and hotel offers their own co-branded credit card, specifically built to lock-in frequent travelers with miles and points.
However, to credit card power users, Uber's rewards were baffling. With only 2% cash back on any Uber ride, there are a number of bank-owned travel cards that offer better cash-back bonuses than those offered by the Uber card:
One could write this off as a low-effort entrance into an established industry for this travel company. However, to do so would ignore Uber's true intention: to combine your restaurant spending data and location history to launch the first data-driven restaurant empire in the world.
Unlike other travel companies, Uber's immediate focus with this card isn't locking you into its platform. Uber wants your restaurant spending data. They want to know which restaurants you visit, which cuisines and dishes you prefer, and how much you spend every time you visit.
This isn't a new endeavor, but rather them doubling down on UberEats' recent success. Available in more than 120 markets globally, UberEats has grown by 2400% between March of 2016 and March 2017. Profits from UberEats have actually eclipsed ridesharing profits in markets like Tokyo, Taipei, and Seoul.
Uber already knows how you travel around your city, oftentimes better than than city planners. This dataset is powerful, allowing Uber to watch and predict where millions of people move around cities and neighborhoods around the world. This data can have a real world impact: Uber has already learned to charge wealthy users more per ride and classify riding patterns as one-night stands... the possibilities are nearly endless.
This new spending data will allow them to not only see where you go every day, but what you do once you arrive. Uber will be able to watch various neighborhoods travel into Chinatown for dinner, or into The Mission for al pastor tacos and drinks next door. They'll know exactly which coffee and dessert places are popular with each demographic. They'll know exactly which restaurants in every neighborhood get you to spend far too much on pizza, locally-brewed IPAs, and fries dipped in chipotle aioli.
The evidence for this theory lies in the new Uber Credit Card incentives.
Instead of locking in card users as Uber riders with bigger rewards (both up-front with higher cash-back percentages or on the back-end as ""bonus redemption"" categories for accrued points), the Uber Credit Card gives out an industry-leading 4% back on any restaurant purchase (including UberEats!).
Not only is this card sold as your best option for dining out, it beats nearly every credit card on the market for restaurant rewards spending:
The US Bank Cash Card was the only card with no annual fee that beats the Uber card, with a stunning 5-5.5% back on fast food spending. I've never seen one of these cards in real life. (side note: The art of churning, or credit card reward maximization, is a very deep rabbit hole to jump into. I'm sure schemes exist that give 4%+ back on restaurant spending, but none apart from Uber's card are widely accessible to non-churners).
Uber's insistence on beating the market rate for this spending data (by paying more than nearly every other card's rewards) lends credibility to the argument that Uber really wants your credit card data.
They've actually already tried to acquire this unique data set... twice. In late 2016, Uber began asking for permission to track your location even when you're not using the app. However, this controversial initiative was killed this August, perhaps in anticipation of the Uber Credit Card announcement.
In addition, Uber just reintroduced ""Local Offers,"" a way for users to get free rides by linking their Visa card to their Uber account. This new program transparently pays consumers for their transaction data: it's no coincidence this was brought back only two months before the Uber credit card.
Edit: Uber publicly denied that they would receive an individual's spending habits from the Uber card. However, that doesn't discount their previous attempts to acquire this data and don't prevent them from acquiring this data in the future from the card or other avenues.
Ok, so what? Cool graph, but why does Uber want it?
Uber wants to own the entire restaurant industry. Armed with data about your eating habits, Uber will be able to predict and satisfy your every craving.
This is a huge and growing market: the amount of food dollars spent on restaurants and take-away has increased from 34% in 1974 to 50% in 2014. Avocado toast sales jumped 5200% between 2014 and 2016, according to a recent post from Square. Stereotypes are real, it seems.
This isn't a new business strategy: companies have always fought for control of your food purchases because of something we'll call the ""while I'm here"" effect. When consumers visit a store to purchase something they need (like food), they're far more likely to purchase ancillary products.
Retailers like Walmart have leveraged this effect for years. When Walmart's growth stalled in 1988, founder Sam Waltman started selling groceries: by cutting prices on everyday food items, Walmart was able to decimate grocery chains and drive sales of high-margin, everyday items like socks, toothpaste, and laundry detergent.
This theory works for online businesses as well. The success of Amazon's recommendation engine has alone been the subject of Fortune Magazine articles, and Jet.com's success relies on your inability to say ""no"" to one-time discounts on thousands of items based on what you've already added to your cart. Surprise, surprise: Walmart purchased Jet.com for $3 billion in 2016.
Uber has already made moves in this direction, starting with Uber-affiliated ""ghost kitchens""  virtual restaurants with no retail location that sell food exclusively through food delivery apps. No expensive rents, wait staff, or tables for customers: just a kitchen and an iPad with WiFi.
This is not a new concept - early this year, TNW investigated Green Summit, a NY-based startup that now operates four kitchens (three in NYC, one in Chicago) that support 16 different restaurants, available exclusively on UberEats competitors Seamless and GrubHub.
Uber has already launched ghost restaurants in San Francisco, Chicago, Atlanta, Denver, Philadelphia and Toronto, according to Elyse Propis, UberEats' program manager of restaurant innovation.
Instead of owning the kitchen, Uber partnered with local establishments to cook and fulfill the orders placed at Uber-owned ghost restaurants. Starting in 2016, one of their trial restaurants brought in ""about $2,000 per week in sales"" and surging ""up to $5,000 per week during [special events] like the recent Mayweather-McGregor fight.""
This extra revenue can represent a significant sum for small restaurants, who can eliminate many of the costs associated with running a food establishment, like rent for a large seating area and wait staff to bus tables and take orders.
Uber isn't alone. Deliveroo, which has raised almost $1 billion since their inception in 2014, was caught planning to secretly open 150 ghost restaurants across the UK by the end of 2017. Their ghost kitchens operate out of shipping containers at industrial sites hidden under overpasses.
The next wave of delivery restaurants won't be on Main Street: they'll be tucked away into hidden kitchens in the backs of nondescript buildings.
I like cheap delivery food. Why should I be worried?
From a restaurateur's perspective, these restaurants are powerfully efficient. The front of the house in any restaurant is expensive and space-inefficient, especially as wages and rents are increasing in cities around the world. Ghost restaurants can deliver the same food at a lower price point.
From a consumer's perspective, ghost restaurants offer certain advantages as well. Hyper-targeted cuisines based on the food you were looking for already is enticing - as new trendy dishes are discovered, delivery restaurants will be able to rapidly change their menu and brand based on customer demand. Instant restaurant brand, menu, and price A/B testing is finally possible.
However, there are downsides to this model. As most ghost kitchens have no address, their restaurants are almost never represented on review platforms like Yelp or Foursquare. Users are left powerless to report sometimes legitimate concerns about a meal (i.e. undercooked meat), and in-app review systems become meaningless when the platform owner has a financial incentive to promote certain restaurants over others.
Restaurant owners could also skirt accountability by closing brands associated with public scandals. Chipotle couldn't change their name after the string of food-borne illnesses caused by tainted ingredients, but ghost restaurants could simply close up shop and relaunch instantly under a different name.
Lastly, there have always been large issues with trusting large technology companies with our data. Last week, Uber announced that 57 million accounts had been compromised by hackers in 2016, only disclosing the leak as part of a board investigation into Uber's business practices.
Uber can't be the only company trying to do this.
You're right. Uber could face steep competition from online restaurant directories like Yelp or Foursquare which have access to similar datasets. Amazon could make a move into this space as well, given their recent expansions into the food industry with the acquisition of Whole Foods.
Restaurant spending is only a portion of overall food spending, and Amazon's powerful inventory management platform is a powerful contender in the grocery space, especially with their newest 365 perfectly located distribution centers in every major city in the US.
However, no other platform can rival Uber's delivery speed and reliability, supported by their ~2.5 million drivers around the world. When a freshly-baked pizza can go from amazing to disappointing in twenty minutes, only the services with near-instant delivery can win. Amazon, Yelp, and Foursquare will be left in the dust without a powerful last-mile distribution network, which they don't have quite yet.
But wait, I like eating at restaurants. It's fun.
Uber won't be able to single-handedly kill the restaurant industry. Delicious food, combined with a wonderful setting and service will always draw in friends and families looking for a wonderful night out.
You also can't ignore the newest trend of ""instagrammable dining experiences,"" or restaurants that specifically build their service around helping customers take the perfect picture for Instagram or Snapchat. Smaller, niche restaurants will survive: the mediocre won't be tolerated.
Not many have been able to recreate that same social proof and content with delivery at scale: SUGARFISH, an LA-based sushi restaurant, packages and manicures their delivery orders in disposable, Instagram-ready bento boxes.
Chains with strong brand loyalty will also survive: it's nearly impossible to replace restaurants with signature dishes like In-N-Out, Starbucks, or McDonald's (side note: McD's now delivers via UberEats). Despite Uber's best wishes, they won't be able to replace an In-N-Out burger. It's here to stay.
Coffee shops and other working spaces also have a strong competitive advantage to Uber's ghost restaurants: free WiFi, good AC, and the productive attitude that comes with the low murmur of conversation and a latte. Delivery can't compete here, by definition: it's not fun to work out of your apartment.
So that's it? Uber just wants to start fake restaurants?
This dataset is far more powerful. Armed with your spending data, Uber could dig far deeper into the restaurant industry:
The possibilities are endless when you can predict consumer demand.
Yes, Uber would face steep competition from Amazon and other companies that already have powerful inventory management systems for goods that have a near-infinite shelf-life (e.g. books). Yelp and Foursquare could also compete in this space, armed with years of consumer culinary preferences.
However, none of these competitors have Uber's last-mile delivery network, a key component for successfully delivering goods that can go bad in minutes or hours. Amazon and Yelp are all investing in their last mile-delivery networks, they aren't quite there... yet.
Existing food delivery apps like Postmates, Grubhub, and Seamless could also compete. However, their datasets exclude in-house restaurant orders, weakening their predictive models, and their delivery networks are far smaller than Uber's.
What happens now? Should I use the card?
In short, yes. We already sell every piece of data about our lives. Facebook, Google, Amazon, Twitter, etc. already package and sell your data to businesses around the world who try to sell you more things.
As long as your data is collected ethically and stored securely, consumers benefit from the marketplace finding value in otherwise worthless data.
So... you might as well enjoy 4% back. You're already selling your transaction data to Chase: why not look for the highest bidder?
The place to discover your next favorite thing.
12.9K 
62
Thanks to Kevin Liu, Laz Alberto, Justin Hilliard, Kate, Zak Kukoff, Eli Levinson, Josh Vorick, and Niv Dror. 
12.9K claps
12.9K 
",124
https://medium.com/hackernoon/a-quick-starter-guide-to-using-leveraged-trading-at-bitmex-5383de4cb320?source=tag_archive---------4-----------------------,A Quick Starter Guide to Leveraged Trading at BitMEX,6 July 2019: AntiLiquidation.com is a BitMEX Anti-Liquidation Tool & Position Calculator. This free tool will save you up to 70% of your...,BambouClub,10,"6 July 2019: AntiLiquidation.com is a BitMEX Anti-Liquidation Tool & Position Calculator. This free tool will save you up to 70% of your capital by avoiding liquidation. It also enables up to 700x leverage via tight Stop placement. Use it to improve your PnL at BitMEX.
AntiLiquidation.com is configurable so you can input your own risk numbers. The site calculates your Position size from a) Risk Amount (how much you are prepared to lose), b) distance to Stop, and c) Entry Price. A checklist takes you through the structuring of a good trade that avoids all possibility of Liquidation and 'blowing up' your account.
There is a a Guide to accompany use of the AntiLiquidation tool: How to Use the BitMEX Anti-Liquidation Tool.
Other Guides:
How to Create Synthetic High Leverage at BitMEX with NO Risk of Liquidation
How BitMEX Stop-Losses Can Save you 70% of your Capital, and How to Set up a Stop
BitMEX provides a means to turn bear markets into a profitable trading opportunity. People say BitMEX is risky, which it is when you don't know what you are doing. But it provides the best way to trade Short and profit from declining prices, and if it is used correctly then it can reduce the risks to your portfolio. (The acid test of whether you trade on BitMEX responsibly is, while you might get Stopped out quite a lot, you never get Liquidated.)
The global crypto market reached its peak on 8 January 2018 of $828 billion, and then fell by 70% to $243 billion on 1 April 2018. People reacted in three ways.
a) Those who HODL'd (i.e. nearly everyone) lost 70%, assuming they matched the market. Most traders under-perform the market so they lost more than 70%.
b) Those who sold for USD in January did well  they preserved their portfolio value measured in USD and increased it in BTC.
c) Those who sold their portfolios for USD and shorted Bitcoin and ALTs though the first Half of 2018 made an additional fortune to the one they made in the run-up to 8 January.
So who was taking the greater risk, the HODLers or those who made the effort to learn how BitMEX works and how to Short on low leverage?
As an example, this Short trade made me a 500% profit at BitMEX in the Bitcoin panic on 7 March 2018, the day of the Binance API hack:
You can buy 1 Bitcoin ($11,670 at time of screenshot) with 0.1 BTC ($1,167) by buying a 10x leveraged position. You pay only 10% Initial Margin. You can also short the Bitcoin price (profit from a fall in its price) by Selling the Contract. The most you can lose is your Margin.
(Ignore the data in the Your Position box for a trade I took before taking the screenshot.)
Quantity: The quantity of the trade is $11,670. This is your position. But the money you place at risk is less than this, depending on what leverage you choose. The higher the leverage, the less you place at risk, but the greater the probability of losing it.
Cost: The cost is 0.1015 BTC i.e. $1,184.
Cost = Initial Margin+ (2 x Taker Fee)
Cost = (1/Leverage x Position) + (2 x Taker Fee)
= (1/10) x 11,670 + (11,670 x 0.0015)
= $1,184.50
This is the maximum you can lose. You lose the entire amount should the price fall by 10% from $11,670 to $10,500. If the price was to crash to $5,000 your loss is still limited to $1,167 which is the value of your Initial Margin.
Order Value: The value of your position is 1 BTC i.e. $11,670. (0.9889 BTC to be exact.) Fees are calculated on this amount.
Available Balance: This is how much you have available for trading. Cost must be lower than Available Balance to execute the trade.
Note that your profit can exceed 100%, indeed it is unlimited, but your loss is limited to 100% (i.e. $1,167) , however much the Bitcoin price falls.
Again, as with the Long, your profit can exceed 100% by orders of magnitude. (Profit is unlimited with Longs, but there is a theoretical maximum limit with Shorts which is the profit when the price has fallen by 100%, as the price cannot fall further and become negative.) Your loss is limited to 100%, however much the market rises.
This asymmetry (unlimited profit, limited losses) is the beauty of the BitMEX Limited Risk contract, which is a BitMEX innovation. Trading Futures Contracts on the CME or CBOT, for example, there is no such limited risk facility. With standard futures contracts the Exchange will Margin Call the client for Maintenance Margin to supplement his Initial Margin when the price approaches the Bankruptcy Price, and you can lose a lot more than your Initial Margin. So you would get a Profit/Loss Scenario like this for a Long:
The mechanics of the BitMEX solution are that BitMEX sets a so-called Liquidation Price a fraction above the Bankruptcy Price (in the case of Longs) or a fraction below the Bankruptcy Price (Shorts).
When the market moves adversely against your position and approaches the Bankruptcy Price, and breaches the Liquidation Price, the Liquidation Engine takes over your position and liquidates it automatically at market. It add any tiny profit made by the Exchange to the Insurance Fund, or deducts any loss made from the Fund.
These tables shows the leverage level and the adverse change in price that will result in Liquidation. The greater the leverage the smaller the adverse change in price that will cause a Liquidation.
Long: Liquidation price < Entry Price
Short: Liquidation price > Entry Price
The above tables show that Shorting is safer than going Long, in that a larger percentage change (and USD change) is required to cause Liquidation when you go Short than when you go Long, for a given level of Leverage.
The above tables also show that even with the minimum 1x Leverage there is a small but real risk of Liquidation when Long. But there is no risk of Liquidation when 1x Short.
The BitMEX Exchange offers Long and Short leveraged trades of up to 100x Leverage. Never use more than 25x because the difference between the Liquidation and Bankruptcy Prices at high leverage stacks the statistical odds against a winning trade. Don't worry about it at low leverage. This is explained in Why You Should Never Trade 100x at BitMEX: The Liquidation Price vs. Bankruptcy Price Gap Means you Lose
Example: you buy a $9,255 100x leveraged XBTUSD position with 1% margin, i.e. your stake is $93 of Initial Margin. Your Bankruptcy Price (Entry Price less 1% Margin) is $9,163 but your Liquidation price set by BitMEX is $9,240. The price just has to fall $15 (0.16%) from your Entry to trigger your Liquidation and 100% Loss. That is a trade for suckers. Trading with 100% leverage on a repeated basis (Long or Short) will inevitably result in losses. The BitMEX Insurance Fund wins. Its current holding is 6,909 BTC, or $65 million. That money came from salami-slicing the testicles of 100x bulls via the Liquidation Engine.
But you still want to try high leverage, right? For reasons explained in this essay How to Create Synthetic High Leverage, if you want to trade with leverage > 10x then don't do it with the Slider Bar. Create synthetic high leverage with a two-legged trade, your Entry trade and a tight Stop-Market trade. (Tight means close to your Entry Price.) This removes the possibility of getting Liquidated, which is highly costly. You might well get Stopped Out but this is less costly as you then make no charity payment to the Insurance Fund. Always avoid selecting high leverage from the BitMex Slider Bar. And always use a two-legged trade: you Entry trade and a Stop order.
To use Market or Limit is one of your most important decisions.
It is not widely known that BitMEX charges extremely high fees to takers (those who use Market tab in the screenshot) but actually pays market-makers to trade (those who use the Limit tab). A marker-maker is defined as someone who places a Limit order and does not take the market price to open or close a trade. For all Bitcoin contracts:
BitMEX fees for market trades are 0.075% of your total leveraged position (not just your margin) for both entry & exit.
Example:
Calculate total fees on a $1,000 trade with 100x leverage. i.e you pay $1,000 Margin to open a $100,000 position.
Total Fees = 100 [leverage] x $1,000 [Margin] x 0.00075 [Rate for Market order] x 2 [Entry + Exit] = $150
Fees are 15% of your $1,000 trade in the example. Why pay 15% for market trading when BitMEX pays you 5% for trading with Limit Orders? (An additional benefit of Limit trading is that your trading is likely to be less frequent and more disciplined and profitable.)
Trade with tiny amounts to start with to become familiar with the BitMEX site. Then you can increase your leverage as you gain competence. E.g. you might open a safe position of $100 at 2x Leverage which means your Cost is $50. This $50 trade is illustrated:
The most you can lose is the Cost: 0.005 BTC = $50. You lose that if the market price falls 33% to the Liquidation price of $6,693.
When you press Buy Market, this confirmation screen pops up. Your order is not placed until you confirm Buy in this screen.
I suggest these practices in making your first few trades.
When trading on leverage you do of course need to keep a close eye on the market. Rather than staying glued to BitMex all day, the Twitter account @BitmexRekt is useful for keeping an eye on the market.
Take this Tweet as an example:
This Tweet indicates the XBTUSD market has experienced a sudden drop. (If the Tweet says 'Liquidated Short on XBTUSD: Buy ...' then the market has risen quickly.)
When a Long position is liquidated it means the price has fallen and breached the Liquidation Price. The BitMEX Liquidation Engine then takes over the position and closes it by Selling 495,600 contracts at the market price.
1 XBTUSD contract = 1 USD on BitMEX. (Warning: Other Futures contracts on BitMEX have different contract sizes. E.g. the contract size of the ETH Futures Contract is 1 ETH, or about $800 at time of writing.) So the trader who got liquidated for 495,600 Contracts lost a position of $495,600. He does not, however, lose $495,600. The amount of his losses depends on the leverage he was using. The greater the leverage, the smaller the loss. With the maximum 100x leverage the loss is 0.5212 Bitcoin, about $5,700 or 1.15% of the $495,600 position.
That's it. Good luck.
#BlackLivesMatter
6.3K 
27
",125
https://medium.com/@ren-heinrich/i-made-3-million-in-crypto-these-are-the-26-rules-i-learned-dc1895b05bce?source=tag_archive---------9-----------------------,I Made $3 Million in Crypto. These are the 26 Rules I Learned.,I got to know blockchain technology at the beginning of 2016. That was also the time I bought my first cryptocurrencies. I watched tons of...,Ren & Heinrich,9,"I got to know blockchain technology at the beginning of 2016. That was also the time I bought my first cryptocurrencies. I watched tons of YouTube videos, read a lot of articles to learn about how blockchain and different consensus algorithms actually work, I read tons of whitepapers to explore different blockchain-based projects. While accumulating my knowledge, back then I also joined trading groups hoping to maximize my gains, believed anonymous people's recommendations on the internet, and invested my money blindly. I made losses and gains. Probably the profit I made is nothing compared with a lot of crypto trading experts in the space. I do not own a Lambo or a luxurious villa. But it helped me significantly in terms of improving my life quality. For example, I do not have to work 40-60 hours a week to make the ends meet. I can finally afford to buy a decent property for my family, I can take my kids to go for short holidays whenever there is a long weekend during the school semester and long holidays during summertime.
In this article, I summarized 26 points or rules which helped me in making profits.
1. Know the asset you are investing in. If you decide to buy Bitcoin, read about Satoshi Nakamoto's white paper first, learn how blockchain technology works, what is a public and a private key, what is proof of work, etc. If you are buying Ethereum, learn about smart contracts, how initial coin offerings and decentralized finance platforms work, etc. There are a lot of free resources on the internet with easy-to-understand explanations.
For more information, check out my big step-by-step guide about how to properly research crypto projects and crypto currencies here:
ren-heinrich.medium.com
2. Know the crypto regulations in your country. What kind of cryptocurrencies are you allowed to buy and hold in your country? What kind of documents do you need to provide when buying crypto? How does the crypto tax regulation work? When you want to sell, always contact your crypto broker/exchange in advance to ask for the cash-out procedure, contact your bank in advance to ask what you need to provide in case you have a large amount of money flow into your account. Remember, always research these things in advance, even if you do not plan to sell immediately because it probably takes a very long time for those institutions to check and verify everything.
3. Do not invest more than you can afford to lose. Do not use your emergency money because crypto prices are very volatile and you might not get your money back when you need it urgently. Invest an amount that will not put you in a difficult situation if you lose it.
4. Never go all-in. After you have decided the total amount, you want to invest, for your first-time investment only spend part of it to buy whichever cryptocurrency you have researched and think is promising. Always have some cash at the side to buy the dip. This also reduces your risk of losing all the money when your expectation does not work out.
5. Keep your cryptos in your hardware wallet and keep your private keys and seed phrases safe. Never leave your crypto in exchanges, because they might get hacked or go bankrupt. In those cases, you will lose everything and never get it back.
6. Keep your private keys and seed phrased safe. Never save your private keys and seed phrases in your laptop or your smartphone, because they might get hacked as well. You can write them down on a piece of paper and put it in a place only you can find.
7. Do not day trade. Unless you are a very experienced trader, it is not a good idea to day trade, because you are trading against countless bots and whales who can influence the market. Chances are that you will lose a lot of money.
8. Do not leverage trade. Lots of platforms provide high leverage trading, which makes the gains look very attractive. But unless you have a lot of experience in leverage trading, chances are your position will get liquidated.
9. Accumulate knowledge about the crypto/blockchain space, such as new protocols, new developments, new projects, new regulations, general sentiments, price models, and so on. Blockchain and crypto are still at the early development stage. That is also why there are a lot of opportunities in this space. The market changes very fast, there are lots of news and developments every day. That is what makes the whole space exciting and interesting. Spend some time to read the news, you do not need to read every detail, but just the headlines and summaries are enough.
Here's an article where I a list of 8 free Bitcoin indicators that I am using to analyse long-term trends:
ren-heinrich.medium.com
10. Focus on the long term. Long-term means anything from a year to 5 years, or even more. Let's look at Bitcoin which has the largest market cap at the moment. It just crashed more than 50% in the past week; tens of thousands of traders got their positions liquidated during a short period of time. However, if you zoom out to look at the long-term price development, you will see that anyone who held Bitcoin for more than 4 years is in profit.
11. Do not panic sell when the price is experiencing a fast drop unless you are already in significant profit and thinking about exiting. Normally whales use this strategy so that they can get cheap coins. Do not fall for that trick. Also, negative news affects the price of crypto, such as bankruptcy of exchange, some country bans the use and mine of crypto. In the past, prices tend to recover after some time.
12. Do not buy when the price just reaches an all-time high. Because all the people who bought at low price earlier are aiming to sell their coins at this level to take profit. Normally, after prices experience parabolic growth, either a huge downtrend will follow or a price correction will happen.
13. Do not buy when the price is in a downtrend. Because the price is likely to get lower and lower. It is better to wait until the price has stabilized, then place a buy order.
14. Do not give up when the price reaches the bottom, it is normally when the accumulation starts.
15. Accumulate periodically to average your cost. The price of most cryptos out there follows the price of Bitcoin. Nobody knows whether the price of Bitcoin will increase or decrease tomorrow. A lazy strategy would be spending a small proportion of your monthly salary to accumulate on a specific day every month.
16. Set a realistic profit target and exit strategy. Remember that you are not there to maximize profit, but to increase your fiat buying power. It is impossible to buy exactly at the bottom and sell at the very top, all we do is to buy low sell high, to make a profit during the uptrend development. Here are two good videos from DataDash and Ivan on Tech about exit strategy I would recommend.
17. Take initial investment out once you are in profit. This way, you get your money back, even if you lose, you are only losing the profit you have made.
18. Know the risk-reward ratio. A coin is more likely to go from $1 to $2 than from $1000 to $2000, even though the increase is 100%. If your profit goals have been reached, there is no point to wait for some coin to go from 3k to 6k, while it could actually reach 6k, you are also taking a risk of losing your profit because the market is unpredictable.
Addendum 24.09.2021: Thanks for all your feedback. I realized that this point needs some more clarification. As some people pointed out this indeed has more to do with market cap rather than price. If a crypto currency has a total market cap of say $100 million it's potential to double and reach $200 million are much higher than for a currency that has a market cap of $500 million.
However, if you are new to crypto I would recommend you to start with the established coins and projects such as Bitcoin and Ethereum. Only after you gained some experience you should focus on other, smaller coins.
19. Diversify. Do not put all your eggs in one basket. In order to reduce risks, focus on large-cap cryptos which actually have real use cases. It also does not hurt to diversify into other assets, such as stocks and gold.
20. Do not follow random social media account recommendations and buy a token or coin you don't know. A lot of people post on social media saying ""X token will moon by the end of this month"" or ""Y coin is the next unicorn"". Do not follow any of these advices until you have done your own thorough research.
21. Do not follow everything said by big influencers. Many of them want to get clicks and views and increase their followers. As a result, they always tell what people want to hear. For example, after a huge price drop of Bitcoin, some of them say ""Bitcoin will moon tomorrow"", just because a lot of people whose portfolios are in the red want to hear it.
22. Read/listen to opinions from different sources, both positive/ bullish and negative/ bearish. Nobody knows everything. That is why it is important to listen to different sources. If you have followed No1 and No9, then you already have enough knowledge to make your own judgment. And if you have been in the space long enough, you will be able to distinguish between reliable and unreliable sources.
23. Do not follow pump-and-dump groups. They are very risky. Normally when the price reaches a certain level, a whale will dump, and the price will experience a very fast drop. Chances are that the price will never recover and you will lose money.
24. Do not fall for scams. Such as free giveaways which require you to send your crypto to another address, comments on Twitter and YouTube where people say they made $$$ with some person, impersonators of big influencers. Here's a story I wrote about how my friend Steve lost $20,000 to a cryptocurrency scam and what you can learn from it:
ren-heinrich.medium.com
25. Control your emotions, do not be jealous of others when they made a lot of money. Feel happy for them and learn from other people's experiences. Do not get frustrated if you lose money. Learn from your mistakes. It is a long-term learning process. I also lost money before after I started to buy Cryptos.
26. Patience is the key. If you are about to lose your patience, refer back to rule No10, zoom out Bitcoin price chart and look at the long-term trend.
!!!PLEASE BEWARE OF IMPERSONATORS AND SCAM MESSAGES. I WILL NEVER ASK YOU TO SEND ME MONEY OR CONTACT ME BY WHATSAPP!!!
",126
https://www.cantorsparadise.com/the-black-scholes-formula-explained-9e05b7865d8a?source=tag_archive---------7-----------------------,"The Black-Scholes formula, explained",Introduction to the most famous equation in finance,Jrgen Veisdal,12,"The Black-Scholes model is a mathematical model simulating the dynamics of a financial market containing derivative financial instruments. Since its introduction in 1973 and refinement in the 1970s and 80s, the model has become the de-facto standard for estimating the price of stock options. The key idea behind the model is to hedge the options in an investment portfolio by buying and selling the underlying asset (such as a stock) in just the right way and as a consequence, eliminate risk. The method has later become known within finance as ""continuously revised delta hedging"", and been adopted by many of the world's foremost investment banks and hedge funds.
The goal of this article is to explain the Black-Scholes equation's mathematical foundation, underlying assumptions and implications.
Happy reading!
The Black-Scholes model is a mathematical model simulating the dynamics of a financial market containing derivative financial instruments such as options, futures, forwards and swaps. The key property of the model is that it shows that an option has a unique price regardless of the risk of the underlying security and its expected return. The model is based on a partial differential equation (PDE), the so-called Black-Scholes equation, from which one can deduce the Black-Scholes formula, which gives a theoretical estimate of the correct price of European stock options.
The original Black-Scholes model is based on a core assumption that the market consists of at least one risky asset (such as a stock) and one (essentially) risk-free asset, such as a money market fund, cash or a government bond. In addition, it assumes three properties of the two assets, and four of the market itself:
In subsequent extensions of the original model, these assumptions have been revised to adjust for dynamic interest rates for the risk-free asset (Merton, 1976), transaction costs for buying and selling (Ingersoll, 1976) and dividend payouts for the risky asset (Whaley, 1981). In this essay, assume we are working with the original model, unless stated otherwise.
The Black-Scholes equation is the partial differential equation (PDE) that governs the price evolution of European stock options in financial markets operating according to the dynamics of the Black-Scholes (sometimes Black-Scholes-Merton) model. The equation is:
Where V is the price of the option (as a function of two variables: the stock price S and time t), r is the risk-free interest rate (think interest rate akin to that which you would receive from a money-market fund, German government debt or similar ""safe"" debt securities) and  is the volatility of the log returns of the underlying security (for the purposes of this article, we are considering stocks). A neat derivation of the equation is available on Wikipedia, based on John C. Hull's ""Option, Futures and Other Derivatives"" (1989).
If we rewrite the equation to the following form
Then the left side represents the change in the value/price of the option V due to time t increasing + the convexity of the option's value relative to the price of the stock. The right hand side represents the risk-free return from a long position in the option and a short position consisting of V/S shares of the stock. In terms of the greeks:
The key observation of Black and Scholes (1973) was that the risk-free return of the combined portfolio of stocks and options on the right hand side over any infinitesimal time interval could be expressed as the sum of theta () and a term incorporating gamma (). The observation is sometimes known as the ""risk neutral argument"". This because the value of theta () is typically negative (because the value of the option decreases as time moves closer to expiration) and the value of gamma () is typically positive (reflecting the gains the portfolio receives from holding the option). In sum, the losses from theta and the gains from gamma offset one another, resulting in returns at a risk-free rate.
The Black-Scholes formula is a solution to the Black-Scholes PDE, given the boundary conditions below (eq. 4 and 5). It calculates the price of European put and call options. That is, it calculates the price of contracts for the right (but not obligation) to buy or sell some underlaying asset at a pre-determined price on a pre-determined date in the future. At maturity/expiration (T), the value of such European call (C) and put (P) options are given by, respectively:
Black and Scholes showed that the functional form of the analytic solution to the Black-Scholes equation (eq. 1 above) with the boundary conditions given by eq. 4 and 5, for a European call option is:
The formula gives the value/price of European call options for a non-dividend-paying stock. The factors going into the formula are S = price of security, T = date of expiration, t = current date, X = exercise price, r = risk-free interest rate and  = volatility (standard deviation of the underlying asset). The function N() represents the cumulative distribution function for a normal (Gaussian) distribution and may be thought of as 'the probability that a random variable is less or equal to its input (i.e. d1 and d2) for a normal distribution'. Being a probability, the of value N() in other words will always be between 0  N()  1. The inputs d1 and d2 are given by:
Very informally, the two terms in the sum given by the Black-Scholes formula may be thought of as 'the current price of the stock weighted by the probability that you will exercise your option to buy the stock' minus 'the discounted price of exercising the option weighted by the probability that you will exercise the option', or simply 'what you are going to get' minus 'what you are going to pay' (Khan, 2013).
For a European put option (contracts for the right, but not obligation, to sell some underlaying asset at a pre-determined price on a pre-determined date in the future) the equivalent functional form is:
In order to calculate what the price of a European call option should be, we know we need five values required by equation 6 above. They are: 1. The current price of the stock (S), 2. The exercise price of the call option (X), 3. The time to expiration (T - t), 4. The risk-free interest rate (r) and 5. The volatility of the stock, given by the standard deviation of historical log returns ().
We can estimate any stock's volatility by observing its historical prices, or, even simpler, by calculating other option prices for the same stock at different maturity/expiration dates (T) and exercise/strike prices (X), if we know they have been set according to a Black-Scholes model. The resulting value, , is a number between 0 and 1, representing the market's implied volatility for the stock. For Tesla, at the time of writing this article, the value averaged at approximately 0.38 for 4-5 different option prices around the same expiry/maturity date. Input into equation 6 above, we find that the call option we're interested in should be prices somewhere around $7.
Although it is interesting to understand how options issuers arrive at the price of their call and put options, as investors it's hard to ""disagree"" with such prices, per se, and so difficult to turn this knowledge into actionable investment theses.
We can however get a lot of milage out of the Black-Scholes formula if we instead treat the price of an option (C or P) as a known quantity/independent variable (found by looking at different maturity/expiration dates T and different exercise prices X). This because, if we do, the Black-Scholes functional equation becomes a tool to help us understand how the market estimates the volatility of a stock, also known as the implied volatility of the option. This is information we can disagree over, and trade against.
Because American options can be exercised at any date prior to expiration (so-called ""continuous timeline instruments""), they are much more difficult to deal with that European options (""point in time instruments""). Primarily, since the optimal exercise policy will affect the value of the option, this needs to be taken into account when solving the Black-Scholes partial differential equation. There are no known ""closed form"" solutions for American options according to the Black-Scholes equation. There are, though, some special cases:
First, check if it is optimal to exercise the option early, by investigating whether the following inequality is fulfilled:
For S = stock price, X = exercise price, D1 = dividend paid, t = current date, t1 = date of dividend payment, T = expiration date of option.
If the inequality is not fulfilled, early exercise it not optimal. If C() is the regular Black-Scholes formula for European call options on non-dividend-paying stock (eq x), the value of the American call option is then given by a version of the same equation where the stock price (S) is discounted:
If the inequality is fulfilled, early exercise is optimal and the value of the American call option is given by the following, awful, mess of an equation (I tried to break it up by each term to make it more readable):
Where as before S = price of stock, T = date of expiration of option, X = exercise price and r = risk-free interest rate,  = volatility (standard deviation of the log of the historical returns of the stock), and D1 is the dividend payout. In addition,  is given by:
a1, a2 by:
and b1, b2 by:
It should go without saying that Black-Scholes model is precisely that, a theoretical model that tries to estimate how a market behaves, given the assumptions stated above and the inherent limitations of our own numerical estimations of risk-free interest rates (r) and future volatility (). It should here be highlighted that not all the assumptions of (especially the original model) are in fact empirically valid. For instance, significant limitations arise from:
These should be accounted for in any and all investment strategies, for instance by hedging with out-of-the-money options, trading on multiple exchanges, hedging with volatility hedging and Gamma hedging, respectively.
As briefly mentioned it was Fischer Black and Myron Scholes who in 1973 showed that dynamically revising a portfolio according to certain rules removes the expected return of the underlying security (Black & Scholes, 1973). Their model built on previously established works by Bachelier, Samuelson and others. Robert C. Merton was the first to publish a paper expanding on the understanding of the model and who coined the term ""Black-Scholes options pricing model"". Scholes and Merton was awarded the 1997 Nobel Memorial Prize in Economic Sciences for their discovery of the method of divorcing stock options from the risk of their underlying securities. As Fischer Black passed away in 1995, he would not be eligible to receive the award, but was acknowledged as a contributor by the Nobel Academy.
I am not a mathematical economist, nor is any part of this or any article I publish meant as financial advice. For those interested in reading more about options trading, I especially recommend the now famous book The Big Short* by Michael Lewis and perhaps also my own essays on ""Brownian Motion in Financial Markets"" and ""Event-driven investments, inflection points, and how I made 32x my money in two weeks"".
A good, free, Black-Scholes calculator is available on WolframAlpha.
This essay is part of a series of stories on math-related topics, published in Cantor's Paradise, a weekly Medium publication. Thank you for reading!
* This essay contains Amazon Affiliate links
Medium's #1 Math Publication!
1.4K 
12
1.4K claps
1.4K 
",127
https://themakingofamillionaire.com/you-will-never-be-rich-if-you-keep-doing-these-10-things-8c9677bc06b0?source=tag_archive---------6-----------------------,You Will Never Be Rich If You Keep Doing These 10 things,No matter how much money you earn,Desiree Peralta,12,"There are no secrets to getting rich overnight, but there are some proven systems that can help you get rich.
The reality is that everyone has the same opportunity to be rich. Some may face greater obstacles than others. But the opportunity is there for everyone to seize.
Have you ever wondered why some people are rich, and others are not? Mostly it's because of their lifestyle and mindset.
Many people spend their entire lives lamenting the situation in which they were born, while others strive every day to change it, and that differentiates us.
Also, they still believe in easy money and that everything is a matter of luck. That is why there are still companies that steal money from others by promising to double it in a week.
There are still a lot of people that do nothing because they feel that one day ""their time will come"" and believe that wealthiness is a matter of divine luck.
But having money or ""financial freedom"" is more a matter of habits and effort than ""luck or easy way,"" and there are many habits that do not allow you to achieve that goal.
From living beyond your means to having a single source of income, I bring you 10 signs that say you will never be rich.
In this world, there is nothing completely true. Everyone has the potential to make their future as they wish, and in reality, the limit is the very head of each person.
But throughout my brief life and from a few books I read, I have discovered that most affluent millionaires have one thing in common: None are employed.
Some reasons are:
In school, we learn that hard work will get us ahead in life. But ""that's only half the story,"" says Ric Edelman, a top financial adviser.
When you are an employee, you are used to a salary. If that salary gives you to live comfortably, take vacations, and pay your bills, then you are hardly looking for something else.
However, almost no one becomes a millionaire by saving from an employee's salary. So one reason you won't be a millionaire is that you only have one job.
In my case, I spent 4 years of my life waiting for my salary to increase and working extra and giving my best, but my bosses only increased me when I wanted to go to another company, not because ""I deserved it.""
I realize that bosses don't really care about your performance as long as you do your work correctly, and if you are in a company that only cares about the money, you will not earn anything extra for your efficiency.
So this year, I decided to make a change; instead of waiting for a salary increase, I started working only the necessary in the company and using my energy and time for personal projects.
The result was to increase my monthly income by 150% between investments and entrepreneurship. I have learned a lot about investments, and I hope in 2021 to become totally independent.
medium.com
If in January you put $1000 in a bank account that generates 1% annual interest, you will currently have $10 earned. However, if in January 2020, you had bought 3 shares of Netflix for a value of $324. Currently, you would have $580 earned.
I'm not telling you that you should invest all your money in stocks; that requires analysis and knowing the market. What I mean by this is that there are better ways to use your money than leaving it in a saved account.
Saving in a bank account monthly will not make you a millionaire. Invest wisely and use those savings to create additional forms of income will.
There are many (and even safe ways) in which you can use your savings today that can help you generate more income tomorrow. For example, one of the investments that I am currently doing is buying courses. Knowledge is one of the best ways you can invest your money.
I learned to invest my money wisely, and I was trying different forms of investmentfor example, deposit certificates and shares.
In total, I have spent $700 dollars on courses this year. But that earned me $2200 for improving my writing. In trading, I have spent $900, but that has allowed me to earn $3000 so far.
That money in a bank account would have allowed me to buy a new handbag, but now I have new forms of income that will allow me to retire earlier in the future.
There are many businesses that promise you ""financial freedom."" Today there are still many people who believe that they will get rich with them, but they do not really work to make you rich.
According to David Olariyone, some of them are:
If you want to earn a lot of money, work, invest your time and money, learn about what really works, and be consistent.
Everything I've accomplished so far has been hard work. From the outside, maybe other people will think it is luck, but every day (even on vacation), I have been getting up early to grow in those personal projects.
I learned from a young age that easy money did not exist when my father did not want to give me money for a cell phone, and I had to save from the school lunch to buy it. Now I know why he didn't give it to me, and I appreciate it.
If you don't learn how to drive properly, you will never get a license. It is not safe for someone who doesn't know basic rules to ride in a vehicle. The same goes for money.
If you don't know how money works, you will never make money work for you.
I know people who do not know how taxes work or why companies reduce certain amounts of their salary. I always wonder how they know that they are not being scammed if they do not care about what the companies are using their money on.
Financial education enables people to better visualize their opportunities, make smart decisions, know where to get help, and take action to resolve conflicts.
For example, in my country, there is a law that those who earn a certain amount of money and are studying, the government will be returned annually 10% of the taxes they spent on education. Anyway, I think I only know 3 people who use that benefit out of the 200 people who studied with me.
Thanks to this benefit, the government has returned me up to $ 1000 for education. Using credit cards wisely, I got a ""free"" roundtrip flight to Boston, and I know how to make money work for me.
Something that helped me a lot to learn how money works were working for 1 year in a bank. Although I am a programmer, that experience helped me understand at a logical level how all processes work.
There I realized that loans and credit cards, in general are a problem instead of a solution if they were not for a specific purpose. Many people believe that credit cards are to spend and spend. But I learned that they are really to have benefits when you use them.
Having goals allowed me to buy my first car at age 19, my first house at age 22, and travel 2 times a year every year. Every year I make a budget in excel and what I want to achieve with that money.
That allows me to work with more passion for achieving it and don't spend my money on unnecessary things.
The best way to save and increase your income is to have a goal for that money. If you do not know what you want to have money for, then it would not be logical for you to save; it would not make sense to save it.
Do you want to buy a house? Get a new car? Travel the world? Those are valid goals that you can write in order to save for it.
Most of the people who do not have a goal with their savings do not have a single dollar in the account. They live from day to day and enjoy the now. If you don't know what your financial goals are, you can't start working to accomplish them.
""The number one reason most people don't get what they want is that they don't know what they want,"" T. Harv Eker in his book
The first thing you must-do if you want to be rich one day is to know the reason you want to be rich.
Investing is essential to good money management because it ensures both present and future financial security.
Not only do you end up with more money in the bank, but you also end up with another income stream. Investing is the only way to achieve both growing wealth and passive income.
If you do not invest your money, it means that you plan to live only with your monthly salary and that you have no plans to retire until age 65.
If you want to retire early or become financially independent, investing is the way to do it.
Any investment is better than none, but you need to choose carefully.
The higher a return offered by an investment, the greater the risk of losing your money. In order to protect your wealth, you want to hold a mix of high- and low-risk investments.
Last year I got a mutual fund that leaves me 7% safely. There I have the money that I want to save in the long term. For short term savings, I have certificates and stocks. Also, I started buying properties. I currently have one house. In the coming years, I will continue to buy for rent and Airbnb.
medium.com
Living only on your employee salary is a sign of conformity. Not looking for other sources of income means that you have no aspirations or goals to grow.
Although you are financially well off with your salary, and it allows you to have a quiet life, having different sources of income allows you to achieve financial independence, eventually.
Millionaires, on average, have seven streams of income. They've learned that diversification and creating multiple streams of income is the key to long-term wealth creation. This is the key to achieving what many people dream of: financial freedom.
Having multiple incomes has a lot of benefits; the principal is security. If there's anything the last decade has taught us, it's that no job is safe.
The economy takes wild swings beyond anyone's control, and unfortunately, for most people, their only source of income is from their day job. When that reliable 9-5 disappears, they find themselves in serious financial trouble.
Three years ago, I only worked, I had a fixed monthly salary, and savings and my goals were only according to that income. My dreams were limited. After I started learning how to have other income, I started dreaming bigger, and now I feel like there is nothing I can't achieve in the next years.
To become rich is to step out into the unknown and conquer fear. Exploring new thoughts and ideas and incorporating them into our lives means getting out of that comfort zone we are accustomed to.
The more comfortable you are in your little cocoon, the fewer risks you will take, and fewer opportunities will come your way.
The more contracted you become with fear, the fewer people you will meet, and the fewer new strategies you will try.
The minute you become comfortable, you stop growing.
A moment of fear can screw up your life for eternity. Being comfortable and fearful has killed more ideas, opportunities, more action, and more growth than anything else combined.
If you want to grow, you have to take risks and make mistakes.
Instead of being afraid to take risks, see each opportunity to learn new skills.
If I hadn't made my dreams to better income goals a reality, I'd still be waiting for a raise, riding an old car, and looking for a better job. Now I know that life is more than looking for a perfect position in a company.
Nothing in this life is a chance or divine luck.
Everything that happens to us in this life results from our actions. Getting rich can be very hard work. Thinking that become rich is luck means that you don't want to work or take the risk to achieve anything.
People become wealthy through thousands of strategic decisions and actions that make them wealthy. They develop skills; they create products; provide services; and beyond all, provide value to other people.
It's natural to look at someone who is wealthy and point to some ""lucky"" reason that they were successful. It's impossible to know the thousands of decisions and actions that lead to success.
For example, I could write a blog post, have it go viral, accumulate a large audience, and make a lot of money. That doesn't just happen randomly, though.
For most writers, it will take hundreds if not thousands of posts that are carefully edited and targeted before one goes viral. No one sees the year of work that went into creating a platform and audience before that perfect post took off.
Wealthy people spend years developing skills, building a reputation, and creating value before they get paid.
Blowing your entire paycheck (and then some) each month isn't an ingredient in the recipe for financial success. Neither is draining your savings or running up card balances. Most of the people who will never have financial health are those who spend more than they earn.
If you want to be poor all your life, fill out the credit card and pay only the minimum, buy cars that are more expensive than your capacity, and never make extraordinary payments on your loan.
But keep in mind that being rich in social media is not as important as actually being rich. Impressing people who don't care about you won't help you live a peaceful life.
To stop spending, start by tracking where the money goes every month. Try to zero in on nonessential areas where you can cut back. Then create a realistic budget that ensures you have enough to pay the bills and enough for investment.
Currently, I use my credit cards for any benefit they give me, for example, to generate points for trips or discounts in specific stores. I do not have any loan, and I will only take one when it is for an investment and the earnings are better than the interest.
The general population has a love/hate relationship with wealth. Some resent those who have money while simultaneously hoping for it themselves.
There are exceptions, but the reason a vast majority of people never accumulate a substantial nest egg is that they don't understand the nature of money or how it works.
I believe that being rich begins in the mind. You may earn a lot of money now, but if you don't know how to manage it, it won't last forever.
If you want to be rich, then you must educate yourself and know how the world works, because if you continue to believe that everything is a matter of luck and not work and knowledge, then I regret to tell you you will stay in your comfort zone.
This article is for informational purposes only. It should not be considered Financial or Legal Advice. Not all information will be accurate. Consult a financial professional before making any significant financial decisions
",128
https://medium.com/@blakeross/heroes-give-superheroes-borrow-74205349de69?source=tag_archive---------2-----------------------,"Heroes Give, Superheroes Borrow",Thoughts on Mark Zuckerberg's and Priscilla Chan's charitable contributions.,Blake Ross,4,"Last week, Mark Zuckerberg and Priscilla Chan got a big truck and loaded it up with stacks of twenties worth $44,999,999,937.63 (they stopped for gas).
Unfortunately he posted during lunch, so it went largely unnoticed. In all, Zuck Truck garnered just 19 likes and 2 comments:
But the world was equally divided behind the two commenters, as you can see, and the media loves a cage match. Soon experts began attacking other critical components of Mark's plan, like whether the rubber bands around the cash might suffocate small African warblers if improperly disposed.
All this sound and fury are overshadowing what I believe to be the real depravity of the couple's announcement. So let's dispense with the myths and get to the root evil:
You're probably reading this and saying, ""Hey, you're just defending Mark because he's your friend!"" And you're probably reading that and saying, ""Hey, you're only calling yourself Mark's friend to earn some positive news coverage for yourself!"" But that's not true. If I wanted to earn positive news coverage, the best way for me to do that would be to donate $45 billion. Do you agree? If you do, scroll back up and start reading again from the section labeled ""Myth #2.""
The above is known in computer science as an infinite loop. It's a way to forever trap incompetent people on a treadmill of stupidity, draining their mental resources, and ultimately depleting their blood cells of the vital vitamins and precious nutrients that sustain life here on Earth.
Anywho, you made it to this point, so you'll be able to grok the real intellectual abomination of the Chan Zuckerberg Initiative:
Mark refuses to use his considerable influence to shatter philanthropy's ""glass ceiling.""
We keep hearing about these angelic moguls who pledge to donate ""everything."" BILL GATES TO DONATE ENTIRE FORTUNE! WARREN BUFFETT GIVES IT ALL AWAY! Now comes Mark to donate ""virtually 100%.""
We're so distracted arguing over whether or not they'll really donate everything that we forget to challenge the bedrock assumption: that ""everything"" is actually the maximum they can give. Mark even reinforces this with classic subliminal messaging:
Here's the question CNN should be asking: If that's truly his daughter's name, why won't he release ""Max's"" birth certificate?
He won't release it because, if you're one of the 90 million homeowners living the American Dream, you know in your bones that Mark's $44,999,999,937.63 contribution is hardly the maximum possible. Think about it: When you purchased your home, you made a downpayment with your money and then borrowed more money. You did this because getting a house with a gazebo was important to you. Well, didn't Mark say human opportunity etc. was important to him?
If Mark truly wanted to save the world, he would have given away all his money, and then borrowed more money from someone else and given away all their money. This assumes, of course, that Mark's credit score is above 700 (see CreditKarma.com, the fastest way to check your FICOTM score online).
Yet in a 2,000-word screed to his daughter  whose real name, I'm told, is Diego  the word ""leverage"" doesn't appear a single time. Mark Zuckerberg is doing less to cure AIDS than the average American is doing to procure a gazebo.
And therein lies the ugly truth:
By declining to donate even a mere 105% or 110% of his net worth on margin, Mark reinforces the narrative that the billionaire class owes no debt to society once they donate every cent of their money.
This is not an easy thing to say; Mark was once my personal hero. Sure, I've overlooked moral failings before, like when he activated Facebook Safety Check during the drunk driving arrest of Michael Phelps  but refused to activate it for any other country's Olympic criminals:
But all the open letters in the world can't wash away the fundamental human truth: A hero is someone who leaves everything he has on the field, plus the stuff you loaned him. We must always look past soaring rhetoric to uncover hidden financial motives, and that is why I have decided to pen this essay today.
Disclosure: This is a sponsored post for CreditKarma.com.
",129
https://www.inbitcoinwetrust.net/could-bitcoins-bull-market-end-next-month-770583f2dfc6?source=tag_archive---------8-----------------------,Could Bitcoin's Bull Market End Next Month?,My family's financial welfare depends on bitcoin's success. That's why it's so important I publish this article.,Mark Helfman,7,"I wrote three books about bitcoin and publish a newsletter about cryptocurrency. My family's financial welfare depends on bitcoin's success. I don't want to answer the question in my title, but I feel compelled to bring it up.
When you look at the data, trends, and patterns of behavior we saw as bitcoin's price approached its four previous market cycle peaks, you will see the same things today as you did for each of those other times.
That history suggests bitcoin's bull run will end next month at a price of roughly $90,000.
I realize that seems crazy. On March 11, 2021, I published a detailed post for subscribers of my Crypto is Easy newsletter going deep into the data and correlations. I summarize them below and recommend you read that other post for a more nuanced and informative explanation. This article hits only the high notes.
Also, make sure you caught my post from earlier this month, Bitcoin's Market Cycle Peak is a Lot Closer Than You Think.
Yes, some say institutions will never let the price crash. No more bear markets. Only up forever. They're all FOMOing in, buying up even the slightest dip. ""Wall of institutional money.""
Sounds great.
To sustain this bull market, we need institutions to HODL, not just buy. After all, they can't push prices higher when they're selling their bitcoin right after they get it.
Look at the realized cap HODL waves, which show the movements of bitcoin among older and newer wallets based on the value of the bitcoins that move.
Unlike the HODL waves you're probably used to, this metric quantifies the influence those HODLers have on the market. Rather than calculate a percentage of people or wallets, it calculates the percentage of bitcoin's market value.
We know institutions accumulated a lot of bitcoin from September to December.
If they're still HODLing that bitcoin, we should see it reflected in some growth in the band that represents people who acquired bitcoin in the past 3-6 months.
We don't see that at all.
In fact, we see a steep drop dating back months.
This chart lumps all the short-term HODLers, people who've had bitcoin for three months or less. Blue circles are market cycle peaks.
Those shorter-term bands keep widening, resulting in spikes on this chart. You can see the spikes lead to market cycle peaks.
As you can tell from the arrow, this most recent spike is approaching (but not quite at) levels we only see at market cycle peaks.
As I explained in previous updates, it's unlikely institutions or their custodians are moving money from old wallets to new ones within their own portfolios, at least not in any significant amount.
In other words, those short-term bands are not rising from big HODLers moving to newer wallets that they control. Rather, they're shipping off their bitcoin to somebody else.
Certainly, some institutions are HODLing. It's just not enough to change the trend.
It was until February.
Since then, flows into Grayscale bitcoin trust have fallen off a cliff and even went negative in March.
Flows into the Purpose bitcoin ETF have dried up, too  from 2,200 bitcoins on the first day to an average of 200 or less since then. On top of that, 3iQ sold some bitcoin from its fund.
That signals fatigue or disinterest among institutional investors and traditional retail speculators, at least in the very short-term.
(Yes, NYDIG says it has a few billion dollars of institutional money lined up. For a $1 trillion asset like bitcoin, a billion dollars doesn't seem like enough to move the market.)
Sure, depending on which model you pick. S2F, expanding cycles, hash cycles, four-year cycles, halving cycles, the list goes on.
There's a model for every outcome you want to see, and they all disagree with each other.
While I don't make decisions based on data models, I found a few that fit my bias. They all tell us the market's approaching its peak.
All of these signals meet next month at a price of roughly $90,000.
For this summary, I'll overlap all of them on one chart so you can see the trajectory:
In my longer update, I delve into this data more deeply. Really interesting stuff.
Bottom line?
Every time we see those signals converge, we get a crash of +77% and a bear market.
Yes. Anything you can do, I can do better 
I don't offer predictions or statistical proof, just food for thought. The more coincidences you see, the more likely it is that they mean something.
We're long overdue for a severe crash (not a dip) or at least a long consolidation, and maybe we will get that before the market cycle peak. If so, you can throw out all this data. Bitcoin will crash, the market will reset and lay a strong foundation for that big, long bull market that everybody expects.
Without that crash or multi-month consolidation, we will head to the market cycle peak a lot sooner at a much lower price than you expect.
Are you sure?
Bitcoin's price has gone up for more than two years. In January 2019, it was $3,200. In January 2020, it was $7,000. In January 2021, it was $25,000.
On those dates, the total altcoin market was $37 billion, $53 billion, and $215 billion.
Higher each year.
We've been in this bull market for a while, even if it hasn't felt like one or it hasn't met the dictionary definition of ""bull market.""
Now, the momentum has shifted. This plane's running out of fuel.
Do you want to hop on a plane when it's already running out of fuel? Or do you want to wait until it lands so you can fill up the tank and then start zooming again?
Bitcoin's price can go a lot higher, but it needs to land and refuel. Otherwise, it will crash.
Not a 30% crash like we've seen so many times. Not even a 50% crash like we got in 2020 and 2019.
A far bigger crash.
We still have room to run. You can realistically double your portfolio in a month or two, and if you have a lot of altcoins, you could do even better. That's an opportunity you can't get in any other market.
At great risk, though.
Long-term, none of this matters. Bitcoin's price will almost certainly go way higher in the future than it is today.
If you're willing to HODL the peak and wait six months to three years for your investment to get back to even, then you don't have to worry about a thing. No matter what happens in the coming months, you'll be fine in the long run.
For me, that's a big risk to take and a long time to wait. I don't want to be underwater for that long.
You probably don't, either.
No. We've been here before four times already, with each market cycle.
Four times.
Same patterns, Same relationships. Same things we see at the end of each market cycle.
It's not different. It's not different at all.
It's exactly the same.
Why should we expect a different outcome?
Mark Helfman publishes the Crypto is Easy newsletter. He is also the author of three books and a top bitcoin writer on Medium and Hacker Noon. Learn more about him in his bio.
Originally published at Voice.com.
In Bitcoin We Trust is a place where Bitcoin believers...
1.4K 
20
1.4K claps
1.4K 
",130
https://entrepreneurshandbook.co/the-exact-steps-i-followed-to-make-1-500-of-passive-income-every-month-8b592d50e530?source=tag_archive---------4-----------------------,"The Exact Steps I Followed to Make $1,500+ of Passive Income Every Month",I'm 29 and refuse to trade time for money. Here's how you can do it too.,Julia Horvath,15,"""I wanted movement and not a calm course of existence. I wanted excitement and danger (...). I felt in myself a superabundance of energy which found no outlet in our quiet life.""  Leo Tolstoy in Family Happiness
At 25, I had an epiphany. I hated to sacrifice half of my waking hours to work for someone else. Even though my job at a multinational company was okay, a feeling of suffocation crept over me with every day I turned on my computer.
My dream is to travel to every country. Meanwhile, I want to read a book every month and watch a movie every week. I want to explore every corner of the city I live in. I crave to have the time to laugh and go crazy with my friends and fall asleep late. I yearn to utilize my time in a fulfilling way.
As Jack Kerouac would put it, I'm
mad to live, mad to talk, and desirous of everything at the same time.
I realized, however, I won't have the time to do all these things as long as I had a 9-5 corporate job. The enjoyment of my zest for life would be limited to after-hours and weekends until I turned 65  the legal age of retirement where I live.
Who knows if a) I would live that long and b) if I did, what my health would allow at that age?
My mom passed away from breast cancer at age 48. This defining experience taught me you can't assume you have a whole lifetime in front of you to do all the things you want to do.
Therefore, I made a decision: I'd do what it takes to escape the work-hustle fetish culture and stop to trade time for money.
Two months later, I quit my job and fastened my seatbelt on a flight to Southeast Asia. I had no idea what I was doing but was wrapped in a fundamental sense of freedom.
You can't assume you have a whole lifetime in front of you to do all the things you want to do.
Fast forward to today and I make $1,500+ on autopilot, aka passive income. No, $1,500 isn't massive wealth I accumulate every month but it's enough to cover all my costs of living and even put a little aside. I don't have to worry about making ends meet.
I work 10-15 hours a week to generate additional, regular income, and I have more than enough to live, save up, enjoy life, and tinker with passion projects that'll hopefully generate more passive income later.
I'm also an ordinary person in my 20s; had little savings and didn't take out a loan.
How did I do it?
Passive income is neither impossible nor rocket science. All you need is time, determination, and the will to take a few calculated risks.
There are many ways to make passive money online (affiliate marketing, dropshipping, influencer marketing, etc.). Nevertheless, I'll focus solely on selling digital products.
Why?
Because that's how I make my money and, therefore, have expertise in.
I quit my job to do this. Can you quit yours too? It depends.
As a turbulent ENFP, I had savings of $18,000 when I quit, burned up $8,000 within 6 months of travel while I did nothing, and essentially started to put real effort into my new business once I panicked and had $10,000 left.
Whether you can quit your job largely depends on your savings, expenses, and willingness to take risks. Overall, it feels good to have enough money to live off from a year and have a buffer on top of that.
While I started like that I can attest to significantly higher stress levels once I got below this threshold and, therefore, don't recommend it.
Whatever you decide, don't assume you can generate income within a few months. It took me over a year of hard work and zero cash flow to figure it all out. In the beginning, there's too much you don't know. No matter how good you are, you'll inevitably make mistakes, all of which take time.
If you decide you can't quit your job, after all, that's okay. I know several people who built successful online businesses while employed. If you can afford it, try to reduce your hours (e.g. from 40 to 20). Otherwise, dedicate time on the weekends, in the evenings, etc.
For further inspiration and the conviction you can do it, read Jon Morrow's story about how he became a multimillionaire selling online courses while he can't move anything but his face. He's one of the guiding lights of my journey and I came back to this piece whenever I felt like giving up.
I sell Hungarian language courses. Hungarian is considered to be one of the most difficult languages in the world (along with Mandarin Chinese and Arabic). It's also my mother tongue.
I'm also with a partner who learns Hungarian and who started this business together with me. This gave us a unique approach and angle to other people's struggles.
When you look for your niche, here's what you should ask yourself:
What is it you know only a few others know, but many want to learn?
While 10 Mio people speak Hungarian, only a few of those speak English at my level and even less want to teach it.
When I started out, there was one proper autodidactic online course and a few apps floating out there. At the same time, language-forums, related Facebook groups, and the Hungarian-learning subreddits had thousands of members.
This way I was confident I had a business.
While these paragraphs are just the tip of the iceberg of my market research you get the idea.
Millionaire online creator Pat Flynn's book Will It Fly? is packed with amazing content to help you decide whether your idea could work or not. It's what I used to decide if I should go through with mine.
Also, Jon Morrow's article 21 Warning Signs You Chose the Wrong Topic For Your Blog will help you with the decision. Don't let the title deter you  these will work not only for blogs but any type of topic you decide to pursue online.
Here are the crucial things to keep in mind when you decide about your niche:
If you remember only one thing, let this be it. You have a business if you help people with key pressing issues they face. Don't start with nice-to-have things you feel or wish people might like. Solve their most painful problems and you're guaranteed to make money.
You're building passive income so you have more free time to focus on your passions.
Hungarian isn't my passion. Don't get me wrong  I like it and find the nitty-gritty of the language interesting. Most people who want to learn it are lovely and I love to talk to them and hear their background stories.
My passions are my partner and my friends, travel, getting to know myself, indoor bouldering, and writing.
All of them would cease to be passions as soon as I'd burden them with the enormous responsibility to provide me a living.
Passions cease to be passions as soon as you burden them with the enormous responsibility to provide you a living.
Don't make this mistake, as it'll leave you disappointed, burnt out, and dispassionate.
Or, to put it differently, to be an expert, all it takes is to know more than the people who want to learn it and your willingness to advance.
The only key is transparency. Be honest about where you are, what you know, and what you don't know.
If your niche is a topic you only got into recently, say so and take your audience on a journey with you as you learn.
I'm not a linguist or Hungarian teacher. I base my content on my experiences with my Hungarian-learning partner and, after 3 years of being into it, my experiences as a tutor and talking to hundreds of Hungarian-learners.
I'm dedicated to unlock and explain the most difficult grammar rules. I dug deep into my mother tongue to make it more accessible for its learners.
Nevertheless, I'm not a certified expert and don't plan to be.
Learners trust me specifically because of my different approach. Many of them worked with teachers before and want to try something new when they buy my courses.
You don't have to be either. You can turn your perceived shortcomings into advantages. People don't want your certifications. They're looking for someone they can relate to and trust.
I cannot emphasize this enough. Most people skip this step and later wonder why their social media audience doesn't convert.
My business runs almost entirely without social media. All I have is a Facebook-page of 1,000 fans which came somehow over time. I use Facebook solely for running ads (more on this later).
Here's why your email list will be your superpower, lovechild, and most valuable asset you ever build:
An email list is the only thing you control. You're not subject to any algorithms, policies, and changes of heart of social media providers. While only ~5% of people will see the smart observation you post on social media 30-50% of people will see what you send them via email:
An email list indicates strong interest and, therefore, a higher propensity to buy.
It takes a second to follow anyone on Twitter. It takes typing, opening another tab, checking your emails, and the confirmation of your subscription to join their email list. It also takes trust. The hurdle is bigger and people who are willing to take it will be a lot more likely to open their wallets later on.
Think of all social media trends of the last 5-10 years. If I wanted to follow them all I would've gone crazy.
From Facebook to Instagram, Twitter, and now TikTok and Clubhouse  people change and try new platforms frequently. All the while, they've been diligently reading their emails every day since the mid-90-s.
While social media trends come and go, you can trust email to stick around and be a reliable tool to build trust, connect with your audience, and, ultimately, sell your products.
There are several online resources to start an email list. Author and online millionaire Pat Flynn's is a great one.
You'd think 500 people are too few to make real money. You're wrong. I had 500 people on my email list when I pre-sold my first course and it was a massive personal success (see below).
I'm not gonna lie, however it isn't easy to get to the first 500 people on your list. It took me almost a year. Although to be fair, I didn't give it my all and tried to rush after multiple projects at the same time.
Little did I know how valuable my little email list would eventually become.
Either way, here's how to get your first 500 subscribers:
This can be an ebook, a video series, a daily email with little nuggets of help, a PDF, or any other kind of digital material.
The easiest way to create a good freebie is to ask yourself this question I learned from online millionaire Amy Porterfield:
What does your audience need to understand, be aware of, or believe to want your product?
By now, I have several so-called lead magnets or freebies and tried all of the types I mentioned above. They each work, given they're helpful, solve a quick pain your audience has, and are related to your product.
Start with one such freebie and make it the best you can. Don't be afraid to give away too much value for free. Your help will pay off later. Also, in the end, each subscriber can translate directly into income.
I also hate cold emails, but you don't have to see it like that. You don't sell anything yet; you just want to create an audience of people who could potentially be interested in what you have to say.
Don't be shy to aim for the whole bandwidth of your social network. In the end, you don't know whom other people know and whom they'll forward you to.
To build my list is the only thing I use social media for. I used my personal Facebook account to find and post my free material to all Hungarian-learning groups.
Also, I played around a lot with Reddit.
My partner spent a lot of time tinkering with Pinterest and made one article go viral there.
There are hundreds of tutorials for each social media channel and how to use them as a powerful marketing tool.
Where you put your efforts will be a combination of which channel you find easy to use and where your audience is.
Once people start hopping onto your list, email them at least monthly. Otherwise, the value of your list will deteriorate quickly. After all, you can't expect people to be excited about you and your product idea if they haven't heard from you for a year.
You can email them what you're up to and come up with helpful blogposts or the helpful resources of other people.
You don't have to build an entire product before you make money. You can generate income with a product that exists only in your head.
In fact, this is what you should do, as it gives you direct feedback about whether your idea is viable or a waste of time.
How to do this and see great results, however, goes beyond the scope of this article. That's why I wrote an in-depth tutorial with every step of how you can pre-sell your product idea to your audience.
As I wrote above, I had 500 subscribers during the 6 months I pre-sold and built my first course.
It was a massive personal success. I made almost $3,000 before I launched it. That's $3,000 for a product that didn't even exist yet.
Of course, $3,000 in 6 months wasn't enough to live while it was far from passive income, too.
At that time, my partner and I were together in this and worked our asses off all the while. While this meant tremendous emotional support, it also meant to divide the money by two.
Nevertheless, this $3,000 showed us to make money from digital products is possible.
I don't need fancy social media accounts and hundreds of thousand people I reach to achieve it. All I need is a dedicated audience I was now determined to grow.
This goes parallel with the previous step. After the initial pre-sale, we saw the product idea was viable (ie. people were ready to spend money on it), so we proceeded to create the product.
At the same time, we left pre-orders open until launch and grew our email list.
Once you launched your first product, it's time to automate the process and make the leap towards passive income.
It's time to build a proper sales funnel.
Roughly, here's what this looks like:
In the end, all passive income that comes from digital products boils down to these 3 crucial steps.
People wrote entire books about email funnels. In the end, though, all you have to do is be helpful and offer a lot of value for free in your emails before you pitch and ask for money.
My longest funnels are 10+ emails long and span over 2 weeks. These are also the most successful ones.
Every few months, it's time to remind your email list of your product(s). You think it's pushy and if people decided once your product isn't for them you should respect their decision.
In fact, the opposite is true. I sell a lot more products per subscriber with flash sales than through my sales funnel. Here's why.
See one of my flash sales above. I made almost $5,000 within a week. All I did was send a few emails.
You shouldn't overdo flash sales, however. I do them every 4-5 months.
Viola  after you completed these steps you likely built at least some passive income.
Like me, you might not make enough yet to ditch all work and do nothing but sip cocktails on a tropical beach. Nevertheless, you'll be on track to fulfill this dream.
Once you built one product, you can expand further, focus on growing your list, create a new product, or think of other passive income streams.
My current monthly $1,500 consists of two digital products as described above which I sell through an email list that meanwhile grew to 4,000+. I do 2-3 yearly flash sales. I also started a Steady account (the German equivalent of Patreon).
I promise all of these got easier after I built and sold my first product.
Maybe you noticed you don't need a lot of money to recreate all of this. Your biggest expense is you  you have to eat, sleep, and turn lights on. That's where your focus should lie when you make the decision to build an online business.
In fact, you don't need social media following or even a webpage at the beginning at all. Focus on the essentials, and don't get lost while you do nice-to-have-things that don't matter.
A product, an email list, and a sales funnel with a freebie as a lead magnet. Narrow your focus. To become scattered is one of the biggest hurdles you'll face and can derail the whole thing.
Have patience and accept this won't happen overnight.
Self-employment alone can already bring many advantages and a lot of freedom. Passive income, however, goes a step further.
I have a lot more free time. Time is one of the most valuable assets in life. It's also the only thing you won't get back (unlike money).
Therefore, I have the freedom to design my day and, in the end, make the most of my life without waiting for the weekend.
I can wake and sleep as I please. Self-employment and passive income helped me get back to my natural sleep cycle as a night owl.
At the same time, I can work on passion projects without pressure.
Self-employment fulfills me with meaning. I don't feel like I live half of my life working on someone else's dream.
Overall, I have a strong sense I don't waste a minute of my life. I no longer have a long bucket list of things I want to do when I have more money, more stability, or more security at some point in the distant future. I just do them. Life is too short for that sh*t.
As I said, I'm average, had zero starting capital, and took on zero debt. All I had was a thirst for life that never ceased.
To be fair, I also had a loving partner with the same vision and we shared the workload at the beginning. While this helps you can do the steps I described above alone, as well.
If I can do it, you can do it too.
What are you waiting for? Figure out that niche and start to work on it.
If you need more help, join my Self-Letter. It's a weekly email where I help you learn more about yourself, embrace your creativity, and make money while you live in alignment with your personal values.
""Want to get in on an exciting new virtual event hosted by Entrepreneur's Handbook? Our first Startup Summit is April 8th and already has several hundred registrants. There are a few early-bird tickets left at a special price  Click here to reserve your seat now.""
How to succeed in entrepreneurship
11.1K 
82
Thanks to Eva Keiffenheim. 
11.1K claps
11.1K 
",131
https://medium.com/personal-growth/the-hidden-sign-that-you-have-a-poor-persons-mindset-13203d676a4c?source=tag_archive---------8-----------------------,You May Have A Poor Person's Mindset And Not Know It,The problem you don't see,Jane Hwangbo,7,"Have you ever noticed that the poorest, most miserable people you know tend to be buyers of lottery tickets? C'mon lucky numbers 3, 10, 16, 22, 4, 50.
I'm sure there's a smart-sounding statistic somewhere lodged in a research report by some well-meaning PhD student who had to get funding from the So-And-So Foundation to prove his or her point, but I digress.
The correlation is common enough that I'll assume you know what I mean.
Big dreamers tend to be the ones who never seem to be able to get their dreams off the ground.
People who view themselves as great lovers tend to be the ones left at the altars.
And the most educated people in the world tend to be the dumbest ones in the room. Not always, but way often.
""But Ms. Hwangbo, whatever your last name is, what on God's green earth does this have to do with money?"" you may be asking. It would be a fair question.
After all, you are reading this article because you're pissed that someone may be raining on your grand financial plans, right? I applaud you for seeking the truth, despite this fact. It's a great quality. In fact, valuing truth no matter what may be one of life's most useful qualities ever.
The big problem is this. We try too hard with our money.
We overcompensate for everything with money. We can't stand that we're insecure about ourselves. We feel like we can never do as well as the latest who's-it on Instagram, which leads us to make a series of small financial mistakes that snowball over time.
Back in your parents' day, the people to beat were on TV instead of on social media, or they were your next door neighbors, the Joneses. The underlying problem was the same. The negative comparison bias caused people to take out mortgages they couldn't afford, play in the stock market when they couldn't take the losses, and run up their credit cards.
Instead of just being okay with feeling inferior, we do all the wrong things with our money in order to convince ourselves that we're better than okay. And when our insecurity comes calling again which it always does, we do whatever we're doing that's taking us the wrong direction and do it more.
In the case of people who buy lottery tickets every week, instead of changing how they think about money and using it in more productive ways, they buy more tickets. Every purchase feels like a last ditch effort, which fuels more last ditch financial efforts because by definition last ditch efforts rarely work. This inevitably creates a financially desperate situation, and kaboom. It's sad. Lots of lives go up in smoke this way. Not by buying lottery tickets per se, but by taking on more student loans, leasing expensive cars, and buying big houses.
Maybe this is starting to sound a bit too existential. Isn't life though? Since we developed this newer part of our brain, we can overthink ourselves right out of relevance, which may be why our average brain size is now shrinking, but again, I digress.
Minimalism's pretty hot right now. The coolest people you know are subscribing to a financially minimalist lifestyle, which I would applaud if it weren't another form of trying too hard.
Minimalism unfortunately feels like a way of fitting in. Another mantra to live up to.
People say they want a financially successful life, but what they really mean is that they want the pleasure and ease of having it without the pain of creating it. It's the most common goal ever.
Their highest value when it comes to money is therefore pleasure. Pleasure, unfortunately for everyone pursuing money for this reason, is a pretty low quality human value. Even if you attain it, your happy feelings come and go and never really accumulate.
Apes and cats value pleasure too. Pursuing pleasure rarely makes higher-functioning human beings happy. You can try to use your money to become a happy person this way, but it probably won't work. You have this newer part of your brain as I mentioned before, that needs something deeper to be satisfied.
Fear is the killer of all pleasure, so in order to reach and sustain as much pleasure from money as possible, you have to do dick moves like not paying the doorman a tip or demanding that the world give you everything for free even though it really chaps your hide that you yourself don't make enough money from other people to make a decent living.
Welcome to the millennial economic paradigm where it sucks for everyone just a little bit more. Sorry to those millennials who do the opposite. You guys are my heroes.
Remember that old saying? Nothing in life is free? Well, you're in denial of it. It's what you're not willing to pay for that makes wealth elusive to you. Either with time (sustained, difficult effort) or with more money (what those Wall Street bastards call ""investment""). Bad long-term plan.
You want the life of the rich before becoming the kind of person who becomes rich. By rich, I also mean being compassionate and hard-working, commensurate with the dollars earned.
Nope, the real problem isn't your lack of money. Your problem is that you're in the market for a hack. You want the outcome of financial success without the process. You want the yacht, the Ferrari, the fabulous Instagram life, but no thank you to the work and the boring minute stuff. You've got a poor person's mindset.
Pleasure is never long-lasting and pain is inevitable, so if you think about it that way, you're sort of screwed. You want the wrong things out of your money. You want it to relieve your pain, instead of providing the means to some other value.
You have to figure out what you value more highly than seeking pleasure and ease when it comes to your money. You have to value something better. What might that be?
Ever get the feeling when you read stories about mega-successful and financially wealthy people that:
a) They never did anything for the money, and
b) They would have done whatever they did that made them successful anyway, even if they had never seen the kind of success they did?
The difference between them and people with a poor person's mindset is exactly that. Successful people value the process and the work itself over the outcome, the party, the accolades, and the money. In some way, they value the pain. It's a better pain to build a business out of their art than say, the pain one gets from passing a kidney stone. The pain from doing their work and failing over and over again is meaningful pain, and always worth going through.
They don't try to avoid discomfort and uncertainty because frankly, avoiding these things is not what makes a great freakin' wealthy life.
There are things in your own life, whether you've identified them or not, that are worth the pain, that you'd be willing to go through a ton of suffering for, and still come out the other side with a vaguely goofy smile on your face. If you don't know what these things are, hoping that money will solve your ""purpose"" problem is pretty darned unlikely to succeed.
You'll never have enough money.
Let me repeat that. If you're not sure of what you like about money other than pleasure and avoidance of pain, look at what you're willing to do for the money  as in, the work itself. That is what you really value.
If the work you're willing to do is sleep around until you nail a wealthy spouse, you're in the world's oldest profession.
If you're willing to study to get multiple school degrees for the money, you're a professional student or scholar. You value ideas over execution.
If you're willing to withstand failure and humiliation over and over again while trying to create a product or service, you're an entrepreneur.
If you're willing to practice your instrument every day for hours on end and face the possibility of rejection during a performance, you're a musician.
You are what you're willing to do for your money. The money will never change that.
Now don't get me wrong. Not all values are equal. Some values are healthier for reinforcing self-worth and confidence than others and therefore better values. Your job is to pick the ones that reinforce the quality of life you're trying to build, and pick them well.
But please, stop going around thinking that what you want is the money. You'll only attract people whose highest value is to take it from you.
Sit down with a pen and piece of paper tonight, and think about what you might be willing to take some major pain for.
It doesn't have to be what you do for a living, or even anything you believe you do well yet. It doesn't even have to be one thing. It could be multiple things.
Ask yourself what you do already or have done in the past that you're pretty sure no amount of suffering could keep you from doing if you gave yourself permission. Improvement in doing would be enough to keep doing it.
Keep the money 'what-ifs' out of your mental equation. Baby steps.
If you're hopeful that money will be the solution to all of your problems, you're already screwed. You're misguided about what your actual problems are.
It all comes down to you. What do you value? What are you willing to practice?
p.s. Sorry if this article offends anyone who views ""purpose"" as lofty, sacred, and above biting commentary. I can only reply, please try to get over it. The value of purpose is already above all else.
This blog is dedicated to one of my favorite writers, Mark Manson, author of The Subtle Art Of Not Giving A F*ck. He kinda says everything I say, but better.
Sharing our ideas and experiences.
3.1K 
139
",132
https://medium.datadriveninvestor.com/the-definitive-guide-to-why-life-is-terrible-for-millennials-130104f8b2df?source=tag_archive---------8-----------------------,A Definitive Guide to Why Life Is So Terrible for Most Millennials,There's hard evidence behind our burnout.,Jessica Wildfire,10,"My in-laws still try to give young people advice, especially when they're struggling. Their greatest hits include, ""downgrade your phone"" and ""stop spending so much money at Starbucks.""
Yes, seriously.
They're like many so-called boomers. They prefer to attribute everyone's problems to their personal shortcomings.
They grow uncomfortable when you talk intelligently about how much our politics and economics have changed over the last 30 years, and how little they understand the world now.
They get defensive, and that leads to preaching.
They tell you how hard they worked when they were your age, and they use their own children as counter-examples to prove it's still possible to thrive during this century, ignoring larger trends and the general erosion of the middle class. My own dad sometimes holds me up as an example, citing my PhD and 6-figure income, completely dodging how many hours I work between various jobs and side hustles. I'm not allowed to mention that because it's viewed as ""complaining.""
In reality, I can literally see the guilt creep over my relatives' faces when I describe my life with a hint of honesty.
A lot of successful millennials simply lie to their parents.
I do. So do most of my friends.
We do our best to present a cheerful facade, making everything look easy and breezy. Amongst ourselves, we ask why they ""just don't get it."" The truth is simple. You know, maybe they don't want to.
It's also possible that nobody has ever distilled for them exactly what has changed and to what extent.
Here's an attempt.
Our parents and grandparents are fond of telling us they had little to no trouble buying a house, investing, and saving money for retirement. They could live easily on roughly $50,000 a year.
A lot has changed since then.
According to CNBC, the cost of living has skyrocketed, even beyond inflation. Housing prices have gone up enormously. In 1990, the average cost of a house was about $79,000. In 2010 the price had gone up well over $220,000. Now, the average reaches above $360,000 for a standard, four-bedroom house. Average mortgage and rent payments range anywhere from $1,000 to $2,000 per month. If you live in a smaller city like I do, then you might enjoy a lower cost. It still eats through a third of our monthly income.
From there, it gets worse. According to Fox Business, groceries for a family of four average almost $900 a month. By the time you factor in other basic necessities like car payments and internet, you're looking at monthly expenses around $3,000 for the average family.
Here's a look at groceries alone:
Meanwhile, the average income has gone nowhere. Most of us are still making $50K a year. Sometimes, we're making much less. In fact, wages have actually dropped by 9 percent since 2006.
So when you look at the economic data, it becomes clear why younger Americans can't afford to live like their parents did on the same salary. Often, members of a household have to combine their incomes. They also have to find jobs that pay well above the average. If they have kids, then a third of that second income goes toward childcare.
To enjoy the same lifestyle our parents did, we work two and three times harder  often at multiple jobs.
It makes us a little bitter.
Millennials grew up with the notion that college held the keys to their prosperity. Our culture reinforced this myth from every angle. Employers made degrees a requirement, even for entry-level jobs.
We had no choice.
To make matters worse, everyone told us that the prestige of our university mattered. If we wanted the best jobs, we had to graduate from competitive schools. So we enrolled in the best we could. Many of us didn't have wealthy parents to fund our education. If scholarships didn't cover full tuition, we took out loans and worked low-wage jobs.
The price of college has gone up astronomically over the last 20 years. Because wages have remained flat, the average teen now has to work more than 2,200 hours in order to afford a single year of college. At a public institution like Michigan State University, you'd have to work 60 hours at minimum wage just to afford a single credit hour. A year of college is typically 30 credit hours, so the math comes to 1,800.
So to afford tuition alone, you'd have to work full time for 4 years, and you'd have to save every single penny you earned.
For most of us, that's impossible:
Prior generations were able to afford college because it was cheaper. Textbooks didn't cost as much. There were more scholarships and grants available, and the money went further.
Our parents enjoyed these benefits, then they spent the next 30 years electing politicians who systematically defunded higher education, forcing institutions to shift the cost onto us. They also stood by while CEOs weaseled their way onto university boards of trustees and forced cutthroat business thinking onto a largely nonprofit enterprise.
All the while, we were told college was worth the investment. Billionaires like Steve Jobs and Oprah gave commencement speeches about how, even though they didn't go to college, it was still a great idea. In private they mocked formal education as something ""for bozos.""
The average American now carries roughly $33,000 in student loan debt by some estimates, for a degree that's almost worthless.
We're told this is our fault.
Three out of four millennials don't own stock of any kind. They cite various reasons. The most common one is simple:
They don't have the money.
Let's put that argument aside for a minute and ask a different question: Why would a millennial invest in the stock market? Every day, we read stories about ordinary people getting defrauded and tricked into making bad investments. We remember what happened to our parents during the 2008 recession. While a handful of people managed to recover, many never did. They had to delay retirement and take out loans just to survive. We know it could easily happen to us.
We're not lazy or dumb.
We're scared. We don't invest because we're skeptical of the stock market, for good reason. We know it's run by speculators and hedge fund managers. We know we're missing out, but we also know the extreme risk involved with investing even a little bit of money at a time in our lives when we have almost none in the first place.
A single chart shows how much we've suffered:
Charts like this make us angry, because we work practically all the time, and all we want is a financial future that doesn't depend on us risking our livelihoods on apps like Robinhood.
People keep giving us platitudes like ""don't invest what you can't afford to lose,"" or ""make enough to withdraw your original investment."" This advice is infuriating because it assumes we have something to lose.
We don't.
As the above chart makes painfully clear, most of us have nothing.
Moreover, interest rates on debt often rival stock returns. So on a practical level, we have to decide what's smarter. To a lot of millennials, it doesn't make sense to invest if you're paying hundreds of dollars a month on loans, none of which even goes toward the principal.
Millennials simply don't want our futures tied to the whims of Wall Street. We're tired of being told it's the only way.
We want something else.
Our entire lives, millennials were told to find meaningful, fulfilling jobs instead of pursuing mere employment. We were told to chase our ""passions,"" and not to worry about money so much.
We listened to that advice and followed careers in fields like teaching and social work. We became nurses and paramedics, or doctors and veterinarians. Maybe we became artists or writers. Thanks to a deregulated economy, companies can hire us as freelancers and contractors  so that's what they do. All of that led to our current state, where we contribute a lot to society but make very little money from doing it.
Everyone takes advantage of us, justifying our low pay with neoliberal philosophies and free market ideology.
To get ahead, most of us put our talents to work through side hustles, making extra money through platforms like Patreon and OnlyFans. Or we start blogs and podcasts. This might sound interesting, even fun.
We certainly make it look that way.
According to another article by CNBC, 64 percent of millennials report having a side hustle. Some of us sell goods and services, while others sell their bodies through erotic content. On average, it brings in roughly $11,000 a year. It's not always extra spending cash.
A third of us need that money to get by:
In some ways, it's not fair or accurate to call it a side hustle or a gig. And while the work can be rewarding, that doesn't change the fact that we're forced into this situation because our employers refuse to pay us what we're worth, for no other reason that their incessant greed. So we work 40 or 50 hours a week at one job, and then we spend another 20 hours on a side hustle. As an added insult, we're taxed for the extra money we earn.
That's why we're exhausted.
Obviously, not everyone in our demographic is struggling. Most of us know someone who's doing well. Their parents were able to pave a smooth path for them into financial independence. They work at Fortune 500 companies, and life doesn't feel that difficult for them.
That's not the point.
Privileged millennials might as well live on a different planet. They inherited their parents' value systems, and they don't spend a minute thinking about what life is like for the rest of us.
These people and their parents respond to our plights with a stunning lack of empathy and intellect. Instead of listening and trying to understand, they immediately fall back on faulty arguments and bad logic. They call us lazy, and tell us we don't know how to save money. They lecture us like we're 12-year-olds who never took an economics course.
Either they don't get it, or they don't want to.
While the social and economic system currently works well for a handful of people, it's crushing the rest of us.
In general, millennials expect a lower quality of life in several regards, ranging from spending power and net worth to relationships and self-perception. They're more vulnerable to just about everything, including health problems and substance abuse.
Most millennials never have a chance to get ahead, because they're always falling behind. The minute we manage to save a few thousand dollars, an emergency expense comes along to wipe it out. Then we start over. The pandemic has made everything worse for us. Our support structures have withered, and many of us have been left to fend for ourselves. To pretend otherwise is nothing short of gaslighting.
As Anne Helen Petersen deftly argues, it's insulting to be constantly reminded of your shortcomings, and then blamed for your hardships when politicians have spent decades stripping away the tools for social mobility that helped our parents get where they are. It hurts to see so many of our parents applaud these politicians, telling us leaders like Donald Trump have done a great job with the economy simply because the stock market continues to rise, creating a fantasy bubble for brokers and investors.
It's irritating to listen to retirees gloat about how they earned their wealth completely by themselves, refusing to acknowledge even for a moment that government programs and policies helped them achieve a solid foundation first, many of which don't exist for us.
We're tired of it.
Our parents tell us to spend less money on coffee and phones, but they forget one crucial component of capitalism.
Consumption drives the economy.
If everyone suddenly stopped eating out and going to movies, businesses would shut down. The stock market would spiral. In fact, that's exactly what's happened over the last year. We've stopped consuming, and now unemployment has soared.
You can't brag about your son's 6-figure income at a marketing company, when it's his job to convince people to buy things they don't need. That makes your advice to ""consume less"" hopelessly naive.
Besides, many of us do pinch pennies. Millennials have less disposable income than any other demographic. We simply don't have the cash to spend, and we don't have the time. Most of us yearn for a simpler life, away from the ouroboros of capitalism. We want nothing to do with a financial system that depends on someone else always losing. We're tired of the zero-sum games. We want fulfilling jobs where we actually contribute something to society, other than personal brands and marketing plans.
We despise the system our parents created and then forced us into. We want to tear it down and replace it with something fair.
We're tired of being punished for that.
We're tired of being judged.
We want out.
Gain Access to Expert View  Subscribe to DDI Intel
empowerment through data, knowledge, and expertise.
8.7K 
96
8.7K claps
8.7K 
",133
https://medium.com/@awilkinson/the-berkshire-hathaway-of-the-internet-391a8ee83db?source=tag_archive---------9-----------------------,The Berkshire Hathaway of The Internet,Warren Buffett is famous for handshake deals and one-page contracts. He often buys multi-billion-dollar companies after a few phone calls...,Andrew Wilkinson,8,"Warren Buffett is famous for handshake deals and one-page contracts. He often buys multi-billion-dollar companies after a few phone calls, usually without ever meeting the management team in person or visiting their facilities. Not only that, but he typically pays below market prices and avoids dealing with investment bankers. Despite all this, he has amassed a collection of over 65 wholly owned companies, ranging from See's Candies, to Dairy Queen, to Fruit of The Loom.
Today, his holding company, Berkshire Hathaway, is worth almost $500 billion.
So, if he pays below market, refuses to negotiate, and won't even visit them in person, why do wealthy entrepreneurs and families sell to him? Because he makes it easy.
Selling your business is usually a long and frustrating experience, full of stops and starts, hard-nosed negotiation, and emotional flareups.
It typically takes 6-12 months and the process looks something like this:
Now let's look at Warren Buffett's process for buying a business:
Starting to understand why people like selling companies to Warren Buffett, even if it costs them a little? In addition to making the process painless, he also commits not to meddle with the core business, to keep existing management in place, and to hold the business forever.
I now own almost ten operating businesses, so there's usually somebody trying to buy one of them. In the past, I've gone by the ""I'll always pick up the phone"" policy, so whenever a potential buyer has been interested in one of my businesses, I'd always talk to them to see if their number made sense. Everything has a price, or so I thought...
Over the years, I've heard the right price many times. I've gotten excited and mentally decided to sell various businesses over the years, but in reality, I've only sold one. Instead of rolling around in my solid gold lambo making it rain, I've instead experienced the frustrating, distracting process that I mentioned above. In the past five years, we've been through five sale processes for different businesses and only had one deal go through, and on average, I've spent 6 to 8 months per year in a process.
In short, my life has sucked for the past five years. Collectively, I've spent months on the phone doing pointless calls with potential buyers. Talking to investment bankers. Negotiating deals that will never close. Forcing my accounting team to do endless diligence. All time that could have been spent growing my businesses or enjoying life. It's been a huge waste of time and a massive distraction from running my companies.
The funny thing is, had somebody just pulled a Buffett and made it easy, I would have sold every time. I was mentally and emotionally ready to do a deal, but I'm an entrepreneur. I'm high paced. I make a decision, and I want it to happen. I couldn't understand why buyers felt they needed to dig into accounting minutae. In one deal, we spent a month doing an audit over a .5% revenue fluctuation between two months. It didn't need to be this complicated: all our businesses are easy to understand, asset-light, and have brain-dead simple financial statements.
After five years of it, I'm done. The process is broken. We're rolling up our sleeves, flipping sides, and becoming the buyer we've always wanted. We're building the Berkshire Hathaway of the internet. We call it Tiny.
When we started Tiny earlier this year, we set out to mirror Buffett and make it enjoyable and easy for owners to sell us their business. Our first acquisition was Dribbble, in January, and it took a little more than two months in total.
The owners, Dan and Rich, had been peppered with acquisition interest for years. When they decided to consider a sale, we were able to offer them a quick, straightforward conversation about what they wanted and how we could could make it happen. In weeks, we knew we had a deal and shook hands on the terms. There was no complex diligence process, no earnouts to haggle over, and no renegotiation once we'd shook hands. In short, we made it painless and did whatever was needed make them comfortable. We took the Buffett road.
Our process? Simple. We focused on three key things:
We spent about a week coming to terms on a fair price and key terms, shook hands on a deal, and closed in a little over two months. No fuss, no muss.
Sure, we could have spent another 3 months in diligence. We could have tried to renegotiate the terms and ground them down. We could have spent weeks picking apart their management team. But we would have lost the deal, and if we didn't lose it, they would have been miserable and our relationship would have started off on the wrong foot.
Like Berkshire, we also agreed to leave the existing team in place and stay the hell out of their way. We're always available to help with important hires (if the seller wants to step back, replacing them), and anything else they need (accounting, legal issues, strategy, etc), but they get as much or as little attention as they ask for. Unlike more traditional buyer we plan to leave the companies we buy to keep doing what they're doing for the longterm. We bought them because they are awesome, and we want them to stay that way.
Since we closed the Dribbble deal in January, I'm happy to report that we've bought another 3 companies, with each deal taking less than a month to close.
Let's be real, we're a long way from being Berkshire Hathaway. We're Tiny, both literally and figuratively. We're just wannabes at this point. Our market cap is many billions of times smaller than Berkshire's, but we want to use Buffett's methodology to keep buying more and more wonderful internet businesses. While he focuses on railroads and fast food chains, we want to buy simple, profitable internet businesses with great teams.
We know that there are thousands of phenomenal entrepreneurs out there who want to sell their business but don't want to deal with the brain damage. They don't want to freak out their staff. They don't want their business to be put at risk by a short-term oriented buyer who will try to pump and dump. They don't want to answer to a micro-managey board. I know that, because I was one of them, and I talk to more and more people like me every day.
Spread the word. There's a better way to sell your business :-)
You should follow me on Twitter 
PS: I get why private equity firms and other traditional buyers are the way they are. They are fiduciaries managing money for huge institutional investors. When a deal goes south, the last thing they want to tell their investors is ""I had a good feeling about it"". I get why they feel the need to do this sort of diligence, but I think it gives us a special advantage over the establishment.
",134
https://medium.com/@nellsonx/how-to-properly-invest-in-bitcoin-blockchain-and-other-cryptocurrencies-in-2017-32d59e2ff435?source=tag_archive---------1-----------------------,How to invest in Bitcoin properly. Blockchain and other cryptocurrencies,Guide for beginners,Alexandr Nellson,23,"People keep asking me how to properly invest into the crypto-economy. What do I need to know? How can I hedge against losing money? How should I choose the right cryptocurrency for my portfolio, which will skyrocket in the future?
In this guide you will find an exhaustive list of answers to many of these questions. But first, before giving you an explanation on how to invest, let me address the question: why should you even need to invest your hard-earned money? I'll try to explain this as simply as possible.
Over the last 20 years with the invasion of the Internet, many people have earned amazing amounts of money based on one simple thing  the Network Effect. What is that? In the 70-s there was an engineer, Robert Metcalfe, who invented what we now call the Ethernet. In order to make a profit on his invention, he began selling network cards. At that time people didn't understand why computers needed to connect to the network. Robert realized that it was necessary to find a reassuring argument to prove that his network card was just the right panacea needed to cure all their growth-related illnesses. Robert said:
The network's value is proportional to the square of the number of users in this network.
When you buy 10 computers without a network card their conditional costs are equal to 10. However, with network cards the conditional costs of 10 computers will increase to 45! This is 4.5 times more profitable! ""Wow"",  exclaimed the corporate workers and then they began buying Robert's network cards.
It turned out that Robert's idea is quite reasonable! It can be used it to assess Telecom companies which have rolled out across the globe with redoubled force. The capitalisation of Telecom companies depends on the number of its users. The dependence is not a straight line but a quadratic. At the dawn of the social networks, venture capitalists quickly remembered the cherished formula and applied it to such things as Facebook, Twitter, Skype, etc. So the Metcalfe's law was proven to be correct and gave us millions of Ethernet grids which evolved into what we now call the Internet.
Simple truth  if the number of network users is steadily growing, the exponential growth of our investments are expected to grow with it. Thus, Facebook was worth more than $ 300 billion. The formula is simple:Look for a network which is growing steadily at the very beginning => invest put money => wait => profit! The question is  why Bitcoin? Because it's a network and it's growing. It is growing quickly. Check yourself:
Of course, these figures are approximates, because there is no way to determine the exact number of users. The numbers listed above are my own assessment. However, to make it look more convincing here is a graph of Blockchain.info wallets
Question: How long will it grow and when will the growth slow down?
Answer: Blockchain technology belongs to a class of ""No Way Back"" technologies. This is a way to refer to technologies that Homo sapiens started to use and now cannot imagine how they lived without it. Here's a graph showing the growth in adoption of TV, electricity and other technologies:
Growth will be carried out at the S curve until saturation occurs.
There are about 3.5 billion people who use the Internet, and about 20 billion connected devices or just bots. Considering that the Bitcoin network has properties that are not offered to us by any Government or Corporation, we can assume that there is a high probability that the majority of connected people and machines will use this network.
What are these properties?
The list of benefits can go on and on, but you get the idea. This is a kind of manna from heaven, which is better money in comparison with paper dollars. But most importantly:
It's just the economic miracle of mathematics, cryptography and computer science.
Investment in the blockchain is the best investment opportunity since the start of the Internet.
In current economic circumstances, fraud is carried out in several ways:
What's surprising here? How this bubble can burst if there is no alternative? For some reasons no one admits the idea that an alternative to the urrent outdated system is already there. Which is great! That suggests that a Black Swan is not such an unlikely event. In the case that a negative event has at least some probability, smart financiers are recommended to hedge risks. Let's guess, what's the alternative?
Right  it's Bitcoin.
Investments in the blockchain as protection from the fraud of Governments and Central banks.
There is another argument in favor of the Blockchain: robots and artificial intelligence. The reality is that we don't know exactly when computer algorithms will be able to solve all the problems that we can, including creativity.
Today is 2017. The computing power of a device which cost $1000 approximately equals the computational abilities of the mouse. In accordance with Moore's law in 2025 (and perhaps earlier, e.g., in 2022) the cost of computing by a human's biological brain will be compared with the cost of computing by computers.
Let me give you some real live examples from different areas:
The algorithm that broke one of the strongest players of Go is available on Github and it is called ""Tensor Flow"". Any school student can click on the ""Fork"" button and make anything that comes into his mind. This accessibility of really smart technologies will inevitably lead to the emergence of a new generation of smart devices. Autonomous robots will be able to earn money and consequently make economic decisions. And as soon as the first striking example occurs  the Homo sapiens may get nervous. His job may be under threat.
There are more Bots than humans. This amount will increase and Bots will become smarter. Thus, the risk of devaluation of your (and my) intelligence is real. And the longer you deny it, the faster your brain will depreciate in value. The point is that the blockchain is convenient for a robot economy . It is understandable, it is reliable, and it is easy to integrate. The Blockchain for the Internet of Things is a new megatrend. It is obvious that without the blockchain  a robot economy would simply be impossible. The solution is right here:
Investments in the blockchain as a hedge against the depreciation of your own intellect.
In addition to the von Neumann architecture, there will be quantum computers in the future. The Moore's law for quantum computers will work in the square.
So, you have some incentive ideas in your head and you already want to buy Bitcoins. Your blood starts to boil. The brain anticipates an incredible profit. All the free energy is concentrated on the sources of investment.
Calm down! Turn ON your Brain
The blockchain is a really cool technology and it is growing rapidly. But there is no guarantee that you will be able to make profit. I will list some necessary conditions that must be followed to increase the probability of a profitable outcome.
A mandatory condition for safe investments in the crypto-economy is basic computer literacy. If you don't know how computers work, then think twice before putting your wealth on the line. Then the question arises: How do I know if I have this basic computer literacy? These bullet points will help you understand a little bit more about it:
If you answered ""Yes"" to even one question  your computer literacy want to be better, and you need to take this into account when investing in the crypt-economy. Otherwise, you have a high probability that something will go wrong.
Check my tutorial to know how to store Bitcoins and altcoins in a secured way.
You have already realized that there are no freebies, and risks, in fact, are more than you thought! You need some time to determine how much to put down for investment. General recommendation: from 1% to 10% of an available financial assets you currently have. If you have confidence in your own computer literacy, then the percentage can be increased up to 30% and even 50%.
It is very important to start from amount of your financial assets. An appropriate financial assets are currency, stocks, bonds, shares. Property, cars and other things that can be touched, are not included here. Although, I've heard a story about one guy who sold his apartment and bought Ether on all the money. Heroes must be known in person, so let me know if you read this!
So, if financial assets are greater than $100k  you're lucky:). Buy cryptocurrency in the amount of from 1% to 10%, depending of your risk appetite.
If your financial assets are in a range of $10k-$100k  you have something to lose. You are neat, consistent and purposeful. Allow yourself a little risk and roll up to 3%-20%. Your well-being is not significantly affected in a case of losses, but it can teleport you into the category above sooner than you would want to leave your current work.
If financial assets you have are up to $10K  you are still ahead. The amount is already nice, but it is not enough to feed itself. When there is a stable source of income, start from 5%, and when you will be confident  raise the rate up to 40%. Think about your strategy: 5% for retirement. When you got paid  put 5% on the blockchain. If you maintain schedule within 1-2 years you will be pleased with your investments.
If you do not have anything, then best suited strategy is ""all-in"", because you have nothing to lose. With such financial discipline  hardly something will help you! So is there any chance that something will changes in your life .
Great! Bitcoins are for you! Governments, banks and even your parents think that you can't have something for possessions, without their permission. Prove to them that this is not true. If you are reading this article, probably your computer literacy is much better than most of the people around you. So don't believe them. They are stupid. By the time you finish school, banks may disappear altogether, Governments may become autonomous code, your parents may lose their job in connection with the invasion of artificial intelligence, and your kettle might be richer off than your entire family. But if you'll have a few bitcoins and tens of Ether, your coevals will look at you differently. Girls will love and parents will be proud. Look at Vitalik Buterin. His first bitcoins appeared when he was 16 years old. It was interesting. He began to write and study. Then he didn't go to University, but created Ethereum. Now, professors of the world call him Mr. Vitalik Buterin. If you will start to accumulate anything from 6, 8, 10, 12 years you will have a good future.
Prepare to wait a long time.
If you're not ready to freeze your money less than for 3 years  close the tab and move on. There is nothing for you. Blockchain market is in the embryonic stage. It may be rocking back and forth. The clear strategy is Buy & Hold. Bought and keep. I recommend to plan it as a long-term investment with some simple strategy of fixing. For example, detecting 10% each time when the net asset valuation increased 5 times from the previous commit.Well, it's simple! But this strategy will only work if you define the investment horizon in a radius of 3-10 years.
Here's what is thing about. There are all sorts of technology is growing unevenly. The guys from Gartner explained it very simply:
New technology occurs and all at once are amazed from the new technology. This is called the peak of inflated expectations. But the technology is still not working as it should work. Therefore, the technology comes at some point of disillusionment. And then it either disappear or becomes productive.This principle works for all technologies.
So. In our history, all looks the same.
Any blockchain system grows very similar. I am telling this in order to give you one simple idea:
Do not buy when everyone amazed.
If you see that in the last month something has increased several times, this is a great reason to look at buying, study, and find a more optimal time to enter when the market goes down.
Strategy ""Buy & Hold"" also means that if you find an interesting asset, it is the time to start buying when it falls, and purchasing until it starts to grow.
Previously, I have discussed about assets that already existed and were traded on the markets. But where did they all appear? After all, it's better to be at the forefront of ""something great"". Essentially there are two options:
Both approaches in different cases makes sense. Sometimes, these two approaches should be combined. But in the second case, we have the opportunity to participate in the creation of something which is risky. Thus was created Ethereum, and many more.
Question: how to determine that this is something where I put the money is great, but not some sort of regular wiring?
Answer: Behold the root. We look at three things:
If at least one item has not converged, then don't risk. Of course, there are a lot of additional factors:
Generally, all these criterias are not exhaustive. However it is enough to understand that making such decisions requires analysis and understanding of whole picture.
Guys from cyber.fund collect significant ICO in their radar
Yes, there is such a thing. Recently a lot of my friends sent me a project in the field of bitcoins and blockchain with a question: Look? What do you think?
99% of them are not the Blockchain investments at all. Basically, there are different companies registered in different jurisdictions and they provide variety of services. Here an example of such company. If the property is not registered on the blockchain and has its ends on paper in some jurisdictions, it is not a blockchain investment at all. Such an organization will never be more marginal than organization consisting of a code and not paying a bribe to all sorts of crooks. Of course, there are many situations where this approach is needed. Especially in the services at the junction with the Fiat economy, because the Government learned very well how to keep the ball of money holders.
This article is NOT about investing in such kind of companies. It is highly recommended NOT to invest your hard-earned bitcoins in such enterprises because risks are much higher.
Anyway, a journey to the blockchain investments begins from buying Bitcoins, as all existing cryptocurrencies and assets are traded primarily to Bitcoin. Of course, you can start from mining. But this article is not for those guys who are willing to wrestle over how to buy hardware to solve problems with electricity, cooling, to suffer with the software, and then monitor it around the clock. Such guys are not needed in this article. They'll figure it out. This article is for the lazy you. You want to make a decision, press the button (alone and green), and immediately get incredible profit.
If you are lucky and your country do not prohibit the purchase of bitcoins (e.g like in Russia) you can buy Bitcoins by credit card or Paypal.
But at first  know how to store your money! Start from this article.
So, go to Localbitcoins. This is such a great global Bazaar where everyone can handle Bitcoins for the national currency. Vendors have build great reputation. So of course you can be cheated, but traders do value their reputation and the whole thing works quite well.
Sign up. Go to the Security tab and go through the quest to get the ""Strong"" title.
You can skip this step, of course, but I have warned you. Yeah. By the way, remember that the password must be unique and long. Next, select the seller. Not all sellers are good for you . The amount may be not suitable for you or a transaction method. Sometimes seller can be muddy. Your task is to find a guy who has a large number of transactions, faster transaction processing, good rating, etc. Have a good time.
Once the ad has attracted, go into it and look for seller's terms which are written on the right. Read them carefully! If you agree, enter the quantity you wish to purchase and write a short message. For example :
I'm here for the first time. I can be blunt. But I'll try not to be blunt. And Yes! I will use bank transfer from ""name of the bank"".
And here the solemn moment! Big green button! Press. After a while you will get the answer. Send money according to specified details.
After payment go to the ad and click on the ""I paid"" button. The seller may wait for while, and after some time, from 1 minute to hours, you will see updated balance in your account, and the transaction is deemed closed. If you like it, feel free to give good feedback to the seller. They love it and you can get feedback in return. You will be back, right?
Congratulations! But it is early to rejoice. While bitcoins are stored in Localbitcoins it's not your bitcoins, but their. So we go to the next step.
Thus, we assume that you successfully create secured wallet and your private key is only you and no one else.
Well, you copied the address from your local wallet and now we return to the section Wallet on Localbitcoins. Paste the address from clipboard in the address box, then enter the amount of bitcoins  every last Satoshi. And click ""Send from wallet"".
You can see the address where money were sent, and the most valuable information is txid or transaction id. By this ID you can very easily check Localbitcoins, if he is lying to you or not. Copypaste this ID. Next, go to the website of any block explorer and enter this ID in the search. For example, here or here. By the way, it is the first historic transaction that Satoshi started.
I should mention a little about block explorer. One of the sweetest pieces of blockchain technology is the ability to overlook this database (it's called the blockchain or chain of blocks) in real time to any person and robot on the Earth! It has nothing to do with auditor. Every last bit can be seen and proven. The government and banks do not want to give you such openness, because all the crooks will immediately light up.
So, you look in your wallet and get proud of your bitcoins. But, intuition tells you that it can't be actually so great. There must be a catch. The source code of the bitcoin nodes (they confirm a transaction) is open-source, the entire historical database is available and verifiable, the source code for the wallet is opened. There must be a catch. The catch is that your bitcoins exist only as digital recording and this fact is accepted by all other users of Bitcoin network. For proving the right of ownership you can only provide a digital signature calculated using your unique private key. It turns out that this is the nature of digital ownership of new generation. We have to live in this digital illusion, in which the key religion and science is math, not even physics. That's the trick.
Enough sentimentality. Now you have your bitcoins, but the path only begins, because the blockchain technology, which made the Bitcoin has spawned with a lot of interesting things.
This section generally worthy to write a separate book . I will not stoop to the recommendations exactly where to invest, but just go through some areas to plant ideas.
Decentralized infrastructure. Any computer in any network does three things: (1a) receives some data, then (2) makes some transformation, i.e. calculates, then decides that (3) save for longer, and that (1b) forwards in response to the request. Our brain works identical. From this you can make the assumption that all decentralized infrastucture will move in three directions:
And here the fun begins:
After reading this point, dear reader, you hopefully realised that investing in cryptocurrencies is interesting, cool and really promising, but it seems difficult. So you need to decide (1) to invest your bitcoins independently or (2) to entrust this task to someone. Each approach has its pros and cons.Let's see them in order.
Pros:
Cons:
Where to invest?So, you weighed all the ""pros"" and ""cons"" and decided to do it yourself. This is commendable decision. My initial idea was that all people can and want do it themselves. Today, unfortunately, this is not quite true. At this stage, such projects like cyberFund may help you.
Roughly speaking, it is possible examine the state of the economy in one place. Cyber.fund adjust the price and number of tokens in all significant blockchain systems. But the most important is cyberRating. The complete methodology is disclosed in this paper.
Once you realised where to invest, you need to send bitcoins to the exchange, where the target asset is being traded . In our case it will be Binance. The truth is that Binance may disappear tomorrow and we will got a new meme in the Internet.
Therefore, you make these operations at your own risk. I estimate the probability of closing for each particular exchange during the year is 10%. In this series, and Coinbase and Poloniex and all sorts of banks like Xapo.
But now we use Binance because there are a lot of currencies and good liquidity. Decentralized trade is one of the most promising areas, but it is still at the very beginning. I think with the development of projects such as BinanceDEX for one year or two years it will be a worthy alternative to Localbitcoins and Binance with good liquidity.
If Binance for some reason do not satisfy you, there are still a lot of other exchanges. Have fun choosing them. The process is the same everywhere:
Keep in mind that all serious. Exchanges are real.
Here comes the fun part. This step depends entirely on what you decided invest. Without this step, you will not be able to guarantee the long-term safety of your investments. I can say that on every official website of every blockchain system there are links to different wallets. Which trust or not to trust  it is your decision. I will list the tools that I use myself:
This is not an exhaustive list, but may cover 80% of the needs for a beginner. Each of these software requires that you saved the private key. Do it.
I have described in details the best way for storing Bitcoins and Ether in this article
After all of these procedures, you need to withdraw assets from Binance to your wallets. Save the private key as closely and carefully as you can. After these steps, the property becomes yours. Next you are faced with the task of monitoring the value of your portfolio.
You can use such Google Spreadsheet Add-ons for fund management and portfolio tracking
That's all. Still have questions? I will answer them with pleasure!
Please, write your comments and questions below. And one more thing: let's spread the knowledge. Recommend and Share this article. Thank you for reading!
Here's a cool song for dessert.
",135
https://towardsdatascience.com/machine-learning-in-finance-why-what-how-d524a2357b56?source=tag_archive---------9-----------------------,"Machine learning in finance: Why, what & how","Machine learning in finance may work magic, even though there is no magic behind it (well, maybe just a little bit). Still, the success of...",Konstantin Didur,12,"Machine learning in finance may work magic, even though there is no magic behind it (well, maybe just a little bit). Still, the success of machine learning project depends more on building efficient infrastructure, collecting suitable datasets, and applying the right algorithms.
Machine learning is making significant inroads in the financial services industry. Let's see why financial companies should care, what solutions they can implement with AI and machine learning, and how exactly they can apply this technology.
We can define machine learning (ML) as a subset of data science that uses statistical models to draw insights and make predictions. The chart below explains how AI, data science, and machine learning are related. For the sake of simplicity, we focus on machine learning in this post.
The magic about machine learning solutions is that they learn from experience without being explicitly programmed. To put it simply, you need to select the models and feed them with data. The model then automatically adjusts its parameters to improve outcomes.
Data scientists train machine learning models with existing datasets and then apply well-trained models to real-life situations.
The model runs as a background process and provides results automatically based on how it was trained. Data scientists can retrain models as frequently as required to keep them up-to-date and effective. For instance, our client Mercanto retrains machine learning models every day.
In general, the more data you feed, the more accurate are the results. Coincidentally, enormous datasets are very common in the financial services industry. There are petabytes of data on transactions, customers, bills, money transfers, and so on. That is a perfect fit for machine learning.
As the technology evolves and the best algorithms are open-sourced, it's hard to imagine the future of the financial services without machine learning.
That said, most financial services companies are still not ready to extract the real value from this technology for the following reasons:
We will talk about overcoming these issues later in this post. First, let's see why financial services companies cannot afford to ignore machine learning.
Despite the challenges, many financial companies already take advantage of this technology. The figure below shows that financial services' execs take machine learning very seriously, and they do it for a bunch of good reasons:
There is a wide range of open-source machine learning algorithms and tools that fit greatly with financial data. Additionally, established financial services companies have substantial funds that they can afford to spend on state-of-the-art computing hardware.
Tanks to the quantitative nature of the financial domain and large volumes of historical data, machine learning is poised to enhance many aspects of the financial ecosystem.
That is why so many financial companies are investing heavily in machine learning R&D. As for the laggards, it can prove to be costly to neglect AI and ML.
Let's take a look at some promising machine learning applications in finance.
Process automation is one of the most common applications of machine learning in finance. The technology allows to replace manual work, automate repetitive tasks, and increase productivity.
As a result, machine learning enables companies to optimize costs, improve customer experiences, and scale up services. Here are automation use cases of machine learning in finance:
Below are some examples of process automation in banking:
JPMorgan Chase launched a Contract Intelligence (COiN) platform that leverages Natural Language Processing, one of the machine learning techniques. The solution processes legal documents and extracts essential data from them. Manual review of 12,000 annual commercial credit agreements would typically take up around 360,000 labor hours. Whereas, machine learning allows to review the same number of contracts in a just a few hours.
BNY Mello integrated process automation into their banking ecosystem. This innovation is responsible for $300,000 in annual savings and has brought about a wide range of operational improvements.
Wells Fargo uses an AI-driven chatbot through the Facebook Messenger platform to communicate with users and provide assistance with passwords and accounts.
Privatbank is a Ukrainian bank that implemented chatbot assistants across its mobile and web platforms. Chatbots sped up the resolution of general customer queries and allowed to decrease the number of human assistants.
Security threats in finance are increasing along with the growing number of transaction, users, and third-party integrations. And machine learning algorithms are excellent at detecting frauds.
For instance, banks can use this technology to monitor thousands of transaction parameters for every account in real time. The algorithm examines each action a cardholder takes and assesses if an attempted activity is characteristic of that particular user. Such model spots fraudulent behavior with high precision.
If the system identifies suspicious account behavior, it can request additional identification from the user to validate the transaction. Or even block the transaction altogether, if there is at least 95% probability of it being a fraud. Machine learning algorithms need just a few seconds (or even split seconds) to assess a transaction. The speed helps to prevent frauds in real time, not just spot them after the crime has already been committed.
Financial monitoring is another security use case for machine learning in finance. Data scientists can train the system to detect a large number of micropayments and flag such money laundering techniques as smurfing.
Machine learning algorithms can significantly enhance network security, too. Data scientists train a system to spot and isolate cyber threats, as machine learning is second to none in analyzing thousands of parameters and real-time. And chances are this technology will power the most advanced cybersecurity networks in the nearest future.
Adyen, Payoneer, Paypal, Stripe, and Skrill are some notable fintech companies that invest heavily in security machine learning.
Machine learning algorithms fit perfectly with the underwriting tasks that are so common in finance and insurance.
Data scientists train models on thousands of customer profiles with hundreds of data entries for each customer. A well-trained system can then perform the same underwriting and credit-scoring tasks in the real-life environments. Such scoring engines help human employees work much faster and more accurately.
Banks and insurance companies have a large number of historical consumer data, so they can use these entries to train machine learning models. Alternatively, they can leverage datasets generated by large telecom or utility companies.
For instance, BBVA Bancomer is collaborating with an alternative credit-scoring platform Destacame. The bank aims to increase credit access for customers with thin credit history in Latin America. Destacame accesses bill payment information from utility companies via open APIs. Using bill payment behavior, Destacame produces a credit score for a customer and sends the result to the bank.
In algorithmic trading, machine learning helps to make better trading decisions. A mathematical model monitors the news and trade results in real-time and detects patterns that can force stock prices to go up or down. It can then act proactively to sell, hold, or buy stocks according to its predictions.
Machine learning algorithms can analyze thousands of data sources simultaneously, something that human traders cannot possibly achieve.
Machine learning algorithms help human traders squeeze a slim advantage over the market average. And, given the vast volumes of trading operations, that small advantage often translates into significant profits.
Robo-advisors are now commonplace in the financial domain. Currently, there are two major applications of machine learning in the advisory domain.
Portfolio management is an online wealth management service that uses algorithms and statistics to allocate, manage and optimize clients' assets. Users enter their present financial assets and goals, say, saving a million dollars by the age of 50. A robo-advisor then allocates the current assets across investment opportunities based on the risk preferences and the desired goals.
Recommendation of financial products. Many online insurance services use robo-advisors to recommend personalized insurance plans to a particular user. Customers choose robo-advisors over personal financial advisors due to lower fees, as well as personalized and calibrated recommendations.
In spite of all the advantages of AI and machine learning, even companies with deep pockets often have a hard time extracting the real value from this technology. Financial services incumbents want to exploit the unique opportunities of machine learning but, realistically, they have a vague idea of how data science works, and how to use it.
Time and again, they encounter similar challenges like the lack of business KPIs. This, in turn, results in unrealistic estimates and drains budgets. It is not enough to have a suitable software infrastructure in place (although that would be a good start). It takes a clear vision, solid technical talent, and determination to deliver a valuable machine learning development project.
As soon as you have a good understanding of how this technology will help to achieve business objectives, proceed with idea validation. This is a task for data scientists. They investigate the idea and help you formulate viable KPIs and make realistic estimates.
Note that you need to have all the data collected at this point. Otherwise, you would need a data engineer to collect and clean up this data.
Depending on a particular use case and business conditions, financial companies can follow different paths to adopt machine learning. Let's check them out.
Often, financial companies start their machine learning projects only to realize they just need proper data engineering. Max Nechepurenko, a senior data scientist at N-iX, comments:
When developing a [data science] solution, I'd advise using the Occam's razor principle, which means not overcomplicating. Most companies that aim for machine learning in fact need to focus on solid data engineering, applying statistics to the aggregated data, and visualization of that data.
Merely applying statistical models to processed and well-structured data would be enough for a bank to isolate various bottlenecks and inefficiencies in its operations.
What are the examples of such bottlenecks? That could be queues at a specific branch, repetitive tasks that can be eliminated, inefficient HR activities, flaws of the mobile banking app, and so on.
What's more, the biggest part of any data science project comes down to building an orchestrated ecosystem of platforms that collect siloed data from hundreds of sources like CRMs, reporting software, spreadsheets, and more.
Before applying any algorithms, you need to have the data appropriately structured and cleaned up. Only then, you can further turn that data into insights. In fact, ETL (extracting, transforming, and loading) and further cleaning of the data account for around 80% of the machine learning project's time.
Even if your company decides to utilize machine learning in its upcoming project, you do not necessarily need to develop new algorithms and models.
Most machine learning projects deal with issues that have already been addressed. Tech giants like Google, Microsoft, Amazon, and IBM sell machine learning software as a service.
These out-of-the-box solutions are already trained to solve various business tasks. If your project covers the same use cases, do you believe your team can outperform algorithms from these tech titans with colossal R&D centers?
One good example is Google's multiple plug-and-play recommendation solutions. That software applies to various domains, and it is only logical to check if they fit to your business case.
A machine learning engineer can implement the system focusing on your specific data and business domain. The specialist needs to extract the data from different sources, transform it to fit for this particular system, receive the results, and visualize the findings.
The trade-offs are lack of control over the third-party system and limited solution flexibility. Besides, machine learning algorithms don't fit into every use case. Ihar Rubanau, a senior data scientist at N-iX comments:
A universal machine learning algorithm does not exist, yet. Data scientists need to adjust and fine-tune algorithms before applying them to different business cases across different domains.
So if an existing solution from Google solves a specific task in your particular domain, you should probably use it. If not, aim for custom development and integration
Developing a machine learning solution from scratch is one of the riskiest, most costly and time-consuming options. Still, this may be the only way to apply ML technology to some business cases.
Machine learning research and development targets a unique need in a particular niche, and it calls for an in-depth investigation. If there are no ready-to-use solutions that were developed to solve those specific problems, third-party machine learning software is likely to produce inaccurate results.
Still, you will probably need to rely heavily on the open source machine learning libraries from Google and the likes. Current machine learning projects are mostly about applying existing state-of-the-art libraries to a particular domain and use case.
At N-iX, we have identified seven common traits of a successful enterprise R&D project in machine learning. Here they are:
Small projects may require significantly less effort and a much smaller team. For instance, some R&D projects deal with small datasets, so they probably don't need sophisticated big data engineering. In other instances, there is no need in complex dashboards or any data visualization at all.
",136
https://medium.com/@noahbradley/minimum-wage-artists-4f8e00024a4?source=tag_archive---------5-----------------------,Minimum Wage Artists,"On the reality of ""starving artists"" in the games industry",Noah Bradley,3,"To my fellow artists, creatives, illustrators, and concept designers,
Do not work for less than minimum wage.
Companies like Fantasy Flight Games ask artists to do a fully painted illustration and sign away nearly all rights to that piece for just $100. That is rude, disgraceful, and downright wrong. We need to bring this to light so it can stop.
For a company pulling in tens of millions of dollars in revenue annually, surely they are capable of paying their freelance artists a reasonable rate.
$100, assuming the average artist will take around 15 hours to complete the work (factoring in communication, research, revisions, actual painting, and invoicing), will earn the artist somewhere around $7/hour, less than the current US federal minimum wage.
By most estimates, a freelancer should mentally divide any hourly rate they earn by half to compare it to any in-house, full time position (as a freelancer has additional taxes, has to factor in time for accounting and advertising, has to pay for health insurance, and has all of the overhead to supply their own equipment and work space).
So that puts our functional wage for that job closer to $3.50/hour.
""But I work a lot faster than that! I can make good money!"", they scream.
You really can't.
If you do an $100 piece every working day of the year, ignoring all holidays and never taking a vacation even for Christmas, you will earn a grand total of $26,100 at the end of the year, before taxes.
You will work like a dog to earn less than even the median American income to do a skilled job that helps your employer earn the position of one of the largest analog game companies.
But the thing is, you won't make that much. You're not going to make a livable income. You're not magically going to fill every hour of your day with doing paying work for companies. You will be like the majority of freelance artists in this field and earn below the poverty line. I've known far too many friends in this field who are at the top of their game and struggle to earn even $15,000. Most don't even earn $10,000 in a year.
But nobody talks about it. We all keep our mouths shut because money and rates are taboo subjects and we fear losing our jobs. We fear to lose the jobs that don't even pay us enough to live.
Still others will say ""But you're doing a job that you love! Don't you enjoy doing art? Why do you have to bring money into it?"" Please. Don't get me started. I know more than a few carpenters who love their job, but a remarkable few who don't think they deserve to be paid for their work.
Something has to change.
Artists need to be paid more. Not so we can sit comfortably and drink martinis or lounge on a beach all of the time. But just so we can be paid fairly and earn a decent, sustainable living. We're not asking for much.
We're only asking for enough.
So artists, do not be lured in by the prospect of working for ""a big company"" or for the illusory ""prestige"" of working on easily recognized IPs. A bad job is a bad job, and nearly anything Fantasy Flight Games will give you is a bad job.
I made the mistake of doing a few jobs for $100 when I was starting out years ago. I hope you won't make that same mistake.
Follow Noah Bradley on twitter
",137
https://medium.com/yueeh/%E6%BD%AE%E6%BD%AE%E5%BF%85%E5%82%99-richart-gogo-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97-de46637d7c5f?source=tag_archive---------1-----------------------,Richart + @GoGo ,",,",Yueh,11,",,
S : $110 !C :, Richart 
 Richart 
,,,
,Richart ?
Richart  2016 ,,()
, Richart ,;,,, + Richart App 
, Richart App ;,,  
  , Richart App  Richart , 300 ()
, Richart , @ GoGo ,
, Richart VISA ;
,;,,
2018/06/07 :,!,:
yueeh.com
 Richart App ,;,,,
,,,
,Richart ; @ GoGo 
,?
,;,, Richart App ,
,
Richart  5  Richart App ,
 ATM , Richart  15 ,, 20  LINE 
, ATM ,
,   ATM , Richart App   ,,
Richart  0.45% ,;, 1% ,
 2018/07/16   1% ?:
yueeh.com
 UI/UX ,,Richart App ;App ,,,
 UI,
 @ GoGo , @ GoGo 
,?
,, 1%, 10,000 , 100 ()
, Richart ;  
 3.5% ,:
(1) 0.5% ,(2) 2% ,(3) Richart , 1%,
: (2)  (3)  500 ,,,
()
, Richart , 1% ,,Richart (?)
  ,Apple , 3.5% ;, Airbnb 
, @ GoGo 
 icash 2.0 ,,icash 2.0 / ,,  
 icash 2.0 ( ),, icash 2.0  9023 ,()
(2019/01/19 ) GoGo  LED , icash 
Richart  & @ GoGo ,
, Richart 
 Facebook :Yueh  Life & Tech,
www.facebook.com
  ,
all about LIFE and TECH.
1.2K 
9
",138
https://medium.com/@nellsonx/how-to-properly-store-bitcoins-and-other-cryptocurrencies-14e0db1910d?source=tag_archive---------5-----------------------,How to store Bitcoins and other cryptocurrencies properly.,Power Guide for saving your money,Alexandr Nellson,6,"I will tell only a few possible stories that are waiting for you in the case of lack of computer literacy:
Clearly, this is not a complete list. But maybe it will give you some ideas when something goes wrong. Now you see, cryptocurrencies  is not bucks. You are fully responsible for it's safety. Neither Jesus Christ nor the Pope, nor Vladimir Putin nor Donald Trump will not be able to help you if you do something wrong. But those guys can't take your money if you do everything in a right way.
With great power comes great responsibility.
My overall recommendation is to increase computer literacy. A specific recommendation is act strictly according to instructions without unnecessary actions.
Here is one basic rule:
Do not give ANYONE your private key.
This means - do not use any of these wallets:
Cold storage in the context of Bitcoin refers to keeping a reserve of Bitcoins offline.
Methods of cold storage include keeping bitcoins:
I will share my own method which I use. The main thing to understand is the basic rule:
Do not give ANYONE your private key.
Step 1  Create a bootable USB flash drive Create a bootable USB flash drive with permanent encrypted storage. Tails will be perfect, because it has an Electrum (Bitcoin wallet) and encryption in the box. Set up two passwords for login and for encryption.Tutorial here
Step 2  Create a cold wallet.Load with Tails (without the Internet). Create your cold wallet using Electrum. Come up with a complex password and write down 12-13 words (seed) on paper or think of something other place.This seed  is your money!!! These 12 words will allow you to regain access to the wallet from any computer in the future.You must also select the check box on the option ""View Transaction Before Signing"" in the wallet Settings. Thus, you will cause Electrum to show the transaction details before enter a password (i.e. signature) and send the transaction to the network.
Step 3  Create a ""watch-only"" wallet in the main OS or mobile phone.Copy your Bitcoin public address from Electrum into a text document on a flash drive and shutdown the Tails.Create a 'watch-only' wallet using your public address or public key in any wallet app you like on main OS or on mobile phone. For example in Electrum you need to do the following steps:- Create new wallet- Choose ""standart wallet""- Choose ""Use public or private keys""- Paste your PUBLIC key in the next window > Create
Now we can view the balance of our bitcoins and generate new addresses from public key, but we can't spend any of our bitcoins.
Step 4-Spend Bitcoins in a secured wayTo send your Bitcoins in a secured way: run Tails again (do not use Internet).Then make the transaction > a window will appear (as we have enabled the checkbox """"View Transaction Before Signing"").At first click ""Sign"" and then when you enter the password, click ""Save"". Do not send the transaction to the network and do not enable internet, just copy the transaction file to the flash drive.
Start in the main OS > Open your wallet > In the menu choose something like ""Load transaction from file"" > load the signed transaction from a flash drive > and push ""Broadcast"" button
Thus the signed transaction is sent to the Blockchain from the 'watch-only' wallet, and your private key is not leaked anywhere from your Tails usb.
Very important. Always check address AFTER you have copied it in the field. Just going back to wallet, remember the first 2 digits and last 2, and then go to the form and check whether these are the numbers that you copied! First, we sometimes make mistakes. Practice shows that quite often. Second, now there are so smart viruses that will determine when you copied the bitcoin address to the clipboard, and replace it to another address.
The same logic can be used in relation to all other cryptocurrencies.
I agree that this scheme is not the most comfortable to use, because you need to make a lot of manipulations with multiple OS and usb devices.
If you are looking for a good usability  look at buying a hardware wallet. These hardware wallets works by the scheme described above, but also they have some unique advantages:
Some of these wallets (e.g. Ledger Nano S wallet) support not only Bitcoin but also Ethereum, Litecoin, Dogecoin, ZCash, DASH, Stratis and more. Also it has integrations with many wallet apps like Electrum, BitGo, Copay, GreenBit and others.
That's all. I hope you got the main rule. Please, write your comments and questions below. And one more thing: let's spread the knowledge. Recommend and Share this article. Thank you for reading!
",140
https://medium.com/startup-grind/i-was-wrong-about-ethereum-804c9a906d36?source=tag_archive---------0-----------------------,I was wrong about Ethereum,"I was wrong about Ethereum because it's such a good store of value... no wait, let me try again.",WhalePanda,9,"I was wrong about Ethereum because it's such a decentra... nope.I was wrong about Ethereum because everyone is using it as a supercomputer... No.
But, then again I wrongly assumed that no startup would need or even dare to ask $50 million in funding. I also wrongly assumed that people would use common sense and that leading developers would speak out against this sort of practice. Quite the opposite it seems.
Greed.Greed from speculators, investors and developers.
Speculators and investors: No.Developers: Absolutely.
So let's think for a minute and think what determines the price? Supply + demand. Pretty straightforward.
The tokens that are available on the market, right? But with every ICO there are more tokens that are being ""locked up."" Obviously the projects will liquidate some, to get fiat to pay for development of their project, but they also see the rising price of Ethereum.
So at that point greed takes over and they think, totally understandable, ""We should probably just cash out what we really need and keep the rest in ETH, that's only going up anyway it seems.""
And obviously there are new coins being mined, but if you look at the amount of ETH these ICOs raise, at this point, it's just a drop in a bucket.
You have the normal investors (who are already very late to the game at this point... as usual), but the buy pressure that these ICOs are creating is crazy and scary. Take TenX for example, it's an upcoming ICO at the end of the month. The cap is 200,000 ETH (at current ETH price of $370) that's $74,000,000 for a startup.
Here's the best part: it's only 51% of the tokens. Effectively giving it an instant $150 million valuation (if it sells out, which it probably will).Another example is Bancor, a friend of mine runs a trading group, he collected 1,100+ BTC to put into Bancor.
This needs to be converted into ETH before the sale starts. These are decent size players, but not even the big whales who participate in these ICOs.
It can go quite a bit higher, there are so many coins being taken off the market by these ICOs, that it can still continue for a while and everyone is seeing this and thinking: ""Why aren't I doing an ICO."" There are lots more coming.
At one point it will crash, hard. What the trigger will be?
Bug(s) in smart contracts.
Major hack.
Big ICO startup that fails/fucks up,
Network split.
Even something as silly as not having a decent ICO for a couple of weeks, which creates sell pressure from miners and ICO projects can cause a big crash.
Usually when a bubble like this pops we could easily see 70-80% loss of value (for reference: Bitcoin went from $1,200 to $170 after 2013-2014 bubble). This is however quite the unusual situation and I'm not sure to what kind of bubble I can really compare it.
I'm sure most of you have seen ""Wolf of Wall Street."" Just re-watch this clip and see if you find any similarities with the current situation. (bonus clip)
What I really find interesting is what the ICO startups will do, Bitcoin had hodlers and investors mainly, individuals who most of the time had a full-time job and didn't need to sell.
With Ethereum there is this huge amount being held by companies who need to pay bills. Will they panic dump to secure a ""healthy"" amount of fiat funding, will they try to hold through a bear cycle?
Everyone loves making money, you can't blame traders or investors from taking advantage of this hype. That would be silly. People will buy literally anything if they can make a quick buck out of it.
The responsibility here is with the developers, Consensys and the Ethereum Foundation but they don't take responsibility since they're getting more money. This will end with the regulators stepping in.
The developers of a project assign these crazy tokensale caps, more money than any startup would ever need.
The Ethereum foundation members+ core developers use their own celeb status to actively promote these projects as advisors, for which they're compensated well, luring in people who have no clue what they're buying.
Consensys promotes all of this since it's the marketing branch of Ethereum. The more fools that buy in, the better.
Have you heard of primalbase? It's an ICO with a token for shared workspaces. Why would a shared workspace need its own token? It doesn't, it really really really doesn't.
First thing that an advisor should've said in this case was: ""Don't do it, it's stupid, it makes no sense."" But well there we have Mr. Ethereum himself.
We all know that Vitalik has a cult-like following with the Ethereum investors so it will be very easy for primalbase to launch their ICO and use Vitalik's face and name to get itself funded.
This is just one example, if you go through all of these ICOs you find a lot of familiar names and faces.
Nothings wrong with being an advisor, but when you're just sending people to the slaughterhouse...
The sad part is that a lot of people will lose a lot of money on this, some of them obviously more than they can afford to lose, that's how it always goes.
It isn't capped. Yes, I know they're planning to switch to PoS (which it already kind of is). Do you think they managed to create the first software implementation ever without any bugs? Doing such a major change on a (currently) $30 billion market is completely irresponsible, borderline insanity.
Even if we assume that there are no bugs, what about the miners? The miners who bought their equipment to mine Ethereum, the miners that supported the network for years. ""But they knew we were switching to PoS."" Of course they knew, and do you think they'll just give up on such a profitable coin?
Some might switch immediately to Zcash and Ethereum Classic but there will be another fork and we'll have ETHPoS and ETHPoW, with of course all the Ethereum tokens being on both chains.
Ethereum's fees are lower. They are, sometimes, by a bit. If you're trying to send something when no token sale is active obviously, else you have people spending $100's to get in on the token sale and clog up the network.
Also doesn't apply when you send something from exchanges since for example with Poloniex it's about $1.9 vs $0.28 for Bitcoin. Oh and another exception is when you actually use it for smart contracts, which require more gas to process than a normal transactions from account A to account B.
You know.. the actual reason why Ethereum was created.
Bitcoin isn't as decentralized as it should be, we all know that, but compared to most other coins, Bitcoin is very decentralized.
Vitalik has called himself a benevolent dictator in the past. He is the single point of failure in this project and if he gets compromised in any way that's the end.
There is no way of knowing if this happens and since people blindly follow everything he says, he has the power to do anything. Satoshi was smart enough to remove himself from the Bitcoin project.
Don't have to spend much time on this: see DAO and split that lead to Ethereum and Ethereum Classic.
But but but.. all those big banks use Ethereum. No, they don't. They use ""an"" Ethereum, which is a (private) fork of Ethereum. By that definition 99% of all altcoins are using Bitcoin.
Still a separate chain. The fact that we're talking about a private blockchain here actually makes altcoins more like Bitcoin than ""an Ethereum"" that EEA uses like Ethereum.
You can compare it to 2013-2014 when some companies started to get interested in blockchain vs Bitcoin, only difference here is that for Ethereum it's part of their marketing campaign to lure in potential investors.
If you think I'm just full of crap, which is fair, I am just some random popular guy on Twitter who has been around from before Ethereum.
Have a look at what Vlad has to say about the current state of Ethereum here and here. Vlad Zamfir is probably the smartest guy on the Ethereum team, and I say this while I don't agree with him on many of his opinions, I do respect him.
If you're an actual developer, be realistic and honest with your investors. Do you really ever need more than $5 mill? Finish a MVP first and then do a tokensale, if you really really need to do an ICO.
Plenty of rich crypto investors and traders now that would love to be part of your project and who would be happy to just invest for equity. Yes, it will probably be less than what you can get in an ICO, but at least you didn't sell out and it shows you actually really care about your product/service/...
If you're a trader or investor, be realistic about the bubble. I know you hear this a 100 times when you're trading but: don't invest what you can't afford to lose.
I have some Ethereum, not as a long term investment, but because the price is going up and I need it to invest in tokens which I can quickly flip as soon as they come on the market.
That's just the type of market we're in. Everyone is making a lot of money, awesome right?
Stories, tips, and learnings from and for startups around...
3.9K 
132
",141
https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------3-----------------------,Deep Learning the Stock Market,Update 25.1.17  Took me a while but here is an ipython notebook with a rough implementation,Tal Perry,17,"Update 25.1.17  Took me a while but here is an ipython notebook with a rough implementation
In the past few months I've been fascinated with ""Deep Learning"", especially its applications to language and text. I've spent the bulk of my career in financial technologies, mostly in algorithmic trading and alternative data services. You can see where this is going.
I wrote this to get my ideas straight in my head. While I've become a ""Deep Learning"" enthusiast, I don't have too many opportunities to brain dump an idea in most of its messy glory. I think that a decent indication of a clear thought is the ability to articulate it to people not from the field. I hope that I've succeeded in doing that and that my articulation is also a pleasurable read.
Why NLP is relevant to Stock prediction
In many NLP problems we end up taking a sequence and encoding it into a single fixed size representation, then decoding that representation into another sequence. For example, we might tag entities in the text, translate from English to French or convert audio frequencies to text. There is a torrent of work coming out in these areas and a lot of the results are achieving state of the art performance.
In my mind the biggest difference between the NLP and financial analysis is that language has some guarantee of structure, it's just that the rules of the structure are vague. Markets, on the other hand, don't come with a promise of a learnable structure, that such a structure exists is the assumption that this project would prove or disprove (rather it might prove or disprove if I can find that structure).
Assuming the structure is there, the idea of summarizing the current state of the market in the same way we encode the semantics of a paragraph seems plausible to me. If that doesn't make sense yet, keep reading. It will.
You shall know a word by the company it keeps (Firth, J. R. 1957:11)
There is tons of literature on word embeddings. Richard Socher's lecture is a great place to start. In short, we can make a geometry of all the words in our language, and that geometry captures the meaning of words and relationships between them. You may have seen the example of ""King-man +woman=Queen"" or something of the sort.
Embeddings are cool because they let us represent information in a condensed way. The old way of representing words was holding a vector (a big list of numbers) that was as long as the number of words we know, and setting a 1 in a particular place if that was the current word we are looking at. That is not an efficient approach, nor does it capture any meaning. With embeddings, we can represent all of the words in a fixed number of dimensions (300 seems to be plenty, 50 works great) and then leverage their higher dimensional geometry to understand them.
The picture below shows an example. An embedding was trained on more or less the entire internet. After a few days of intensive calculations, each word was embedded in some high dimensional space. This ""space"" has a geometry, concepts like distance, and so we can ask which words are close together. The authors/inventors of that method made an example. Here are the words that are closest to Frog.
But we can embed more than just words. We can do, say , stock market embeddings.
Market2Vec
The first word embedding algorithm I heard about was word2vec. I want to get the same effect for the market, though I'll be using a different algorithm. My input data is a csv, the first column is the date, and there are 4*1000 columns corresponding to the High Low Open Closing price of 1000 stocks. That is my input vector is 4000 dimensional, which is too big. So the first thing I'm going to do is stuff it into a lower dimensional space, say 300 because I liked the movie.
Taking something in 4000 dimensions and stuffing it into a 300-dimensional space my sound hard but its actually easy. We just need to multiply matrices. A matrix is a big excel spreadsheet that has numbers in every cell and no formatting problems. Imagine an excel table with 4000 columns and 300 rows, and when we basically bang it against the vector a new vector comes out that is only of size 300. I wish that's how they would have explained it in college.
The fanciness starts here as we're going to set the numbers in our matrix at random, and part of the ""deep learning"" is to update those numbers so that our excel spreadsheet changes. Eventually this matrix spreadsheet (I'll stick with matrix from now on) will have numbers in it that bang our original 4000 dimensional vector into a concise 300 dimensional summary of itself.
We're going to get a little fancier here and apply what they call an activation function. We're going to take a function, and apply it to each number in the vector individually so that they all end up between 0 and 1 (or 0 and infinity, it depends). Why ? It makes our vector more special, and makes our learning process able to understand more complicated things. How?
So what? What I'm expecting to find is that that new embedding of the market prices (the vector) into a smaller space captures all the essential information for the task at hand, without wasting time on the other stuff. So I'd expect they'd capture correlations between other stocks, perhaps notice when a certain sector is declining or when the market is very hot. I don't know what traits it will find, but I assume they'll be useful.
Now What
Lets put aside our market vectors for a moment and talk about language models. Andrej Karpathy wrote the epic post ""The Unreasonable effectiveness of Recurrent Neural Networks"". If I'd summarize in the most liberal fashion the post boils down to
And then as a punchline, he generated a bunch of text that looks like Shakespeare. And then he did it again with the Linux source code. And then again with a textbook on Algebraic geometry.
So I'll get back to the mechanics of that magic box in a second, but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one. Where Karpathy used characters, we're going to use our market vectors and feed them into the magic black box. We haven't decided what we want it to predict yet, but that is okay, we won't be feeding its output back into it either.
Going deeper
I want to point out that this is where we start to get into the deep part of deep learning. So far we just have a single layer of learning, that excel spreadsheet that condenses the market. Now we're going to add a few more layers and stack them, to make a ""deep"" something. That's the deep in deep learning.
So Karpathy shows us some sample output from the Linux source code, this is stuff his black box wrote.
Notice that it knows how to open and close parentheses, and respects indentation conventions; The contents of the function are properly indented and the multi-line printk statement has an inner indentation. That means that this magic box understands long range dependencies. When it's indenting within the print statement it knows it's in a print statement and also remembers that it's in a function( Or at least another indented scope). That's nuts. It's easy to gloss over that but an algorithm that has the ability to capture and remember long term dependencies is super useful because... We want to find long term dependencies in the market.
Inside the magical black box
What's inside this magical black box? It is a type of Recurrent Neural Network (RNN) called an LSTM. An RNN is a deep learning algorithm that operates on sequences (like sequences of characters). At every step, it takes a representation of the next character (Like the embeddings we talked about before) and operates on the representation with a matrix, like we saw before. The thing is, the RNN has some form of internal memory, so it remembers what it saw previously. It uses that memory to decide how exactly it should operate on the next input. Using that memory, the RNN can ""remember"" that it is inside of an intended scope and that is how we get properly nested output text.
A fancy version of an RNN is called a Long Short Term Memory (LSTM). LSTM has cleverly designed memory that allows it to
So an LSTM can see a ""{"" and say to itself ""Oh yeah, that's important I should remember that"" and when it does, it essentially remembers an indication that it is in a nested scope. Once it sees the corresponding ""}"" it can decide to forget the original opening brace and thus forget that it is in a nested scope.
We can have the LSTM learn more abstract concepts by stacking a few of them on top of each other, that would make us ""Deep"" again. Now each output of the previous LSTM becomes the inputs of the next LSTM, and each one goes on to learn higher abstractions of the data coming in. In the example above (and this is just illustrative speculation), the first layer of LSTMs might learn that characters separated by a space are ""words"". The next layer might learn word types like (static void action_new_function).The next layer might learn the concept of a function and its arguments and so on. It's hard to tell exactly what each layer is doing, though Karpathy's blog has a really nice example of how he did visualize exactly that.
Connecting Market2Vec and LSTMs
The studious reader will notice that Karpathy used characters as his inputs, not embeddings (Technically a one-hot encoding of characters). But, Lars Eidnes actually used word embeddings when he wrote Auto-Generating Clickbait With Recurrent Neural Network
The figure above shows the network he used. Ignore the SoftMax part (we'll get to it later). For the moment, check out how on the bottom he puts in a sequence of words vectors at the bottom and each one. (Remember, a ""word vector"" is a representation of a word in the form of a bunch of numbers, like we saw in the beginning of this post). Lars inputs a sequence of Word Vectors and each one of them:
We're going to do the same thing with one difference, instead of word vectors we'll input ""MarketVectors"", those market vectors we described before. To recap, the MarketVectors should contain a summary of what's happening in the market at a given point in time. By putting a sequence of them through LSTMs I hope to capture the long term dynamics that have been happening in the market. By stacking together a few layers of LSTMs I hope to capture higher level abstractions of the market's behavior.
What Comes out
Thus far we haven't talked at all about how the algorithm actually learns anything, we just talked about all the clever transformations we'll do on the data. We'll defer that conversation to a few paragraphs down, but please keep this part in mind as it is the se up for the punch line that makes everything else worthwhile.
In Karpathy's example, the output of the LSTMs is a vector that represents the next character in some abstract representation. In Eidnes' example, the output of the LSTMs is a vector that represents what the next word will be in some abstract space. The next step in both cases is to change that abstract representation into a probability vector, that is a list that says how likely each character or word respectively is likely to appear next. That's the job of the SoftMax function. Once we have a list of likelihoods we select the character or word that is the most likely to appear next.
In our case of ""predicting the market"", we need to ask ourselves what exactly we want to market to predict? Some of the options that I thought about were:
1 and 2 are regression problems, where we have to predict an actual number instead of the likelihood of a specific event (like the letter n appearing or the market going up). Those are fine but not what I want to do.
3 and 4 are fairly similar, they both ask to predict an event (In technical jargon  a class label). An event could be the letter n appearing next or it could be Moved up 5% while not going down more than 3% in the last 10 minutes. The trade-off between 3 and 4 is that 3 is much more common and thus easier to learn about while 4 is more valuable as not only is it an indicator of profit but also has some constraint on risk.
5 is the one we'll continue with for this article because it's similar to 3 and 4 but has mechanics that are easier to follow. The VIX is sometimes called the Fear Index and it represents how volatile the stocks in the S&P500 are. It is derived by observing the implied volatility for specific options on each of the stocks in the index.
Sidenote  Why predict the VIX
What makes the VIX an interesting target is that
Back to our LSTM outputs and the SoftMax
How do we use the formulations we saw before to predict changes in the VIX a few minutes in the future? For each point in our dataset, we'll look what happened to the VIX 5 minutes later. If it went up by more than 1% without going down more than 0.5% during that time we'll output a 1, otherwise a 0. Then we'll get a sequence that looks like:
0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,1,1,1,0,0,0,0,0 ....
We want to take the vector that our LSTMs output and squish it so that it gives us the probability of the next item in our sequence being a 1. The squishing happens in the SoftMax part of the diagram above. (Technically, since we only have 1 class now, we use a sigmoid ).
So before we get into how this thing learns, let's recap what we've done so far
How does this thing learn?
Now the fun part. Everything we did until now was called the forward pass, we'd do all of those steps while we train the algorithm and also when we use it in production. Here we'll talk about the backward pass, the part we do only while in training that makes our algorithm learn.
So during training, not only did we prepare years worth of historical data, we also prepared a sequence of prediction targets, that list of 0 and 1 that showed if the VIX moved the way we want it to or not after each observation in our data.
To learn, we'll feed the market data to our network and compare its output to what we calculated. Comparing in our case will be simple subtraction, that is we'll say that our model's error is
ERROR = (((precomputed) (predicted probability))2 )^(1/2)
Or in English, the square root of the square of the difference between what actually happened and what we predicted.
Here's the beauty. That's a differential function, that is, we can tell by how much the error would have changed if our prediction would have changed a little. Our prediction is the outcome of a differentiable function, the SoftMax The inputs to the softmax, the LSTMs are all mathematical functions that are differentiable. Now all of these functions are full of parameters, those big excel spreadsheets I talked about ages ago. So at this stage what we do is take the derivative of the error with respect to every one of the millions of parameters in all of those excel spreadsheets we have in our model. When we do that we can see how the error will change when we change each parameter, so we'll change each parameter in a way that will reduce the error.
This procedure propagates all the way to the beginning of the model. It tweaks the way we embed the inputs into MarketVectors so that our MarketVectors represent the most significant information for our task.
It tweaks when and what each LSTM chooses to remember so that their outputs are the most relevant to our task.
It tweaks the abstractions our LSTMs learn so that they learn the most important abstractions for our task.
Which in my opinion is amazing because we have all of this complexity and abstraction that we never had to specify anywhere. It's all inferred MathaMagically from the specification of what we consider to be an error.
What's next
Now that I've laid this out in writing and it still makes sense to me I want
So, if you've come this far please point out my errors and share your inputs.
Other thoughts
Here are some mostly more advanced thoughts about this project, what other things I might try and why it makes sense to me that this may actually work.
Liquidity and efficient use of capital
Generally the more liquid a particular market is the more efficient that is. I think this is due to a chicken and egg cycle, whereas a market becomes more liquid it is able to absorb more capital moving in and out without that capital hurting itself. As a market becomes more liquid and more capital can be used in it, you'll find more sophisticated players moving in. This is because it is expensive to be sophisticated, so you need to make returns on a large chunk of capital in order to justify your operational costs.
A quick corollary is that in less liquid markets the competition isn't quite as sophisticated and so the opportunities a system like this can bring may not have been traded away. The point being were I to try and trade this I would try and trade it on less liquid segments of the market, that is maybe the TASE 100 instead of the S&P 500.
This stuff is new
The knowledge of these algorithms, the frameworks to execute them and the computing power to train them are all new at least in the sense that they are available to the average Joe such as myself. I'd assume that top players have figured this stuff out years ago and have had the capacity to execute for as long but, as I mention in the above paragraph, they are likely executing in liquid markets that can support their size. The next tier of market participants, I assume, have a slower velocity of technological assimilation and in that sense, there is or soon will be a race to execute on this in as yet untapped markets.
Multiple Time Frames
While I mentioned a single stream of inputs in the above, I imagine that a more efficient way to train would be to train market vectors (at least) on multiple time frames and feed them in at the inference stage. That is, my lowest time frame would be sampled every 30 seconds and I'd expect the network to learn dependencies that stretch hours at most.
I don't know if they are relevant or not but I think there are patterns on multiple time frames and if the cost of computation can be brought low enough then it is worthwhile to incorporate them into the model. I'm still wrestling with how best to represent these on the computational graph and perhaps it is not mandatory to start with.
MarketVectors
When using word vectors in NLP we usually start with a pretrained model and continue adjusting the embeddings during training of our model. In my case, there are no pretrained market vector available nor is tehre a clear algorithm for training them.
My original consideration was to use an auto-encoder like in this paper but end to end training is cooler.
A more serious consideration is the success of sequence to sequence models in translation and speech recognition, where a sequence is eventually encoded as a single vector and then decoded into a different representation (Like from speech to text or from English to French). In that view, the entire architecture I described is essentially the encoder and I haven't really laid out a decoder.
But, I want to achieve something specific with the first layer, the one that takes as input the 4000 dimensional vector and outputs a 300 dimensional one. I want it to find correlations or relations between various stocks and compose features about them.
The alternative is to run each input through an LSTM, perhaps concatenate all of the output vectors and consider that output of the encoder stage. I think this will be inefficient as the interactions and correlations between instruments and their features will be lost, and thre will be 10x more computation required. On the other hand, such an architecture could naively be paralleled across multiple GPUs and hosts which is an advantage.
CNNs
Recently there has been a spur of papers on character level machine translation. This paper caught my eye as they manage to capture long range dependencies with a convolutional layer rather than an RNN. I haven't given it more than a brief read but I think that a modification where I'd treat each stock as a channel and convolve over channels first (like in RGB images) would be another way to capture the market dynamics, in the same way that they essentially encode semantic meaning from characters.
",142
https://medium.com/the-mission/financial-fridays-it-s-financial-suicide-to-own-a-house-ba03ef98c8e8?source=tag_archive---------9-----------------------,Financial Fridays: It's Financial Suicide To Own A House,I am sick of me writing about this. Do you ever get sick of yourself? I am sick of me.,James Altucher,10,"I am sick of me writing about this. Do you ever get sick of yourself? I am sick of me.
But every day I see more propaganda about the American Dream of owning the home.
I see codewords a $15 trillion dollar industry uses to hypnotize its religious adherents to BELIEVE.
Lay down your money, your hard work, your lives and loves and debt, and BELIEVE!
But I will qualify: if someone wants to own a home, own one. There should never be a judgment. I'm the last to judge. I've owned two homes. And lost two homes.
If were to write an autobiography called: ""My life  10 Miserable moments"" owning a home would be two of them.
I will never write that book, though, because I have too many moments of pleasure. I focus on those.
But I will tell you the reasons I will never own a home again.
Maybe some of you have read this before from me. I will try to add. Or, even better, be more concise.
IT'S NOT AN INVESTMENT
Everyone has a story. And we love our stories. We see life around us through the prism of story.
So here's a story. Mom and Dad bought a house, say in 1965, for $30,000. They sold it in 2005 for $1.5 million and retired.
That's a nice story. I like it. It didn't happen to my mom and dad. The exact opposite happened. But...for some moms I hope it went like that.
Maybe Mom and Dad had their troubles, their health issues, their marriage issues. Maybe they both loved someone else but they loved their home.
Here's a fact: The average house has gone up 0.2% per year for the past century.
Only in small periods have housing prices really jumped and usually right after, they would fall again.
The best investor in the world, Warren Buffett, is not good enough to invest in real estate. He even laughs and says he's lost money on every real estate decision he's made.
He's a liar also. So I don't know. But that's what he says.
There's about $15 trillion in mortgage debt in the United States. This is the ENTIRE way banks make money.
They want you to take on debt. Else they go out of business and many people lose their jobs.
So they say, and the real estate agents say, and the furniture warehouses say, and your neighbors say, ""it's the American Dream"".
But does a country dream? Do all 320 million of us have the same dream?
What could we do as a society if we had our $15 trillion back? If maybe banks loaned money to help people build businesses and make new discoveries and hire people.
HOUSING IS NOT AN INVESTMENT, PART II
Let me tell you the qualities of a good investment:
a. It's not the bulk of your net worth. Good investments are usually part of a diversified set of investments you make in your life, including the investment you make in yourself (acquiring more skills, having more experiences, etc).
b. It doesn't require heavy debt. (see above, i.e. $15,000,000,000,000)
c. You can get your money back when you need it.
From hard experience I know when I needed money most, it's exactly at those moments I can't get it. The house can't get sold.
And the bank that was so friendly lending the money, starts calling within 12 hours of not getting their check. And then starts suing me.
Usually when I make an investment, I'm not the one getting sued. Except when I buy a house.
ISN'T RENTING LIKE THROWING MONEY DOWN THE TOILET?
No, renting is like ""making money"". And I will tell you how.
Let's say you want to buy a $500,000 house at a 6% mortgage.
You put $200,000 down.
The entire house would rent for about $2500, give or take. So that's 80 months or almost eight years worth of rent you just gave to the bank in a single check.
Do you ever get that bank money back?
No, because after mortgage debt (most of which cannot be written off in taxes), property maintenance, and taxes (which go up with inflation and are almost never considered in the price of the house), closing costs, buying costs, title insurance, property upgrades, etc. the homeowner might spend close to $1,000,000 in the lifespan of the house. Or twice that.
So instead of writing that $200,000 check in one day (as opposed to spreading rent out over eight years and the landlord is in charge of all maintenance, taxes, etc so you don't have to deal with it), you could invest in yourself.
Can you get more than 0.2% a year investing in yourself?
I hope so. Simple example: If you take two or three courses in a month on WordPress development, you can take freelance jobs making $5000 a month.
I know 14 year olds doing that. Illustration, ghostwriting, 3D rendering, are other skills you can learn. And many more. There are 1000 ways to make more.
How much do those courses cost? Often nothing. But definitely less than a mortgage.
Every investment in the world is judged by its SAFETY VERSUS ALTERNATIVES. A house investment is not safe versus the alternatives.
HOUSE OWNER: IT'S GOOD TO HAVE ROOTS
The average house owner, owns their house for 4.5 years. Some own for much longer, some own for less. That's just an average.
4.5 years is not ""roots"".
Why do people move? Because jobs are no longer as stable as they once were.
And they are no longer in one or two cities but all over the country or world.
So the original reasons for owning a house (a guaranteed easy commute into an urban area where the jobs are) are no longer valid, as demonstrated by the increasingly short lifespan of house ownership.
This is a trend that is continuing forever.
OPPORTUNITY COST
The other day my sink broke. How come? Because hair falls out in the shower, stuff gets put in the toilet that shouldn't go there, food gets caught in the pipes, and a million other things.
My house is 150 years old. It used to be a hotel. Things break. Pipes crumble in the hands of the plumber.
I email the landlord, who calls a plumber, who gets new pipes that are paid for by the landlord. The landlord wasn't expecting it but that's what they signed up for.
Meanwhile, I read a book on the couch in the other room.
The same thing when Hurricane Sandy came over the river. People were canoeing in the street outside my house. The water filled two feet in my house.
""This is the first time in 100 years the water got this high,"" the landlord told me. So he ripped up floors, cleaned out mold, fixed furniture, and took care of it.
This time I was upstairs reading a book.
FLEXIBILITY
Some people like to know where they will be in 30 years. They feel comfort in that.
When you rent, you never know if you will be kicked out eventually or if the house will get sold and you have to move.
So there is no judging here. But I like flexibility in my life. I like to know I can move. And in my area, so many houses are for sale, I always know I can find a good place to rent.
And with so many houses for sale, I know those people are stuck while I am not.
Will it always be that way? No. Things cycle. But America has a tendency to overbuild. And then people overbuy. And then rentals are available.
I always look at rentals. Right now there are better houses for less rent available within a mile of my house. But I like my landlord and house and I don't blow a good thing if I have it.
I live right on the river and can watch the leaves turn green and in the summer there are giant parties in the park next to my house.
And on Sunday nights they show movies outside next door and the whole town shows up. I watched ""Bladerunner"".
But I still want the ability to pick up and move at a moment's notice if I want to. Freedom makes me happy.
PROPERTY RIGHTS ARE THE BASIS OF AMERICA
Many people like to own real estate because of the word ""real"". It feels more real than money.
Or stocks. Or bonds.
I get that. It is real. And in America, nobody can take your land from you if you own it.
But not many people own their land. The bank owns it. Hence the $15 trillion in debt.
And people will never own it (the 4.5 year average thing).
But this is a judgment call again.
I like to know I can live out of a single bag. I've been doing that all my life.
When I moved to NYC I lived out of a garbage bag. Before I got married I lived in a dive hotel. After I got divorced I lived in the same hotel.
I like feeling like I could lose everything and survive. Maybe this is why I have lost everything sometimes. But it's also how I keep surviving and learning more each time.
This will sound corny so please skip to the next part: but property rights are not real.
Loving who you are and where you are and what you are doing is the only thing that is real.
Live in your heart and not your home and you will never feel lonely or the need to establish roots.
Share that love with the people around you. And then, they also, will feel less need for roots.
That is the best investment. That is the best return on investment. That is the best home to live in.
The America Dream has us chained us to the land so they can feed us like pigs in a trough with debt, with factory/cubicle jobs that we can't escape because it's so hard to move (until they kick us out with 2 weeks severance), with forced friends in our neighbors, with supposed roots for our kids even though the statistics show those roots are a lie.
Freedom is more important than a dream.
Everyone has the story. They have bought and sold three houses and made money on each of them.
I believe them. Perhaps many people are phenomenal investors.
Others live in a good, secure neighborhoods that they want their kids to grow up in.
I believe those people also. But I've also seen the pain they've gone through when jobs were not as stable as they thought or marriages are not as stable as they thought and that mortgage would've been nice in their hands instead of in the bank's hands.
We need a little bit of breathing room in order to survive when the noose is put around our neck.
WHAT DO I DO THEN?
You can rent. Just like some houses are bad and some are good, some landlords are better than others. Like anything that is an important life decision, it takes research.
You can find roots with a good landlord. You can even paint the house and knock down walls and do whatever you want.
If you believe in housing as an investment, there are companies that just own houses that you can invest in on the stock market.
So you get all the benefits of a long-term investment in housing and get your cash out in five seconds if you need it.
But what should you do with all of that extra cash you have if you don't own a house?
Maybe nothing. Having cash is a nice thing. It reduces stress.
But also you can invest in yourself. Or companies that are growing.
If companies aren't growing, I can tell you that housing prices will go lower. Because housing prices depend on the stability of employment.
By definition then, companies will always grow faster than housing, in aggregate.
Average income for people age 18-35 has done from $36,000 to $33,000 in the past twenty years. While debt has increased 100x. Not good.
WHY DO PEOPLE ALWAYS ARGUE FOR HOUSING
There's something called ""investment bias"". Your brain thinks, ""I've just made the biggest investment of my life so it must be right"".
Your brain loves you. It doesn't want you to think it made a bad decision for you. It's scared you won't use it anymore.
So it tells you, ""that $200,000 down was the best decision you ever made. Everything else involves flushing money down the toilet, or no roots, or no stability!"" So it's hard to consider the alternatives.
It's a lot of work to own a house also. Have you ever spent time in the Death Star? I mean Home Depot. That place is huge. And I only need that one special color of paint.
But where is it? The stormtroopers at Home Depot are never around when you need them.
And what about that ""snake"" that can clean my toilet. Where is it? And how do I use it? And is it gross? Why do they call it a snake?
It's no wonder that plumbing is one of the highest paid professions in America.
And how long does it take to paint a house. Or who do I go to? And will they overcharge me if they pave the driveway?
Did I calculate that into my total cost of owning a house?
I like to sit in the garden area of Home Dept. There's thousands of flowers and plants and it smells like dirt.
To be honest, that's the closest I will ever get to hiking  sitting in the garden area of Home Depot.
I'm pathetic. And I flush my rent down the toilet. And I don't have roots. And I refuse to fix my toilets or shovel my driveway or deal with my flooded basement. All I like to do is read.
And one day I'll move. Maybe next to an ocean. And take a walk on the beach. Last week, a friend told me the sun sets in the West.
Maybe one day I'll move to California. Five years until my youngest graduates.
I'll sit on the porch and watch the sun set and have cash in the bank (I hope) while someone is fixing my toilet.
When the sun has 15 minutes yet to live that day, maybe I will feel like I'm falling in love.
A network of business & tech podcasts designed to...
387 
19
",143
https://medium.com/@allenfarrington/gauge-theory-does-not-fix-this-625f98de3246?source=tag_archive---------2-----------------------,Gauge Theory Does Not Fix This,"or, why do intellectuals oppose Bitcoin?",allen farrington,25,"or, why do intellectuals oppose Bitcoin?
I should start off by saying that I have nothing against Eric Weinstein. Readers need not worry that this is another Talebenning. It's a little suspicious that Weinstein claims Taleb is ""incredibly subtle"", but we all have our foibles. I liked Taleb once, too, after all.
Unlike Taleb, who is a bullying, cowardly, bullshitting charlatan, Weinstein seems like a perfectly nice and well-intentioned guy. He is certainly extraordinarily intelligent, which might seem like it makes this whole episode all the more bizarre, but I think it points to a deeper truth.
Being smart doesn't really matter. In fact, it could well be a handicap. I am remound of Robert Nozick's essay, Why Do Intellectuals Oppose Capitalism? I won't repeat the entire argument here; readers can bookmark the link above and digest in their own time (it's not long). But the gist of it is that people whose profession or primary intellectual pursuits consist of ""wordsmithery"", as Nozick calls it  competitively putting forward essentially verbal arguments in the hope of enacting influenceseem inclined to find unfair and unjust a dynamic in which this gets you nowhere. They are used to ""central planning in the classroom"" in which rewards are dished out on the basis of perceived merit  i.e. ""politics""  and there is no ""anarchy and chaos of the marketplace.""
So far, this probably doesn't sound like Weinstein at all. He certainly doesn't ""oppose capitalism"", nor is he a ""wordsmith"". Quite the contrary, he is a ""numbersmith"" of the variety Nozick goes out his way to exclude, and as Managing Director of Thiel Capital, he could hardly be more capitalist. If Peter Thiel is Dr Evil, then Weinstein is Dr Evil's evil cat.
But there is a subtler undercurrent to Nozick which I feel has some heft here: intellectuals tend to have grand and all-encompassing theories that it is entirely within their power to shape and perfect, and which are constructed such that they are essentially unfalsifiable. In the appropriate intellectual domain, this isn't even a bad thing, necessarily. But of course, economics in real life is a highly inappropriate domain for such tomfoolery, and the intellectuals get very upset that nobody seems to be in charge, shaping and perfecting reality to an unfalsifiable theory that could have been their own with a little more politicking.
Now we are getting somewhere. Bitcoin is a microcosm of ""capitalism"" and the intellectual response to Bitcoin a microcosm of Nozick's argument. It is falsifying grand and all-encompassing theories of economics, finance, and politics left, right, and center. Weinstein has one such theory, and I'm sorry to have to be the one to say that it is not going to survive contact. Interesting as it may be, Bitcoin does not have to bend to fit it. It has to bend to fit Bitcoin. If it breaks, nobody will care.
In other words, Gauge Theory does not fix this.
Gauge Theory: What Is It Good For?
So what is this Gauge Theory all the cool kids are talking about? Here's Weinstein explaining it:
...
...
...
...
I included all the ellipses above so readers might be tricked into watching that video in its entirety before they saw the text that followed. If you fell for this, I sincerely apologize for having wasted so much of your time, but I also feel this experience is an important one to fully grasp what we are dealing with here.
Notice, by the way, that Rogan is a journalistic genius. He doesn't tell Weinstein he hasn't explained shit; he sets Weinstein so much at ease by playing dumb that Weinstein makes it totally clear on his own that he hasn't explained shit. Anyway ...
A glance at Wikipedia makes clearer the gravity of the issue:
a gauge theory is a type of field theory in which the Lagrangian does not change (is invariant) under local transformations from certain Lie groups.
Okay, so what's a field theory? What's a Lagrangian? What is invariance? What is a local transformation? What's a Lie Group? This doesn't bode well for something purporting to ""explain"" ... well, anything, really.
So what actually is it? By far the best resource I can recommend if readers really want to explore this is a paper by Juan Maldacena, The Symmetry And Simplicity Of The Laws Of Physics And The Higgs Boson.
There are two interesting things about this paper. First, whereas Weinstein is clearly very smart but is really more of an entertainer than an academic, Maldacena is actually a genius, with next to no public profile beyond his discipline. He is a physicist at the Institute for Advanced Study, and the only reason he hasn't won the Nobel Prize for his discovery of the Anti-de Sitter/Conformal Field Theory Correspondence is because he is too young and it happened too recently.
But beyond these biographical details, the reader can get an even clearer sense of this by comparing the paper just cited to Weinstein's performance on Rogan: Maldacena actually explains the intuition, meaning, and relevance of gauge symmetries in English, something Weinstein seems unable to do.
The second interesting thing is that Maldacena finds an economic analogy to be the most accessible introduction to the layman before moving on to particle physics. I really do recommend just reading the paper itself, but I will extract and condense the relevant discussion:
""We imagine we have some countries. Each country has its own currency. Let us imagine that the countries are arranged on a regular grid on a flat world. Each country is connected with its neighbors with a bridge. At the bridge there is a bank. There you are required to change the money you are carrying into the new currency, the currency of the country you are crossing into. There is an independent bank at each bridge. There is no central authority coordinating all the exchange rates between the various countries. Each bank is autonomous and sets the exchange rate in an arbitrary way. The bank charges no commission. For example, assume that the currency in your original country is dollars and the one in the new country is euros. Suppose that the exchange rate posted by the bank at the bridge between two countries is 1.5 dollar = 1 euro. Then if you have 15 dollars the bank converts it to 10 euros as you cross the border. If you decide to come back your 10 euros will be converted to 15 dollars. Therefore, if you go to a neighboring country and you come right back, you end up with your original amount of money. Another rule is that you can only go from one country to the neighboring country. From there you can continue to any of its neighbors and so on. However, you cannot fly from one country to a distant country without passing through the intermediate ones. You can only walk from one to the next, crossing the various bridges and changing your money to the various currencies of the intermediate countries. The final assumption is that the only thing you can carry from one country to the next is money. You cannot carry gold, silver, or any other good.
...
Where is the symmetry? The gauge symmetry is the following. Imagine that one of the countries has accumulated too many zeros in its currency and wants to drop them. This is fairly common in the real world in countries with high inflation. What happens is that one day the local government decides that they will change their currency units. For example, instead of using Pesos now everybody needs to use ""Australes"". The government declares 1,000 Pesos will now be worth 1 Austral, or 1,000 Pesos = 1 Austral. So everybody changes all prices and exchange rates accordingly. If you needed to pay 5,000 Pesos for a banana, now you will need to pay 5 Australes. If your salary was 1 million Pesos, it will now be 1 thousand Australes. Suppose the neighboring country is the USA. If the exchange rate was 3,000 Pesos = 1 Dollar, it will now be 3 Australes = 1 Dollar. See figure 5. We call this a ""symmetry"" because after this change nothing really changes, nobody is richer or poorer and the change offers no new economic opportunities. It is done purely for convenience. You can see this gauge symmetry in action in some Argentinean banknotes in figure 6. It is called a ""gauge"" symmetry because it is a symmetry of the units we use to measure or ""gauge"" the value of various quantities.
This symmetry is ""local"", in the sense that each country can locally decide to perform this change, independently of what the neighboring countries decide to do. Some countries might like to do it more frequently than others. In the real world, Argentina has eliminated thirteen zeros through various actions of this ""gauge symmetry"" since the 1960s, so that 1 Peso of today = 10^13 Pesos of the 1960s.
...
Now, in physics the countries are analogous to points, or small regions, in space. The whole set of exchange rates is a configuration of the magnetic potentials throughout space. A situation like the one in figure 7, where you can earn money, is called a magnetic field. The amount of gain is related to the magnetic field. The speculators are called electrons or charged particles. In the presence of magnetic fields, they simply move in circles in order to earn money. In fact, the total gain along the circuit is the flux of the magnetic field through the area enclosed by the circle. Now imagine that you are a speculator that has debt instead of having money. In that case you would go around these countries in the opposite direction! Then your debts would be reduced in the same proportion. In the example of figure 7, your debts would be reduced by a factor of 1/1.5 by circulating in the direction opposite to the green arrow. In physics, we have positrons, which are particles like the electron but with the opposite charge. In fact, in a magnetic field positrons circulate in the opposite direction as compared to electrons.
In physics, we imagine that this story about countries and exchange rates is happening at very, very short distances, much shorter than the ones we can measure today. When we look at any physical system, even empty space, we are looking at all these countries from very far away, so that they look like a continuum ... When an electron is moving in the vacuum, it is seamlessly moving from a point in spacetime to the next. In the very microscopic description, it would be constantly changing between the different countries, changing the money it is carrying, and becoming ""richer"" in the process. In physics we do not know whether there is an underlying discrete structure like the countries we have described. However, when we do computations in gauge theories we often assume a discrete structure like this one and then take the continuum limit when all the countries are very close to each other.
Electromagnetism is based on a similar gauge symmetry. In fact, at each point in spacetime the symmetry corresponds to the symmetry of rotations of a circle. One way to picture it is to imagine that at each point in spacetime we have an extra circle, an extra dimension. See figure 9(a). The ""country"" that is located at each point in spacetime chooses a way to define angles on this extra circle in an independent way. More precisely, each ""country"" chooses a point on the circle that they call ""zero angle"" and then describe the position of any other point in terms of the angle relative to this point. This is like choosing the currency in the economic example. Now, in physics, we do not know whether this circle is real. We do not know if indeed there is an extra dimension. All we know is that the symmetry is similar to the symmetry we would have if there was an extra dimension. In physics we like to make as few assumptions as possible. An extra dimension is not a necessary assumption, only the symmetry is. Also the only relevant quantities are the magnetic potentials which tell us how the position of a particle in the extra circle changes as we go from one point in spacetime to its neighbor.
Back to the Wikipedia crib, then: ""field theory"" just means that it helps to understand the system in question in mathematical terms as consisting of a little arrow at every point that means something helpfully numerical; ""Lie groups"" can be read as a special class of the more easily intuited ""symmetry""  rotating or flipping while preserving size and relative position; ""local transformations"" means we only enact these flips and rotations on the little arrows at certain well-defined locations within the system as a whole while leaving everything else untouched; ""Lagrangian"" is a technical specification of the dynamics of the entire system arrived at by a calculation on all the little arrows, and that the Lagrangian is ""invariant"" under the ""local transformations"" means that flipping and rotating the arrows only locally doesn't change the outcome of the calculation, hence also doesn't change this way of specifying the system as a whole.
Got it? Good. You are ready to go on Rogan.
In the course of introducing this analogy, Maldacena cautions us to,
""keep in mind that our goal is not to explain the real economy. Our goal is to explain the real physical world. The good news is that the model is much simpler than the real economy. This is why physics is simpler than economics!""
It's almost as if he doesn't have an unfalsifiable grand and all-encompassing theory of everything. What a wuss. Economics is totally a Gauge Theory! LFG!!!
Economics As A Gauge Theory
I want to try to paint Weinstein's ideas in as generous a light as I can, at least to start with.
In all seriousness, and without meaning to be snarky towards Weinstein, I couldn't find an explication of his theory by him that I am actually happy to endorse. This talk is not bad, but wanders absolutely all over the place and drowns the audience in superfluous formalisms that don't help much in understanding the core contention.
The best explication I found came from the theoretical physicist Lee Smolin, with whom Weinstein has collaborated, from his paper, Time And Symmetry In Models Of Economic Markets. I quote the relevant section at length as it really does frame the issue nicely:
""The proposal of Malaney and Weinstein is that to construct models of economies that have real dynamics and time dependence in them- so that for example, preferences of households can change in time-it is necessary to hypothesize that the dynamics is con- strained by much larger groups of gauge invariances.
As we have seen in the discussion above, the need for gauge invariance stems from a fundamental fact about prices, which is that they appear to be at least in part arbitrary. It seems that each agent in an economic system is free to put any value they like on any object or commodity subject to trade. How do we describe dynamics of a market given all this freedom? To get started we recall that in the Arrow-Debreu description of economic equilibrium, there is a gauge symmetry corresponding to scaling all prices. This may suffice for equilibrium, but it is insufficient for describing the dynamics out of equilibrium, because away from equilibrium there may be no agreement as to what the prices are. There is then not one price, but many views as to what prices should be. Each agent should then be free to value and measure currency and goods in any units they like- and this should still not change the dynamics of the market. It should not even matter if two agents trading with each other use different units. Thus we require an extension of the gauge symmetry in which the freedom is given to each agent, so they may each scale their units of prices as they wish, independently of the others.
There is a further difficulty with price which is that even after issues of measurement units are accounted for different agents will value different currencies or goods differently. Different agents have different views of the economy or market they are in, they have diverse experiences, strategies and goals, and consequently have different views of the values of currencies, goods and financial instruments. Consequently, in a given economy or market it is often possible to participate in a cycle of trading of currencies, goods or instruments and make a profit or a loss, without anything actually having been produced or manufactured. This is called arbitrage.
In equilibrium, all the inconsistencies in pricing are hypothesized to vanish. This is what is called the no arbitrage assumption. But out of equilibrium there will exist generically inconsistencies in pricing. In fact, we are very interested in the dynamics of these inconsistencies because we want to understand how market forces act out of equilibrium on inconsistencies and differences in prices to force them to vanish. This is essential to answer the questions the static notion of equilibrium in the Arrow-Debreu model does not address.
However, in analyzing the dynamics that results from the inconsistencies, we need to be careful to untangle meaningful differences and inconsistencies in prices from the freedom each agent has to rescale the units and currencies in which those prices are expressed. This is precisely what the technology of gauge theories does for economics.
How does gauge theory accomplish this? As in applications of gauge theory to particle physics and gravitation, the key is to ask what quantities are meaningful and observable, once the freedom to rescale and redefine units of measure are taken into account. The answer is that out of equilibrium the meaningful observables are not defined at a single event, trade or agent. Because of the freedom each agent has to rescale units and choose different currencies, the ratios of pairs of numerical prices held by two agents in a single trade are not directly meaningful.
To define a meaningful observable for an economic system one must compare ratios of prices of several goods of one agent, or consider the return, relative to doing nothing, to an agent of participating in a cycle of trading. This might be a cycle of trades that starts in one currency, goes through several currencies or goods and ends up back in the initial currency. Because the starting and ending currencies are the same, their ratio is meaningful and invariant under rescalings of the currency's value. This is true whether one agent or several are involved in the cycle of trades. We say that these kinds of quantities are gauge invariant.
Such quantities, defined by cycles of trades such that they end up taking the ratio of two prices held by the same agent in the same currency have a name: they are called curvatures. The ratios of prices given to a good by different agents also have a name: they are called connections. The latter do depend on units and hence are gauge dependent, the former are invariant under arbitrary scalings of units by each agent and are gauge invariant or gauge covariant (this means they transform in simple fashion under the gauge transformations.)
It is interesting that the quantities that are invariant under the gauge transformations include arbitrages, which should vanish in equilibrium. This does not mean that they are irrelevant, indeed, they may be precisely the quantities one needs to understand how the non-equilibrium dynamics drives the system to equilibrium. That is, it is natural to frame the non-equilibrium dynamics in terms of quantities that vanish in equilibrium. These are the quantities that the law of supply and demand acts on, in order to diminish them.
There is a precise analogy to how gauge invariance works in physics. In gauge theories in physics, local observables are not defined because of the freedom to redefine units of measure from place to place and time to time. Instead, observables are defined by carrying some object around a closed path and comparing it with a copy of its configuration left at the starting point. These observables are called curvatures. The result of carrying something on a segment of open path is called a connection and is dependent on local units of measure. But when one closes the path, one makes comparison to the starting point possible, so one gets a meaningful observable, which is a curvature.
In general relativity exactly the same thing is true. Here curvature corresponds to inconsistencies in measurements, for example, you can carry a ruler around a closed path and it comes back pointing in a different direction from its start. The dynamics is then given by the Einstein equations, which are expressed as equations in the curvature. But in the ground state-which is roughly analogous to equilibrium in an economic model-the curvature vanishes. The state with no curvature, called flat spacetime, is the geometry of spacetime in the absence of matter or gravitational forces. It is the state where all observers agree on measurements, such as which rulers are parallel to which.
But while the curvatures vanish in the ground state, the physics of that state is best understood in terms of the curvatures. For example, suppose one perturbs flat spacetime a little bit. The result are small ripples of curvature that propagate at the speed of light. These are gravitational waves. The stability of flat spacetime is explained by the fact that these ripples in curvature require energy.
Similarly, it may be that the stability of economic equilibrium can be studied by modeling the dynamics of small departures from equilibrium. These are states where prices are inconsistent, ie where arbitrage or curvature is possible. By postulating that the dynamics is governed by a law that says that inconsistencies evolve, one gets a completely different understanding of the underlying dynamics than in a theory that simply says that inconsistencies or curvatures vanish.
If economics follows the model from physics, then the next step, after one has an- swered the question of what are the observables, is to ask what are the forms of the laws that govern the dynamics of those observables. Given what we have said, the choice of possible laws is governed by a simple principle: since only gauge invariant quantities are meaningful, the dynamics must be constructed in terms of them alone.""
Here is my attempted translation of all this: if a market is not at perfect equilibrium over a prolonged stretch of time (ignore for now that this is meaningless anyway  we are supposing it isn't true), there is a serious problem of measurement in comparing the dynamics of the market at any two times because of a lack of an invariant measure.
The paragraph beginning ""there is a further difficulty ..."" is particularly astute in identifying essentially radical subjectivism and resultant uncertainty meaning no individual's perspective can be meaningfully privileged. This is true not only because their preferences can change, but because even their preferred scale of preferences is somewhat arbitrary and psychological.
This is the ""gauge symmetry"" Maldacena alluded to above and that is similar to experiences with redenominations of currency: it means the same if we value one chicken at $1 and 10 chickens at $10 (if that's even true  it might not be), or ten chickens at one cow and one hundred chickens at ten cows, but none of these denominations make any more sense than any others.
However, Weinstein would argue, this idea of symmetry in the measurement opens an important conceptual door: an arbitrage cycle. The existence of an arbitrage cycle in an out-of-equilibrium market is an economic observable that is gauge invariant, and, Weinstein would therefore posit, fundamentally meaningful in terms of the dynamics of the entire system.
This notion of invariant measures across agents and across time may sound vaguely familiar to those who have listened to Weinstein's thoughts on this before, as they are usually put forward more straightforwardly in his criticisms of official inflation statistics.
And here he absolutely has a point  one with which Bitcoiners I'm sure are intimately familiar. Relative to what exactly is 2.3%, or whatever, supposed to be meaningful? If it's the ""average basket"" then who is the average person? If your personal costs are increasing at a rate more like 30% per annum, how much better is this supposed to make you feel?
Michael Saylor has recently popularized the notion that ""inflation is a vector"", that every good or service has its own rate of inflation, that is hence experienced differently by every individual depending on their purchasing habits, and that reducing the concept as a whole to a single number is a ""metaphysical abstraction"".
I would take this even further and argue that even the vector is a metaphysical abstraction  it's just a more useful one for conceptualizing the workings of the economy than the single number. In reality, none of the entries in the vector really exist. It's a conceptual aid, not an observable. Every price of every sale is meaningful only at that point in spacetime, and the capital structure facilitating the exchange is reflexively affected by its having happened.
This is where Weinstein's approach starts to creak at the joints. Having correctly identified the relevance of radical subjectivism in general, one of its many consequences of ill-defined inflation, and the genuinely interesting realization that arbitrage cycles are phenomena worthy of study, he seems to make the entirely uncalled-for leap to something like: neoclassical economics is a naturally occurring gauge theory.
Economics As Literally Anything At All Besides A Gauge Theory
Pretty much all you need to grasp the essential error in all this is available in a single paragraph on Weinstein's website under the heading, Neo-Classical Economics And Gauge Theory:
""Economic theory is based around the hidden assumption that consumer tastes are absolutely 'stable' over time despite the fact that a world with static tastes cannot even be considered a plausible simplification of the world in which we live. Many rationalizations have been given for this fiction which are at times both ingenious and embarrassing. The key problem for economic theory is that the field simply failed to develop mathematical methods for analyzing changes in dynamic preferences.""
This final sentence should be ringing some serious alarm bells. Allow me to translate:
The mathematical apparatus of mainstream neoclassical economics does not cleanly solve every theoretical economic problem. The solution is that we need more and more complicated math.
Um, no.
This is not to say that some mathematics is not useful in economics. But a decent caricature of Weinstein's position is that until everything has been mathematized, economics remains unscientific. Which is true, but has exactly the opposite significance to what Weinstein is after: economics is not a science. There are no possibilities for controlled experiments and the fundamental building blocks do not behave in ways that can be coherently mathematically described.
Weinstein seems to think he has found a key component that might bridge the gap: an invariant for measurement. But unfortunately, he is simply so wedded to mathematical formalisms and their hoped-for application in economics that he lacks the proper context in which to place this insight.
Arbitrage is a fundamental concept. With enough conceptual leeway, we could interestingly argue that all profit-seeking capital formation and deployment is some or other form of loosely-defined arbitrage. But then, rather than claiming that this profundity can be used to root economics in a sweeping mathematical formalism, I would instead encourage the reader to go read Israel Kirzner's Competition and Entrepreneurship, in which more or less what I just said is explained totally straightforwardly and with zero equations, as far as I recall.
And that's basically the end of that. You don't need particle physics or algebraic geometry. You just need Kirzner's realization that entrepreneurship is, by its nature, non-exclusionary. It is a price discrepancy between the costs of available factors of production and the revenues to be gained by employing them in a particular way  or, profit. In other words, it is perfectly competitive. It does not rely on any privileged position with respect to access to assets; The assets are presumed to be available on the market. They are just not yet employed in that way, but could be, with capital that is presumably homogeneous. Anybody could do so  they just need the incentive of profit and guts.
In other words, markets are always out of equilibrium. The arbitrage cycles Weinstein identifies as a meaningful invariant are the motivating force of all economic activity. They are everything. Ironically, his insight may be so profound that its true significance has gone over his head. He has narrowed down his search for the economic holy grail all the way to ... entrepreneurship.
Weinstein basically doesn't fully grasp that subjective value cannot be mathematized. Nor can the intuition, motivation, taste, and creativity that drives entrepreneurship and competition and from which literally all economic activity follows, some of which can admittedly be helpfully mathematically characterized, albeit with a pinch of salt.
If you want to know why academic economics is such a mess, go read Principles of Economics by Menger, A General Mathematical Theory of Political Economy by Jevons, and Pure Elements of Political Economy by Walras, and decide which you like the most and which makes the most sense. Then go check which had the most academic influence. Then be sad.
Here, Weinstein describes the flourishing that followed these works, nowadays called The Marginal Revolution, as, ""the introduction of the differential calculus formally into economic theory,"" which is about the most ridiculous description of it I have ever come across. In case the reader is unfamiliar, The Marginal Revolution solidified the centrality of subjectivism over cost- and labor-theories of value and spurred a variety of methodologies as to how to deal with it. Some methodologies involve differential calculus and other methodologies are good.
In the same interview, Weinstein tellingly later says that, ""I think that George Soros's theory of reflexivity has not been taken seriously because we haven't had the mathematics to incorporate it within the standard canon."" I'm honestly not sure Weinstein has ever really read or thought about Soros' line of thinking here in any depth because the entire point of The Alchemy of Finance is that the principle of reflexivity renders finance irresolvably unscientific. Amongst many wonderfully quotable extracts, Soros writes,
""The attempt to transpose the methods and criteria of natural science to the social sphere is unsustainable. It gives rise to inflated expectations that cannot be fulfilled. These expectations go far beyond the immediate issue of scientific knowledge and color our entire way of thinking.""
Weinstein says shortly thereafter, ""there is no question that agents move markets. But what [Soros] is saying is that markets move the minds of agents. And you have to ask yourself the question: what is the mathematics of moving a mind?""
No, you really don't need to ask yourself that. If you find yourself asking yourself that, stop and read Kirzner immediately.
Weinstein's intellectual lineage (and associated inflated expectations) on this topic can of course be traced not from Menger, but from Walras and Jevons through Pareto and Marshall to Paul Samuelson, whom Weinstein consistently praises, and whose only flaw Weinstein deems to have been not being quite mathematical enough, despite being probably the single worst and most insidious influence on academic economics in the twentieth century.
The reader may not be familiar with Samuelson, although he was world-famous in his heyday, and while I don't want too much of a digression, two biographical details seem pertinent.
First, he wrote the once-standard English language textbook on economics, rather obnoxiously called Economics, believed to be the best-selling economics textbook in history, which, from its first edition in 1948 up until its 12th edition in 1985 predicted in its introduction that the Soviet economy would overtake that of the US before too long. Naturally, this date was pushed back every time, and although the embarrassment was finally removed in 1985, in 1989, Samuelson claimed that, ""contrary to what many skeptics had earlier believed, the Soviet economy is proof that ... a socialist, command economy can function and even thrive.""
Mhmm.
Second, as will be mostly meaningful and possibly infuriating to long-time readers of mine, Samuelson claimed that, ""the ergodicity assumption is essential to advance economics from the realm of history to the realm of science."" In other words, we must assume something we absolutely know to be false in order to pretend economics is scientific, which we absolutely know it is not.
Mhmm.
This is the thread Weinstein is picking up, and the results are every bit as silly as you might imagine.
In fact, it's all a shame because, as I argued above, and as Maldacena demonstrated without even really meaning to, there are areas of economics in which gauge symmetries are a useful abstraction. But they aren't a scientific analysis. The absolute most you could sensibly say would be something like: if you already understand gauge symmetries, that's a useful shortcut to grasping the mechanics of xyz, but if not, don't worry about it.
Whatever To Do About Bitcoin?
It's important to understand the key difference between Weinstein and Maldacena. Weinstein is not really an academic, he is an academically-oriented performer, and there is a large part of this entire debacle that is purely performative. It sure has driven a lot of engagement. I mean, jeez, look at what you are reading right now!
That might be fine if he were just wowing podcast hosts with his musings on interdisciplinary linkages between particle physics and economics, but it is problematic with Bitcoin because Bitcoin is not a theory. It's a fact. It works, miraculously without having been embedded in a Gauge Theory, and without there even being such a BIP on the horizon, at least as far as I am aware.
It's even more unfortunate in this case because, although my initial reaction to Weinstein's thoughts on the matter was that they were little more than complexity-laundering bullshit, I have since been convinced by more patient and tolerant peers that they contain a nugget of insight. When Weinstein says we need to embed Bitcoin in a Gauge Theory, this is what I think he means:
for an aspirationally universal monetary tool, the inevitable transactional information leak beyond assurance of validity would be preferable if curtailed at local consensus rather than global consensus.
But notice how I said that without referencing particle physics? And notice how it can be discussed without proposing radical-to-the-extent-they-are-not-meaningless protocol overhauls by learning about Lightning, taproot, cross-input signature aggregation, and so on?
The critique of local versus global state is answered by Lightning. You might even go as far as to say, if you are being exceptionally cheeky, that Bitcoin is the Gauge Field that makes possible in the first place the kind of local transformations Weinstein is interested in. If Bitcoin's Lagrangian turns out to be invariant under these transformations, all the better. If not, who cares?
Which brings us full circle: Gauge Theory does not fix this. Taking the time to understand it, and doing real work on it, might fix it just a teeny, tiny bit. But spouting off grand and all-encompassing theories without having understood it, and without offering anything workable as an alternative, will make you no friends and will influence nobody.
We would love to have you, Eric. Really, we would. From what I can tell, you'd be an excellent advocate. The story of your interactions with the Boskin Commission and the inadvertent discovery of the disingenuous political bent of mainstream economics will find in us a near-perfect audience. But you are going to have to knuckle down and listen, learn, and stop pretending Bitcoin is a subset of your theory of everything.
Your theory of everything is a subset of Bitcoin.
It's nothing personal, but we don't trust. We verify. Do let us know when our verification ought to begin.
follow me on Twitter @allenf32
thanks to Gigi for edits and contributions
",144
https://medium.com/on-banking/high-frequency-trading-on-the-coinbase-exchange-f804c80f507b?source=tag_archive---------6-----------------------,High Frequency Trading on the Coinbase Exchange,by Andrew Barisser,Andrew Barisser,7,"by Andrew Barisser
I've recently started trading bitcoins algorithmically on the new Coinbase exchange. After reading about High-Frequency-Trading in the book Flash Boys by Michael Lewis, I decided I'd give it a shot myself, albeit in a clumsier, more amateurish way. The experience has been fascinating, both on a technical level, and in a strategic sense. Writing logic that controls money itself is a strange thing. Setting it loose for the first time, knowing that any bug could literally throw away cash, was terrifying.
Bitcoin is an incredibly open system that is particularly friendly to no-name developers. The exchanges have open API's that allow anyone, literally anyone, to trade. There's no premium access, no expensive trading floor credentials. It's totally open. I love that.
As I've designed my trading bot, I've come to realize how much strategic depth there is to these sorts of games. The exchanges are already rife with trading bots; these are shark infested waters. Bots dance around each other in a chaotic swirl. They employ so many diverse strategies. It's like so many microbes competing in the primordial ooze. Entering into this environment, I had to be immediately cognizant of other bots.
Algorithmic traders need to occupy a particular niche. They profit from market inefficiencies. In a perfect market, what they do would not be profitable. It is precisely because markets, in their native state, are not ideally smooth, continuous, and well functioning, that algorithmic traders may extract any value. In rectifying the little mistakes, the little instances of slippage that occur in markets, one may eke out small profits. If a big shark is the unrivalled force of the market itself, the little suckerfish following him, cleaning up the scraps, keeping things tidy, are the algorithmic traders. They too have their place.
Another paradox is that I cannot reveal my trading strategy without also compromising it. To a small extent, explaining my strategy would be an invitation to competitors, for whom the marginal cost of setting up the software is very low. Much more threateningly, however, if my bot's exact strategy were known, it could be depredated. If you could always predict its every step, you could trick it into giving up money, again, and again, and again. This is something else that keeps my paranoia alive, the fear that someone out there will observe my bot, and in the to and fro of its orders, figure out its strategy. I imagine myself coming back to my bot, seeing its balance empty, because some mastermind gamed it algorithmically, draining pennies with each cycle.
On the other hand, my bot's strategy is exceedingly conservative, and would be difficult to game. It is basically a sophisticated market maker. It provides liquidity to the Coinbase exchange. This means that it looks at the order book and observes where the orders are thin. Perhaps there is very little order depth on the buy side. It can place limit orders, like little traps, at varying depths on the buy and sell sides. It varies the exact way it does this based on recent market conditions. If a large trade is then suddenly executed, it may overwhelm the availability of offers at the best price. Such a large offer may then trigger one of my offers, lying in wait, at a more advantageous price. This is market-making 101. It's pretty much the least opinionated strategy out there, although I have tempered my own implementation with some additional price-prediction logic. On the whole it's an exceedingly boring strategy.
Market-making also delivers real social utility. The deeper the liquidity provided by market makers, the more difficult it is to cause erratic spikes in price. Market makers also reduce the bid-ask spread, a concept most people aren't even aware of: a testament to successful practitioners on Wall Street.
Other bots employ widely varying strategies. Some rectify the spread between separate exchanges. This strategy is completely dependent on speed. If someone drops 1000 BTC on BitfineX, the price on Coinbase plunges in synchrony because someone raced to execute a market order. Other strategies revolve around tricking other bots, for which there are endless tactics. They often involve elaborate posturing, fooling others' logics into fatal missteps. Still others are designed to intimidate human beings with massive buy or sell orders. I'm sure still others abound of which I have no idea.
On a practical level, my bot must be very quick. If it is delayed even by a few seconds between cancelling and placing orders, market conditions can cause the new orders to become inappropriate. The orders I place follow a sound logic assuming that the bot has a correct understanding of the state of the order book. This assumption does not hold for long. Within a second, a flurry of significant orders could have skewed the actual order book, such that the new orders I've devised are now plain wrong.
I must also be on the lookout for hostile bots, who may place and quickly remove large orders with the intention of tricking other bots. The faster my bot can maintain awareness of the order book, the less susceptible it will be to such tactics. My bot even has additional logic to prevent it from being tricked by fake volume walls from other bots.
In the pursuit of speed, I've had to think about technical details I was not very familiar with. I've had to parallelize a lot of mundane, boring functions. I can't cancel obsolete orders in serial, it would take too long for my JSON requests to go back and forth across the internet. Because Coinbase does not offer a single API endpoint to cancel all orders simultaneously, something I've been asking for, I cancel them with many separate requests in parallel. Similarly, Coinbase lacks an endpoint for creating multiple orders at once. So I must issue multiple requests simultaneously. A synchronous solution would take several seconds, which is far too long.
It's funny how a human sense of time is wholly inappropriate to that of bots. Even a fraction of a second can be hopelessly long. Trying half as hard, or moving half as fast, don't guarantee half the profits; they yield zero (or worse). Delving into algorithmic trading, one must inhabit the lifecycle of a bot, stretching one's own concept of time to milliseconds. It is on this scale that I still see my bot as dumb and slow.
At least the bot's awareness of the order book is very fast. It streams a websocket feed of new orders. Each item is a permutation of the order book, so I must maintain the book's state and make little changes as they arrive. This process is also parallelized. It's actually amazing how fast this is, there are about 20 order permutations per second.
So my bot mainly provides liquidity. It earns a small but steady amount from this. It holds roughly equal amounts of bitcoins and dollars, so abrupt price changes can leave it with losses in a given denomination. But on the whole it is making decent profits compared to a 50/50 basket of bitcoins/USD.
My bot performs best when volume is high, but price swings are low. As a provider of liquidity, it smoothes the erratic undulations that would otherwise occur without market makers. In this it is providing a useful function, thus high volume periods are the most lucrative. In some cases, sharp swings, back and forth, can cause my bot to persist in holding the wrong asset. Thus it is possible to lose money. I've found that low-volume regimes are the most dangerous. My bot seeks to estimate the trading rate and moderate the depth of its orders accordingly. This limits the risk of being caught in large swings, at the cost of having its orders executed less often.
I've also found that there is a significant amount of noise around my balance. As the price oscillates, my bot periodically loses money. It may be losing money 45% of the time. But if it is gaining in the other 55%, it will win massively over the long run. Given these odds, measuring the bot on a frequent basis would lead one to observe more instances of loss versus infrequent observations. It's like checking on your stock portfolio. If you had a guaranteed strategy returning 10% per year, but with a normal amount of noise, you'd observe losses almost 50% of the time if you observed your balance often enough, even as you employed a successful strategy. The law of large numbers only works... over longer timescales.
While at first this bot was merely a distraction, I've come to realize that what it does, albeit simplistically, is really necessary. One of the biggest problems with Bitcoin is the way it is traded. The illiquidity of exchanges is a huge problem. Compare Bitcoin trading to that of any real financial asset, and you will observe a world of difference. Financial folks extract tremendous value in the maintenance of efficient markets in other assets. This does not just happen magically. Bitcoin needs better functioning markets if it is to attract serious players. It's also a profit opportunity. Even at current trading volumes, a lot of value can be captured by smoothing out market fluctuations. If Bitcoin were to grow, the need for liquidity would also increase. I've learned that infrastructure isn't just servers and github repos. It's also financial middlemen who make markets work. The mere fact that I could dabble in this, as nobody, illustrates the wonderful openness of Bitcoin.
Follow me at @abarisser 1GgwA7c2ovgWDBoVYsHT5VYXw2QBey1EdF
A Place For Finance, Banking, Investment
292 
4
",145
https://medium.datadriveninvestor.com/a-1000-bitcoin-investment-wont-make-you-rich-b353f215f746?source=tag_archive---------3-----------------------,A $1000 Bitcoin Investment Won't Make You Rich,The simple math that tells us how rich we could really get from Bitcoin,Jamie Bullock,4,"I've only been experimenting in crypto for a relatively short period of time. Yet during this time I've seen many articles and videos claiming only a tiny amount of Bitcoin will make you incredibly rich.
I'm confident my math skills are good enough to assess that it's unlikely anyone is going to become seriously rich from Bitcoin based on a $500 to $1000 investment.
Let me explain...
Many newcomers to crypto start by assessing assets based on price. So, Bitcoin looks like it's worth a lot at $50k, compared to Cardano at only $1.20. Cardano appears to be worth a mere 0.0024% of Bitcoin's value.
Well, what if I told you if Cardano actually has over 1000x that, at roughly 3.7% of Bitcoin's market value? This is due to a metric called market capitalization.
The market capitalization of a token is given by the following formula:
For Bitcoin at current prices, this would give us roughly:
So the market cap of Bitcoin currently sits around a tidy 1 trillion dollars. When we evaluate a given asset, it's important we always look at capitalization and not price, since this tells us the potential price increase the market could support.
So what if we invested $1000 now...? How rich could we get?
The first scenario we'll consider is buying our Bitcoin now.
Everyone has their own definition of what ""rich"" means, but for this piece, I'm going to define it as having enough money to give up work.
A middle-class income in the US would be around $85k per year. The median age is around 40 and the average life expectancy is around 78. So let's say we need to cover our salary for 40 years to give up work. This gives us a ""rich"" figure of:
So to get rich from Bitcoin we'd need it to generate at least 3.4 million dollars to support a middle-class lifestyle for 40 years.
Is this possible based on Bitcoin invested now? Let's find out...
The market capitalization of gold is estimated at around 11 trillion dollars. Bitcoin is often described as ""digital gold"" and there is speculation it may one day reach gold's value, so let's start with this.
To reach a market cap of $11T, Bitcoin would have to go up 10x in value. This doesn't seem inconceivable since Bitcoin has already gone up by a factor of at least 10x since 2015, and a factor of over 75 million since it first launched!
So what would we need to invest now to be rich in say, 5 years, and Bitcoin reaches $11T market cap?
So to have 3.4m in 5 years based on a 10x rise in Bitcoin price, we'd need to invest $340k.
Investing only $1k would give us $10k. This is a nice sum of money and a sizeable gain, but it's hardly early retirement.
OK, so we've looked at a scenario where Bitcoin reaches the market cap of gold. It shows that unless we invest a huge sum of money, we're not giving up work any time soon!
But what if Bitcoin had a bigger increase in value?
This time, let's work backward from the figure we want to achieve and work out how much Bitcoin would need to go up to support that.
To retire early we'd need a return of $3.4M / $1k. So Bitcoin would need to go up by a factor of over 3000x. Based on current prices this gives:
So to retire early on a $1k investment in Bitcoin, its market cap would need to reach 3 quadrillion dollars! This is 10x the current total wealth in the world.
Some readers will be thinking: but Bitcoin is currently at all-time highs, now isn't the time to be buying!
So in our third scenario, let's consider the crypto market crashes, and Bitcoin goes back down to its 2015 price of $315. To make even $1 million from a $1000 investment, the price would still need to go up 1000x. Obviously, our $3.4m target gain would require a 3000x increase or more. So it doesn't really matter when we invest, we still need to achieve the same relative increase.
The gain from any investment is a function of the percentage increase in value and the capital invested. Realistically, to make life-changing sums of money from investing in any asset two things need to happen:
In reality, no-one knows if the price of Bitcoin will go up or down, but some predictions have it reaching $500k per coin by 2030. If this were to be the case a $1k investment now would net you around $10k in 9 years.
That's 5 times what we'd have if we put our money in an ISA with an 8% return. A very respectable increase.
Still, I wouldn't start planning the retirement party quite yet...
Thank you for reading. If you'd like to find out more about my experiences investing in Crypto, you can learn about why you need a plan to survive the volatile market.
DISCLAIMER: This article presents my own learnings based on personal experience. It should not be considered Financial or Legal Advice. Consult a financial professional before making any major financial decisions.
empowerment through data, knowledge, and expertise.
1.4K 
28
1.4K claps
1.4K 
",146
https://medium.com/incerto/how-to-legally-own-another-person-4145a1802bf6?source=tag_archive---------2-----------------------,How To Legally Own Another Person,Even the church had its hippies -Coase does not need math -Avoid lawyers during Oktoberfest -The expat life ends one day -People who have...,Nassim Nicholas Taleb,20,"Even the church had its hippies -Coase does not need math -Avoid lawyers during Oktoberfest -The expat life ends one day -People who have been employees are signaling domestication  You win elections by not caring about wining elections
In its early phase, as the church was starting to get established in Europe, there was a group of itinerant people called the gyrovagues. They were gyrating and roaming monks without any affiliation to any institution. Theirs was a free-lance (and ambulatory) variety of monasticism, and their order was sustainable as the members lived off begging and from the good graces of townsmen who took interest in them. It is a weak form of sustainability, as one can hardly call sustainable a group of a people with vows of celibacy: they cannot grow organically and would need continuous enrolment. But their members managed to survive thanks to help from the population, which provided them with food and temporary shelter.
Sometimes around the fifth century, they started disappearing -there are now extinct. The gyrovagues were unpopular with the church, banned by the council of Chalcedon in the Fifth Century, then again by the second council of Nicaea about three hundred years later. In the West, Saint Benedict of Nurcia, their greatest detractor, favored a more institutional brand of monasticism and ended up prevailing with his rules that codified the activity, with a hierarchy and strong supervision by an abbot. For instance, Benedict's rules[i], put together in a sort of instruction manual, stipulate that a monk's possessions should be in the hands of the abbot (Rule 33) and Rule 70 bans angry monks from hitting other monks.
Why were they banned? They were, simply, totally free. They were financially free, and secure, not because of their means but because of their wants. Ironically by being beggars, they had the equivalent of f*** you money, the one we can more easily get by being at the lowest rung than by joining the income dependent class.
Complete freedom is the last thing you would want if you have an organized religion to run. Total freedom is also a very, very bad thing for you if you have a firm to run, so this chapter is about the question of employees and the nature of the firm and other institutions.[1]
Benedict's instruction manual aims explicitly at removing any hint of freedom in the monks under the principles of: stabilitate sua et conversatione morum suorum et oboedientia  ""stability, demeanor compatible with norms and customs, and obedience"". And of course monks are put through a probation period of one year to see if they are effectively obedient.
In short, every organization wants a certain number of people associated with it to be deprived of a certain share of their freedom. How do you own these people? First, by conditioning and psychological manipulation; second by tweaking them to have some skin in the game, forcing them to have something significant to lose if they were to disobey authority -something hard to do with gyrovague beggars who flouted their scorn of material possessions. In the orders of the mafia, things are simple: made men (that is, ordained) can be wacked if the capo suspects lack of allegiance, with a transitory stay in the trunk of a car -and a guaranteed presence of the boss at their funerals. For others professions, skin in the game come in more subtle form.
Ironically, you could do better having an employee than a slave -and this held even in ancient times when slavery was present.
Let us say that you own a small airline company. You are a very modern person, having attended many conferences and spoken to consultants, you believe the company is a thing of the past: everything can be organized through a web of contractors. It is more efficient to do so, you are certain.
Bob is a pilot with whom you have entered a specific contract, in a well defined drawn-out legal agreement, for precise flights, commitments made long time in advance, which includes a penalty for non-performance. Bob supplies the copilot and an alternative pilot in case someone is sick. Tomorrow evening you will be operating a scheduled flight to Munich as part of an Oktoberfest special and Bob is the contracted pilot. The flight is full; with motivated budget passengers -some of whom went on a preparatory diet; they have been waiting a whole year for this Gargantuan episode of beer, pretzels, and sausage in laughter-filled hangars.
Bob calls you at 5 P.M. to let you know that he and the copilot, well, they love you... but, you know, they will not fly the plane tomorrow. You know, they had an offer from a Saudi Arabia Sheikh, a devout man who wants to take a special party to Las Vegas, and needs Bob and his team to run the flight. The Sheikh and his retinue fell in love with Bob's manners, the fact that Bob never had a drop of alcohol in his life, has an expertise in fermented yoghurt drinks, and told him that money was no object. The offer is so generous that it covers whatever penalty there is for a breach of a competing contract by Bob.
You kick yourself. There are plenty of lawyers on these Oktoberfest flights, and, worse, retired lawyers without hobbies who love to sue as a way to kill time, regardless of outcome. Consider the chain reaction: if your plane doesn't take off, you will not have the equipment to bring the beer-fattened passengers back from Munich -and you will most certainly miss many round trips. Rerouting passengers is costly and not guaranteed.
You make a few phone calls and it turns out that it is easier to find an academic economist with common sense and ability to understand what's going on than find another pilot, that is, an event of probability zero. You have all this equity in a firm that is now under severe financial threat. You are certain that you will go bust.
You start thinking: well, you know, if Bob were a slave, someone you own, you know, these kind of things would not be possible. Slave? But wait... what Bob just did isn't something that employees who are in the business of being employees do! People who are employees for a living don't have such opportunistic behavior. Contractors are too free; they fear only the law. But employees have a reputation to protect. And they can be fired. People who like employment like it for a reason. They like the paycheck!
People you find in employment love the regularity of the payroll, with the special envelop on their desk the last day of the month, and without which they would act as a baby deprived of mother's milk. Then you realize that had Bob been an employee rather than what appeared to be cheaper, that contractor thing, then you wouldn't be having so much trouble.
But employees are expensive... You got to pay them even when you've got nothing to do for them. You lose your flexibility. Talent for talent, they cost a lot more. Lovers of paychecks are lazy ... but they would never let you down at times like these.
So employees exist because they have significant skin in the game -and the risk is shared with them, enough risk for it to be a deterrent and a penalty for acts of undependability, such as failing to show up on time. You are buying dependability.
And dependability is a driver behind many transactions. People of some means have a country house, which is inefficient compared to hotels or rentals, because they want to make sure it is available if they decide they wanted to use it at a whim. There is an expression ""never buy when you can rent the three ""Fs"": what you Float, what you Fly, and what you ...that something else"". Yet many people own boats, planes, and end up with that something else.
True, a contractor has downside, a financial penalty that can be built-into the contract, in addition to reputational costs. But consider that an employee will always have more risk. And conditional on someone being an employee such a person will be risk averse. By having been employees they signal a certain type of domestication.
Someone who has been employed for a while is giving you the evidence of submission
Evidence of submission is displayed by having gone through years of the ritual of depriving himself of his personal freedom for nine hours every day, punctual arrival at an office, denying himself his own schedule, and not having beaten up anyone. You have an obedient, housebroken dog.
Employees are more risk averse, they fear being fired more than contractors do being sued.
Even when the employees ceases to be an employee, they will remain diligent. The longer the person stays with a company, the more emotional investment they will have in staying and, when leaving, are guaranteed in doing an ""honorable exit"".
So if employees lower your tail risk, so do you lower theirs as well. Or at least, that's what they think you do.
At the time of writing, firms stay in the top league by size (the so-called SP500) only about between ten and fifteen years. Companies exit the SP500 through mergers or by shrinking their business, both conditions leading to layoffs. Throughout the twentieth Century, however, expected duration was more than sixty years. Longevity for large firms was greater; people stayed with a large firm for their entire life. There was such a thing as a company man (restricting the gender here is appropriate as company men were almost all men).
The company man -which dominated the twentieth Century -is best defined as someone whose identity is impregnated with the stamp the firm wants to give him. He dresses the part, even uses the language the company expects him to have. His social life is so invested in the company that leaving it inflicts a huge penalty, like banishment from Athens under the Ostrakon. Saturday nights, he goes out with other company men and spouses sharing company jokes. In return, the firm has a pact to keep him on the books as long as feasible, that is, until mandatory retirement after which he would go play golf with a comfortable pension, with as partners former co-workers. The system worked when large corporations survived a long time and were perceived to be longer lasting than nation-states.
About in the 1990s, people suddenly realized that working as a company man was safe... provided the company stayed around. But the technological revolution that took place in Silicon valley put traditional companies under financial threat. For instance, after the rise of Microsoft and the personal computer, IBM which was the main farm for company men, had to lay off a proportion of its ""lifers"" who then realized that the low-risk profile of the position wasn't so much low risk. These people couldn't find a job elsewhere; they were of no use to anyone outside IBM. Even their sense of humor failed outside of the corporate culture.
Up until that period, IBM required its employees to wear white shirts -not light blue, not with discreet stripes, but plain white. And a dark blue suit. Nothing was allowed to be fancy, or invested with the tiniest amount of idiosyncratic attribute. You were a part of IBM.
Our definition:
A company man is someone who feels that he has something huge to lose if he doesn't behave as a company man -that is, he has skin in the game
If the company man is, sort of, gone, he has been replaced by the companies person, thanks to both an expansion of the gender and a generalization of the function. For the person is no longer owned by a company but by something worse: the idea that he needs to be employable.
A companies person is someone who feels that he has something huge to lose if he loses his employability -that is, he or she have skin in the game
The employable person is embedded in an industry, with fear of upsetting not just their employer, but other potential employers. [2]
An employee is -by design- more valuable inside a firm than outside of it, that is more valuable to the employer than the market.
Perhaps by definition an employable person is the one that you will never find in a history book because these people are designed to never leave their mark on the course of events. They are, by design, uninteresting to historians.
Ronald Coase is a remarkable modern economist in the sense that he is independent thinking, rigorous, creative, with ideas that are applicable and explain the world around us -in other words, the real thing. His style is so rigorous that he is known for the Coase Theorem, an idea that he posited without a single word of mathematics but that is as fundamental as many things written in mathematics.
Aside from his ""theorem"", Coase was the first to shed lights on why firms exist. For him contracts can be too costly to negotiate, they entail some amount of transaction costs, so you incorporate your business and hire employees with clear job description because you don't feel like running legal and organizational bills every transaction. A free market is a place where forces act to determine specialization and information travels via price point; but within a firm these market forces are lifted because they cost more to run than the benefits they bring. So the firm will be at the optimal ratio of employees and outside contractors, where having a certain number of employees, even when directly inefficient, is better than having to spend much resources negotiating contracts.
As we can see, Coase stopped one or two inches short of the notion of skin in the game. He never thought in risk terms to realize that an employee is a risk management strategy.
Had economists, Coase and Shmoase, had any interest in the ancients, they would have discovered the risk management strategy relied upon by Roman families who customarily had a slave for treasurer, the person responsible for the finances of the household and the estate. Why? Because you can inflict a much higher punishment on a slave than a free person or a freedman -and you do not need to rely on the mechanism of the law for that. You can be bankrupted by an irresponsible or dishonest steward who can divert your estate's funds to Bithynia. A slave has more downside, and you run a lower financial risk by having the steward function fulfilled by a slave.[3]
Now, enters complexity and the modern world. In a world in which products are increasingly made by subcontractors with increasing degrees of specialization, employees are even more needed than before for special tasks. If you miss on a step in a process, often the entire business shuts down -which explains why today, in a supposedly more efficient world with lower inventories and more subcontractors, things appear to run smoothly and efficiently, but errors are costlier and delays are considerably longer than in the past. One single delay in the chain can stop the entire process.
Slave ownership by companies has traditionally taken very curious forms. The best slave is someone you overpay and who know it, terrified of losing his status. Multinational companies created the expat category, a sort of diplomat with a higher standard of living representing the firm far away and running its business there. A bank in New York sends a married employee with his family to a foreign location, say a tropical county with cheap labor, with perks and privileges such as country club membership, a driver, a nice company villa with a gardener, a yearly trip back home with the family in first class, and keep him there for a few years, enough to be addicted. He earns much more than the ""locals"", in a hierarchy reminiscent of colonial days. He builds a social life with other expats. He progressively wants to stay in the location much longer but he is far from headquarters and has no idea of his minute-to-minute standing in the firm except through signals. Eventually, like a diplomat, he begs for another location when time comes for a reshuffle. Returning to the home office means loss of perks, having to revert to the unchanged base salary, and the person is now a total slave -a return to lower middle class life in the suburbs of New York City taking the commuter train, perhaps, god forbid, a bus, and eating a sandwich for lunch! The person is terrified when the big boss snubs him. Ninety five percent of the employee's mind will be on company politics... which is exactly what the company wants. The big boss in the board room will have a supporter in the event of some intrigue.
All large corporations had employees with expat status and, in spite of its costs, it was an extremely effective strategy. Why? Because the further from headquarters an employee is located, the more autonomous his unit, the more you want him to be a slave so he does nothing strange on his own.
There is a category of employees who aren't slaves, but these represent a very small proportion of the pool. You can identify them at the following: they don't give a f*** about their reputation, at least not their corporate reputation.
After business school, I spent a year in a banking training program -by some accident as the bank was confused about my background and aims and wanted me to become an international banker. There, I was surrounded with the corporate highly employable persons (my most unpleasant experience in life), until I switched to trading (with another firm) and discovered that there was some people in a company who weren't slaves.
One type is the salesperson whose resignation would cause the loss of business, and, what's worse, he can benefit a competitor by take some of the firm's client there. Salespeople had a tension with the firm as the firm tried to dissociate accounts from them by depersonalizing the relationship with the clients, usually unsuccessfully: people like people and they drop the business when they get some generic and polite person trying to get on the phone in place of the warm and often exuberant salesperson-friend. The other one was the trader about whom only one thing mattered: the profits and losses, or P/L. Firms had a love-hate with these two types as they were unruly -traders and salespeople were only manageable when they were unprofitable, in which case they weren't wanted.
Traders who made money, I realized, could get so disruptive that they needed to be kept away from the rest of the employees. That's the price you pay by associating people to a specific P/L, turning individuals into profit centers, meaning no other criterion mattered. I recall once threatening a trader who was abusing the terrified accountant with impunity, telling him such things as ""I am busy earning money to pay your salary"" (intimating that the accounting did not add to the bottom line of the firm). But no problem; the people you meet when riding high are also those you meet when riding low and I saw the fellow getting some (more subtle) abuse from the same accountant before he got fired, as he eventually ran out of luck. You are free  but only as good as your last trade. I said earlier that I switched firms away from the proto-company man and I was explicitly told that my employment would terminate the minute I ceased to meet the P/L target. I had my back to the wall, but I took the gamble which forced me to engage in ""arbitrage"", low risk transactions with small downside that were possible at the time because the sophistication of operators in the financial markets was very low.
I recall being asked why I didn't wear a tie, which at the time was the equivalent of walking down Fifth avenue naked. ""One part arrogance, one part aesthetics, one part convenience"" was my usual answer. If you were profitable you could give managers all the crap you wanted and they ate it because they were afraid of losing their jobs.
Risk takers can be socially unpredictable people. Freedom is always associated with risk taking, whether it led to it or came from it. You take risks, you feel part of history. And risk takers take risks because it is in their nature to be wild animals.
Note the linguistic dimension -and why, in addition to sartorial considerations, traders needed to be put away from the rest of nonfree, nonrisktaking people. My days, nobody cursed in public except for gang members and those who wanted to signal that they were not slaves: traders cursed like sailors and I have kept the habit of strategic foul language, used only outside of my writings and family life.[4] Those who use foul language on social networks (such as Twitter) are sending an expensive signal that they are free -and, ironically, competent. You don't signal competence if you don't take risks for it -there are few such low risk strategies. So cursing today is a status symbol, just as oligarchs in Moscow wear blue jeans at special events to signal their power. Even in banks, traders were shown to customers on tours of the firm as you would with animals in a zoo and the site of a trader cursing on a phone while in a shouting match with a broker is something that was part of the scenery.
So while cursing and bad language can be a sign of dog-like status and total ignorance -the ""canaille"" which etymologically relates these people to dogs; ironically the highest status, that of free-man, is usually indicated by voluntarily adopting the mores of the lowest class[5]. Consider that the English ""manners"" isn't something that applies to the aristocracy; it is a middle class thing and the entire manners of the English are meant for the domestication of those who need to be domesticated.
Take for now the following:
What matters isn't what a person has or doesn't have; it is what he or she are afraid of losing
So those who have more to lose are more fragile. Ironically, in my debates, I've seen numerous winners of the so-called Nobel in Economics (the Riksbank Prize in Honor of Alfred Nobel) concerned about losing an argument. I noticed years ago that four of them were actually concerned when me, a nonperson and trader, called them publicly a fraud. Why did they care? Well, the higher you go in that business, the more insecure you get as losing an argument to a lesser person exposes you more than other people.
Higher up in life only works under some conditions. You would think that the head of the CIA would be the most powerful person in America, but it turned out that he was more vulnerable than a truck driver... The fellow couldn't even have an extramarital relationship. You can risk people's lives but you remain a slave. The entire structure of the civil service is organized that way.
The exact obverse of the public-hotshot as slave is provided by the autocrat.
As I am writing these lines, we are witnessing a nascent confrontation between several parties, which includes the current ""heads"" of state members of the North Atlantic Treaty Organization (modern states don't quite have heads, just people who talk big) and the Russian Vladimir Putin. Clearly, except for Putin, all the others need to calibrate every single statement to how it could be misinterpreted the least by the press. I have been exposed to such an insecurity first hand. On the other hand, Putin has the equivalent of f***you money, projecting a visible ""I don't care"", which in turn brings more followers and more support among the constituents. In such a confrontation Putin looks and acts as a free citizen confronting slaves who need committees, approval, and of course feel like they have to fit their decisions to an immediate rating.
The effect of such an attitude as that of Putin is mesmerizing on his followers, particularly the Christians in Lebanon -especially those Orthodox Christians who lost the active protection of the Russian Czar in 1917 (against the Ottoman usurper of Constantinople) and now are hoping that Byzantium is coming back about hundred years later, though the reincarnation is a bit further north. It is much easier to do business with the owner of the business than some employee who is likely to lose his job next year; likewise it is easier to trust the word an autocrat than a fragile elected official.
Watching Putin against others made me realize that domesticated (and sterilized) animals don't stand a chance against a wild predator. Not a single one. Fughedabout military capabilities: it is the trigger that counts.
Universal suffrage did not change the story by much: until recently, the pool of elected people in so-called democracies was limited to a club of upper class people who cared much, much less about the press. But with more social mobility, ironically, more people could access the pool of politicians-and lose their job. And progressively, as with corporations, you start gathering people with minimal courage -and selected because they don't have courage, as with a regular corporation.
Perversely, the autocrat is both freer and -as in the special case of traditional monarchs in small principalities  in some cases has skin in the game in improving the place, more so than an elected official whose objective function is to show paper gains. This is not the case in modern times, as dictators knowing their time might be limited, indulge in pillaging the place and transferring assets to their Swiss bank account -as in the case of the Saudi Royal family.
More generally:
People whose survival depends on qualitative ""job assessments"" by someone of higher rank in an organization cannot be trusted for critical decisions.
Although employees are reliable by design, it remains that they cannot be trusted in making decisions, hard decisions, anything that entails serious tradeoffs. Nor can they face emergencies unless they are in the emergency business, say firefighters. As we [saw/will see] with the payoff function, the employee has a very simple objective function: fulfill the tasks that his or her supervisor deems necessary. If the employee when coming to work in the morning discovers the potential for huge opportunities, say selling anti-diabetes products to prediabetic Saudi Arabian visitors, he cannot stop and start exploiting it if he is in the light fixtures business selling chandeliers.
So although an employee is here to prevent an emergency, should there be a change of plan in anything, the employee is stuck. While this paralysis can stem because of the distribution of responsibilities causes a serious dilution, there is another problem of scale.
We saw the effect with the Vietnam War. Then most (sort of) believed that certain courses of action were absurd, but it was easier to continue the course than to stop -particularly that one can always spin a story explaining why continuing is better than stopping (the backfitting story of sour grapes now known as cognitive dissonance).
We are also witnessing the same problem with the U.S. attitude towards Saudi Arabia. It is clear since the attack on the World Trade Center on September 11 2001 (in which almost all the attackers were Saudi citizens) that someone in that nonpartying kingdom had a hand -somehow -with the matter. But no bureaucrat, fearful of oil disruptions, made the right decision -instead the worst one of invading Iraq was endorsed because it appeared to be simpler.
Since 2001 the policy for fighting Islamic terrorists has been, to put it politely, missing the elephant in the room, sort of like treating symptoms and completely missing the disease. Policymakers and slow-thinking bureaucrats stupidly let terrorism grow by ignoring the roots -because it was not a course that was optimal for their job, even if optimal for the country. So we lost a generation: someone who went to grammar school in Saudi Arabia (our ""ally"") after September 11 is now an adult, indoctrinated into believing and supporting Salafi violence, hence encouraged to finance it -while we got distracted by the use of complicated weapons and machinery. Even worse, the Wahabis have accelerated their brainwashing of East and West Asians with their madrassas, thanks to high oil revenues. So instead of invading Iraq, blowing up ""Jihadi John"" and other individual terrorists, thus causing a multiplication of these agents, it would have been be easier to focus on the source of all problems: the Wahabi/Salafi education and the promotion of intolerance by which a Shiite or a Yazidi or a Christian are deviant people. But, to repeat, it is not a decision that can be made by a collection of bureaucrats with a job description.
The same thing happened in 2009 with the banks....
Now compare these policies with ones in which decision makers have skin in the game as a substitute for their annual ""job assessment"", and you would picture a different world.
Indeed, quite a different world as we can see in the next chapter.
[1] John Mast-Finn.
[2] In some countries, executives and mid-level managers are given perks such as a car (in the disguise of a tax subsidy), which are things on which the employee would not spend his money had he been given cash (odds are he may save the funds); they make the employee even more dependent.
[3] Stanislav Yurin. Ancient dilemma, see Ahiqar, the root of Aesop-La Fontaine:
Ou vous voulez ?  Pas toujours ; mais qu'importe ?
- Il importe si bien, que de tous vos repas
Je ne veux en aucune sorte,
Et ne voudrais pas meme a ce prix un tresor. ""
Cela dit, maitre Loup s'enfuit, et court encor.
[4] I can't resist this story. I once received a letter from someone with a request: ""Dear Mr Taleb, I am a close follower of your work, but I feel compelled to give you a piece of advice. An intellectual like you would greatly gain in influence if he avoided using foul language."" My answer was very short: ""f*** off.""
[5] My friend Rory Sutherland (the same Rory) explained that some more intelligent company people had the strategy of cursing while talking to journalists in a way to signal that they were conveying the truth, not reciting some company mantra"".
[i] http://www.thelatinlibrary.com/benedict.html
Note the Silver Rule Quod tibi non vis fieri, alio ne feceris.
Chapters in Progress from The Incerto Collection (The Black...
",147
https://medium.com/@the-hip-hapa/5-things-not-to-do-in-the-robinhood-app-for-stock-trading-604bd813915e?source=tag_archive---------0-----------------------,5 Things NOT to Do in the Robinhood App for Stock Trading,"As a first-time investor, I've found the Robinhood app super-useful for buying stocks. But as a first-time investor, there's a lot I don't...",Jen Quraishi Phillips,6,"As a first-time investor, I've found the Robinhood app super-useful for buying stocks. But as a first-time investor, there's a lot I don't know. Here are the top 5 mistakes I made in my first months using Robinhood.
When I first started using Robinhood, it was my first time buying stocks directly, ever. So I didn't realize that even Robinhood offers different ways to buy stocks. When you buy in real-time, you often don't get the exact price you want because of delay between when you enter the order and when it processes. But this is not the only way to buy a stock, and definitely not the best.
To access the other ways to buy a stock, you tap on the stock you want, you can then tap ""Buy"", and then ""Order Types"" in the upper right-hand corner. The different kinds of ""orders"" are: Limit Order, Stop Limit Order, Stop Loss, and Limit Order. For myself, I find the ""Limit Order"" most useful. A limit order means that you can tell the app, ""Hey, I want to buy Apple stock, but only if it's $95 or less."" The app will earmark funds for this, and automatically execute the buy when the stock price reaches $95 or less.
Now, when you do a ""Limit Order"", it means you have less money in the kitty (Robinhood calls this ""Buying Power"") for buying other stocks. Which leads me to my #2 mistake.
You may see a great stock you want to buy RIGHT NOW. But darn it, you forgot to add funds, so the opportunity passes you by. Unless you have Robinhood Instant, it'll take about 3 days for your money to transfer from your bank account to the Robinhood app. So if there's a stock you have your eye on, don't even think of buying until you've amassed enough ""buying power"" (Robinhood's term for available cash) in your account.
Even though my ""investment strategy"" (ha ha) is basically to buy low, hold on, collect dividends, and eventually sell high, I still get impatient. When I see money sitting in my Robinhood account, I want to spend it right away because I don't want it losing the potential interest it would be gaining if it were in, say, an IRA or other savings vehicle. But I am working on patience. We are in a volatile market right now, and there's a good chance the stock I want may go even lower before the year's out. The dip in price may be a reaction to a domestic news event like the presidential election, or to global influences like the state of China's economy, or just because it's a rainy Tuesday: who the hell knows.
Although the response to news events has been shown to be minor (and more pronounced in reaction to positive news like stock earnings than to negative news) fluctuations can last as little as 40 seconds, so I will set up something automated to make sure I catch the price I want. Computers now do the majority of trades, cashing in on milliseconds, so being a sausage-fingered human is a major disadvantage.
Ideally, I'd like to set up some kind of simple, tiered queue where, for example: If I have more than $200 in cash AND Apple stock is $99 or less, then it'll buy as many APPL shares as possible; UNLESS Microsoft is less than $34/share, in which case it'll buy Microsoft. But as far as I know, that's not possible in Robinhood yet. So until then, I'll just set up a number of limit buys and whichever gets to its target price first will be executed first.
Since I'm buying most of my stocks in the view that I'll hold them for at least a year (here's why), I try not to look at their value daily. Instead, I track the value of my portfolio monthly in a spreadsheet. The way I look at it, it's kind of like checking your weight every day: fluctuations are normal, so there's no reason to get freaked out by small changes.
Also, I know that studies have shown that the ""pain"" from a potential loss of money is twice as powerful as the ""high"" from a gain. So don't fall prey to your fears and tap ""sell"" the moment your stock starts to fall. You want to look at the big picture over time. Set up some Google news alerts for yourself so you can stay aware of any major information involving the companies you hold stock in. And if you're really intent of not letting a stock dip below a certain price, you can set up Robinhood to automatically sell it (""stop loss"" order) before it gets to that point.
Currently I'm using three apps to research stock history and prices: Robinhood, ClosingBell, and WikiWealth. Robinhood only gives 1 year of stock history, which may be enough for some. But I often like to use Yahoo Stocks or another tool to look at prices for 5 years or longer, as well as to research dividend history.
WikiWealth (screenshot to left) is based around Warren Buffet's stock picks and gives a simple recommendation for stocks. It also shows things like the company's cash flow margin and potential returns, if you're interested.
ClosingBell compiles analyst and user ratings and news, but I actually use it more for an alert system. You can tell it to send you a notification when, say, GoPro hits $8 a share. It also has handy lists of things like ""Top Picks in Communication Services"" so you can learn more about the top stocks in different sectors, which is great if you're looking to diversify your portfolio and aren't sure where to start.
ClosingBell does have a Robinhood integration, but honestly I haven't used it much.
Okay, that's the 5 things I've learned *not* to do while using Robinhood. I know #1 and #3 are kind of the same thing, but they're the most important. Since I started using Robinhood, I have been learning a lot about the markets, but of course I still have a lot to learn. I found out, for example, that for tax reasons, I could likely be having a better outcome if I were investing inside of an IRA rather than through Robinhood. But as a newbie, I've found Robinhood to be a great way to get my feet wet and sooooo easy since it's on my iPhone. And hey, in 5 months my $1600 investment has gained almost $300 in value. Not too bad, especially considering the market conditions. Now I just need to learn how to do even better, and more about the tax implications of buying stocks. If you have tips or links to help, please feel free to put them in the comments section. Thanks for reading.       -
Jan. 2018: I've published an update to this post here with more things I've learned.
Jan. 2021: I have lost confidence in Robinhood. I've moved my entire portfolio to Schwab (except for 3 stocks I forgot about, and Bitcoin cuz Robinhood doesn't let you transfer it).
The last straw for me was the company's actions around Gamestop, when they stopped individual users from buying the skyrocketing stock while hedge funds continued. But they also said checking/saving accounts were insured that were not and continue to have multiple outages that stop people trading for DAYS. I no longer trust that if I truly need to buy/sell a stock on Robinhood, I'll 100% be able to do it. So for now, I'm letting my portfolio chill at Schwab while I try out new apps. Good luck to you all!
",148
https://richculturemedia.com/the-one-word-that-explains-why-economics-professors-are-not-billionaires-e7754a606ef4?source=tag_archive---------8-----------------------,The One Word That Explains Why Economics Professors Are Not Billionaires,This would change the financial experts you listen to,David O.,8,"It matters who you learn about money from. There is a reason 99% of the world's population competes for 1% of the resources. And it is not the fault of the rich or the ultra-rich. Instead, it is those the 99% listen to for financial advice. There is one word in the definition of economics that is the root of the problem. And that is the word, ""scarcity"". Economics operates on the premise that there is scarcity and hence resources have to be rationed. This is why economics professors look to manage what is. They cannot think of possibilities or abundance. Hence, they find it difficult to amass wealth or become billionaires. But it doesn't end here.
Economics is all about how to manage what exists. From the very definition, you can see why it is difficult for a student or master of economics to amass great wealth. The first reason is that they will always be thinking of managing what they have. They will never think of having more or creating more. The second reason is that even when they get an opportunity to have more, based on how they have been taught, their mindset does not allow them to take advantage. They think their gain is another person's loss. They think they have to give up something valuable to achieve something greater. The concept is called opportunity cost.
Management is a great skill and it has its place. But the world cannot go forward with management. Throughout history, there have been great managers. But in the last 300 years, we have had inventors, innovators, and adventurers that have changed the world with discoveries. These discoveries have changed the way we live and has made a tremendous impact on the economy.
A great economy is not a product of great management. A great economy is a product of invention, innovation, and discoveries
Economists look at what exists and think of how everybody can manage that. That kind of thinking is what drowns people into poverty. This is why economists can't be billionaires. If an economist becomes a billionaire tomorrow, you can be sure it won't be long before the money is gone.
Big money runs from those who try to keep it and runs to those who dare to dream big
If you still doubt this point, then consider an illustration. If a brilliant economist 1,000 years ago is given the task of predicting and creating a financial model for this present world (with the current world population), what do you think the economist will predict? There is no way that the economist can predict the internet. Can you imagine today's economy without the internet? Think about this.
The financial models today are based on this exact ignorance. Economists think that the resources we have today are the same ones we will have tomorrow.
The media celebrates millionaires and billionaires like they are a special breed of people. And they are not. If you have listened to enough of them, you will understand that it is no big deal. They are just people who focused on doing something that produces a lot of money if they are successful. And they will be successful if they take every possible advantage and don't give up.
Many of the rich people try so hard to show regular people that the path to wealth isn't difficult at all. But the average person has been stocked with the wrong information for so long that it is difficult to change. And that programming came from school.
School teach economics and not money
There is a difference between teaching economics and money. Economics is the study of scarcity. Money is a study of value and the creation of value. And the public gets their understanding of the subject of money from learning economics. Once a person's mind is conditioned by the idea of scarcity, it is difficult to break out.
This is why rich people are either self-made (who stopped school early) or they are the children of the rich (who were taught differently about money at home)
Your financial life is the way it is because of the people you listen to. Economists lecture the public about money and hence they plunge the world into the same mindset that they have. The media don't call millionaires and billionaires to talk about the economy, rather they call them to talk about themselves and their opinions. The media calls economists to talk about the economy. And guess what?
If a billionaire argues with an economist, the economist would win. This is because the economist is logical. However, the economist will continue to be an employee while the billionaire will keep making billions.
It matters whose insight you are listening to about money.
The problem of poverty will never be solved by money. The central banks can print as much money as they wish, if the money is not met with productivity growth, the result will be inflation. It could also lead to hyperinflation and the fall of a currency.
Productivity growth is not championed by economists. Productivity growth is championed by entrepreneurs. Entrepreneurs that are illogical end up rich. This is because they look at what could be and go forward to create it.
Economists sit down and come up with research. The annoying part of the research is that they present it as the gospel truth to which there can be no deviation. And this research is designed to limit the thinking of those who read it.
If you read an economic research, it puts your mind in a box that you may never be able to break out from
This is just like the 4-minute mile challenge. When people have tried and failed to run a mile in 4 minutes, scientists came out with an explanation of how a human couldn't run a mile in 4-minutes. They gave all kinds of explanations about the muscles, tendons, and bone structure. Yes, even scientists did that. It took one person who is not even a professional athlete to break the record.
I heard a story of a class that was given a particular complex equation to solve. The professor told the students that all the great professors had tried to solve the equation and had failed. Then the professor wrote the question on the board. All the students tried and failed. After the professor was gone, a young guy who had been absent walked in, looked at the equation and solved it in one glance. The interesting part was that the young guy was not even among the best students.
It matters the people you allow to paint your financial mindset.
Why do we look at the situation of the economy to determine our financial choices? When the media start talking about a recession, everybody starts behaving like they are in a recession. The truth is that people create their own recessions. People keep talking about a 2020 recession, and I have not felt it thus far. Yes, there was an event but how it affects you depends on you. In fact, recessions are the best times to get rich.
Where there is no optimism or inspiration, and people look up to the economic report, poverty is bound to be created
In a real sense, the economy is nothing more than the behavior of the producers and consumers in a country. This is why countries with higher levels of optimism and inspiration produce more entrepreneurs. If your behavior is altered or influenced by the news, you have put yourself in a difficult place to create wealth.
The state of the economy is one thing and it is entirely different from your financial reality. The state of the economy is the problem of the people in government. It is not your problem. It has nothing to do with your financial results. If you have no job, it is not because people are not hiring.
If you have no job or work, it is because you have not figured out a way to help another person get a result to which they will gladly pay you for
What possible result can I help someone get because I don't seem to have any result myself? Go online and learn. Become knowledgeable at something. Go deep to the extent where you know it more than 90% of all people. Then, tailor it to an audience that it doesn't appeal to (but needs it). And be different from everybody else.
Your financial future is connected to your decisions. Decide to be the best thing ever to happen to a specific audience.
Think about the number of times you hear these words and phrases every day: not enough, bad, need, don't have, lack, worse, etc. These are words that are consistent with the idea of scarcity. And they might be everything you associate with finances. And they will shape your experience.
Now, think about these other words and phrases: rich, plenty, too much, excess. How often do you hear them being used in a financial context? How often do you hear that on the news? Even when the times are good, they are anticipating the bad times.
Entrepreneurs use the concept of scarcity to sell their products and services. And that is good to drive up their value. But the use of the concept in monetary policy and governance makes people line up with buckets in front of a tap whereas there is a huge river bank 50 meters away.
What is the one word that makes people rich? Abundance: the idea that there is so much available. The school system has wired a kind of remorse in people when they make insane amounts of money. They believe they are taking from what should be someone else's. And that is not true.
If you decide to make a lot more money today, and the money in circulation can't keep up, the Fed will print more money to match up
In today's world, various economies need people who will command more money. The US needs people that would justify the money printing they have been doing. The real reason anyone is without money is that they don't have something strong and compelling they want to do with the money.
People solve problems. The financial system is looking for people who can solve problems (and have proved they can) to give money. Think about this. When you wake up each morning say this to yourself aloud:
There is too much money in the world looking for someone who has a solution to a problem. I have decided to solve a problem and hence I am qualified to have a lot of money!
Think abundance.
Why are economic professors not billionaires? It is because they think within the box of the existing available resources. They are financially logical. If you want to be super rich, you have to be financially illogical. Think illogical, do illogical.
If nothing else, I hope you now know why you should not listen to economics professors to make (personal) finance decisions.
Cheers.
Inspiring your financial success
12.9K 
187
12.9K claps
12.9K 
",150
https://medium.com/paylater/building-your-credit-history-b5c2c01494e4?source=tag_archive---------6-----------------------,Building your credit history,"Loans  as with finance, in general  can do wonderful things for you when properly managed. It can help solve urgent problems, cater to...",Carbon,2,"Loans  as with finance, in general  can do wonderful things for you when properly managed. It can help solve urgent problems, cater to substantial expenses and grow businesses.
However, when mismanaged, credit quickly becomes a source of frustration and can cause real damage to your future financial prospects.
We've highlighted a few ways below to ensure you are properly managing credit:
Limit your exposure to debt
Realize there is such a thing as 'too much credit'. Taking on multiple loans at a time increase the risk of missing a payment and then getting stuck in nasty cycles of debt  constantly taking additional loans to pay off previous loans which you're already struggling to service.
Only apply for loans when you need it
A loan is a serious obligation and should be treated as such  it probably shouldn't be used to impress your boys at the club! Assess your needs prior to applying  is the cash need important enough to take out a short term loan?
Start with the end in mind
Only take loans when you are certain you will have the means to repay on or before the agreed due date(s). Be sure to confirm all interest/fees associated with the loan prior to applying. Only proceed when you are comfortable that you will be able to service the expected repayments.
Avoid late repayments at all costs
Late repayments or defaults on loans are not only a breach of the contractual agreement between a borrower and a lender; they also come with very real consequences that can be hard to shake off in the long-term.
Services such as Paylater reward you for on-time repayments with points, which can be used towards obtaining more attractive loan terms.
Get familiar with credit reports
If you don't know what this is  you're not alone! A lot of our new users start off unaware of the meaning of credit reports or the existence of the credit bureaus that store information on them. Financial institutions are required to provide these bureaus with data on the accounts you hold with them  this applies to current accounts, overdrafts, loans, and soon, even payments such as PHCN bills will be reported!
Three of the registered bureaus in Nigeria are XDS Credit Bureau, CR Services Credit Bureau, and CRC Credit Bureau. You can order a self-report from any of the following organizations to review your credit history.
Let us help you get better at managing credit  get in touch with us today or download the Paylater app on the Google Play Store.
Serving the next billion with digital financial services.
7.6K 
9
",151
https://themakingofamillionaire.com/i-retired-at-35-here-are-5-lessons-from-my-first-6-months-of-freedom-4f0bdbdb9587?source=tag_archive---------7-----------------------,I Retired at 35  Here Are 5 Lessons from My First 6 Months of Freedom,"The good, the bad and the downright weirdness of it all",Charlie Brown,7,"In October 2020, I officially retired from work and life as I knew it. I was 35.
Through a series of largely intentional  but some fortuitous  events, I now have enough passive income to oversee all my basic living costs forever.
This is not a life I thought I would lead. Growing up, I thought early retirement was reserved for the rich, and 'early' meant 55. I grew up poor and just assumed I'd be working for money forever.
But if you're open to it, life sometimes has a funny habit of taking you down avenues you never even knew existed.
It's been 6 months since I pressed the big red financial freedom button. Here's what I've learnt in the weirdest half a year of my life.
Someone on Twitter recently asked at what age their followers got their first job.
I was 6. I worked for my artist mother, selling her paintings and notecards to customers who loved the novelty of buying from a little blonde girl.
Working is in my blood, and I can't imagine life without it.
Of course, no one who retires at 35 actually retires in the traditional sense; I'm not about to join my grandma's Bridge club. I still want to work. I just want to work for something other than money. I want to work on meaningful projects, something that I care about, whether I get paid or not.
My guess is, if you're on the path to financial independence, you don't want to stop working either.
The problem is, I have 29 years of working for money behind me, and divorcing work from money takes some time. After all, society's work rhetoric has been drilled into our heads since we were born.
""Don't worry about the work; just think of the money.""
""You should take that job because it pays better.""
""It doesn't matter if you don't like your job; the pay is more important.""
The weight of society can be against you on this one. So what do you do?
You take some time to divorce the two. I took a couple of clients on a freelancing site who paid me peanuts for crappy work. It didn't take long to realize that money was no longer my primary motivator.
Earning money isn't the devil. After all, if you're still earning cash, it gives your investments more time to grow, which is no bad thing.
But working purely for the money completely defeats the point of reaching Financial Independence in the first place. Don't fall into the trap. Find the work that is meaningful to you and chase after that instead, not the dollar bills.
My parents-in-law cannot understand why my husband and I chose to sell our wine business for a life of, as they see it, nothing. (They only think it's nothing because they don't ask what we do now, but that's an issue for my therapist and me to unpack).
For almost everyone in the working world, your job is your identity. It's called Workism, and it makes us miserable. When literally the first question anyone ever asks is 'what do you do?' how do you explain you're a financially independent 35 years old who doesn't 'need' to work without sounding like an entitled, lazy layabout?
It's time to forge a new identity, one that is rooted in something more meaningful than your job title.
Ryan Nicodemus of the Minimalists once said he no longer asks people he meets what they do for a living. Instead, he asks them what they're passionate about. He finds that the ensuing conversation is far more meaningful and interesting because he's asked someone what they actually care about, not just their job title.
Hello, nice to meet you. My name is Charlie, and I am a financially free writer, feminist, full-time traveller, and wine-lover with a penchant for food, family, and friends.
Everyone can benefit from this way of thinking, not just the financially free. You are not only your job title, so why define yourself that way?
Your conversations with strangers could be so much more interesting if you forget the work identity and ask them what they really care about instead.
The Financial Independence Retire Early movement isn't mainstream, however normal it seems to those down the rabbit hole. Considering 55% of the population are too scared to regularly check their bank balance when you have a spending spreadsheet and know your net worth to the nearest dollar, there's definitely a disconnect.
So the questions will come. Sometimes they're curious, sometimes they're suspicious, most of the time, they're just people trying to wrap their heads around the idea.
Are you just on holiday? What do you do all day? Aren't you bored? What is your long-term plan? No, really, you can't be doing this in 5 years, surely?
Questions are great because they open discourse and conversation. I don't want to persuade everyone I meet to head down the financial independence path not everyone has or wants the opportunity to do so, and I'm mindful of that.
But being open to alternative ways of living is one of life's great skills. Financial independence, the tiny house movement, communal parenting. Whatever it is, if you're open to listening and learning, you may discover something wholly unexpected that you can apply to your own life.
So when I receive questions about my financial independence, I'm more than happy to answer them. Maybe someone will take a nugget of an idea to help them better shape their own financial life. What could be better than that?
You think you won't miss your old job. Then you remember something a colleague said that cracked you up. You feel the loss like a gut punch.
You wake up in the middle of the night, remember that you're financially independent, and start to spiral. What if my entrepreneurial post-retirement venture doesn't work? What do I do if I run out of money? What if I lose friends because they don't understand what the hell I'm doing?
Perhaps it was naive of me to expect to be totally ay-OK with retiring at such a young age, leaving my old life in the dust.
But achieving financial independence is basically turning your life upside down, inside out. You're sticking one finger up to accepted financial norms, and that takes time to process.
The sucker punches diminish as time goes on and your new life becomes the norm, but they are a good reminder of the magnitude of what you have achieved. Gaining financial freedom is no small thing. And it's worth being reminded of that sometimes.
For the first month, I didn't think about being retired. I was too busy trying to wrangle my life into something that resembled what I wanted. I was moving from the most trapping of situations, complete with a house and a bricks-and-mortar business, to life on the road. I was establishing writing plans. I was fighting off parental naysayers.
There was no time to compute.
But around a month in, my husband Sam turned to me and said:
""You are aware that you never have to work on anything, or for anyone, you don't want to again, aren't you?""
The truth be told, I wasn't aware. I've had my fair share of bad jobs. Recruitment. Administration for men who only thought I was good enough to make the tea. Even in my own company, I felt intensely beholden to my customers, my employees, rules, and regulations, always anticipating their needs, not mine.
Shaking off all of that is one of the greatest lures of financial freedom. That you have the freedom to discover what you like to do and how you like to do it. The freedom to shape your day into how you'd like it to look. The freedom to give back to the world in a sustainable, meaningful way.
There are no two ways about it. It's fricking awesome.
I was recently part of a writers group discussion where someone asked where we all see ourselves in 5 years. One said they wanted to be a novelist with a couple of books under their belt. Another said they wanted to build a copywriting and marketing company.
When my turn came, I said I have no idea, and I was super happy about that.
With financial freedom comes the opportunity to throw that question in the trash. Will I still be travelling? Still, be writing? Still without a permanent home?
Who knows.
Life generally laughs at your attempts to control it anyway, so why not lean into the uncertainty and leave some parts of it up to chance?
That's the beauty of a financially free future. You don't only get to write the rules; you get to shrug it off when the rules fly out of the window. And that is worth all the questions, the identity crises, and the sucker punches in the world.
This article is for informational purposes only. It should not be considered Financial or Legal Advice. Not all information will be accurate. Consult a financial professional before making any significant financial decisions.
",152
https://medium.com/hackernoon/why-you-should-never-use-upwork-ever-5c62848bdf46?source=tag_archive---------1-----------------------,"Why you should never use Upwork, ever.","Note. I never wanted to write something like this, or expose the people involved. However, I feel that's it necessary for something to get...",Shadi Al'lababidi Paterson,10,"Note. I never wanted to write something like this, or expose the people involved. However, I feel that's it necessary for something to get done for my own ends, and also for those thinking of foraying into the world of freelancing and in particular, using Upwork's platform, to seriously reconsider.
Freelancer/Consultant looking to grow your income online? Let's hang out on Twitter 
<%% UPDATES %%>
Medium support asked me to take down all images. Yes it was after I spoke to Upwork, however it was Monday morning California time and they were very reasonable. No need for the tin-foil hats.
I created a post that replicated the original, which you can view here, which has the images included.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
I've been contacted by all involved parties and many others. A full follow post will be up soon.
I can't even start to express my gratitude for the support, but also the stories shared. Please remember, the collective is stronger than the few. Voices echo on the internet.
</%% UPDATES %%>
Now, I'm not going to get into the recent changes in Upwork's fees to take 20% from freelancers. I'm not going to get into the additional fees they charge, the poor support they offer, the weak quality of jobs they have. I'm not going to talk about the impossibility of competition they offer due to being seriously undercut by those that live in countries with lower costs of living. I'm not going to talk about the stupidity of their algorithm which focuses on getting the most money from a client rather than providing the best service.
No. You see, I actually managed to make money from Upwork, you can see my profile here  (http://www.upwork.com/o/profiles/users/_~01f4aa82c79a722261/)
I was doing quite well, I even made a video demonstrating my skills, specifically for Upwork. It became one of my main income streams and ways to gain leads. You can see my feedback, the projects I've worked on, and all of my happy clients. Very, very clearly.
Let's take us back exactly 11 days ago, that's all it took.
2 weeks previous to me having to write this, I was invited to interview for a standard Analytics Gig setting up a Saleforce account with GA. Very basic stuff. I tell the client expected hours and quote, then he disappears. This isn't unusual in itself. About 10 days later I get the standard -
'Hey, sorry for the delay, I need XYZ done in a very short amount of time, etc... etc...' . Anyone that's done freelance knows that this is just how clients can be. Usually followed by a 'We can also only pay you about half, it's the arbitrary number we budgeted for, even know we know nothing about the work we're actually trying to get you to do'.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
Of course, I expect nothing less from a 'Director of Sales' at a company. But honestly this is just the everyday life of a freelancer, nothing to write home about.
I'm going to fast forward over the next week of little to no contact, where I'm trying to get a basic bullet point spec sheet so that I can accurately come within budget. Even though said client want's it 'Turned around ASAP'.
The client eventually gave me login details, alas, still no job. (They have to actually give you the job through Upwork). As a seasoned freelancer, I'm always skeptical and very suspicious. Especially when a client-to-be only has a 3.5 star rating yet has over 40 previous jobs.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
After 3 weeks of sporadic talks, the client trying to get me to work without actually giving me the job, and myself getting fed up. I decided to not go through with it.
This is just usually a huge red flag for me and it should be for any freelancers reading this also. In order to get a job done properly, you need solid communication, a client willing to talk to you and sort you out with what you need within at least 48 hours. If they're constantly bargaining to get your price down, to undervalue the work that needs to be done. If they try and validate a low price with the promise of 'Long term work'. Get out and move on.
NOTE: The client gave me their login details to Salesforce, this would prove integral later in the story.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
These messages really do say it all. Reading back, I can see how what I wrote may be provocative. But you have to remember, he's essentially been keeping me on the ropes for what is now 17 days. Which for Upwork, is very unusual. I simply wanted to cut if off and move on.
When I get this reply from him (Btw, 12pm my time is 3am his time), I was really quite surprised. I considered my words critically. But ultimately felt that his threat was directly aimed at my livelihood and my reputation. For those that don't freelance, this is the equivalent of logging a complaint directly to your boss. It's no joke, just because it's over the internet from a stranger. This is worth $2000+ a month to me, a large portion of my income.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
Finally, I get hit with this.
Then, I receive this -
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
He tried to also jeopardize some long standing client work of mine by explicitly going after clients he could find on the internet and writing to them about my unprofessional-ism and advising them not work with me. Yes, he went out of his way and took his time at 3am in the morning to try and break all of my working relationships 
(There was a screenshot of him writing to my client. He used the words ''fuck face'' in a very interesting way. My client respectfully asked for this to be taken down and I of course obliged)
As well as said client actually writing 'Shadi is a cock face', through said client's contact form. Seriously, that happened.
In this situation, what would you do? Well, I decided to cover my bases, contact Upwork, they have the chat logs, they have support. So I created a ticket.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
No prizes for those who guess how this ended.
The next morning I wake up to find a message banner across the Upwork homepage.
'You're suspended... all financial transactions will be blocked ...'.
Woah, wow. Yes, he found out that I created a complaint and decided to retaliate.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
Yes, that's right. He said I hacked into their salesforce account. Now, apart from the obvious limitations of someone that has the most rudimentary knowledge of how HTTP Headers even work. Despite the fact that Salesforce uses 2-factor. Remember, he actually gave me the login details.
'Holy crap, is that actually happening', I believe is the polite and SEO friendly way is what I said to myself.
Now, on this beautiful Sunday morning, I wake up to this.
(Image removed by request of Medium  view original here  )
(Here was said website, Medium also made me delete all links to it. Said website is actually my personal website)
Yes, that's right.
To reiterate what this means, specifically to me. Over $500 of current funds being processed, along with another $750 coming in from a client I'M STILL WORKING WITH, will be blocked. As well as a client that I currently have an hourly with. Note, they haven't actually stopped me from taking in money from clients, just withdrawing it from their platform. Opening up new jobs and any other actions related to this.
Essentially, the clients I have now can still be charged, Upwork can still take their commissions, but there is figuratively no way for me to withdraw this money. Not to mention I've just lost a large part of my potential income.
'Upwork, are you serious? Wow.' , perfectly sums up my thoughts.
Freelancing is no joke, and it's not always pretty. I've had truly amazing client relationships over the years, I'd say freelancing is 90% fun up until the point where you actually have to ask for your money. A common theme is a client disappearing for weeks on end, they're on their business trip, they're in the middle of a sprint. Yes, we've all heard it.
If you're a new freelancer. Don't be disheartened. My advice to you is this:
Find your community. Whether that be 8760, or whatever that pertains to your industry. Talk to people, especially if you're young. Offer your time, in return you may not be getting a bunch of money, but you will be learning valuable skills. Especially the soft skills needed for remote work, as well as a portfolio and references.
If you're considering Upwork. I would strongly suggest you not to. Not just because of what happened to me. Remember Upwork takes 20% up to $500, then 10% up to $10000. + VAT and processing fees and FX rates.
If you want to charge $100 per hour on Upwork. You can essentially charge something like $78 per hour direct to a client, without Upwork, and get paid the same. It's not just you saving money, but also the client (Who has an extra 1.35% charged on top, now). That's huge labor savings of over 20% for a client, and you avoid all of the Upwork bureaucracy. Take your experience and portfolio and join these online communities, there is always plenty of work.
You're like the Comcast of Freelancing. Your customer Support seems blind to facts.
Now, one of three things could of resulted in why this happened.
It's almost a year since I've joined you. I've worked on exactly 29 different projects thanks to you. Of those 29 projects, only 1 went to a dispute, which was solved after I paid the $200 fee or whatever it is for the process (See the recurring theme here, by the way?).
After 1 day, you auto-banned my account. After 11 days, you permanently banned it. You've blocked all money going out. No explanation from Support, no nothing.
So please tell me, how a freelancer, who has gone to great lengths, such as creating a very embarrassing video (I actually hired a professional video guy to do this), can be banned so easily. How should other freelancers either starting out or in the same position as myself, view this?
After much deliberation, I decided it was worth calling out someone for what they truly are.
(Names now redacted due to legal stuff)
#UpYoursWork
I hope that was enjoyable. If it was, give it a , so others can find it.
Freelancer/Consultant looking to grow your income online? Let's hang out on Twitter 
Hacker Noon is how hackers start their afternoons. We're a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don't take the realities of the world for granted!
#BlackLivesMatter
6.7K 
176
",153
https://medium.com/newco/its-the-advertising-model-stupid-b843cd7edbe9?source=tag_archive---------7-----------------------,Facebook Can't Be Fixed.,"Facebook's fundamental problem is not foreign interference, spam bots, trolls, or fame mongers. It's the company's core business model, and...",John Battelle,5,"Mark Zuckerberg has announced his annual ""personal challenge,"" which in the past has ranged from eating meat he personally kills to learning Mandarin.
This year, his personal challenge isn't personal at all. It's all business: He plans to fix Facebook.
In his short but impactful post, Zuckerberg notes that when he started doing personal challenges in 2009, Facebook did not have ""a sustainable business model,"" so his first pledge was to wear a tie all year, so as to focus himself on finding that model.
He sure as hell did find that model: data-driven audience-based advertising, but more on that in a minute. In his post, Zuckerberg notes that 2018 feels ""a lot like that first year,"" adding ""Facebook has a lot of work to do  whether it's protecting our community from abuse and hate, defending against interference by nation states, or making sure that time spent on Facebook is time well spent....My personal challenge for 2018 is to focus on fixing these important issues.""
The post is worthy of a doctoral dissertation. I've read it over and over, and would love, at some point, to break it down paragraph by paragraph. Maybe I'll get to that someday, but first I want to emphatically state something it seems no one else is saying (at least not in mainstream press coverage of the post):
You cannot fix Facebook without completely gutting its advertising-driven business model.
And because he is required by Wall Street to put his shareholders above all else, there's no way in hell Zuckerberg will do that.
Put another way, Facebook has gotten too big to pivot to a new, more ""sustainable"" business model. The company is on track to earn at least $16 billion in profits in 2017. Wherever the number lands (earnings for the year come out later this month), it's at least 50 percent growth on the year before. As a stock, Facebook is breaking out in a massive way  it's priced at roughly 36 times earnings  a healthy premium to the S&P's average of around 25. Facebook's financials from its first year through 2016 are in the opening image above. Here's the stock price in 2017:
The stock is trading at $186 or so today; it began the year at roughly $120. As we all know, a stock price is the market's estimate of future earnings. If Zuckerberg decides to really ""fix"" Facebook, well, that stock price will nosedive. And that will lead angry shareholders to sue the stuffing out of the company, and likely demand the CEO's head on a pike.
If you've read ""Lost Context,"" you've already been exposed to my thinking on why the only way to ""fix"" Facebook is to utterly rethink its advertising model. It's this model which has created nearly all the toxic externalities Zuckerberg is worried about: It's the honeypot which drives the economics of spambots and fake news, it's the at-scale algorithmic enabler which attracts information warriors from competing nation states, and it's the reason the platform has become a dopamine-driven engagement trap where time is often not well spent.
To put it in Clintonese: It's the advertising model, stupid.
We love to think our corporate heroes are somehow super human, capable of understanding what's otherwise incomprehensible to mere mortals like the rest of us. But Facebook is simply too large an ecosystem for one person to fix. And anyway, his hands are tied from doing so. So instead, he's doing what people (especially engineers) always do when the problem is so existential they can't wrap their minds around it: He's redefining the problem and breaking it into constituent parts.
Here are two scenarios for what might come of Zuckerberg's 2018 quest:
Which do you think will happen?
Yeah, me too. If Zuckerberg picks #2, his business would shrink by tens of billions of dollars. It'd still be an awesome business, and he'd probably be a lock for Man of the Year. But it'd get his assed sued into oblivion by angry shareholders.
Then again....Zuckerberg is one of several tech founders who hold super majority shares that give him absolute control over the future of his company. The stated reason for this structure, popularized by Google founders Sergey Brin and Larry Page, was to ensure that the capitalistic vagaries of Wall St. don't force visionary companies to hew to corporatist rationale as they mature. (Page and Brin actually name-checked the New York Times as their inspiration.)
Will Zuck drop acid? Let's just say this: He certainly could.
Covering the biggest shift in business and society since...
17.9K 
148
",154
https://towardsdatascience.com/a-comprehensive-guide-to-downloading-stock-prices-in-python-2cd93ff821d4?source=tag_archive---------9-----------------------,A comprehensive guide to downloading stock prices in Python,Download historical stock prices with as little as one line of code!,Eryk Lewinson,6,"The goal of this short article is to show how easy it is to download stock prices (and stock-related data) in Python. In this article I present two approaches, both using Yahoo Finance as the data source. There are many alternatives out there (Quandl, Intrinion, AlphaVantage, Tiingo, IEX Cloud, etc.), however, Yahoo Finance can be considered the most popular as it is the easiest one to access (free and no registration required).
The first approach uses a library called yfinance and it is definitely the easiest approach that I am aware of. The second one, withyahoofinancials, is a bit more complicated, however, for the extra effort we put into downloading the data, we receive a wider selection of stock-related data.
Let's get right into it!
We need to load the following libraries:
You can pip install the libraries you are missing :)
yfinance is a very convenient library, which is my go-to library for downloading stock prices. It was previously known as fix_yahoo_finance. The short history of the library is that is started as a fix to the popular pandas_datareader library. With time, Yahoo Finance changed the API and the connected functionalities were deprecated. That is when fix_yahoo_finance was introduced to once again make downloading data from Yahoo Finance possible. It worked as both a patch to pandas_datareader, and as a standalone library.
Without further ado, below I show how to quickly download the stock prices of Tesla:
Running the code results in the following table:
By default, the function downloads daily data, but we can specify the interval as one of the following: 1m, 5m, 15m, 30m, 60m, 1h, 1d, 1wk, 1mo, and more. The command for downloading data can easily be simplified to one line:
However, I wanted to show how to use the arguments of the function. I provided the start and end date of the considered timeframe and disabled the progress bar (for such a small volume of data it makes no sense to display it). We can download the stock prices of multiple assets at once, by providing a list (such as ['TSLA', 'FB', 'MSFT']) as the tickers argument. Additionally, we can set auto_adjust = True , so all the presented prices are adjusted for potential corporate actions, such as splits.
Aside from the yf.download function, we can use the Ticker() module. Below I present a short example of downloading the entire history of Tesla's stock prices:
Running the code generates the following plot:
The advantage of using the Ticker module is that we can exploit the multiple methods connected to it. The methods we can use include:
For more information on the available methods, be sure to check out the GitHub repository of yfinance.
The second library I wanted to mention in this article is yahoofinancials. While I find it a bit more demanding to work with this library, it provides a lot of information that is not available in yfinance. Let's start with downloading Tesla's historical stock prices:
We first instantiated an object of the YahooFinancials class by passing Tesla's ticker. Having done so, we can use a variety of methods to extract useful information. We started with historical stock prices. The usage of the method is pretty self-explanatory. One thing to note is that the result is a JSON. That is why I had to run a series of operations to extract the relevant information and convert the JSON into a pandas DataFrame. Running that snippet results in:
The process of obtaining the historical stock prices was a bit longer than in the case of yfinance. Now it is time to show where the yahoofinancials shines. I briefly describe the most important methods:
I presented a case of using the library for downloading information on a single company. However, we can easily provide a list of companies, and the methods will return appropriate JSONs containing the requested information for each company. Let's see an example of downloading the adjusted close prices for multiple companies:
Getting the data into the pandas DataFrame required a bit more of an effort, however, the code can easily be reused (with slight modifications also for different methods of the YahooFinancials class).
In this article, I showed how to easily download historical stock prices from Yahoo Finance. I started with a one-liner using the yfinance library and then gradually dived deeper into extracting more information on the stock (and the company). Using the mentioned libraries, we can download pretty much all the information available at Yahoo Finance.
You can find the code used for this article on my GitHub. As always, any constructive feedback is welcome. You can reach out to me on Twitter or in the comments.
Liked the article? Become a Medium member to continue learning by reading without limits. If you use this link to become a member, you will support me at no extra cost to you. Thanks in advance and see you around!
I recently published a book on using Python for solving practical tasks in the financial domain. If you are interested, I posted an article introducing the contents of the book. You can get the book on Amazon or Packt's website.
",155
https://towardsdatascience.com/detecting-credit-card-fraud-using-machine-learning-a3d83423d3b8?source=tag_archive---------8-----------------------,Detecting Credit Card Fraud Using Machine Learning,Catching Bad Guys with Data Science,Lukas Frei,8,"This article describes my machine learning project on credit card fraud. If you are interested in the code, you can find my notebook here.
Ever since starting my journey into data science, I have been thinking about ways to use data science for good while generating value at the same time. Thus, when I came across this data set on Kaggle dealing with credit card fraud detection, I was immediately hooked. The data set has 31 features, 28 of which have been anonymized and are labeled V1 through V28. The remaining three features are the time and the amount of the transaction as well as whether that transaction was fraudulent or not. Before it was uploaded to Kaggle, the anonymized variables had been modified in the form of a PCA (Principal Component Analysis). Furthermore, there were no missing values in the data set. Equipped with this basic description of the data, let's jump into some exploratory data analysis.
Since nearly all predictors have been anonymized, I decided to focus on the non-anonymized predictors time and amount of the transaction during my EDA. The data set contains 284,807 transactions. The mean value of all transactions is $88.35 while the largest transaction recorded in this data set amounts to $25,691.16. However, as you might be guessing right now based on the mean and maximum, the distribution of the monetary value of all transactions is heavily right-skewed. The vast majority of transactions are relatively small and only a tiny fraction of transactions comes even close to the maximum.
The time is recorded in the number of seconds since the first transaction in the data set. Therefore, we can conclude that this data set includes all transactions recorded over the course of two days. As opposed to the distribution of the monetary value of the transactions, it is bimodal. This indicates that approximately 28 hours after the first transaction there was a significant drop in the volume of transactions. While the time of the first transaction is not provided, it would be reasonable to assume that the drop in volume occurred during the night.
What about the class distributions? How many transactions are fraudulent and how many are not? Well, as can be expected, most transactions are non-fraudulent. In fact, 99.83% of the transactions in this data set were not fraudulent while only 0.17% were fraudulent. The following visualization underlines this significant contrast.
Finally, it would be interesting to know if there are any significant correlations between our predictors, especially with regards to our class variable. One of the most visually appealing ways to determine that is by using a heatmap.
As you can see, some of our predictors do seem to be correlated with the class variable. Nonetheless, there seem to be relatively little significant correlations for such a big number of variables. This can probably be attributed to two factors:
Before continuing with our analysis, it is important not to forget that while the anonymized features have been scaled and seem to be centered around zero, our time and amount features have not. Not scaling them as well would result in certain machine learning algorithms that give weights to features (logistic regression) or rely on a distance measure (KNN) performing much worse. To avoid this issue, I standardized both the time and amount column. Luckily, there are no missing values and we, therefore, do not need to worry about missing value imputation.
Now comes the challenging part: Creating a training data set that will allow our algorithms to pick up the specific characteristics that make a transaction more or less likely to be fraudulent. Using the original data set would not prove to be a good idea for a very simple reason: Since over 99% of our transactions are non-fraudulent, an algorithm that always predicts that the transaction is non-fraudulent would achieve an accuracy higher than 99%. Nevertheless, that is the opposite of what we want. We do not want a 99% accuracy that is achieved by never labeling a transaction as fraudulent, we want to detect fraudulent transactions and label them as such.
There are two key points to focus on to help us solve this. First, we are going to utilize random under-sampling to create a training dataset with a balanced class distribution that will force the algorithms to detect fraudulent transactions as such to achieve high performance. Speaking of performance, we are not going to rely on accuracy. Instead, we are going to make use of the Receiver Operating Characteristics-Area Under the Curve or ROC-AUC performance measure (I have linked further reading below this article). Essentially, the ROC-AUC outputs a value between zero and one, whereby one is a perfect score and zero the worst. If an algorithm has a ROC-AUC score of above 0.5, it is achieving a higher performance than random guessing.
To create our balanced training data set, I took all of the fraudulent transactions in our data set and counted them. Then, I randomly selected the same number of non-fraudulent transactions and concatenated the two. After shuffling this newly created data set, I decided to output the class distributions once more to visualize the difference.
Outlier detection is a complex topic. The trade-off between reducing the number of transactions and thus volume of information available to my algorithms and having extreme outliers skew the results of your predictions is not easily solvable and highly depends on your data and goals. In my case, I decided to focus exclusively on features with a correlation of 0.5 or higher with the class variable for outlier removal. Before getting into the actual outlier removal, let's take a look at visualizations of those features:
Box plots provide us with a good intuition of whether we need to worry about outliers as all transactions outside of 1.5 times the IQR (Inter-Quartile Range) are usually considered to be outliers. However, removing all transactions outside of 1.5 times the IQR would dramatically decrease our training data size, which is not very large, to begin with. Thus, I decided to only focus on extreme outliers outside of 2.5 times the IQR.
Visualizing our classes would prove to be quite interesting and show us if they are clearly separable. However, it is not possible to produce a 30-dimensional plot using all of our predictors. Instead, using a dimensionality reduction technique such as t-SNE, we are able to project these higher dimensional distributions into lower-dimensional visualizations. For this project, I decided to use t-SNE, an algorithm that I had not been working with before. If you would like to know more about how this algorithm works, see here.
Projecting our data set into a two-dimensional space, we are able to produce a scatter plot showing the clusters of fraudulent and non-fraudulent transactions:
Onto the part you've probably been waiting for all this time: training machine learning algorithms. To be able to test the performance of our algorithms, I first performed an 80/20 train-test split, splitting our balanced data set into two pieces. To avoid overfitting, I used the very common resampling technique of k-fold cross-validation. This simply means that you separate your training data into k parts (folds) and then fit your model on k-1 folds before making predictions for the kth hold-out fold. You then repeat this process for every single fold and average the resulting predictions.
To get a better feeling of which algorithm would perform best on our data, let's quickly spot-check some of the most popular classification algorithms:
The results of this spot-checking can be visualized as follows:
As we can see, there are a few algorithms that quite significantly outperformed the others. Now, what algorithm do we choose? As mentioned above, this project had not only the focus of achieving the highest accuracy but also to create business value. Therefore, choosing Random Forest over XGBoost might be a reasonable approach in order to achieve a higher degree of comprehensiveness while only slightly decreasing performance. To further illustrate what I mean by this, here is a visualization of our Random Forest model that could easily be used to explain very simply why a certain decision was made:
Fraud detection is a complex issue that requires a substantial amount of planning before throwing machine learning algorithms at it. Nonetheless, it is also an application of data science and machine learning for the good, which makes sure that the customer's money is safe and not easily tampered with.
Future work will include a comprehensive tuning of the Random Forest algorithm I talked about earlier. Having a data set with non-anonymized features would make this particularly interesting as outputting the feature importance would enable one to see what specific factors are most important for detecting fraudulent transactions.
As always, if you have any questions or found mistakes, please do not hesitate to reach out to me. A link to the notebook with my code is provided at the beginning of this article.
References:
[1] L.J.P. van der Maaten and G.E. Hinton, Visualizing High-Dimensional Data Using t-SNE (2014), Journal of Machine Learning Research
[2] Machine Learning Group  ULB, Credit Card Fraud Detection (2018), Kaggle
[3] Nathalie Japkowicz, Learning from Imbalanced Data Sets: A Comparison of Various Strategies (2000), AAAI Technical Report WS-00-05
",156
https://medium.com/yueeh/2018-%E7%B6%B2%E8%B3%BC-%E8%B6%85%E5%95%86%E7%A5%9E%E5%8D%A1-%E5%9C%8B%E6%B3%B0%E4%B8%96%E8%8F%AF-koko-combo-icash-%E8%81%AF%E5%90%8D%E5%8D%A1-79989437238a?source=tag_archive---------8-----------------------,2018  & : KOKO COMBO icash , 5%; @GoGo ,Yueh,8," 5% ,:
 Richart   ,,2018 ;, KOKO, COMBO icash (COMBO ),
 Richart  GoGo ,:KOKO  RichartCOMBO  GoGo !
:Richart , KOKO COMBO icash ,;,COMBO  Mastercard, GoGo  VISA 
, KOKO  Richart , KOKO  + COMBO ,
 Richart ,, KOKO App  + 

, Email;,,!(,)
 10 ,~
1/18: App 1/19:1/30:,
,, KOKO COMBO icash  COMBO :
 2018/12/31 , COMBO , 5% :PChome,,
, KOKO ;, 5%!
??
!, 7-11 (:7-11 ); 4 ;OK  20 ; 1/3 ,
!
, Apple PaySamsung Pay  Android Pay , KOKO COMBO icash 
 PTT :, KOKO COMBO icash , 5% (:~);, KOKO , 5%
107/12/31 , KOKO COMBO icash  icash  NT$500  7,500 OPENPOINT , OPENPOINT  3 ( $2,000 )
KOKO  0.3% ,,
,, 300 ;,, 6,000 ,
,,
 KOKO , Richart + @ GoGo  
, NT$1,800 
107/12/31, KOKO COMBO icash , 30 , NT$888  ()  icash , 3 , KOKO  NT$500
2018/07/16~2018/10/15, Apple Pay ,~ 6 ~ 63 
107/12/31 ,KKBOX  6 
 KOKO COMBO icash  2018 ,;,
,,,KOKO  Richart ( KOKO )
 KOKO  0.15%( 1.8%,Richart  1%), NT$5,000 , 6/21  12/21 
,KOKO , Richart 
:KOKO  /  50 ( KOKO ~XD)
,B11F-EC15613,S221769146
iOS  / Android 
 KOKO COMBO icash ,
yueeh.com
yueeh.com
 Yueh  Life and Tech 
www.facebook.com
** This is NOT a sponsored article.
all about LIFE and TECH.
392 
7
",157
https://medium.com/@hwayne/is-wealthfront-worth-it-38b5d90e0250?source=tag_archive---------8-----------------------,Is Wealthfront Worth it?,"I like money. That's pretty normal. What's weird is that I like thinking about money, to the point where a 'quiet night in' usually means...",Hillel Wayne,9,"I like money. That's pretty normal. What's weird is that I like thinking about money, to the point where a 'quiet night in' usually means making pickles and doing my finances. Last year I got my taxes in the day after I got my W2. I'm just into money.
A while back a friend recommended Wealthfront, a robotic investing service. They manage your investment money for you, claiming to get much higher returns than the average investor or even most professionals. They also do automatic rebalancing, tax lost harvesting, and all sorts of other fun things, all for just 0.25% of your portfolio annually. At the time I was just getting into investing, so I figured hey, might as well try it out. I eventually put 9,999 dollars in it (the max before fees kick in) and then continued investing in Vanguard, managing that part of my portfolio myself.
About a month ago I contacted Vanguard to get a few questions answered. Here's how it went.
Me: ""Can you tell me a little more about my risk profile?""Vanguard: ""Sure, here's 30 pages of analysis, projections, and references.""
Emboldened by this, I contacted Wealthfront too.
Me: ""Can you tell me a little more about my risk profile?""Wealthfront: ""Your risk is ten.""Me: ""Yeah, but what does that mean? Do you have historical performance for a risk ten portfolio?""Wealthfront: ""We have a graph.""Me: ""Can you tell me what were the average returns per quarter?""Wealthfront: ""It's different for everyone.""Me: ""Can you tell me what were my returns per quarter?""Wealthfront: ""No.""Me: ""Can you at least give me the raw data?""Wealthfront: ""No.""Me: ""...""Wealthfront: ""Have you considered setting up an autodeposit? 500 dollars a month could increase your account by 76,000 in ten years!""Me: ""Isn't that just a 4% compounded return?""Wealthfront: ""...""Me: ""...""Wealthfront: ""76,000!""Me: *click*
I like money. I also like being very petty. So I decided to manually answer all my questions using the absolutely amazing Portfolio Visualizer and see what it says about Wealthfront. Here we go.
Disclaimer: I'm not a financial person. I'm a programmer who's Dunning-Krugerred his way into thinking he's an expert in everything. Please don't take this as financial advice. Take it as some guy just complaining on the internet a lot.
One of the counterintuitive things in investing is what ""high risk"" means. For funds, 'high risk' doesn't mean 'likely will lose a lot, but might gain a lot'. It means 'high median return, high standard deviation.' This means low-risk funds are for short-term investments, while high-risk funds are for long-term investments, where the fluctuations will even out. I'm 24, debt-free, with no major expenses on the horizon. I can afford to be very risky. With that in mind, let's look at what happens when we invest $10,000 in Wealthfront's max risk distribution:
Not bad. You're making about 3% per year, and if you adjust to the bottom of the stock market crash (please don't ever do this) you're looking at a solid 11% percent increase! These numbers are actually a little high, because we're not accounting for the .25% Wealthfront fee, but it gives us a rough gauge.
Now let's compare it to investing in just the S&P 500:
Huh.
Okay, that's not a completely fair comparison. We're comparing a single, volatile index to a diversified portfolio. Generally, diversification is better than throwing everything into one fund and hoping for the best. That's because different kinds of investments do better at different times, and by only picking one you miss good years on the rest and be oversensitive to bad years on your own. This pretty awesome chart from BlackRock sums it up nicely:
Sometimes US markets give the most return. Sometimes it's world markets. Sometimes it's fixed income. What's important is that it changes year to year, so you're better off playing the broad game. Wealthfront advertises their diversification as a major asset in their favor:
WEALTHFRONT OFFERS A PERSONALIZED AND GLOBALLY-DIVERSIFIED INVESTMENT PORTFOLIO OF INDEX FUNDS
So let's look at their max risk distribution. Here's the actual funds it contains:
Let's break this down. About a third of your money is following the total US market and 50% is international. Then you have a bit following energy for some reason and finally a slice of bonds. It doesn't have as high gains right now as the S&P, but it'll probably do better in the long run.
So yes, Wealthfront gives you diversification. But what if we did it ourselves? Wealthfront had a 95% stock/5% bond allocation, so let's follow that. Vanguard recommends about 40% of your stocks be international, which is about 35% of the total portfolio. So let's do 60% domestic market, 35% international market, and 5% bonds. Here's what we get.
Our own allocation does better overall, and that's not even accounting for the Wealthfront fee. More importantly, it's roughly correlated with the Wealthfront allocation, and moves up and down in the roughly same magnitude. If the Wealthfront balance has diversity, then so do we, and it only took us a few seconds to think it up. Diversification may be a valuable advantage, but by no means is Wealthfront the only way to get it.
But what if you want less risk? Maybe you have a short term goal. Maybe the market is volatile and we flinch (DON'T DO THIS). With Wealthfront, it's easy to make a lower-risk, diverse portfolio. Here's the distribution they give us for a risk of 4:
This has performed better than ""max risk"" Wealthfront portfolio over the past 8 years, mostly because it lost less in the 2008 crash. Bonds just did better for a while. Regardless, with Wealthfront you just pick a number and it does the rest for you. And it figures out, based on a basic questionnaire, your risk tolerance, which can be pretty reassuring.
Again, though, this isn't anything new or special or even convenient that Wealthfront provides. Here's a piece from one of my favorite charts:
Look at the ""average annual return"" and ""years with a loss."" Choose the graph with the most years you're comfortable with. Make that investment percentage in bonds. Split your remaining money 60/40 US/International stocks. Let's pick '4' here, too, for 40% bonds, 35% US stock, 25% international:
As for the questionnaire, that's not particularly new either. Once again, the services Wealthfront offers are already widely available, with lower fees and more transparency.
Wealthfront advertises it regularly 'rebalances' your portfolio. It lists this as one of the big advantages of using the service, claiming it leads to a .4% increase over the lifetime of your investment. That'd more than make up for the .25% fee.
A study performed by David Swensen, Chief Investment Officer of Yale University, found that threshold-based rebalanced portfolios earned an average of 0.4% more per year over 10 years than portfolios that were not rebalanced.
Thing is, rebalancing isn't actually that complicated an idea. Over time, as your assets gain and lose value, your portfolio's risk changes. Like we have a long string of good years and your 60/40 stock/bond ratio becomes 80/20. Now you've got a riskier portfolio, and should move money from stocks into bonds to go back to your old ratio.
This sounds like it'd be difficult, especially if you've got a lot of funds you're trying to rebalance regularly. In this case, Wealthfront would be providing a great service. Fortunately our lazy portfolio has only three funds and experts say you're just fine rebalancing annually, so Wealthfront is charging you a chunk of your returns to save you a few minutes each year.
This is the big one. We can, with just a little bit of effort on our part, do the same thing Wealthfront does minus overhead. The problem is that there's still some effort and some expertise required. While I think it's important to understand how your money works, it can be a really scary topic for a lot of people. Several of my friends have said that even if Wealthfront is a less optimal investment, it's at least some investment, and one that's easier to get into than a do-it-yourself.
And this is true: Wealthfront is a completely no-effort investment, and a 4% long-term increase is still much better than the 0.12% return you'd get from Citibank. Even if our lazy portfolio is better, it can never give us that.
Fortunately, this kind of service, once again, isn't unique to Wealthfront. Vanguard, for example, also offers ""all-in-one"" funds, aka ""funds of funds"", which do exactly the same thing Wealthfront does. Two of them are target retirement funds, which automatically adjust their risk tolerance as you get older, and life strategy funds, which let you pick a risk tolerance and stick with it as long as you want. Let's compare the highest risk all-in-ones to Wealthfront's max risk:
Once again we do better by skipping the middleman.
Tax-loss harvesting is the idea that if you sell assets for less than you bought them, you get a tax write-off. Then you buy a similar (but not too similar) asset and hey, deferred taxes. Not free, because you'll have to pay extra capital gains of them later, but deferred's not a bad second. Wealthfront does this for you automatically, swapping between similar ETFs.
This seems like a pretty useful benefit, and I haven't researched it too in depth, so I don't have a lot to say on it. The only things I'll point out is that the jury's still out on how useful it is and it can be dangerous to have two robo-advisors harvesting for you. That's because if you sell one asset and buy a ""similar enough"" one, that's called a wash, and you can't write that off. Robo advisors are careful enough not to give you washes, but if you have two and they're not communicating, they can't ensure this, and you may end up underpaying on your taxes.
Wealthfront says they aim at a tech-savvy crowd of people who want to use innovative, disruptive technology and leave the big, slow investment companies in the dust. That sounds pretty cool. Then again, Vanguard has full-featured apps on all mobile platforms, lets you do your own analyses, and has two-factor auth. Wealthfront doesn't even have an Android app yet. An Android app! They've been promising one since 2014!
As always, it depends. If you really value tax-lost harvesting or have only a little to invest, it might be right for you. Otherwise, I personally don't see the point. Where it innovates it doesn't provide much value, and the value it does provide isn't that innovative. And, of course, getting info from them is like pulling teeth.
My Wealthfront money is still there, mostly to keep my tax forms reasonable. As soon as the 2015 year ends, though, I'm planning to transferring to the rest of my self-managed portfolio. I just think it'll provide better value for my time and money. And, of course, I like thinking about my money, and would rather not have a computer think about it for me. That's just less fun.
",158
https://entrepreneurshandbook.co/how-i-slowly-became-a-middle-class-millionaire-bd621529fb0f?source=tag_archive---------0-----------------------,"How I Slowly Became A ""Middle-Class"" Millionaire",Learn the rules of the game,Barry Davret,5,"Sixteen years ago, I had no job, no income, less than $1,000 in savings, and debts totaling $22,000. My one shot at entrepreneurship put me in precarious financial and professional jeopardy.
Today, I'm what you'd call a middle-class millionaire. When I add up assets and subtract liabilities, the net balance exceeds the mythical threshold, but you'd never know it from my lifestyle.
My house looks like a generic old home you'd find in Anytown, USA. It needs more work than we can afford, and not a day goes by that I wish I had resisted the drama of home-ownership.
The rest of my life mirrors common middle-class challenges: paying a mortgage, worrying about college for my kids, and nervously peeking at my credit card and bank account balances once a week.
Middle-class millionaires still worry about money since most of their net worth exists in untouchable assets: home equity and retirement accounts. This wealth exists on paper, but you can't spend it, and it always runs the risk of disappearing due to market conditions.
Despite the illusionary aspects of paper wealth, I consider myself fortunate. I make enough to pay the bills and save for the future. My kids go to good schools and never worry about finding their next meal.
Still, it's not easy, and you may not think it's worth the sacrifices.
In previous generations, one parent could stick out the same job for forty years and retire in relative comfort. Those days are gone.
Today, you need to approach your career as if it could disappear at any time. You'll also have to generate multiple income streams. And finally, you must accept that you're never going to become a gazillionaire entrepreneur or crypto lottery winner.
A year after my near financial collapse, my manager told me about an open position, two pay grades above my level. When I told him I wasn't qualified, he said it didn't matter. I needed to demonstrate my commitment to advancing my career.
We argued back and forth until he said something that finally clicked with me. ""If you want to get ahead, you have to play the game they want you to play.""
Every industry has its own unwritten rules, as does every business. Some seem pointless, purely for show, and often just unfair. But it's humans who run businesses, and humans play games. Understand the game they want you to play, and follow the rules.
But wait, you say. Shouldn't advancement depend on performance and results? In revenue-producing positions, like sales, it often does. Money has a way of rewarding performance. Even then, politics plays a role. But if you're in an administrative or expense role, it's full-on game theory.
I left my full-time job in 2003 to move out west. That's when I kicked off my entrepreneurial effort. When it crashed, I re-entered the job market, but I had been out of my previous role for over two years, leaving my skills a bit stale.
Several hundred resumes later, I had landed a total of zero interviews. That's when I decided on a radical strategy change  reinvention.
I had been a software developer in my previous life, a decent one at best. Rather than continue to look for a position in that role, I packaged my experience as a programmer and my experience selling mortgages as a failed entrepreneur, took a few courses on project management, and leveraged all of that into a role as a Project Manager for mortgage processing software.
It's the kind of job title that makes people squint as if doing long division in their head.
That's the point.
That extreme specialization enabled me to position myself as the only person even remotely qualified.
Five years later, I again reinvented myself. No doubt, I will repeat this strategy several times before I hang up my work pants for good.
Continuous reinvention is a way of life. Fair or not, you cannot depend on your skills remaining relevant. Nor can you assume your skills are good enough to keep your job safe from poachers, structural changes in your industry, or declining economic fortunes.
When I secured that miracle job in 2005, my salary barely covered the hefty price tag of living in New York City. I've since left, but my modest salary increases eventually lost ground to the rising cost of living and raising two kids.
Five years ago, I started a side hustle to bring in extra money. That extra cash allowed us to divert money into savings and investment. We also made a few improvements to our home, increasing its value.
Today, my wife and I maintain three income sources. It's stressful and time-consuming, but it's also necessary. A side gig is no longer a luxury but a necessity. A family of four shouldn't need multiple incomes to maintain a middle class lifestyle, but that's the reality.
There's a reason why the 1926 book, The Richest Man In Babylon, still tops the charts of wealth-building books. It offers simple yet practical and timeless advice such as this: ""For every ten coins thou placest within thy purse take out for use but nine.""
For most of my working life, saving ten percent was impossible. Today, we manage it thanks to our multiple income streams. Even with enough money coming in, saving requires discipline.
Tucking loose change under your mattress won't cut it. If you have access to a 401K with matching, take advantage of it. If not, setup another automatic deduction plan.
To build wealth, you need to save and invest. Your salary alone, no matter how impressive, won't secure your long-term future. Start with 1% if you must, and then build from there.
When we shopped for our house, the lure of new construction intoxicated us. Those homes, however, were considered the exurbs  outside the traditional metropolitan area. Historically, these neighborhoods rarely held up in price appreciation.
That turned us off, and instead, we followed the advice of a real estate expert friend.
""Buy the best piece of shit you can afford in a neighborhood with a strong history of price appreciation.""
That's what we did. We approached it strategically, buying a piece of junk in a nice neighborhood we plan on staying for twenty years.
On paper, buying a home is a terrible idea. The maintenance costs alone eat up much of your equity. A saner plan would be to rent a cheap house and then put your savings towards investment.
But that requires a discipline few people possess, myself included.
For all the negatives of homeownership, if you buy in a neighborhood with a history of price appreciation, you'll at least keep up with inflation while paying off your mortgage balance.
That's one method of nabbing a million-dollar net worth, not an exciting one, and perhaps, not a worthwhile one. But for me, striking it rich with a billion-dollar startup was never a realistic possibility.
How to succeed in entrepreneurship
5.6K 
57
5.6K claps
5.6K 
",159
https://themakingofamillionaire.com/spy-vs-qqq-investing-in-different-indexes-782714e03643?source=tag_archive---------8-----------------------,SPY vs. QQQ: Investing in Different Indexes,Comparison of ETFs that track the S&P 500 and NASDAQ 100,Matthew Chin,6,"Earlier last year, I analyzed and compared two of the most popular S&P 500 index exchange-traded funds (ETF), SPY and VOO. An index is just a metric used to measure the stock market performance based on the prices of a subset of companies/stocks. Since an index ETF is conceptually just a bucket of stocks that mirror the performance of an index, I was curious about other funds that followed other indexes. Some of the other indexes we often hear on the news include the Dow Jones Industrial Average and the NASDAQ Composite.
One such fund specifically for the NASDAQ is QQQ. While funds like SPY are made up of the 500 largest companies of the US Stock Exchanges, QQQ is an ETF that is composed of the 100 largest companies traded only on the NASDAQ Stock Exchange. If we look at the historical trends, even with downturns during the Dot-com Bubble at the turn of the millennium and the Great Recession in 2008, the NASDAQ 100 index vastly outperformed the S&P 500 index over this three-decade period.
So why has the NASDAQ 100 performed so much better, making QQQ worth so much more? If we take a look at the top 10 holdings of these two funds that track their respective indexes, we can see they share many of the same companies, albeit at different weights.
Apple, Microsoft, Amazon, Facebook, and Alphabet (Google) all make up the top five holdings, but QQQ weights are roughly twice as much as SPY. As a result, QQQ is way more tech-heavy in addition to high weights in Tesla, PayPal, Adobe, and NVIDIA.
Just based on this, if the technology sector does better than the overall market, a portfolio of QQQ would perform much better than one of SPY. Evidently, this is exactly what happened this past decade, continuing well into the pandemic.
As investors, we can't exactly invest in an index; the closest we can do is the index ETF. So let's take a look at how SPY and QQQ perform over different investment periods.
medium.com
SPDR S&P 500 ETF (SPY) and Invesco QQQ Trust (QQQ) adjusted close share price data were pulled from Yahoo! Finance using some Python code. The date range was limited from 1/1/2000 to the current date to give a 1:1 date comparison since QQQ has only been trading since 1999 while SPY has been trading since 1993. This would still give us two whole decades of data.
After pulling the daily closing price data from Yahoo! Finance, we can see that QQQ experienced a near-exponential decrease in 2000 and an exponential comeback after 2008, while SPY was more steady in its price changes. QQQ even dropped to as low as $17 a share but rose back up to over $300.
But as usual, share price trends are not very useful in comparing the two funds, so instead, we'll look at returns in percentages.
Using a simple percent change calculation, we can calculate the day-to-day price fluctuations of each ETF. Additionally, we can also get a distribution of the difference in percent change between the two funds relative to QQQ. In practicality, a negative percent difference would mean SPY did worse than QQQ, and a positive percent difference would mean SPY did better.
Although the two funds seem to perform similarly on a daily basis (indicated by the symmetrical distribution centered closely to 0% difference +/-5%), the distribution is just slightly negatively skewed, resulting in QQQ having better daily returns.
When we open up the investment period to one year, we see that SPY only outperforms QQQ a third of the time. The distribution becomes a lot less symmetrical and shifts more negative over to an average of -10% with a range between -40% and +60%.
Further increasing the investing periods to five years, QQQ continues to outperform SPY with more frequency and magnitude. SPY underperforms QQQ over 80% of the time, with the average percent difference at approximately -35%!
And since we have two full decades of data, we might as well extend it to a 10-year investment period and see what kind of returns we can expect.
Here, we see that SPY only performed better than QQQ a small fraction of the time with a maximum of only 50% difference, while being outperformed by -100% or more on average. In some 10-year periods, QQQ even outperformed by more than -300% difference! The best 10-year period was from 8/31/2010 to 8/31/2020; SPY grew 405% while QQQ grew% 752%. That means a $100,000 investment in SPY and QQQ would have grown to $405,000 and $752,000, respectively!
By now, it is pretty clear that QQQ has been outperforming SPY in every way over the last 20 years. So is it the better choice to invest in? Based on the analysis of this data, it's easy to say that QQQ is a much better investment, but it might not be that simple. And as any good analytics professional would say, past performance does not necessarily predict future results. Since QQQ has such a large technology holding, who's to say it will continue to outperform all other sectors for another 20 years? Maybe the tech will have a plateau, or maybe it might skyrocket to even higher prices, we don't know.
Ultimately, much of it depends on an investor's risk tolerance and diversification needs. QQQ seems to be a perfect example of higher risk, higher reward. Since it's largely made up of the technology sector, it is much less diversified than SPY. The S&P 500 includes several hundred more companies across various sectors and is typically the benchmark for stock market performance, whereas the NASDAQ 100 is more weighted in newer, less mature companies. However, both funds have consistently trended upwards and even bounced back from several economic downturns. So personally, as a younger investor just starting out, why not have both?
If you're interested in viewing my code, please check it out on my GitHub!
This article is for informational purposes only. It should not be considered Financial or Legal Advice. Not all information will be accurate. Consult a financial professional before making any major financial decisions.
",161
https://medium.com/mel-magazine/i-won-104-million-for-blowing-the-whistle-on-my-company-but-somehow-i-was-the-only-one-who-went-to-7ed8a808d50c?source=tag_archive---------8-----------------------,I Won $104 Million for Blowing the Whistle on My CompanyBut Somehow I Was the Only One Who Went to Jail,"The story of Bradley A. Birkenfeld, the first in our series of Unusual Millionaires",As Told to MEL,8,"Bradley C. Birkenfeld, 51, blew the whistle on the Swiss bank UBS for helping Americans avoid paying taxes, leading to about $15 billion in recovered tax money, fines and penalties. He spent two and a half years in prison, but he later was awarded $104 million by the I.R.S. for his role in exposing the scheme.
The thing about offshore banking is if someone goes to Switzerland to open an account, that's totally legal. Bank secrecy is written into the Swiss constitution. But a Swiss banker isn't technically supposed to leave Switzerland to solicit clients. You can go nude on a beach in France, but if you do the same thing in Minnesota, you'll get arrested. It's sort of like that, but not as interesting.
At the start, I played a role in all of this, too. As a director of UBS KEY clients with a net worth of more than $25 million, part of my job was to go to the United States and drum up new business  even though legally, potential clients were supposed to come to us; we weren't supposed to go to them. UBS did the same thing in Germany, Asia, Scandinavia, the Middle East, South America and Canada. To further help with client prospecting, UBS sponsored events all over the world: Music festivals, art shows, classic car expos, you name it. Our job would be to hide behind the scenes and troll for new clients  or, to be more blunt about it, aid and abet tax evasion.
There were other signs, too: Training documents that told us how to avoid detection at customs and encrypted laptops, which I never took with me. But I honestly didn't think too much of it until April 2005, about four years after I began working at UBS. A colleague of mine brought me a document from the UBS intranet. It was three pages and contradicted everything we were doing, explicitly saying we shouldn't solicit clients in other countries.
I couldn't get it out of my head. It was a full-on cover-your-ass document that made us easy scapegoats for rogue banking. If we got caught soliciting a client or doing anything else illegal  even if UBS told us to  the bank could simply say, ""We told you not to do it. There it is right in the company system.""
melmagazine.com
Of course, the reality was that they didn't want someone at my level finding the document and asking questions. It was meant to be lost amid an endless pit of compliance forms, account-opening forms, PowerPoints and training documents. You'd have a better chance of seeing God than reading everything on the UBS intranet.
That's why I immediately printed out a number of copies, which I handed out to my senior colleagues. I asked if they'd seen it, and they said, ""No, how did you get it?"" I then went to my boss's office and said, ""What the hell is this?"" He told me, ""Don't make a big deal about it."" DON'T MAKE A BIG DEAL ABOUT IT? I wanted to punch the guy right there. Anyone who had written a document like this had to get approvals, both to write it and then to put it on the intranet. This wasn't a rogue or errant posting; this was deliberate.
A month later, I sent a copy of the document to the heads of both legal and compliance in an email and interoffice memo. No response. I sent it again the next month. No response. I sent it a third time. No response.
That's when I started taking documents out of the bank. I grabbed accounting records, emails, phone records, PowerPoints, training manuals, internal memos. I covered my tracks. I didn't use email or my mobile phone. I used pay phones in Switzerland. I hid documents across the border in France in a friend's barn in case they raided my home. I told him it was a box of old clothes I didn't have room for anymore.
It's important to understand that no one else in the bank would do anything. They were too afraid. Their salary was from UBS. Their mortgage was from UBS. Their car loan was from UBS. Their kids were in private school because of UBS. Turning on the bank would be like punching your parents in the face before they send you to college and cover the whole bill. But me? I was an outsider  the American who was unafraid. Someone told me I'd never work in Switzerland again. I didn't give a shit. While I loved Switzerland and made great friends there, I knew I had to stand up for what I believed was right.
Since I couldn't get anyone within the company to respond to me, I resigned in October 2005  six months or so after I first became aware of the document. We did an exit interview as if everything was normal. They even asked me why I was resigning. How cute. I told them, ""It's because you won't give me an answer about this three-page document."" They said I'd never get an answer. I was defiant. ""You're fucking wrong,"" I responded. Then I sued them for the bonus they were obligated to pay me. It was part of my contract, but they tried to renege because I resigned. Eventually, they settled and paid me what they owed me.
Unfortunately for them, this was just the beginning. I attached the three-page document to three UBS internal whistleblowing policies and sent a letter to the chairman and copied the entire Board of Directors. It informed them that they were all on the hook and that none of them could say they didn't know about it. They didn't like that at all.
A few months later, I went to the Justice Department. I thought they'd be thankful, but instead, they were hostile. Think of it this way: You have some civil-servant hack who's uninterested in someone coming in and handing them an investigation they should've been able to crack themselves decades ago. All I asked for in return was a subpoena or immunity. They said no. And in an ironic, almost hilarious twist, they charged me with conspiracy to commit bank fraud for not giving up one of my clients. Meanwhile, all of my bosses got non-prosecution agreements.
I was sentenced to 40 months in prison in August 2009. The judge basically confirmed that the fraud never would've been exposed without me coming forward. The prosecutors even said so. And yet, they kept saying I was hiding a name. Yeah, I was hiding all right. Hiding by walking right up to the DOJ headquarters in Washington, D.C. and dropping this massive, calculated tax evasion scheme in their laps.
I fired my first attorneys after I went to prison. I did my homework and found Stephen Kohn. He brought in Dean Zerbe. These guys are a dynamic duo. They understand the law. They've testified in front of Congress. They've written books. They've fought cases in the tax court. In 2006, Zerbe even wrote the law that allowed for whistleblowers to receive 15 to 30 percent of the recovered tax money they helped the government uncover.
For me, that was $104 million. That's right, the same government that threw me in prison owed me more than $100 million before taxes. (It is, however, extremely important to me that I make clear that I had no idea there was a law like this for a reward when I resigned from UBS in October 2005, as the law passed in December 2006.)
I was still on house arrest when I found out I got the reward. Kohn called me and said I had to sign this check. Since I couldn't travel out of New Hampshire at the time, I asked my attorneys to hold a press conference at the National Press Club in Washington D.C. and announce the reward. I like to think that pissed a lot of people off. I know for sure it pissed off Kathryn Keneally, an assistant attorney general in the Justice Department's tax division at the time. She told The New York Times she was so mad about the reward she threw her BlackBerry across the room when she heard about it. Why, I'm not sure. I was never the enemy or the bad guy they tried to make me out to be.
I've tried to invest the reward money wisely. I collect Formula One memorabilia and antique hockey gear and sweaters from the NHL's Original Six teams. I've also invested in artwork and real estate as well as written a book about my experience called Lucifer's Banker.
I'm hoping to help other potential whistleblowers, too. I'll gladly talk to anyone who finds themselves in a similar predicament to mine, whether they are individuals or they represent governments. It can be anonymous. But most importantly, I want there to be more protections and assurances for people who are putting their careers, families and lives on the line by telling the truth. Unfortunately, they're necessary in this fucked-up world. Because I can tell you firsthand, widespread change only happens when someone from the inside speaks up and tells the truth.
 As told to Ben Feldheim
melmagazine.com
melmagazine.com
melmagazine.com
There's no playbook for how to be a guy
506 
11
",162
https://medium.datadriveninvestor.com/robinhood-lends-your-shares-to-short-sellers-and-keeps-all-the-proceeds-78353ca33fb9?source=tag_archive---------9-----------------------,"Robinhood Lends ""Your"" Shares to Short Sellers (and Keeps All the Proceeds)",How zero-commission trading platforms earn money behind the curtains,Asger Bruhn,5,"Robinhood is currently subject to a lot of criticism.
The reason is the restrictions on trading in a number of stocks pushed by the sub-Reddit (an online discussion board) WallStreetBets.
The affected stocks most notably include video-game retailer GameStop ($GME), movie-theather chain AMC Entertainment ($AMC), Finnish telecommunications and IT infrastructure giant Nokia ($NOK), and software company and former mobile phone producer BlackBerry ($BB).
Share prices of the affected companies has propelled upwards due to masses of retail investors buying large amount of stocks and call options with the aim of triggering gamma and short squeezes.
Robinhood is not the only broker to shut down activity in the so-called meme-stocks. Other zero-commission brokers such as TD Ameritrade, eToro, and Charles Swabb.
Larger, classical brokers, who serve hedge funds and professional money managers also imposed restrictions, pointing to systemic reasons for the temporary lockdowns.
Despite that, Robinhood has been the center of attention. Probably due to them being the broker of choice for a large share of the members of the WallStreetBets (WSB) community.
The increased scrutiny on Robinhoods business model has made many retailinvestors aware of several downsides to the commission-free brokerage services.
What may infuriate WSB users, is the fact that their shares are made available to lent for the very short-selling hedge funds they are battling it out against.
And Robinhood earns plenty of money doing that. Just as they profit from sending their order flow to be cleared through a select few market makers such as Citadel.
The below outlines how share-lending works, and how it's possible that Robinhood (and other similar brokers) investors aren't paid anything when shares are lent to short-sellers.
When investors sign up at Robinhood, they are guided toward what's referred to as a margin account.
Such an account implies the option of trading on margin, i.e. buying shares on credit. Hedge funds use this account type aswell, so there's nothing unnatural about it (though one could argue retailers should be protected from taking a lot of risk they aren't able to manage).
When buying shares on a margin account, the investor doesn't own any shares. Instead, what they own is a commitment from the broker.
It can be thought of as a line of credit for which the interest rate is tied to movements in the price of the chosen security. The broker further provides the investor with a lot of the ownership rights of the chosen security (dividend payments, corporate action votes, etc.).
But ultimately, the broker  Robinhood in this case  owns the shares.
Investors using Robinhood and similar platforms thus don't technically own the shares they invest in.
When an investor wants to take a short position in a company, he or she will sell shares not owned. Doing so, the investor needs to borrow a share from a current owner.
Selling shares without borrowing  referred to as naked short-selling  is prohibited.
The investor wanting to short lets his or hers broker find a share available for rent. The price of renting depends on the availability of shares.
The same share can't be lent several times, so the amount of available shares fluctuate. That is especially true for equities with small floats (number of shares trading at the marketplace).
When short interest is high, the price of renting a share to short is also going to be high. In short, a question of demand and supply.
To illustrate, see the below graph of GameStop. The blue bars show the number of available shares (left axis), while the red curve shows the fee (right axis).
It's quite clear both the number of shares available to rent and the price of doing so fluctuates a lot over time.
In periods with extreme volatility and large trading volume, such as the last couple of weeks, these numbers fluctuate a lot throughout a trading session.
As an example, the fee associated with renting GameStop shares were 83.6 percent at the open on January 26, but fell to 53 percent in the afternoon. The number of available shares fluctuated from above a million to around 350.000.
To compare, Apple  which has a large market capitalization and high liquidity  continuously has above 10 million shares available for rent, with the fee at a mere 0.3 percent.
When the WSB community hoards GameStop shares on margin accounts, Robinhood holds a large amount of shares at its account with the Depository Trust & Clearing Corporation (DTCC, the organisation responsible for clearing equity markets).
Investors using Robinhood have agreed to let Robinhood lend the shares.
With high demand for shares to lent and a low float of shares, Robinhood make premium bucks renting the shares ""bought"" by the WSB community to those hedge funds and other agents wanting to short GameStop.
Further, Robinhood clients have accepted that Robinhood keeps all the proceeds from lending shares. Many other brokers (mainly the prime brokers) share the proceeds from lending with the client.
There's a saying in business: ""When you aren't paying, you're the product."" For Robinhood, this is indeed the case.
Their clients (and their demand for securities) are the product, and market makers buying the order flow and short-sellers borrowing shares to short-sell are the customers.
A share is only allowed to be lent once. But when a hedge fund borrows a share and short-sells it, the buyer of the share can let another short-seller borrow it.
This way, the short float  the percentage of the share float shorted  can rise above 100 percent.
In a way, one can think of this as Robinhood borrowing money from retail investors to buy and hold GameStop shares (in return for interest equivalent to the change in the share price). Robinhood then lends the shares to short-sellers (in return for interest), who sell it to the retail investors. Rinse and repeat.
This question is tricky to answer. The mechanics of the financial markets are highly complex.
There is one way the WSB community could probably lower the short float: Buying shares of GameStop with cash accounts.
Cash accounts don't have margin. Shares held in cash accounts can't be lend to short-sellers.
But be aware of unintended consequences. What happens for example, if most of the share float is held in cash accounts, increasing the incentive (i.e. the fees earned) for others to buy shares and lend them?
Whether a swift and powerful short squeeze will be successful is yet another question. A part of the short float is not squeezable, since it's probably due to institutionals delta hedging ITM (in-the-money) call options. However, that is a discussion for another day.
medium.com
medium.com
empowerment through data, knowledge, and expertise.
167 
3
167 claps
167 
",163
https://medium.com/@jfindallas/the-secret-to-making-2000-in-stocks-overnight-the-anavex-story-342be9c7e5e6?source=tag_archive---------7-----------------------,"The Secret to Making 2000% in Stocks Overnight, the Anavex story.","The rise, fall and rebirth of Anavex Life Sciences, a ghost from the past preying on people's hope.",Jean Fonteneau,19,"During the week of November 2nd, 2015, a promotional ""newsletter"", run by a site claiming to sell stock ""research"" and investment advice named Agora Financial, was sent around urging investors to jump on a little known stock, trading for about $9, that was about to, with almost complete certainty, rocket overnight to $200, or a gain of 2150%, and even further hinting that the stock could be worth as much as $900 or a 10000% gain. In true infomercial fashion the promotion encouraged the audience to act now to not miss the chance to ""be in for the wealth-ride of a lifetime.""
Enters Anavex Life Sciences, a tiny company with less than 10 employees (until recently had less than 5 employees) based in New York City, trading under the symbol AVXL, and the subject of this promotion.
According to the Agora Financial author the value of AVXL was about to rise overnight to $6.5 billion, if not $30 billion(!), a staggering amount for such a small entity that has never generated any revenues, has virtually no assets, and was trading for $0.65 per share(worth about $20 million in total) in early 2015.
The only problem with this very appealing proposition is that the REAL SECRET to making 2150% in stocks overnight is... THAT YOU DON'T, it doesn't happen, and people that claim otherwise are only trying to depart you from your money.
In recent years the Biotechnology sector has enjoyed incredible growth and stock prices appreciation, on the back of new technologies/breakthrough in science, investors increasing interest and M&A activity. (For a more detailed view see a previous article: The Golden Age of Biotechnology). This has allowed very early biotechnology ventures to get listed on the stock market and attract very large amounts of capital, one that has attracted a lot of attention and raised a lot of eyebrows: Axovant(Symbol:AXON)
One of the side effects of stock market exuberance and massive amounts of funds flowing to one sector, like we have seen for biotechnology recently, is that it creates very fertile grounds for some operators to deploy stock promotion operations, exploiting investors enthusiasm and eagerness to participate in the next big run, to separate them from their savings.
Gold mining ventures(especially hailing from Canada...) are often used for ""pump and dump"" schemes(a good definition: here). Operators claim to own rights to vast land areas under which they claim lie untold gold riches that one day will make investors fabulously rich, claim that is almost impossible to verify and offer a great excuse to raise money all the time(mining is expensive and you have no cash flow until you strike gold). In most cases the claims are complete fabrications.
Similarly with biotechnology, the claims of untold riches, rather than being buried underground, are buried in the test tubes, very difficult to verify and an outcome in the distant future. Like mining it offers plenty of justifications to raise money all the time(drug R&D and trials are very expensive and you have no cash flow until you strike gold, FDA approval, so having no revenues is not out of the ordinary), the perfect setup for pump and dump schemes taking advantage investors hopes and dreams of striking it big.
The case of Anavex Life Sciences; AVXL made its debut as a publicly traded company in 2006 via a reverse merger, trading on the loosely regulated OTC market where the reporting requirements are very limited, and claiming to develop a number of molecules and compounds potentially holding the key to the treatment of numerous diseases and conditions(cancer, alzheimer, diabetes, you name it...).
This debut was accompanied by a fanfare of promotional press releases, for instance in early 2007 Anavex was announcing in a PR ""its strategic vision for growth by discovering and developing cutting-edge drugs against neurological diseases"" and shortly after, in another PR, ""its strategic vision for growth by discovering and developing cutting-edge drugs against solid tumors""... All the while the stock was heavily promoted.
The stock was the subject of another round of heavy promotion in 2009-2010, in this 2009 presentation Anavex claimed once again to have a diversified pipeline of drugs under development and to potentially generate $6 billion of yearly revenues by 2020(yes billions...)
None of this ever came to pass but in the meantime, as a result of the constant PR and promotional activities the stock enjoyed 2 episodes of steep price appreciation that allowed for various financing schemes to be arranged and stocks to be sold during the run ups, they were inevitably followed by steep stock price declines.
A number of the people involved with Anavex Life Sciences at the time (2006-2011), namely the CEO, CFO, COO, have now been linked to numerous penny stock operations that were used as the vehicle of ""pump and dump"" schemes listed on various exchanges, mainly in Canada.
Over the years since its debut as a public company, Anavex management seems to have been either, incredibly picky regarding the preferred location from which to conduct its operation or trying to emulate some sort of around the world ""Catch me if you can"" story. From British Columbia to now New York, via Switzerland and Greece, so far no less than 7 different headquarter locations in less than 8 years for Anavex Life Sciences...
One thing that has never changed over the years for Anavex, since 2007, and over the different episodes of promotional activities, is the presence of the same investor relations/PR firm based in Canada, Primoris Group(firm whose direct phone number in Toronto appears on every Anavex PR since 2007 to today). Anavex appears to be an important but odd client for Primoris Group, its main client base seems to be mostly OTC gold/mineral mining ventures from Canada, most of which seem to undergo episodes of sharp price spikes before drifting back down to near $0(refer back to comments above regarding OTC Canadian mining ventures...) While it is claimed that Anavex Life Sciences is now a very different company with a different management this link to Canada and promotional activities endures.
From 2006 until 2013, while it is unclear if and when any real drug development seemed to have taken place, Anavex always kept with its habit of regularly releasing optimistic PR updates, but the stock kept drifting lower and lower until the Spring of 2013, when a new episode of paid promotional activity seemed to have taken place as exhibited by this notice from the British Columbia Securities Commission circulated after it had ordered the suspension of trading in Anavex shares following unusual activity and reports of paid promotions. (All instances of promotional activity surrounding penny stocks schemes are always reportedly financed by ""third parties"" made to appear as having no connection to the company or the company insiders.)
Coincidentally(?) and shortly thereafter, in July 2013 a new financing arrangement, taking advantage of the recent run-up in the stock price created by the paid promotional activity, was announced, to the tune of $10 million with an entity called Lincoln Park Capital(more on that later) and a new CEO was appointed. It seemed like a new era was starting for the company, and the frequency of press releases and promotional activity picked up again, with now a more narrow focus on Alzheimer's disease. At the time the company seemed to have only one full time employee the new CEO.
At the end of 2014 another ""financing"" with Lincoln Park Capital was announced(in that announcement Lincoln Park was described as a ""long time investor"" in Anavex), and shortly after, the news, that an actual trial for the uses of one of the Anavex compound(Anavex 2-73) in the treatment of Alzheimer's disease was going to take place, was released.
This was the start of a steep year long run-up in AVXL stock price, the stock climbing from about $0.65 in late 2014 to more than $14 during the week of November 2nd, 2015. In late October 2015, AVXL even managed to get itself uplisted from the OTC exchange to the Nasdaq giving more credibility to the story and participating in boosting the stock price.
Taking advantage of this incredible price appreciation, 2 days before being uplisted to Nasdaq Anavex inked a new ""financing"" with Lincoln Park Capital this time to the tune of $50 million, this deal was filed with the SEC on October 26, but curiously it was not the object of the usual promotional press release.
When Anavex made a big ""financing"" deal with Lincoln Park Capital in July 2013, it was in the wake of, as flagged by the BCSC, heavy paid for promotional activity. There is very little doubt in my mind that the recent run-up in price has been helped by heavy promotion activity as well, Agora Financial being probably just one among others.
On this data gathered via HedgeChatter(HedgeChatter is a service using data mining algorithms to detect, across all social media channels, unusual activity or manipulation on stocks) we can see that, on Sunday November 8, this tiny obscure company was being talked about on social media and message boards more than any other by a very wide margin with most messages(71%) being flagged as spam.
This small company stock, unknown to probably 99% of the investing public, rising way past the most popular(AAPL) or most debated(VRX, MNKD) stocks on financial media by a very wide margin can only be the result of active promotion and spam.
At $14, based on 33,789,938 outstanding shares of common stock (according to the lastest SEC filing), Anavex Life Sciences was valued at approximately $470 million dollars, once again a staggering amount for a tiny company with less than 10 employees, that has NEVER generated any kind of revenues, $0, or inked any partnership generating milestone payments(a frequent way development stage biotechnology companies can show revenue and are financed) throughout its entire existence, and has virtually no assets recorded on its balance sheet other than the cash regularly delivered by Lincoln Park Capital in its bank account .
Avanex Life Sciences seems also to have had some very peculiar recruitment practices, of the 4 employees(other than the CEO) that appears to be operating in NYC, and that can be identified via LinkedIn, one apparently was recruited as a ""Financial Analyst""(title now changed to ""Business Consultant"" since this publication)in May 2015 after being a buyer for an automotive and industrial parts wholesaler for 9 years(one may wonder with Anavex being so small, having no revenues and having extremely simple financial statements what ""financial analysis"" needs to be performed...).
Another employee recruited sometime in 2014 as ""Director of Business Development and Investor Relations"", other than an ongoing career as a runway model and while having graduated with a neuroscience degree and likely very bright and talented, seems to have no prior professional business experience or experience of the biotechnology industry, which would seems like an odd recruit for a biotechnology company eager to line up partnerships and business deals on the eve of a big drug discovery milestone.
Only the other 2 employees, one recruited very recently in September 2015 and the other in July 2015, show some solid resume with biotechnology industry background.
Let's now examine how the Lincoln Park Capital(LPC) financing arrangements are setup and my interpretation of the motives that lie behind them. On the surface when announced the deals almost sound like LPC is cutting a check for $10,000,000(size of financing commitment announced in July 2013) to Anavex Life Sciences while in reality it fronts no($0 in the most recent $50M agreement) or very little($100,000 in the July 2013 agreement) cash to the company, it only agrees to purchase new shares issued by the company at a discount to the market price that day.
In the deal inked on October 26, 2015, for $50,000,000, Anavex agrees to sell to LPC newly issued Anavex shares at the lowest price quoted that day or at the average of the 3 lowest close of last 10 trading days.
In this agreement LPC makes no commitment of holding these shares for more than 1 day, 1 hour or 1 minute.
Knowing in advance that you will be able to buy a stock at the lowest price of the day or at a certain discounted price in the future gives you incredible risk-free arbitraging opportunities, especially when one contemplates the following subtle detail of the transaction:
In the Purchase Agreement LPC expressly agrees to engage in ""no short selling or hedging"" of AVXL common stock, but in my opinion if one look closely, one detail of the transaction purposefully allows LPC to do exacly that.
With the execution of the ""Purchase Agreement"" Anavex agreed to issue LPC 179,958 newly issued shares for free. LPC can now use these free shares in their account as a ""float"" to sell shares in the market in advance of the shares they will purchase later at a discounted price(the lowest quoted price of the day), effectively engaging in a transaction akin to shorting the shares they will cover via purchase later, while on paper it can be claimed that LPC is transacting from a long position.
One can imagine a scenario like this, LPC starts the day with the full balance of the 179,958 free shares, during that day they sell in the open market at various price points 50,000 shares, end with a balance of 129,958 shares; at the end of the day they go to Anavex that issue them 50,000 new shares at the lowest quoted price of the day and the balance is replenished to 179,958 shares, LPC can now repeat the same transaction the next day, effectively shorting the stock with the appearance of being a long seller.
Using the same scenario example, that same day AVXL stock fluctuates between $10 and $11 and LPC manages to sell the 50,000 shares in the open market at an average price of $10.50, at the end of the day they purchase 50,000 shares directly from Anavex at $10.00 the lowest price of the day, pocketing $0.50 per share, a $25,000 completely risk-free profit and delivering $500,000 of fresh cash extracted from the marketplace to Anavex bank account, basically, in this example, a 5% commission to transfer funds from retail investors to Anavex.
In my opinion, while often characterized as a ""long time investor"" by the company in its PR announcements, Lincoln Park Capital is not an ""investor"" but acting as an intermediary between Anavex Life Sciences and retail investors, allowing for the sale to the marketplace of newly issued shares and the pass through transfer of cash directly from retail investors in the marketplace to Anavex Life Sciences bank account, for this service LPC is remunerated via the discount offered on the share purchases and the issuance of free shares.
If you look it up, you will be able to find out that Lincoln Park Capital has been involved in dozens of such ""financings"" with numerous small ventures listed on various exchanges, number of them also coming with numerous red flags, among them a number of ""Canadian gold mining"" operations(one of the most frequently used narrative to run ""pump and dump"" schemes). (This article from TheStreet use another biotechnology example to shed more light on LPC financing deals)
In most cases the stock price of the ventures involved in deals with LPC declines sharply in the following months likely due to the added selling pressure of Lincoln Park Capital constantly selling newly issued shares and the significant resulting dilution.
(Illustrating the dilution, change in outstanding AVXL shares as a result of various financing arrangements since 2013, number of outstanding shares:September 30 2013: 37,237,588September 30 2014: 47,200,237September 15 2015: 124,503,297 a very significant dilution, one 2015 share representing less than 1/3 of the ownership in the company it did in 2013.)
Offering shares directly to the retail public demands more registration requirements which implies more due diligence and audit and the involvement of underwriters, especially for a Nasdaq listed company, all tests that, in my opinion, Anavex Life Sciences would likely never be able to pass given all the red flags. (Again it is interesting to note that this latest ""financing"" was inked for a very large amount $50M and a long duration 36 months, only 2 days before AVXL was uplisted to the Nasdaq, when the company still qualified as an OTC listed company where the reporting and registration requirements are much lighter).
It appears to me, that throughout its entire existence, and at an accelerated pace in the last 2 years, Anavex Life Sciences has constantly been issuing and selling new shares using similar arrangements(Purchase Agreement, Dilutive Convertible debt issuance) with the coincidental support of heavy promotional activities deemed to have been paid by ""third parties"".
To date $73,000,000 have transited through Anavex Life Sciences, of that amount about $20,000,000 seem to have been extracted from the investing public via various Lincoln park Capital ""financings"" and other arrangements in just the last 1 year, and yet, as we have seen, the company has virtually no tangible assets.
This staggering amount of money, for such a small entity, raised throughout its existence, appears to have been extracted from the venture via various means mostly via lavish salaries and perks. One interesting fact to note is that in a filing from 2013(the last year this expense was broken down F-6) it showed that Anavex had spent more than $12,000,000 in consulting fees to date, an amount that seems in my opinion incredibly high for such a small operation.
In this current incarnation Anavex Life Sciences appears to be very generous with its management, namely the CEO, according to the company filing here, Anavex Lifesciences CEO was awarded compensation worth $2.66 million for the year 2013 and $650,000 for the year 2014, a level of compensation on par with what CEOs in the sector are paid for jobs at companies employing tens if not hundreds of employees. In any case this sounds extremely high for such a small venture or a biotech startup. As far as one can tell it appears that in 2013 and for at least part of 2014 the CEO was the only full time employee of Anavex Life Sciences, so it seems that he was extremely well compensated for the job of managing himself.
Equally at more than $2,000,000+ for the year 2014, the level of ""General & Administrative Expenses"" seems also incredibly high for a venture with even 5 employees, and even taking into account the fact that Anavex rents offices at a very expensive address in the famous CBS building(some ventures often like to setup shop in the most prestigious addresses and have a lot of fancy titles in the hope it will help compensate for the lack of substance). Given the fact that the company seemed to have only 1 full time employee for most of 2014, the CEO job must also come with some very enviable perks.
(On Saturday, November 7, 2015, Anavex Life Sciences presented the trial data that was advertised has revolutionary and groundbreaking in the promotion referenced at the beginning of this article, and was portrayed as the starting point of miraculous gains, I will refrain from commenting on the data at length since a number of people have already given opinions and from what I can gather it appears to have shown not statistically significant results and a flawed trial(a very small ""n"" called Phase2 trial(while designed like a Phase1) that was unblinded and had no placebo control arm.))
In my opinion the trial was successful in its main purpose: creating a run-up in the stock price and give some material for coordinated promotion operations.
To conclude, Anavex Life Sciences(AVXL), appears to be somewhat of a phenomenon in the world of stock promotion schemes for its longevity, sophistication and scale.
In an era where unprecedented amounts of funds are, and have been flowing into the biotechnology sector over the past 5 years, chasing any investment opportunity/asset with the slightest chance of success, it seems very unlikely that any asset holding a possible chance at a major breakthrough in the treatment of Alzheimer's disease(an obviously very large market) would be left drifting and dormant for someone to come around and pick it up for pennies.
In 2015, the biotechnology company Biogen, with a $2 billion/year R&D budget, has signaled that it was setting aside $2.5 billion to further its multi year effort in finding a treatment for Alzheimer's disease.
To think that anybody would believe and buy into the story that a tiny company that was valued at less than $25 million in early 2015 with less than 10 employees, no tangible assets, a checkered track record, a history of stock promotion activities, and spending more money on CEO compensation and office/G&A expenses than anything else, including R&D expenses, was about to deliver a major breakthrough in the treatment of Alzheimer's disease, is incredible.
In my opinion, Anavex Life Sciences(AVXL) is either:
.
For additional information on the Anavex Life Sciences connection with Canada read this blog post: Sandra Boenisch, Vancouver's finest, CFO to the stars  AVXL, NAKD, LINCOLN PARK via @BuyerStrike
In the HedgeChatter report here, a case study detailing how social media activities/spam/promotion were detected and correlated with price movement. Ironically the case used to illustrate the study, Galena Biopharma, after all this spam/promotional activity inked a $55 million financing deal with... none other than Lincoln Park Capital...
All the information used in this report was gathered from filings filed by the company with the Securities and Exchange Commission, from material distributed by the company and from publicly available sources. To consult all up to date Anavex Life Sciences SEC Filings: Here
Always perform your own due diligence before investing in any securities and always refer to the latest official filings of the company with the SEC.
To report securities fraud, stock manipulation or suspicious activity contact theSecurities and Exchange Commission here
Disclaimer: The author currently holds no long or short position in the common stock of Anavex Life Sciences or any related securities or derivatives and does not plan to do so in the future.
.
.
Generally, stock promoters like to latch on the latest sector to have seen a big run up in prices, I can remember in 2000 seeing a few of these fictitious companies claiming to hold the future to optical networking, 3 years ago a number appeared advertising their incredible potential in the rare earth mineral space, and in 2013-14 a few were advertising their potential in 3D Printing on numerous finance related websites. (Often, after the pump & dump has occurred, and the stock is back down to a few pennies, the company is re-branded with a new fictitious narrative and the cycle starts anew, PUGE, featured in the add below as a 3D printing tech company, has now according to PR ""Entered the $150 Billion Online Travel Industry With a Proprietary Algorithm""...)
This past Summer 2015 I came across one completely obvious and blatant pump and dump scheme named Rainbow Biosciences (symbol: RBCC), a 1 employee aquarium supplies company with no assets incorporated in Nevada, based in Florida, (Canada, Florida, Nevada is often a common thread in these schemes) pretending to have morphed into a groundbreaking biotechnology company.
RBCC was advertising investing in its stock via display adds on various finance websites. (Any company advertising its stock in this manner or on which paid promotion are being circulated should be an immediate red flags and always avoided).
RBCC, like most of these schemes, was incessantly distributing promotional press releases updating investors on the ""progress"" of the company, most of it nonsensical and likely completely fabricated. The only purpose of this type of constant promotion and of paid promotional activities is to create a run up in the stock price into which people connected with the operation and holding large blocks of stocks acquired for pennies if not issued to them for free will sell(dump) their holdings.
By the look of the chart the operation seems to have been a great success:
Generally, after the run-up has ended, the dump taken place and the stock inevitably collapsed, the promotional activities will stop and the frequency of PR from the company will decrease if not completely stop. Often a number of months or years later the stock will be used again to reactivate a scheme with a new name and a new story, generally by the same network of people involved the first time around. (Excellent article on RBCC and a network of other stock promotion schemes: here )
(during the week of November 2nd, 2015, I started posting on Twitter the troubling facts concerning Anavex Life Sciences that I came across, I will paste them below as a complement to this report, and will keep updating and adding to the list)
Observations, comments, disagreements or questions reach me on Twitter:Jean Fonteneau
",164
https://themakingofamillionaire.com/4-unusual-side-hustles-i-do-to-earn-an-extra-1-500-a-month-796aa4b302ac?source=tag_archive---------4-----------------------,"4 Unusual Side Hustles I Do to Earn an Extra $1,500 a Month","No cliches like dropshipping or blogging, I promise",U-Ming Lee,6,"Freelancing sometimes feels like cycling between feast and famine. Although I have enough clients to bill regularly to keep the lights on, I still appreciate having side hustles around as an additional income source for savings and investments.
I believe in a simple mantra to build wealth.
Earn more than you spend, invest the savings.
I'm a full-time writer. I've been extremely fortunate in the past few years to have enough work to pay the bills. So, I keep my spending low and limited to my primary source of income.
My side hustle income goes entirely to savings. I invest all of it.
I feel like I can take on bigger risks with my side hustle income than my main income. It's a psychological thing stemming from the fact that I don't rely on the side hustle income to pay for my daily expenses.
Suppose these riskier investments work out in my favour. In that case, the higher compounding rate will boost my wealth more than safer, but lower-yielding investments will. However, if they don't work out, my loss is limited to the side hustle income I've accumulated. My basic needs and quality of life are not jeopardised.
If I had a dollar for every side hustle article I've read that recommends Rover, Fiverr, Freelancer.com, Uber, or dropshipping, I wouldn't need a side hustle.
I've never even looked in that direction because I feel these fields are overcrowded. There are few barriers to entry to these side hustles. Without barriers to entry, new entrants can flood the market and drive prices down.
I don't want to be caught in a race-to-the-bottom in my side hustle.
It's like being on a high-speed treadmill. You have to work up a sweat just to stay in place.
Instead, I've always looked for the side hustles that fly under the radar. These side hustles are typically corporate-type work commissioned by corporate-type clients who don't want to be caught dead commissioning work on portals like Freelancer or Fiverr.
Although these side hustles are harder to find, they fit in two desirable boxes for me when looking for side hustles: they are profitable, and they don't require a significant time commitment.
If you love Googling people to find out more about them, this side hustle's for you.
Corporate clients need human intelligence all the time. Maybe they intend to hire a CxO, and they need someone to verify their credentials, check for criminal records, regulatory sanctions, offshore directorships that might indicate a conflict of interest, or bankruptcy proceedings.
You'll find yourself searching regulatory databases for any hits on the subject. You'd also be looking through the press for any references to their name, looking for any possible negative press like hints they might have been involved in fraudulent practices or other forms of corporate malfeasance.
For more detailed due diligence work, you might get to interview previous managers, CEOs, customers, or other employees at the subject's previous firm to get a better handle on their character.
Occasionally, some clients might be looking for even more detailed stuff, but unless you are a private investigator or an ex-cop, that kind of work will never come to you. Your job is more ""check-the-boxes"" type work, not to be the next Magnum P.I.
Pay: $80-120 per hourFrequency: Once a monthTime: 3-10 hours depending on the level of detail
This side hustle is for people with a good background in business or finance.
Corporate clients typically want to research their commercial opportunities before they make a big decision. You will be their extra set of eyes to assess the opportunity objectively for your client.
I love this side hustle because I am fascinated with business and finance. This side hustle is a way to monetise my learning.
One time, I helped an American fruit farming association assess the market potential in Malaysia for their product.
Another time, I helped a wealthy private investor determine the viability of setting up a 200-bed aged care home in Kuala Lumpur.
I completed a fun project for a PC gaming publisher who wanted to understand the landscape of the Malaysian e-sports industry. I even got to interview and hang out with a few e-sports teams.
To get involved in this side hustle, you'd need to have the basic consulting skills, including making PowerPoint pitch decks, building Excel financial models, and interpreting financial statements and economic data releases. These are somewhat specific requirements, which helps keep the field limited and the pay relatively high.
The work opportunities in this side hustle don't come too often. But when they do, the project can be quite lucrative due to the short and intense deadlines.
Pay: $100-200 per hourFrequency: Once a yearTime: 20-50 hours
In this side hustle, you'll be extracting relevant data from a combination of commercial and free databases for your client.
Corporate customers want data for their analyses but sometimes cannot justify the subscription costs. Since I am a full-time business writer and my side hustles involve corporate clients, I have subscribed to some proprietary databases. I extract the data they need, adjust it to make it relevant for their needs, and send them a report for a fee.
This isn't an attractive side hustle for most people because you will need database access, which can be expensive. This side hustle might work for business school students or anyone with access to a well-stocked finance library. However, you will need to be careful not to fall foul of the licensing arrangements.
Pay: $40-60 per hourFrequency: Once every 2 monthsTime: 3-5 hours
In this side hustle, you'll be designing consumer surveys and analysing the results for your client.
Some corporate clients don't have the in-house expertise to conduct statistically-robust consumer surveys. So, I've designed some consumer surveys for them, surveyed a panel of urban Malaysian consumers, and analysed the results.
This is a fun side hustle for data scientists, market researchers, or anyone with a quantitative bent. You'll need to know your way around statistical packages like SPSS or R. You'd also need to have decent consulting skills like building PowerPoint presentation decks.
A significant challenge with this side hustle is that it is practically impossible to distribute the survey to a verified panel of consumers. So, you'll often have to come in as a subcontractor on a larger consulting project where your design and analysis skills ""plug in"" to their workflow.
Pay: $100-200 per hourFrequency: Once every yearTime: 20-30 hours
I had some good fortune because some of my earliest clients found me on LinkedIn and contacted me out of the blue. Since then, many of my side hustle projects have come from recurring business.
If you're looking to start with these side hustles, good sources of potential clients are any local lawyers, accountants, or bankers with whom you have a relationship. These professionals serve a broad range of commercial clients, some of whom may be looking for the services you provide.
You'll need to give them a commission for handing you the contact, but it beats cold calls or e-mails.
If you have some specialist skills, make sites like Fiverr or Freelancer.com your last resort, not your first port of call. Churning out work at rock-bottom rates is mind-numbing and will erode your self-esteem over the long run. Instead, try looking out for niches that fit your skillsets by serving institutional clients. Side hustles in these areas are more profitable, fun, and feel more meaningful.
This is the first of a two-part story about my freelance income. Part two:
medium.com
Subscribe to my free newsletter where I share my insights on business, finance, freelancing, Southeast Asia and more. Articles out every Friday.
This article is for informational purposes only. It should not be considered financial or legal advice. Not all information will be accurate. Consult a financial professional before making any major financial decisions.
",165
https://medium.com/hackernoon/meet-spoofy-how-a-single-entity-dominates-the-price-of-bitcoin-39c711d28eb4?source=tag_archive---------4-----------------------,Meet 'Spoofy'. How a Single entity dominates the price of Bitcoin.,"This story is about a trader, or a group of traders, or possibly even Bitfinex themselves manipulates the price of Bitcoin. The past few...",Bitfinex'ed,13,"This story is about a trader, or a group of traders, or possibly even Bitfinex themselves manipulates the price of Bitcoin. The past few months I've slowly collected screenshots of a trader I like to call 'Spoofy'. You'll see evidence of spoofing, wash trading, a sketchy scheme associated closely with Bitfinex known as 'Tether' among other shenanigans.
Spoofy makes the price go up when he wants it to go up, and Spoofy makes the price go down when he wants it to go down, and he's got the coin... both USD, and Bitcoin of course to pull it off, and with impunity on Bitfinex.
Phil Potter referred to me in an interview on August 6th, and claimed they have not seen the kind of activity I'm talking about.
This is a video showing activity on July 22nd, 2017 and contains Phil Potters comments along activity that he claims isn't happening.
You be the judge.
Since March 2017, I've been warning people about the issues regarding Bitfinex and their disconnection from the traditional banking system.
The issue regarding Bitfinex's banking woes is essentially on the backburner as prices have gone up significantly since then, but in my opinion, this is a very big driver of why the price of Bitcoin is exceedingly easy to manipulate.
Spoofy is a regular trader (or a group of traders), that function primarily on Bitfinex, and in a limited fashion on some other exchanges who engages in the following practices:
What is 'Spoofing', you may ask?
Spoofing is placing orders which you have no intent on allowing to execute.
The goal of spoofing is to send false signals to other traders that they will act upon. Placing a large bid may indicate bullishness, causing traders to close short positions and possibly even buy Bitcoins.
You can profit from this by placing asks for your own bitcoin, then send false signals for bullishness, and people close short positions by buying your Bitcoins you have on ask orders.
You can also do the opposite. Placing large asks indicating bearishness, causing people to close long positions, perhaps also into your buy orders.
Spoofing by the way, is illegal. It was made illegal in the United States back in 2010 under Dodd-Frank. Bitcoin exchanges are largely still unregulated and do not police for spoofing, wash trading, and other shenanigans.
en.wikipedia.org
Spoofers bid or offer with intent to cancel before the orders are filled. The flurry of activity around the buy or sell orders is intended to attract other high-frequency traders (HFT) to induce a particular market reaction such as manipulating the market price of a security.
Spoofy is able to manipulate the prices so easily because he simply outwhales everyone else on the exchange. Spoofy is a single entity with between $20 million and $60 million USD on Bitfinex, but it could be more.
In order for someone to combat Spoofy (by dumping into his orders before he can remove them all), they'd have to put a lot of money on the exchange.
As a result, Spoofy can operate pretty much with impunity on Bitfinex.
Spoofy shows himself during three scenarios.
Examples:
On July 23rd, there was some large selling, at which Spoofy then single handedly placed over $14 million in false buy orders.
These bids were unusual for Spoofy, because this time he left them up for a little bit. It supported the price and he didn't really have to buy very many Bitcoin with his bids.
However, about an hour later, in most unusual circumstances, somebody did dump some into his order, at which he pulled the rest of the order almost immediately.
From the looks of it, I believed a trader with a lot of longs sold into him, because overall long positions dropped immediately after this.
However, there's a little issue. How do we know this trading is legitimate? How do we know that Spoofy, didn't close into his own order?
How do we know Spoofy himself wasn't selling into his own orders as well triggering a false crash?
We do have evidence of wash trading, and if spoofy is wash trading the price up, he could wash trade it down as he makes positions on other exchanges.
Remember, just like how someone isn't realistically going to send enough Bitcoin to Bitfinex to combat spoofy's false buy orders, nobody is going to send enough USD to combat his relentless market sells either. They couldn't even if they wanted.
Unfortunately, watching the trades normally make it a little difficult to prove if a trade was a wash trade or not.
But Bitfinex gave us a little gift to help prove this theory.
On July 27th, Bitfinex announced an unusual scheme on how they plan to credit ""Bitcoin Cash"" tokens.
https://www.bitfinex.com/posts/212 (http://archive.is/EOcsO)
"" Please note that if there are more shorts than longs at the fork event, this coefficient will be less than one. This coefficient can be roughly computed from publicly available information from our API (longs & shorts) and the Blockchain (cold storage).""
As a result of this, people could get 'Free' BCH, by wash trading their own shorts, predictably, that's exactly what happened.
A single entity (entity could be a trader, or a group of traders), single handedly wash traded 24,000 Bitcoins in shorts. In order to do this, you would need to have at least 24,000 BTC on Bitfinex and the USD to buy them with.
As a result of me constantly alerting people about this on my Twitter, people seemed to be paying some attention to it. Eventually, the exchange admitted that there was in fact wash trading going on and 'sanctioned' the trader.
In my opinion, it's more like they politely asked the trader to claim his positions before the distribution. Someone with $20 Million to $60 million on Bitfinex likely has something no other trader has. A phone number for Bitfinex.
On top of that, as a result of these false signals. It caused other traders to expect a 'big short squeeze' or perhaps some traders 'expecting a big dump', to possibly take positions based on these fake shorts.
https://www.bitfinex.com/posts/214 (http://archive.is/lNWAY)
""After the methodology announcement on July 27th, several accounts began large-scale manipulation tactics in an attempt to obtain BCH tokens at the expense of exchange longs and lenders on the platform, causing the distribution coefficient to artificially plummet.
We have determined that this kind of manipulation  including wash trading and self-funding shorts  is in violation of Bitfinex's terms of service. Those who intended to take unfair advantage of the circumstances surrounding the BCH distribution at the expense of other users have been sanctioned accordingly.""
The fact that the shorts dropped by 24,000 BTC in a single tick, in my opinion is proof this is done by a single entity.
Spoofy isn't limited to just Bitcoin. Shortly after this 'trader' was 'sanctioned' another interesting thing happened.
Here we can see how the ETCBTC shorts simply vanished, from 60,000 ETC short, to a low of 93 ETC.
But let's not just look at ETCBTC, what about ETCUSD?
I'm not sure what to make of these, but it calls into question the legitimacy of this data. The point I'm trying to make by showing the ETCBTC/ETCUSD margin pairs also engaging in very funny business at the same exact time, how are we supposed to know that the BTCUSD longs on Bitfinex are not also subject to this manipulation?
ETCBTC Shorts = Clear evidence of manipulationETCUSD Longs =Clear evidence of manipulationBTCUSD Shorts = Clear evidence of manipulation (and admitted by Bitfinex)BTCUSD Longs = BTCUSD Longs in terms of USD, has never been higher in Bitfinex's history. See the green line.
And of course, we're not even getting into the other margin pairs... such as ETHBTC which is quite frankly, ridiculous.
Spoofy has been spotted on GDAX... however his orders though on GDAX are much smaller. Usually 400-500 BTC spoof bids or spoof asks.
I have plenty of screenshots on my Twitter
People underestimate how much exchanges follow each other. Manipulation on one exchange will affect prices on other exchanges. You have traders that watch all of the exchanges and if one exchange starts to pull ahead, they too buy on cheaper exchanges.
You don't just have people, but you also have bots that will do the same thing, so price reactions can be immediate.
I believe that since Bitfinex lost their traditional banking, sending USD to buy or selling on Bitfinex to get USD involves a lot of friction, and as a result there are far fewer legitimate traders, leaving us with the wash traders and spoofers.
In my opinion, Bitfinex will never re-establish traditional banking relationships, and their 'clever workarounds' will eventually fail. Their Chief Strategy Officer, Phil Potter, even admitted to playing games with banks in an interview that was eventually pulled off of YouTube, but luckily, I saved it.
https://www.youtube.com/watch?v=62cvxPIDBGY
""We've had banking hiccups in the past, we've just always been able to route around it or deal with it, open up new accounts, or what have you... shift to a new corporate entity, lots of cat and mouse tricks.""-Phil Potter, Chief Strategy Officer of Bitfinex
If you think that's the kind of business banks want to do business with, I have some Bitcoin in the Genesis block I'd like to sell you.
Their next Cat and Mouse Trick?
What are Tethers you may ask?
Tethers are 'tokens' that on their website claim to be backed by US Dollars, and 1 TetherUSD (USDT)= 1 USD.
Bitfinex has claimed in a support ticket that they have an 'Institutional' Investor, who supposedly has a Taiwanese bank account, and therefore, can buy Tethers.
SourceArchive
There's only one teensy tiny itsy little bittle problem with that. No institution in their right mind would ever agree to buy over $250 million USD worth of what are legally car wash tokens... unless it's someone like BTC-E, hence why I think they could be tied up in a mess together, again speculation on my part.
Tether is in the same boat as Bitfinex. They were also cut off from banking by Wells Fargo. The Bitfinex lawsuit against Wells Fargo also had Tether as a plaintiff. (Source)
They have repeatedly said themselves that ""Tether"" isn't part of Bitfinex, but when you dig down a little bit, it turns they are all shareholders in each other.
Tether issuance has continued to skyrocket despite no traditional banking, and despite the fact that tethers, by their own admission (and admission of their own lawyers), are not redeemable for money.
When Tether was cut off from banking, Tethers were around $60M, at the time of this post, they are over $310 million.
At the start of the year, the amount of Tethers in circulation was a mere $6M. A ten fold increase from January to April, and a nearly four fold increase from April to August.
You betcha! (archive)
The lawyer for Tether is Stuart Hoegner, and is general counsel for Bitfinex and Tether. Member of Bars of Ontario (LSUC #42037E) and Nevada (NVbar #12774).
Source (Source #2)Archive (Archive #2)
While they are sure to assure everyone that they in fact have 1 USD for every Tether issued, that is pointless if they never actually have to give you that USD for your Tether.
Let's not forget that the entire concept of Tether, even if they were redeemable and legit, will eventually run afoul of US banking regulations. In my opinion, that is the actual reason why they made them legally no cash value car wash tokens, as an attempt to make them legally permissible. I do not think regulators are this stupid.
Let's think about what happens to Tethers if their bank accounts are seized, or if it turns out that money is co-mingled with BTC-E withdrawals from criminals.
The exchange rate of Bitcoin has been heavily manipulated as a result of spoofing, wash trading, and other shenanigans. This is not healthy long term for the price of bitcoin and the possible future collapse of Tether and Bitfinex will cause a dramatic effect on future Bitcoin prices because we will no longer have these false signals coming out from Bitfinex.
All of the money on Bitfinex is essentially in the same position as Tether. Bitfinex has been without banking for nearly five months. Banking is unlikely coming back to Bitfinex.
I don't believe a bank is going to do business with an exchange as such as Bitfinex, and banks are probably hesitant to do business with a institution who has also sued a large bank in what was obviously a frivolous lawsuit and then withdrew the lawsuit a week later.
A bank that would do business with a business like a Bitcoin exchange is going to do a significant amount of due diligence on the exchange. What do you think they are going to find when they do some homework about Bitfinex?
Finally, in my opinion, I believe that a significant amount of the trading on Bitfinex is likely wash trading, and this could be wash trading in both directions up and down. I believe Spoofy is playing on multiple exchanges, but his orders are usually much smaller on the other exchanges.
Spoofy is either:
In point of fact, two out of the top ten BFX token-holders are in our management team. We assure everyone that we feel the loss acutely, both as a company and as individual customers. Source (Archive)
In my opinion, there's absolutely no way that Bitfinex does not know who the spoofer is, or that he's engaging in illegal spoofing. The exchange can easily prevent, or punish this type of activity.
Nobody needs the ability to place $2M to $15M of buy orders for Bitcoin for five seconds. They could prevent a lot of this activity by simply not allowing orders over a certain size to be removed until a timer lapses.
Even some video games have these kinds of restrictions to prevent manipulation of their fictitious markets.
This is a common strategy in financial markets, and Bitcoin is not immune to this.
http://www.zerohedge.com/news/2017-06-05/exposing-legend-how-traders-spoofed-precious-metals-markets
Now, I'll leave you with this graphic. The very first miraculous recovery happened right when Bitfinex was cut off from Wells Fargo.
Don't forget that Bitfinex failed to announce this problem until April 13th, they even proudly announced their BFX token redemption despite knowing nobody could withdraw USD from their exchange.
The period between March 23rd to April 13th, most traders were in the dark about their banking problems.
Except for management.
Was Spoofy buying?
Trade carefully.
For more information regarding Tethers, be sure to see a more complete story regarding tethers here.
-Bitfinexed
#BlackLivesMatter
5.1K 
41
",166
https://medium.com/@myleik/how-i-started-saving-a-ton-of-money-3ced9baf0db6?source=tag_archive---------4-----------------------,How I Started Saving a TON of Money.,I didn't grow up with best education when it comes to money. I learned how to write a check in the 5th grade and besides that I never...,Myleik Teele,4,"I didn't grow up with best education when it comes to money. I learned how to write a check in the 5th grade and besides that I never learned a single thing until I started college in 1997 and got my first credit card which led to another card and then someone (that shall remain nameless) getting a card in my name (make that two) and then kiss my credit history GOODBYE. From that moment on I became completely frightened with the idea of actually knowing my bank account balances (I'd always wing it). I would never even THINK about looking at my credit and if anyone called to collect money from me I'd say ""wrong number"" or just hang up. I hate to admit this but for the most part I didn't KNOW any better.
I started slowly with money. I bought a book by Suze Orman, ""The Money Book for the Young, Fabulous & Broke."" There are much better books out now I'm sure but at this time this was the only thing I understood. Just search for books on money so that you have basic KNOWLEDGE.
Once I got some general knowledge I figured out how to establish my new $0 balance. I stopped letting my account get down to absolutely nothing and started setting the new (don't spend another dime) zero balance at $200 and then $300, next $500 ... You get it. Once you hit $5oo, you're broke. No going out, no spending past that, you're done. That number can eventually get to $1,000 and more with the right mindset. This keeps you safe. You're never fully out there with your pants down with SOME money in your account.
Once I established that principal I started saving. It was SO HARD for me to save so I set up an auto-savings account. I started with $50/week. I didn't even notice it. Before I knew it I was saving $200/month effortlessly. Without even thinking about about it I had saved $2,400 in a year. With $2,400 you can do a decent vacation, pay the graphic designer for the logo for your business, buy your site on GoDaddy, invest in a key wardrobe piece that makes a professional statement, take some of your favorite business peeps to fancy dinners to pick their brains, go to a conference that will enhance your professionally ... I could go on.
Next thing you want to do is think about emotional spending. How many things do you buy because you want people to think you're ""doing good?"" Cut that out. If your car works (even if it's not cute), keep it a little longer. If you just HAVE to look cute in a car, rent one for the weekend. Try repeating the same outfits often, you'll notice that NO ONE NOTICES. People don't notice your clothes as much as you think they do (and if they do, who cares?). Cut out the clothes and purses and knick knacks and don't buy anything for your wardrobe for 6 months. Notice how creative you get.
Start getting rid of things. Purging was a way that I learned to diminish my attachment to THINGS. If I haven't touched it in a year BYE BYE. Give away a box of ""stuff"" every weekend. You'll start to notice that you have a lot of things that you don't really need. I'm not saying empty out the house but clear out the clutter, it will change the way you ""shop."" Now when I go shopping I think, ""Am I going to use this in the next 30 days?"" If the answer is NO it can stay there. When I'm READY to use it, I'll get it (unless it's 75% off). Even then only add another 30-60 days. Even buying something 75% off that won't be used in a year is a waste.
What are your favorite things? How can you do them (and enjoy them) for less? What do I mean? I LOVE books! LOVE. BOOKS. I spend SOOOOO much money on them so I do a few things. As often as I can I buy them used. When I can't I use my Barnes & Noble Membership which guarantees me a discount. I love wine and I found the $4-10 wine at Whole Foods that is awesome and let's not forget $2 Chuck at Trader Joe's. The long and short is to just try to find out how to do everything you MUST DO a bit cheaper.
Next thing you want to do is TROLL your cell phone bill, cable bill, car insurance, etc. Any bill that you have OPTIONS on, give them a call today. It's as simple as saying that you need to lower your bill ... what can you do? I PROMISE doing this will save you on just about every bill. SHOP around! Saving $40 a month on a cell bill equals $480/year. That's a flight purchased 6 months in advance across country or to Miami and/or Chicago for the year.
We will talk about credit next time. Let me know how these tips work for you!
mytaughtyou.com
",167
https://medium.com/the-capital/how-i-transformed-6-000-to-3-000-000-f8404a5daa0c?source=tag_archive---------4-----------------------,"How I transformed $6,000 to $3,000,000",My journey as a college student investor in the wake of the pandemic.,Maher Rahman,19,"Preface: This blog isn't going to be about financial advice, or how to get rich quick.
In fact, If you bought Apple, Microsoft, Zoom, or Tesla leap calls (Call options that expire in over a year) in March 2020 and held until January 2021, you'd have made nearly as much, if not even, more than I did.
With this in mind, what I did isn't all too ridiculous. The market crash on March 2020, coupled with the subsequent bull run all throughout 2020, is by all means completely ridiculous. Even an ETF, ARKW, from March 2020 to the end of the year quadrupled in price. ETFs, which are known to only have 10-15% growth per year.
In all seriousness, what happened between March 2020 to now was a huge shift towards momentum and sentimental trading instead of a pure focus on fundamentals.
Things like Tesla started soaring in price because public sentiment toward Tesla became incredibly popular, and thus, people started wanting to buy Tesla stock. This was true for a lot of the market. In spite of the US Economy taking huge hits thanks to the pandemic, the stock market began rapidly increasing. Some of this can be attributed to fundamentals, as Amazon was doing well, as well as Zoom, but a lot of it was many new retail (regular everyday people) investors entering the market.
A new paradigm began as much more new traders flooded the market thanks to Robinhood and commission-free trading.
The intent behind this blog is to detail my own personal journey as a complete novice retail investor, and the pitfalls, mistakes, and successes I learned along the way. To talk about investing as I learned from scratch with no finance background.
I hope that by showing my successes, as well as grave mistakes, I can let people know that the road to the moon isn't straight, but to use the ultimate power of the internet to their benefit. And while I was lucky with my timing in the market, I think if you never understand investing, you won't be equipped to ride the next wave if it ever happens.
I share this blog because I simply want to let my journey be known to those who care to read about it. But enough of that, let's start from the beginning.
On May 11, 2020, as the Spring semester was ending and summer was getting near, my friend sent a rather simple message into our group-chat. ""VTIQ.""
Since I knew him for several years and I trusted him as an intelligent person studying finance, I decided (In a pretty dumb fashion) to throw all of the money I had at the moment into that stock, which happened to be around $5,000. As the days went by, I began realizing that VTIQ, which was a SPAC (Special Purpose Acquisition Company), merging with a company called Nikola had some major potential. I petitioned my dad to loan me $1,000 to ""learn how to invest,"" and I threw it all into this stock.
I want to emphasize a major point here. I was just beginning to invest, I had little to no experience at this time, so the actions that I took in hindsight worked out, but in reality, this was a really bad decision and not one I would suggest doing again.
I want to urge the readers at this point to avoid a results-oriented thinking approach, and that while this ended up being successful for me, as we will see later on, I'll learn the hard way about throwing everything into something I don't understand.
I ended up selling my VTIQ right before the merger and then decided to buy right back in since I was heavily indecisive. Since Robinhood doesn't show my transactions with VTIQ, I can only show when I bought NKLA. This has something to do with SPACs becoming completely different companies when they merge. I'll explain more about SPACs in the next section since they're heavily important in my personal investing journey.
A few days later, NKLA does the impossible, and soars to $90+, which is when I make the move to sell everything, leading my account to hit $18,000 only a few weeks after I just started.
Later, I learned that the CEO of NKLA said this:
""The entire infotainment system is a HTML 5 super computer,"" Milton said. ""That's the standard language for computer programmers around the world, so using it let's us build our own chips. And HTML 5 is very secure. Every component is linked on the data network, all speaking the same language. It's not a bunch of separate systems that somehow still manage to communicate.""
Now, I'm a programmer, but it doesn't take a programmer to know that this was a bunch of hogwash, so I was really glad that I sold at that moment and was pretty happy to learn that NKLA tanked right after I sold, and it hasn't come close to the $91 that I sold it at. I only sold initially because it was a lot of money, and I wanted to cash out at the time. It was pure luck that $90+ was the peak, and it tanked right after.
Thanks to this whole endeavor, I was hooked. I wanted to know more about stocks, and I started straying away from my friend's advice and started doing things my own way. I wanted to be much more active, and I couldn't keep relying on him solely. This was when I discovered penny stocks and began fully using margin. This, of course, was not wise in hindsight, but as a beginner, I really didn't know any better.
As with many beginners, I had no idea at the time that stock price was rather unimportant, and that percent gains are what mattered. A stock going from $50 to $100 versus a stock going from $1 to $2 were identical in gains, but to me, I thought going from $1 to $2 was more reasonable (Completely untrue). Because of that misunderstanding, I assumed my only way to grow my account was to buy penny stocks using margin.
For those that are unaware, penny stocks are stocks that are really cheap (Not necessarily pennies) that appear to have lots of room to explode and grow.
Margin is essentially a loan that your broker gives you to purchase more shares of stock. The more money you have in your account, the more margin you can get, and the total amount are dependent on the type of stock that you're trying to buy. https://www.investopedia.com/terms/m/margin.asp
Investopedia is a pretty handy resource for any stock terms that I run across that I'm confused about.
I decided to buy and trade several penny stocks such as IDEX, KTOV, and MYT. In hindsight, these were mistakes, and penny stocks are not something I suggest any of my readers do, but at the time, I truly didn't know any better. I was trying to learn, and I decided at the time that jumping straight into the river was the better way to learn how to swim.
There were three penny stocks, however, that I ended up buying and holding as they started to see incredible growth, and started escaping the penny stock territory (meaning, they were no longer cheap shares). They were a Micro-grid company, an electric vehicle company, and a green energy company, CleanSpark, NIO, and Plug Power, respectively.
The question is, how did I find these stocks, and why did I decide to buy them?
In my mind, I knew that Electric Vehicles and Green Energy stocks (ESG, which is shorthand for Environmental Social Governance) would be huge. I would browse Reddit, Twitter and was slowly learning how to read SEC filings, but I still didn't understand most of the finance terminology and fundamentals at this point, so this was purely an emotional buy. I found a few Reddit threads that discussed these stocks, and since what the posts discussed seemed viable, I decided to go with my gut intuition and go into these stocks.
As a note: NIO exceeded $60+ PLUG exceeded $70+, and CLSK exceeded $40+ in price at their peaks.
My first ever buy in the stock market was VTIQ, which was known as a SPAC. a SPAC (Special Purpose Acquisition Company) is a shell company that goes public with the sole purpose of finding a private company and bringing them public.
I believe it's appropriate to dedicate a paragraph to explain SPACs because they're rather complicated at first, but something I started falling in love with the more I learned about them.
A SPAC's entire goal, is to be a means for a private company to go public. Typically, a SPAC is run by a few people who will end up joining the private company in some form as board members or executives, as well as offer large sums of money. In my head, I viewed them as Shark Tank but on a much bigger scale, typically giving hundreds of millions or billions of dollars to a private company to help them with debt or asset purchasing.
Once a SPAC finds a target, and all goes well, the SPAC will merge with the target, and the ticker on the stock exchanges will change. An example of this is VTIQ becoming NKLA.
https://corpgov.law.harvard.edu/2018/07/06/special-purpose-acquisition-companies-an-introduction/ and https://www.reddit.com/r/SPACs/comments/h0bxbk/the_beginners_guide_to_spacs/ give a good introduction into SPACs.
VTIQ brought NKLA public. At the time, I didn't understand any of this, but I could still remember the gains and happiness I felt from buying VTIQ.
Of course, at that time, I set myself on a mission to understand how SPACs better and how they worked.
My SPAC journey was filled with minor escapades with several different SPACs. OPES, FMCI, GRAF, GMHI, HCCH, SHLL, CCH, DPHC, IPOB, etc. I could go on a giant naming spree as I've most certainly traded hundreds of different SPACs as I started looking more into management teams and potential sectors.
I found all of these SPACs through a few Facebook groups that I was in, the subreddit /r/spacs on Reddit, and discord groups (https://discord.gg/spacs), but they were all rather tame since I either entered late, or sold too early before some of them went high in price.
The main SPAC that gave me my first huge boost after NKLA was one called SPAQ. I saw a post on reddit mentioning this SPAC that was near $10, and honestly, I put all of the money that I could into this SPAC. I knew that thanks to how SPACs operated, buying at a low price was no issue, and this one was set to announce something soon. (Apparently, they updated their website recently at the time that I decided to buy into them). The next few weeks, a rumor came out, along with a DA (Definitive Agreement, a statement confirming that the target company and the SPAC are indeed in talks about merging) that SPAQ was merging with an electric vehicle company called Fisker (Now called FSR on the stock market), causing the stock to move from $10 to $20, leading my account to hit the coveted $100,000 mark.
At this point, I was beyond excited and made my first post on Facebook talking about it to my friends, happily sharing that I made 100k.
Things were going well, I transferred around $40,000 into a TD Ameritrade account to try out warrants and kept the rest in my Robinhood account. What are warrants? They're a type of security for a stock that lets you convert them into a share of stock for the cost of $11.50 after a certain time period.
Essentially, if you think a SPAC will do well, then buying a warrant makes the most sense since typically warrants start off as a few cents and can end up becoming a few dollars if the SPAC does well.
Since I heard that if you're bullish (Meaning you think a stock will go up), that buying warrants was the way to go.
And of course, I'm still incredibly bullish, so I thought warrants were a must transition. Since Robinhood doesn't let you buy warrants, I needed to make an account with TD Ameritrade.
I put most of my TD Ameritrade account into GIK warrants and patiently waited on them as I started focusing on my Robinhood account. They were incredibly cheap, and I thought I didn't mind sitting and waiting for them to do something.
Unfortunately, this is where my major mistake occurs.
I've made several minor mistakes on the way. I lost some money on KTOV, I ended up not getting maximum profit on IDEX, I held OPES and FMCI a bit too long and missed out on my maximum profits. But so far, nothing that set me that far behind, until now.
At the time, Zoom recently had an earnings announcement, and they did amazingly well. I was upset, because to me, this felt like it should have been such an obvious buy.
""Why didn't I buy Zoom calls? Am I an Idiot?""
FOMO (Fear of Missing Out) consumed me. I thought, ""Well, there surely must be plenty of stocks that are doing well from the pandemic, like Costco!"".
Their earnings report was coming in a few weeks, so why wouldn't I buy some calls and play it out.
The gravest mistake I've made in my investing journey. I never knew what IV Crush was, I didn't understand options, and I didn't understand how options interacted with earnings calls. I lost a majority of my Robinhood account and confided in my friend in secret. I never told anyone else until now.
I'll be completely honest, I was destroyed mentally. If it wasn't for my ridiculously incredibly friend who was also investing with me who helped spur me back on, this would have been the end of my journey. I would have turned $6,000 to $50,000 and have called it quits.
And I did. For the entire month of October and even most of November, I didn't look at stocks. I still followed along in the chats I was in, but aside from that, I mentally called it quits, despite my friends pushing me along.
I still had NIO, PLUG, and CLSK positions in my Robinhood account, but aside from NIO, they weren't doing super great at the time. I held onto what little positions I had on there and mentally tapped out.
When Palantir had their direct listing (Going from private to public on the stock market, https://www.investopedia.com/investing/difference-between-ipo-and-direct-listing/), I bought some shares of them, but at that point, I was too hurt to really care that much.
Then one day, I'm told by a friend on Discord that GIK found a target, and it's with an EV company called Lightning Emotors, and the price rocketed. I wasn't following the news, but I remember buying GIK warrants awhile back, so I opted to check back into my TD Ameritrade account.
I'm back. I couldn't believe my eyes, I was beyond ecstatic. This was my chance to redeem myself. Luck really was on my side, and was pushing me to give myself a shot again and really get serious again.
I began browsing more Reddit and Discord servers learning information (Reddit.com/r/investing, /r/stocks, /r/spacs, and unfortunately /r/wallstreetbets).
I started getting my Mojo back, I decided.
SCREW OPTIONS
And started looking into SPACs and Warrants again.
SPACs never let me down before, they were always amazing, and I have no idea why I ever decided to turn my back to them.
I started looking into more and more SPACs, SBE, KCAC, PIC, STPK, IPOB, I couldn't even begin to name them all. This was the SPAC revolution part 2, and I was back on my feet.
The year was wrapping up, while October and half of November was when I was mentally tapped out, the end of November and December was when I came back. Not only were SPACs going crazy, NIO, PLUG, CLSK as well as Palantir all decided to pop off at this moment, and the other stock that I was holding in TD Ameritrade, OEG also started doing amazingly well, going from $2 to $10 in the span of a few weeks. It was almost like the grand return of the king.
It was at this point that I decided to make another post on Facebook to give a wrap-up of my current status at nearly $500,000. I had wanted to cry and thought that this was the peak of a return. Little did I know of what January would bring.
Initially, when I first heard about the GameStop debacle, I put in $5,512 as a joke of sorts. I didn't really believe in it, but I also thought, ""what if.""
Little did I know, as a month passed, I started looking more and more into what a ""Short Squeeze"" is.
For the unaware, at the time, GameStop and another company, AMC, had an incredible amount of people who were ""shorting"" them.
Shorting is essentially when someone ""borrows"" a share to sell to someone else expecting the price to go down, so they can get the share at a cheaper price while selling to someone at a higher price.
A short squeeze happens when many people decide to buy into a stock, collecting all available shares and denying a short the ability to get the stock that they borrowed. Since they borrowed the stock in the first place, they are at the stockholder's whim on the price, which drives the price up.
In a very unwise decision that ended up paying really well, I put a lot of my account into GME and AMC and I bought more and more as the days came buy.
And then, the legendary moment happened, one that nobody could have predicted.
Elon Musk tweeted about GameStop and linked to the subreddit WallStreetBets.
This single-handedly made the stocks explode with popularity, particularly GameStop and AMC.
Which lead me to my peak value on January 27th.
Once I saw this, I sold. And created my most recent Facebook post talking about reaching millionaire status:
I know many people who are familiar with WallStreetBets would say I was ""Paper Hands"" or I didn't ""Hold the Line."" But this was too much money in an instance.
I had no words, and I was in pure shock. This was truly a life-changing moment in my life, and I felt like I didn't earn it.
I felt like what happened with GameStop and AMC was such a wild event, that I felt really confident going into, but one that I could never have predicted would go so well.
So what have I learned? What resources can I offer and what was the point of me writing all of this?
There were so many investments and trades that I did not include on here. I never mentioned when I started falling in love with ARK ETFs run by Cathie Wood. I never mentioned my debacles with so many SPACs like the crazy SHLL bull run, nor did I mention the crazy stuff that happened with Dogecoin.
The reason being, investing is a crazy journey. It's simply too much to go into detail, but it's one that should be taken.
I write this because I started off completely unaware. I wasn't a genius investor. As you can see in the beginning, I had to ask my friend what a pump and dump even was.
But I was always learning.
I'm by no means an expert. I don't think I'm the person to go to for advice, nor will I want to trade for people or sell any advice because I personally still feel like I have so much left to go and so much more to learn. But, if I could talk to myself from May 2020 and tell them what I learned now, this is what I would say:
I share this because no matter how much you know, it'll never be enough. So might as well get started and try to dip your toes in the water. Nobody ever learned how to swim from reading a book.
I won't suggest a book or some Twitter personality. There are plenty of people you can follow on Twitter of your choosing, and there are so many books out there that will all teach the same fundamentals. What I will suggest is yourself. Believe in yourself that you understand things well. I've had my dad tell me to buy Moderna when it was $40 because he was thinking the vaccine might be good, and it quadrupled in price. If you think you know something about an industry, trust yourself.
Google is your friend, and so are asking questions. There was not a single resource I used by itself. I browsed Reddit, I messaged people on Discord, I scoured Twitter, I went onto Investopedia to teach myself new terms, and I kept searching for answers.
discord.gg
Reddit.com/r/spacs as well as Reddit.com/r/stocks
When I was confused, I wasn't afraid to admit that I had no idea. I've had people call me an idiot or much worse things for not understanding, but I never stopped asking.
This post isn't an ""Anyone can get rich like me."" Quite the contrary. I recognize the insane amount of luck I had. So many of the plays and investments that I made could have ended poorly. It was quite possible green energy might have been hated by the market. It was possible Oil recovered and killed some of my plays. It was possible that Tesla didn't soar and thus make other EV stocks soar in price. It was possible that GameStop and AMC was a dud and fell flat on its face.
So many things happened that went well. But like I said, it only happened because I took that risk.
And I made mistakes, plenty of them. I hope that being completely open with myself, I can show that despite the success, there were horrible mistakes that could have easily been avoided had I known better.
But you need to make mistakes to learn from them.
And I hope you can learn from my mistakes by seeing them out in the open.
I recognize that not everybody can take those risks, but for those that can, for those that have an extra $50, $100, $1000 dollars that they are willing to learn with. Or those who have time and want to set up a paper trading account.
I implore you to get started, because investing is a skill that can quite literally, pay you dividends.
Thank you for reading this blog, I still have so much more to learn, and this is by no means an end of a journey. I'm only still beginning. I haven't written any white papers or done any financial analysis on stocks. I want to know more, I want to learn more, and I hope that some of you will continue to learn with me. If you're interested in following me along on that journey, here's a link to my twitter.
https://twitter.com/LeftistInvestor
Check out our new platform  https://thecapital.io/
https://twitter.com/thecapital_io
A publishing platform for professionals now available on https://thecapital.io
290 
7
",168
https://medium.com/conversations-with-tyler/cliff-asness-986ba8e92b4f?source=tag_archive---------3-----------------------,Cliff Asness on Marvel vs. DC and Why Never to Share a Gym with Cirque du Soleil (Ep. 5  Live at Mason),"Tyler and investment strategist Cliff Asness discuss momentum and value investing strategies, and disagreeing with Eugene Fama.",Mercatus Center,61,"Tyler and investment strategist Cliff Asness discuss momentum and value investing strategies, disagreeing with Eugene Fama, Marvel vs. DC, the inscrutability of risk, high frequency trading, the economics of Ayn Rand, bubble logic, and why never to share a gym with Cirque du Soleil.
Listen to the full conversation
You can also watch a video of the conversation here.
Read the full transcript
TYLER COWEN: Cliff is one of the most influential figures in global finance. He has a PhD from the University of Chicago, he studied there with Gene Fama. And he is a founder and principal of AQR Capital Management in Connecticut.
Cliff, when I think of your work, the very first word which comes to my mind is momentum. Could you first give us just the super short version of what a momentum trading strategy is?
CLIFF ASNESS: Sure. A momentum investing strategy is the rather insane proposition that you can buy a portfolio of what's been going up for the last 6 to 12 months, sell a portfolio of what's been going down for the last 6 to 12 months, and you beat the market. Unfortunately for sanity, that seems to be true.
COWEN: Seems to be true. Now, you call it insane, but if you were to give us a simple example  on average, statistically speaking, not in a free lunch way, but what kinds of supernormal returns might you possibly earn through a momentum trading strategy?
ASNESS: As one example, if you were running against largecap US equity, something like the Russell 1000, and you bought the onethird of stocks with the superior 6 to 12 months returns, you'd probably make 100, 125 basis points extra longterm, on average. You do that in small stocks. It's more like 250 to 300.
COWEN: Why is it larger for small stocks?
ASNESS: Almost anything we find in investing, almost any regularity, ""this tends to beat this,"" seems to be smaller  seems to be larger, excuse me  for small stocks. That is not quite as good a deal as it sounds, because the risk is also larger. Smallcaps have bigger fluctuations.
People have a lot of theories. Analysts' coverage, Wall Street doesn't cover them as much, perhaps whatever degree of inefficiency in the market is larger for smallcaps. But it's a nearly ubiquitous finding. Anything you find works in largecaps tends to work somewhat better in small.
COWEN: If you have, in terms of excess return, 100, 125 basis points compounded over 20, 30 years, obviously that's going to be a lot of money, correct?
ASNESS: Correct.
COWEN: What's the catch? What's the qualification? Why doesn't everyone here run out? They don't even wait until the talk is over 
[laughter]
COWEN: And as an investment strategy, follow what you have done with momentum. What's the trick?
ASNESS: There are a number of them. One hypothesis is I'm really not very convincing at all. Another is more people do it these days and I'll admit I think the strategy is going to survive that, but it's a concern. It is not something every investor can do. I get this question from clients sometimes and I go, ""Are you going to do it?"" and they go, ""no."" I go, ""That's why.""
With that said, I think it is a fairly unintuitive idea. To some, it's very intuitive. Just buy what's going up. To someone who studies markets, particularly for Gene Fama like I did, the idea that you can beat markets  and we do more than just momentum. (Tyler, he's promised me he'll get to that.) But it's a very unintuitive idea if you think markets are anywhere near highly efficient. I think that dissuades some. It nowhere nearly works all the time.
One thing I should really be careful about. I throw out the word ""works."" I say ""This strategy works."" I mean ""in the cowardly statistician fashion."" It works two out of three years for a hundred years. We get small p-values, large t-statistics, if anyone likes those kind of numbers out there. We're reasonably sure the average return is positive. It has horrible streaks within that of not working.
If your car worked like this, you'd fire your mechanic, if it worked like I use that word. I think it is harder than you might guess, even if something works long term, to have it go away because a lot of investors can't live through the bad periods. They decide why it's never going to work again at the wrong time.
If your car worked like this, you'd fire your mechanic, if it worked like I use that word. I think it is harder than you might guess, even if something works long term, to have it go away because a lot of investors can't live through the bad periods. They decide why it's never going to work again at the wrong time.
COWEN: I think of you as doing a kind of metaphysics of human nature. On one side, there's behavioral economics. They put people in the lab, one-off situations, untrained people. But here it's repeated data, it's over long periods of time, it's out of sample. There's real money on the line, and this still seems to work.
When you back out, what's the actual vision of human nature? What's the underlying human imperfection that allows it to be the case, that trading on momentum across say a 3 to 12 month time window, sorry, investing on momentum, will work? What's with us as people? What's the core human imperfection?
ASNESS: This is going to be embarrassing because we don't have a problem of no explanation. We have a problem with too many explanations. Of course, we can observe the data. The explanations you have to fight over and argue over. I will give you the two most prominent explanations for the efficacy of momentum.
The first is called underreaction. Simple idea that comes from behavioral psychology, the phenomenon there called anchoring and adjustment. News comes out. Price moves but not all the way. People update their priors but not fully efficiently. Therefore, just observing the price move is not going to move the same amount again but there's some statistical tendency to continue.
Take a wild guess what our second best, in my opinion, explanation for momentum's efficacy is? It's called overreaction. When your two best explanations are over- and underreaction, you have somewhat of an issue, I admit. Overreaction is much more of a positive feedback. It works over time because people in fact do chase prices. So if you do it somewhat systematically and before them you make some money.
One of the hard things you find out in many fields but I found out in empirical finance is those might be the right explanations but they're not mutually exclusive.
Remember the movie Highlander? You and I talk about scifi.
COWEN: Of course, yes.
ASNESS: They always say there could only be one. I use this tagline a lot and only one of a thousand people gets it and then one of two thousand laughs. That's not true in these things. There could be multiple explanations.
Both of these things can be true. There could be underreaction and ultimately, people can go too far. Very often in finance, let me give you two other possible explanations also.
Every empirical regularity, meaning something that works or is terrible with some predictability, it does so for one of three reasons. The worst one is complete accident. You've data mined. I sat there in my dissertation in 1990. I found this empirical relationship with a big tstat but I really checked 63 things so the tstat is a bit of a lie, and it never works again.
Momentum in my opinion  this is editorial  has survived 200 out-of-sample tests, through time, different asset classes. I don't believe that one but to be intellectually honest, you never reduce that probability to zero. You just make it lower. As it works again, you go ""chance you're just lucky, smaller.""
Second reason is a behavioral story. Someone out there is making a mistake. I gave you two. Underreaction and overreaction are both the behavioral story. They're both somebody out there making an error, doesn't mean markets are terrible by any means. I'm a big believer in markets but at the margin, they're making an error and you take advantage of it.
The third is risk. In a rational world, you get paid. Momentum would work in a rational world if the short-term 6 to 12 month winner were in some deep, important sense riskier than the losers.
COWEN: But they don't seem to be.
ASNESS: They don't seem to be.
COWEN: Correct.
ASNESS: They do show, there are terrible periods, there are so-called momentum crashes but there are very good periods, of course, and those crashes don't appear to be a particularly painful time. In the world of economics, it's not just the bad periods, it's when you have them. I don't think people have come up with a very good risk story, but I'm not done.
COWEN: Let me give you my intuition in favor of why it might be overreaction and you tell me what you think.
You receive a signal about the world. It's to some extent a private signal and you over-interpret that signal and you think it's a signal about the whole world so you overreact. That leads to some price movement, which is propagated through time. But, at least some people think, past that 12-month time window, momentum ceases, and there's even a bit of price reversal.
Eventually, you learn that you've been overreacting by thinking your private information is more general, more systematic than it is and then things snap back a bit. Does that psychological hypothesis explain this mix of price reversal in the longer term and momentum in the shorter term? Do you think that makes sense or not?
ASNESS: Yeah, I'm going to go with that for now. No, I'm kidding. Yes, it does. I think you're articulating a version of the overreaction idea.
One thing that Tyler just referred to is if anything pride of place came before momentum and is a super important strategy to us. He's asking me about momentum because that was a very early part of my work in adding it to the lexicon. But value investing at a longer time horizon, buying what's cheap in pretty much any way  price divided by something reasonable, price divided by earnings, books, sales. Just buying five-year losers, five-year stocks that have suffered, and selling or underweighting the opposite is also a very good strategy, very much the opposite in spirit to momentum.
Now, in the value world they have the same fight. Does cheap beat expensive because it's riskier? I don't think they've done a very good job of identifying the risk but I find it inherently more plausible that something priced to a long-term lower level might have a risk element to it than a much more short-term phenomenon like momentum.
COWEN: Then there also be overreaction.
ASNESS: Of course. The second big one is behavioral.
COWEN: Right.
ASNESS: If people make errors of any kind, literally almost of any kind, value is going to work because if two stocks have the same fundamentals, if there's an error in the price, it's going to look more expensive so you're going to dislike that one and you're going to like the one that looks cheaper.
One thing nice  and I don't think it's dispositive and I think both momentum explanations can coexist  and by the way, there are others. I've given you my two favorites, there are others.
One thing nice about the overreaction reason for why momentum has worked over time is it is consistent with the behavioral version of value too. If you're going to get overpriced or underpriced, one would imagine at some point, you overshot something.
COWEN: Right.
ASNESS: So it forms a nice integrated system.
COWEN: Let me give you another reason why maybe overreaction is a possible explanation. In finance I find it's one of the biggest puzzles: why do people trade at all? People who are fools, people who are well-informed but not better-informed than the market, people all over, they trade like crazy. It seems only a small percentage of that is liquidating funds to send your kid to college.
That suggests people are systematically overconfident. If the fundamental human bias is overconfidence and that leads to overreaction, do we then have some kind of plausible  these metaphysical human microfoundations  for why securities markets stay imperfectly priced?
ASNESS: I'm very pleased to get a promotion to metaphysical. That's much deeper than I ever thought of it.
COWEN: Is it a promotion or a demotion?
ASNESS: I started  I was never a pure efficient marketer. I was never a person who thought markets price things perfectly  nor is anyone, by the way. The paragons, one of my investing heroes, academic heroes, Gene Fama, likes to shock the class at Chicago. Only the University of Chicago in Gene's class could this be a shock line.
COWEN: Yes.
ASNESS: But kind of day three, he looks at the class and goes, ""markets are almost certainly not perfectly efficient."" And this is University of Chicago with the godfather of efficient markets so you get a [sigh of surprise]. Anywhere else  you guys did not do that. You might notice that. He makes making a point that that's a very extreme hypothesis that all the information's there.
People individually make errors. Errors are not riskless for even informed people to arbitrage away. I think looking at the world, I share your intuition. I think it's backed up by the data that the most common error is two kinds. They're not exactly the same thing but overconfidence and over extrapolation.
COWEN: Are the two of us overconfident? Let's say someone on Twitter says you're totally wrong.
ASNESS: I can't answer for you.
COWEN: Do you underreact or overreact?
ASNESS: I overreact and, therefore, I'm overconfident.
COWEN: If you tell your kids they can't watch their favorite TV show 
ASNESS: You're destroying my entire self-image right here.
[laughter]
COWEN: Do they underreact or overreact?
ASNESS: My kids are probably more grown up than I am.
COWEN: Yeah?
ASNESS: Yeah, but they overreact.
COWEN: They overreact?
ASNESS: I can't find a lot of examples of people who underreact. I'm picking up on your point here slowly.
COWEN: This is my intuition. You look at these other areas. People at work, they're given positive or negative feedback. They may overreact more readily than underreact.
ASNESS: Let me take this another way. I think we are mixing overconfidence with overreaction a little bit. New news, people might be overconfident in how much they understand it, but they don't seem to incorporate it enough.
There is evidence on this outside of just returns to momentum. People look at the actual news that came out, try to gauge what the reaction should be, things like that.
I do think there are things when it comes to processing new information, I do think there is an underreaction in that sense, an anchoring and adjustment. People do all kinds of psychological experiments. You show people a whole lot of numbers and you show them numbers out here. They should move X, they move half-X.
COWEN: Yes.
ASNESS: They're overconfident that they are right. I think that does lead eventually to the overreaction.
I really do think  this is one thing that makes it very hard, when I go to academic seminars where people are fighting about this. There aren't many things in my life where I'm a peacemaker. If I see a fire, I'm far more inclined to throw gasoline on it than water.
COWEN: OK, good.
ASNESS: But at academic seminars, this idea that there could be multiple explanations, I push at times because there'll always be, two people they want to win. Somebody has an overreaction, somebody has an underreaction story. Somebody says it's risk, somebody says it's behavioral.
It could be both and just to make our lives really hard, the mix of what's more important doesn't have to be the same at all points in time. Let me give you an example.
I didn't start out a perfect efficient marketer. I wrote my dissertation for Gene on momentum. I was already showing a heretical drift at that point. He was quite good about it, by the way. The technology bubble, living through that, where momentum worked but the other thing we believe in many strategies actually, but the other biggest, the second biggest one, the tie for the biggest one is value investing again.
Some of you, distressingly fewer and fewer, look like you actually remembered it live.
[laughter]
ASNESS: Too many of you read about this in the history books, something that one of my partners told me that I look like Lincoln before and after the Civil War from the technology bubble to which I was like, we did well because we stuck with our value strategy but we were nearly destroyed by it. When he said that to me, I'm like, ""Yeah, that's about even. Lincoln kept the Union together and ended slavery. I stuck with a value strategy."" That's perspective for you.
COWEN: You need a lot of discipline to make momentum investing work, right, because for 10 years, it might get you nowhere or even underperform.
ASNESS: This gets back to your earlier question, ""Why doesn't everyone do it? Why doesn't all this stuff go away?"" Any of these things, value, momentum, there are other things  quality investing, low-risk investing. Fisher Black of the Black-Sholes option pricing model's discovery in 1970 and then everyone forgot for 35 years. It's another effective systematic form of investing.
If everyone did them yesterday, they would go away. They work in my opinion  again using my version of ""work""  in kind of a sweet spot. Good enough to be really important if you can follow discipline, not so good enough that the world looks at it and goes, ""this is easy."" They're excruciating at times and I hate those times. I won't pretend I'm neutral as to those times.
COWEN: They help keep your market franchised.
ASNESS: I recognize intellectually you need those times while I will whine and cry horribly during those times. I'm not pretending I'm above that.
COWEN: But momentum investing still works today 
ASNESS: I believe it.
COWEN: Stochastically.
ASNESS: Yes.
COWEN: Now, if the momentum anomaly and the value anomaly  those to me seem like the two biggest anomalies in pricing theory.
ASNESS: I think so.
COWEN: Does Eugene Fama admit you are correct?
ASNESS: No.
COWEN: You have an academic record on this. You have a track record, right, statistically and the success of your firm. Will he try 
ASNESS: He's just trying to get me in trouble.
I will tell you. I'm in public here, I'm with someone I admire greatly, this is going to be on the Internet. I'm still more scared of Gene.
[laughter]
ASNESS: With that said, one of the scariest moments  let me take you back  was telling him I wanted to write a dissertation on price momentum. I swear to God I mumbled the second part, ""and I find it works really well.""
Because it failing is a perfect Chicago, Gene Fama, efficient markets dissertation. ""Look at what these crazy people on Wall Street do. They make all these indicators, and they're throwing away their money.""
To his credit, he immediately said, ""If it's in the data, write the paper."" Now, we don't agree fully on it. We don't agree on two things. For all I know, he changed his mind yesterday, but as of yesterday, I don't think we agree on this.
Value investing, remember, I think works for a mix of both behavioral and perhaps some risk reasons. I think they're hard to identify, but I'm more than willing to say that might be a big part of it. I think Gene would say it's mostly or all risk. I don't think he's very positive on the behavioral explanation.
Momentum  Gene's a risk guy, he's an efficient markets guy. I think Gene is still cynical about it. I know his latest paper  he and Ken French write if not the best among the best papers in finance. I read every one to this day. It doesn't mean I agree with every word.
They started out with a socalled three-factor model about 20 years ago. What drives return on an individual stock? The market's return and your sensitivity to that. Value's return and how much of a value stock you are. Size's return, a small firm effect it's called  how small you are.
They've added to that over time. They've added an investment effect of firms that, for instance, reduce their share count tend to do better. There are theories as to why. And a profitability factor. All else equal, more profitable firms seem to outperform less profitable firms. Again, that's a very strong effect that holds up.
I wrote a piece on our blog saying something, ""our factor model goes to six."" For those of you who are curious, it was a reference to Spinal Tap where our amps go to 11.
I take the other side from them. I love these guys. We agree on 9 out of 10 things. I don't see how momentum is not their sixth factor. It adds a tremendous amount of return versus their model.
I take the other side from [Fama and French]. I love these guys. We agree on 9 out of 10 things. I don't see how momentum is not their sixth factor. It adds a tremendous amount of return versus their model.
The numbers I gave you that you can add versus, say, the Russell 1000, 125 basis points, understates the power greatly because momentum is also in geek speak negatively correlated with value. In English, a good year for momentum is often a bad year for value, and vise versa.
That's easy to create, if you just do the opposite with your left hand as your right hand. It's not easy to create two strategies that both go up on average. That's difficult. That shows up statistically in a model. It becomes even stronger.
So, I don't understand why. I think they should have it as a sixth factor. If you can get Gene to leave Chicago, which is far more difficult than anything else we're talking about, he can tell you why it's not a six factor. But it should be.
COWEN: Let me ask you a question about risk, because this key concept comes up again and again in finance. A strategy may appear to have a high return, but risk-adjusted, what are you really getting?
Now, when I read the very latest papers on risk, let me tell you what I see. I see talk of the third moment of probability distributions, the fifth moment, words like coskewness, terms like the Ushaped pricing kernel, and talk of the volatility of volatility. I'm just waiting on the paper on the volatility of the volatility of volatility.
When I read all this as an outsider, I conclude we don't know anything about risk. These are Ptolemaic epicycles. Within a pretty broad range of asset classes, is it possible risk doesn't really explain anything about asset prices? True or false? What do you say?
ASNESS: The epicycle has held up for a long time. They even got the little tiny movements right. That's not bad.
OK. Of everything you said, a fair amount of those risk models I think are utter nonsense. I don't think the fifth moment  I'm going to insult someone I care about now by accident, I don't even remember who wrote this.
I don't think the fifth moment is a good measure. I don't even think skewness  skewness is bad stuff. Even if selling worth makes money on average, occasionally really bad stuff happens, more often than really good stuff.
Coskewness, which sounds like one of the geekier things Tyler said, very hard to identify, very hard to prove, very hard to isolate in the data. But coskewness at least makes sense to me as a real risk factor. What that means is not only do very bad things happen more often than you would imagine, but they happen while other very bad things  largely the market crashing, for instance.
If something has occasional giant losses, but those are at very good times, and makes money on average, very reliably, that might stink occasionally, but it's something you can live with. If something is extremely bad at the same time everything else in your life is extremely bad 
COWEN: Shotgun and can opener time, right?
ASNESS: Yeah, exactly. Well, I use this example in a very different way. If someone says, ""What if we get something five times as bad? How do you invest if we get something five times as bad as the global financial crisis?""
And I say, ""ammunition and canned goods."" And I don't think there's a better answer for that.
But I do think something like coskewness  it's a geeky idea  I think it's very hard to establish and prove. The data is not really even there.
Momentum itself, getting back to that one, it has negative skewness. The geeks call that a bad left tail. Nassim Taleb would call it a black swan event. It has standard deviations you're not supposed to see. Big events.
They have tended to be more, maybe this is luck, but they have tended to only occur in strong markets, not in weak markets. We don't like that, but we don't worry about that as much.
To be honest, when it comes to value, Ken and Gene have never embraced this story. I don't embrace it either, but I give it some credence. Value, buying cheap and selling expensive, has a little better of a risk story on this front, because it has suffered empirically in the Great Depression, in the global financial crisis.
Probably not enough. It probably is not enough to explain it. But that is the exact kind of measure of risk that should work. Does it hurt you? This is a terrible English sentence, and I apologize in advance. ""Does it hurt you when it hurts to be hurt?"" is an English language version of risk.
Any good quantitative measure, no matter how geeky you make it, should get back to that. If it's a good measure of risk, it doesn't just hurt occasionally. It hurts you when it hurts to be hurt.
If it's a good measure of risk, it doesn't just hurt occasionally. It hurts you when it hurts to be hurt.
COWEN: Are there still new and significant market inefficiencies to be found, or has that lode been mined? Are you the end of this tradition, or just the beginning?
ASNESS: I'm going to take a guess that there aren't that many more.
COWEN: But some?
ASNESS: I won't rule it out, because once you're horribly wrong about this, hopefully you learn from that. I would have told you 10 years ago that I would not have guessed that low risk anomaly  this is the idea that stocks  by the way, I use stocks as a shorthand.
One kind of wonderful thing, and if we've done anything, if there's something I can brag about, a lot of these things we've participated in the research  we've written the leapfrogging papers  we have been early in saying, ""Let's go look outside of stocks. Let's look at bonds. Let's go look at currencies. Let's go look at commodities.""
The things that I'm talking to you about in somewhat different forms ""work"" for all these things. But let me talk about stocks, because it's the easiest. It's the most common language.
Stocks that are low risk, backwards to theory, right? Risk should get paid. Stocks that have low betas, low volatilities, low fundamental risks like low leverage, outperform in a fairly strong way.
Fischer Black found that. If anyone wants to be masochistic and ask me why I tell you about leverage aversion and the fact that no one does what you're supposed to do in theory  low risk outperforms.
Other one is profitability. This is a real anomaly. Stocks out there that make more money and make more money consistently  nothing about price, it's not a value factor  more money by their book value, scaled by their sales, outperform.
COWEN: You mentioned Fischer Black on leverage, just to pursue that a little bit. Fischer thought, and maybe you seem to be agreeing with him, it's another human imperfection that at least some of us are too afraid of leverage.
Because we could borrow some money, buy some typically low beta stocks, and actually improve the quality of our portfolio. That's something else which doesn't quite happen as much as it should. That seems to be almost the opposite of overconfidence.
Is it just debt aversion? We were brought up, don't get into debt.
ASNESS: I think it is some debt aversion. It also might be less irrational and more constraint. There may be people who just can't. Mutual funds can literally leverage a little. Most have in their charters, ""we won't leverage."" So, if you're constrained, you have to do something.
Let me step back. I'm going to draw in my hand an efficient frontier. I promise you it is a perfect artistic rendering. Many of you, I know, have seen it before. If you haven't, on the Xaxis you have risk. On the Yaxis you have expected return.
Everyone in the world wants to move up, to the left. You want less risk and you want more expected return.
On the third week of finance class, they teach you  we should all agree, but of course we don't  but we should all agree on the best portfolio of risky assets. We should all own the same one, because it's the highest return for risk.
Those of us who are aggressive should apply some leverage to it. Those of us who are not aggressive, who are conservative, should delever, add cash, make it less risky. Why do you do that instead of simply moving your money from low return to high?
Because you have to get undiversified. If you move your money from low expected return to high expected return risky assets, you lose diversification. Ultimately, if you want as much expected return as the best asset out there, you have to be only in that asset. Applying leverage to the entire portfolio, you maintain the benefit of diversification.
What Fischer showed is, imagine leverage is very costly or just no one would do it. He points out that low-risk assets are orphans now. Low-risk assets function in a portfolio in an important way, to make your return for risk better.
But they often don't make your top line better. They don't literally make you more money. They make you more money for the risk taken. That's very boring, if you don't apply leverage.
You don't make more money. Most people are interested in some version that makes them more money. And Fischer showed that if that's true, which I believe it is and I think he was right, those assets that are orphaned will be a little too cheap, because no one wants them. People who want to be aggressive will a little too much be willing to be undiversified.
Again, leverage is not riskless. In the perfect theoretical world, not everyone should leverage to the moon if you're very aggressive. But neither is concentration, neither is moving your money into fewer and fewer assets.
We don't tell people leverage is riskless. We do tell people we think people think it's over-risky versus concentration.
So I don't even know if it's risk aversion versus another. It's risk aversion. It's more I think people misappropriate the risk of one thing against another. Maybe it's neither a borrower nor a lender be, your idea that people are just taught that leverage is bad. But I think people prefer concentration risk to leverage risk to their detriment.
COWEN: Let me ask you a very practical question about today's markets. Like myself and like Scott Sumner, you're pretty skeptical of the concept of a bubble and just going around and calling everything bubbles. But, your most famous piece is called ""Bubble Logic.""
There can be bubbles. They may be hard to identify. So, if you were pressed, today in the world, the US, the global economy, if you had to pick what is most likely to be a bubble, are you willing to give us your opinion?
ASNESS: Yes, and it's incredibly boring. There's nothing I feel is very, very likely to be a bubble.
COWEN: But there's always a winner and a loser in any horse race.
ASNESS: If I had to pick one 
COWEN: Have to pick one.
ASNESS: If I had to pick one, and it's still going to be a different kind of cop out, it's a diversified portfolio of stocks and bonds. Let me take you through it.
COWEN: OK.
ASNESS: Versus history, on the measures I like. One of the most famous ones  we look at a bunch of different measures  is Bob Shiller's cyclically adjusted PE. It's price divided by long term rolling average of earnings for the S&P 500.
You like low prices. You don't like high prices. That number is more expensive in roughly 90 percent of 100 some odd plus years of history. That is a terrible forecaster for the next month or the next year. Do not do anything. That is not just a legal, it's a human disclaimer.
But over the next 10 years, nothing is perfect even over 10 years. But it's statistically powerful. When things are expensive, you've made less money. When things are cheap, you've made more money.
Bonds are the same idea. A measure for bonds that is very analogous to a PE for stocks is the yield on a government bond minus an economist's forecast on inflation. People call that a real yield. It's what you should care about. The nominal yield doesn't really matter. It's what you consume in.
That is worse lower. This is a yield, not a price, so lower is bad. But that is worse than roughly 90 percent of history.
The portfolio of half stocks and half bonds, if you take those two measures, scale them and combine them, is the worst ever. How is it the worst ever, when the two are 90th percentile? Well, I think you all have probably figured it out. They're not usually 90th percentile bad at the same time.
It's a bit of a puzzle why they are, but they are. Having said that, Tyler, I'm still going to be a coward. We get to about the worst ever. We've hit this level before for portfolio.
I used to think 100th percentile was pretty impressive in my career. Now, you never go past the 100th percentile. You're a math guy. You know how it works.
COWEN: I'm a math guy.
ASNESS: But I will tell you, not all 100th percentiles are equal. If you are the 100th percentile today, but you're 150 percent of the prior 100th percentile excluding the last couple years, that means you've shot off to the moon.
In the technology bubble, the Shiller CAPE is probably 50 percent higher than it had ever been if you excluded the period right around that. Japanese stocks, in '89, '90, got to levels like that. You might call it an inverse bubble. I think US stocks in the early '80s maybe got to a similar extent, well past anything we've ever seen on the downside, in the very early '80s.
I was willing to call those bubbles, and did real time. I wrote that piece you're talking about.
COWEN: Sure.
ASNESS: Because we spent a lot of time  still a subject of measure  but my measure for using the word bubble, remember I'm a Gene Fama student. He hates the word bubble, too. Bubble is an inefficient market phenomenon.
I will use it, but I hope I have a higher standard than many. Many in our field, have I think, dumbed the word bubble down to mean something we think is kind of expensive. That's not a bubble.
A bubble to me is something still subjective, because your answer may not be the same as mine, but is something I've tried my best to come up with future assumptions of growth, be it for a stock, inflation if it's a bond, and current price, and I can't come up with assumptions that would lead any rational investor, subjective again, to want to own this.
When we did that for stocks, anywhere late '99 to 2000, we assumed very aggressive future returns. We took Wall Street's long-term forecasts, which were nuts, they had never been achieved before, current prices, and we came up with, if that happens, we make less than bonds. We were willing to use the word bubble now.
Right now, this 100th percentile, US stocks and bonds. So you invest half your money in stocks, half your money in bonds. Historically, you've made about five percent over inflation. We think it's priced now, at this level, to make about 2.5 percent over inflation.
COWEN: Rather than five?
ASNESS: Rather than five. Is that a bubble? I can't prove to you that that's a bubble. That's an expensive market versus history.
So, most likely, when you take a whole bunch of expensive things and they get very expensive, and you put them together, you are right. You always can pick a most. But if you ask me the follow up question, ""do you think it's a bubble?"" I will say no. I think it's an expensive market.
A bubble is something where you say this cannot last, and I would not say that.
COWEN: Let me tell you my biggest worry. Maybe you can set me at ease on it. I'm not ready to call it a bubble, but bubble related.
I look at the carry trade. Companies, especially in emerging economies, borrowing in US dollars, typically at quite low rates, and forgetting a bit about future currency risk or future revenue and growth risk on their side. And there being this extreme flow of liquid financial capital into those companies, not really quite backed by forthcoming realities which will match the expectations behind that borrowing.
I'm not sure exactly what's the single asset price here I want to call a bubble, but that's my biggest worry, where I think maybe the market isn't pricing that whole combination correctly. Now, are you less worried than I am?
ASNESS: I am less worried than you are. It's hard for me to know how metaphysically worried you actually are.
COWEN: Not that worried.
ASNESS: But emerging markets, on that same measure, just looking at the local stock markets, not the carry trade per se, but you'd think if that flow was gigantic you're talking about, it would show up. On the same kind of Shiller CAPE numbers are considerably cheaper than US or even Western Europe.
So you might very well be right. But again, even though I'm the bubble guy now, I'm the momentum guy, I think as people who try to beat the markets everyday for a living, I'm a startlingly strong believer in efficient markets relative to the norm.
I think what you're saying has truth to it, but maybe largely in prices already.
COWEN: A lot of people from the hedge fund world, they speak to me. They say, ""Tyler, interest rates, or rather bond prices are a bubble,"" because of course right now the low rate is at zero. It's not going to go down much below that. It could go slightly negative. There's a fear right now we're living the world's biggest bond bubble.
Now, personally, I don't think this at all. But what's your opinion?
ASNESS: I'm going to be real careful, again. I do not think the word bubble is justified. That's not careful. That's overly bold, actually. Having said that, I've got to be clear. When I say I don't think it's a bubble, I'm not saying I think this thing is great.
When I talked about bonds, I said they were more expensive than 90 percent of recorded history. Actually, the low 90s, now. That is not a commercial for forward-looking bond returns. That is saying I reserve the word bubble for something that cannot work out.
I sit down and I go, how would it work out for a bond investor? It's very hard to work out for a cash investor. You've got me there. But cash is largely  it's a government set rate. It's not a market rate.
COWEN: It has other services, right?
ASNESS: Yes. But bonds, how would it work out for a bond investor from here? We look at these ridiculous 2 percentish kind of nominal yields.
For me, to come up with a scenario, not a prediction, not something I think is a good bet, but a reasonable scenario that could happen in the next 20 years, for instance, for a workout, I need one word. Japan. It wasn't that hard.
COWEN: Sure.
ASNESS: If your standard, like mine, is a bubble is something where you can't really come up with a plausible scenario where this investment might work out, it's proof by contradiction. We just ended it.
I think it's an expensive asset. I think equities are actually shockingly similarly expensive. I think people focus on bonds for a bunch of reasons. They focus on bonds because the yields seem much more measurable.
I think equity valuations, things like Shiller's CAPE and many other measures, are actually about as good for forecasting equity returns as bond yields are long term forecasting bonds. I think the last 20, 30 years, equities look better versus bonds than they have in a while, but that's because the tech bubble dominates a lot of the last 20, 30 years.
Over the last 100 years, we actually find we're picking on bonds. We're nervous because multiple asset classes look not bubbleish, but pretty darn expensive at the same time. That's what worries me.
That also leads into, if a lot of the world, be they institutions that need formal forecasts of what they're going to make in their portfolio  or my dad. My dad is planning his retirement, 20, 30 years ago. Always had the same sheet of paper. He never showed it to me, but it was, how much do I need to retire?
I'm pretty sure it was off by a factor of 10. I don't know which direction it was off on. My dad was a trial lawyer. It skips a generation. He's not a math guy. But I'm sure he had, how much I need to live on, what I think I can make on my money. The number that fell out was how much he needed to retire.
People still do that. Institutions do it very formally. They make forecasts of what they're going to make in their portfolio. I'm sure there's a lot of my dads out there. They probably use a spreadsheet now.
If they're using anything like history and if we're right that high prices on both stocks and bonds lead to lower than normal returns, it doesn't have to be a bubble. We don't need to see a crash. We don't need to see a fix. But, they're using too high of an assumption.
It's a problem going forward, and it makes the whole retirement problem a bigger problem.
COWEN: We're going to come back to finance, but there's a segment of these conversations always where we do overrated and underrated. So I toss out something, and you give me a short answer. Is it overrated or underrated?
ASNESS: We already have a problem. I'm not good at short answers. But, OK.
COWEN: Bitcoin.
ASNESS: Correctly rated.
COWEN: Correctly rated. In science fiction, the author Robert Heinlein.
ASNESS: Early stuff, underrated. Later stuff, overrated.
COWEN: What's your favorite?
ASNESS: That is a really  Methuselah's Children.
COWEN: Ah, good pick.
ASNESS: I could have gone with the obvious. I'm a bit of a libertarian. I could have gone with, The Moon Is A Harsh Mistress. It's his most famously libertarian book.
COWEN: But it doesn't age so well.
ASNESS: No, no. I like Methuselah's Children.
COWEN: Ben Bernanke.
ASNESS: Fairly rated.
COWEN: Fairly rated.
ASNESS: Actually, overrated by half the world dramatically, and underrated by half the world dramatically. And it might be partisan.
COWEN: Reality TV.
ASNESS: I know I'm modifying it every time. I'm destroying the spirit of your question.
COWEN: That's fine, that's fine. No, no.
ASNESS: I used to think horribly overrated. I never watched any reality TV. I have 11- and 12year olds, and we watch Shark Tank, a reality TV business show. We watch the old Survivors. Just basically teaching game theory to 11year olds in a sneak way. This guy's doing this, so this guy does this.
I will say fairly rated, and much better rated than I gave it credit for in the past.
COWEN: Now, you've told me you're a hockey fan. Wayne Gretzky. Overrated or underrated?
ASNESS: Oh, he's massively, highly rated, and still underrated.
COWEN: What do people miss?
ASNESS: I think hockey fans don't miss this, but the general public misses that he was a guy who was undersized, less fast  slower. We have a word for less fast.
[laughter]
ASNESS: Than other people. This is cliche-sounding, but the people always used to say he was a humble guy. People would say this about him, he skated where the puck was going to be, while everyone else was skating where the puck was. Having watched him, I just think it was true.
I don't know how he got that ability. It's a mutant ability, but he had it.
COWEN: A bit like a momentum investor.
ASNESS: Yes.
COWEN: Now, I think I'm interested in this issue, as I think you are. Extreme performances or performers, and it's measured most readily in sports. So Gretzky is a kind of extreme outlier. In basketball, you could say Kareem AbdulJabbar who would be in the series as an outlier. Maybe Michael Jordan.
In sports or some other area of your choosing, which is the extreme outlier which strikes you as the most amazing? And you just say, ""oh my God, I can't believe there's a Wayne Gretzky,"" or a fillintheblank-there for me, other than Gretzky.
ASNESS: I have no sense if this is actually accurate. But actually, no one could measure this. It can't be accurate. You're not going to believe what I'm going to say. Cirque du Soleil.
COWEN: Please explain.
ASNESS: When I sit there and watch Cirque du Soleil, which both my wife and I like, I literally walk out and go, ""nobody can do this."" And I don't think they are cheating.
COWEN: They're not cheating, right?
ASNESS: But watch it again. It's like a Looney Tunes show, where Daffy Duck dives from up there into a little thing of water down here, and he doesn't die. I don't know how they do it.
Everything else, the crash of '87 was a 20 standard deviation event. Nothing. Wayne Gretzky, pretty good. The Cirque du Soleil people 
COWEN: Off the charts.
ASNESS: This story was from Vegas, and it's not staying in Vegas. But, I was in Vegas, and I was exercising. I know you find that hard to believe, but I was.
The Cirque du Soleil people were in the gym, and you don't want to ever do that. It is one of the most demeaning, humbling experiences.
[laughter]
ASNESS: They exercise exactly as  they did this thing where they just keep leaping over each other, and they go around in a circle, and they did it for like half an hour. And I'm sitting there on the StairMaster on a three.
COWEN: SpiderMan versus Batman? Who wins that fight?
ASNESS: Batman wins every time, because unlike most superheroes, he cheats violently.
[laughter]
COWEN: SpiderMan versus Batman? Who wins that fight?
ASNESS: Batman wins every time, because unlike most superheroes, he cheats violently.
COWEN: Superman races with Flash. They both travel at the speed of light. Yet Einstein tells us there are no simultaneous events. Who wins?
ASNESS: I'm going to ignore the physics, much as the comic books do. This is actually a pet peeve of mine. I'm more of a Marvel Comics than a DC Comics. I know everyone wanted to know that, in the audience.
DC is much better now, but when I was a kid, they exaggerated all the powers much more. Marvel had realistic superpowers. You could run at 500 miles per hour, not the speed of light. DC would go with the speed of light.
You have no idea. Actually, you might, but this is one of the things comic geeks will fight about. And they've had this, about five times in the comics. They've had races between them. Of course, they try to cheat and make them tie. I know you tried to come up with that.
I subscribe to a theory that is on the Internet. It has a name, but I've forgotten the name, but it's a documented theory. But it says the Flash should win, because the specialized power should win.
COWEN: I agree. Related to portfolio theory, in fact. Right, in equilibrium 
ASNESS: Portfolio theory, comparative advantage.
COWEN: Absolutely.
ASNESS: Though it has a little bit of a ""the world must work out fairly"" notion to it.
COWEN: The world does work out fairly, right?
ASNESS: Long-term.
COWEN: Long-term. Very long-term. Mutual funds, overrated or underrated.
ASNESS: Oh, we run mutual funds. This is a hard one.
COWEN: Other people's.
ASNESS: I can't be fired, so I will go overrated. Active management I think is overrated. I believe certain things can win. I've talked about a few of them, but on average I think people try too hard to beat the market and pay too much for it.
I love if people listen to me. I believe in what I'm saying. But if you go spend your life listening to man named Jack Bogle, you won't do terribly.
COWEN: Super practical question. You're sitting in this room or listening on YouTube. Let's say your income is two or three times the national median. So you can save some money, but you cannot operate investment at a significant scale. What's the mistake those people are most likely to make, and what should they do to stop making it?
ASNESS: Most likely it's a hard one. You already said it.
COWEN: In expected value terms.
ASNESS: Overtrading, and I don't mean everyone. In fact, I'd say a minority are daily stock pickers watching the market.
But there is the phenomenon that I still want to look into more, because someone has to be making this money. This is going to sound stupid. No one's figured out who's making the money. But Jack Bogle quotes these numbers a lot.
There are all these paradoxes where the average mutual fund investor seems to get out and in at the wrong times. Remember I talked about value and momentum?
COWEN: Of course.
ASNESS: I like to call them  it's a geeky phrase, maybe I'm the only one who likes it. They are momentum investors at a value time horizon. Remember, I told you value works long term. You have to hold 3, 5, 10 years. Momentum is a 6 to 12 month horizon.
If you're going to be momentum, you've got to really do it. You've got to be disciplined. You've got to come in every day, and you've got to count on these under- and overreaction things.
If you wait five years and buy what's worked for five years, you can call that a negative value investor or a momentum investor working with the wrong numbers. I do think that is one of the things people do too much out there. It's probably the biggest 
Somebody is making that money. Maybe we're making some of that money. Maybe it's the flipside. It's very hard to track.
COWEN: Sure.
ASNESS: I don't think anyone's done a great job of nailing where that money lands. But I think if people came up with good strategies and somehow disciplined themselves to do far, far less.
The worst case is if someone  I don't think many professional traders can make money trading in and out constantly. I think pretty much nobody in an expected sense  of course, some will get lucky  should do that casually. So, if you're doing that, you're making a giant error.
But if you're even looking at it and going, ""I used like this, but the three to five, it's been tough. Get me out."" If you're getting out because you feel sick to your stomach about it, you're making a mistake.
COWEN: What should we do? Here, I'm leaving the ""we"" deliberately ambiguous, to make securities trading more just, more fair. You can pick the ""we"" you want.
ASNESS: Well, just and fair, this is loaded terms, of course.
COWEN: Of course, deliberately loaded.
ASNESS: A finance guy comes in here and starts being wishywashy about the terms just and fair, I'm worried already.
COWEN: You get to fill in the blanks.
ASNESS: I don't know if I'd call this just or fair, the fact that people make this error. They're hurting themselves. I don't really attribute a value judgment. I wish they didn't. It would probably cost us some money. I think the world would be better off, but I don't know if that's just or fair.
I think the world has gotten more just and fair. I'm going to say something potentially controversial. Something that is often attacked, the high frequency trading, has made the world more just and fair, particularly for small investors.
High frequency trading has made the world more just and fair, particularly for small investors.
High frequency traders have  I watched the Republican debates last night, so I know to change the topic to something I'm comfortable with. They do a lot of different things, but the core trading strategy is just to do the other side of whatever you want to do.
So, if you want to trade, sell X, they'll buy it from you. They charge a little thing called the bidask spread. They will buy it from you for a little less than they'll sell it to you for. It's very competitive. They fight with each other to do your trade, so they can't just charge any bidask spread they want.
They get attacked because they charge a bidask spread, and when you ask them to do a lot, they start moving the prices, because they're getting scared you might know something they don't know.
But we've always had to trade with someone else. We've always needed market makers out there. It used to be much more expensive. Worse bidask spreads, worse execution, particularly for the small person.
There is some controversy with high frequency. I mean small investors, not literally small people. You know that, right?
For very large investors, you're trading very large amounts of money. I believe high frequency has made our trading costs cheaper, but there's at least an argument for the other side. Some will say, market impact. The price moving on you when you try to execute and buy or sell a lot of a stock is bigger now. I don't think so, but that's a fair argument.
There's no argument for the little guy. What you worry about in trading is something called front running. Someone figuring out what you're doing and doing it before you. There's the illegal version, where someone actually gets a peek at what you're doing, which they're not supposed to get.
Then there's the completely legal version. People notices trades occurring and prices moving, and think, ""Oh, I better get in front. Maybe this is a wave."" There's nothing illegal about that, but it still costs you money.
No one bothers, no one front runs a small dollar investor. And it's not because people are nice, and kind, and care about  there's no money. You want to rob banks, not people. So, there's no money in it. The small investor, I think, unambiguously has a fairer, cheaper world.
Now, I'll dig myself a hole. I don't think they should trade very much. That was our earlier question.
COWEN: Of course, overconfidence.
ASNESS: Just because it's cheaper doesn't necessarily make them better off if cheaper induces more trading. That's an entire different  but they're getting cut a fairer deal by Wall Street. Whether they use that to harm or help themselves, open question. But I think they're getting cut a fairer deal than used to.
COWEN: High frequency trading, it's getting us back to Superman versus Flash, right? You liked that question.
ASNESS: I have made this observation many times. It is literally the only part of my field where the speed of light is relevant.
COWEN: That's right.
COWEN: Hedge funds. Your company is much more than a hedge fund. You've written a lot on hedge funds. You know them very well.
For most people, is it worth it? The data on hedge fund returns, I've put a lot of time in trying to what is actually the net return. Forget about the risk-adjusted return, but just the net return. How much comes from linear and nonlinear strategies?
It makes my head spin. It confuses me in the kind of way where as a naive outsider, I get a little scared. What should I think of hedge funds and how good they are? Overall, again not a question about what you're doing.
ASNESS: Sure.
COWEN: But in the end 
ASNESS: I will try to separate those two. It's hard sometimes.
I think if you have to go buy one of every hedge fund that will take your money, which is a subset of hedge funds. Some of them are closed. You'd probably be better off figuring out what their average exposure to the stock market is and go buying an index fund.
I live in Greenwich, Connecticut. In some parts of the world, if you said, ""my daddy runs a hedge fund,"" I'd say, ""what's a hedge fund?"" In Greenwich, Connecticut, the kids say, ""what kind of hedge fund is your daddy running? Is he event arbitrage? Trend following? What does dad do?""
In some parts of the world, if you said, ""my daddy runs a hedge fund,"" I'd say, ""what's a hedge fund?"" In Greenwich, Connecticut, the kids say, ""what kind of hedge fund is your daddy running? Is he event arbitrage? Trend following? What does dad do?""
I am going to be persona non grata, but I think hedge funds  and there is a lot of complexity to this answer. They are a universe of smart people, they are doing some good strategies, some I mentioned to you, already. They seem to have grasped the momentum strategy, not so much value oddly enough. But they seem to definitely incorporate the momentum strategy.
There are so-called arbitrage strategies  they don't use the word like academics. Academics or almost academics like me, use it to mean riskless profits. They mean, a trade that has reliably worked over time where they go long and short, fairly similar things, they are clearly not riskless. But, something like a merger, A is buying B, if the deal closes, it's going to go to here. A is going to fall, B is going to rise.
The day it is announced, it only goes to here, because there is some chance this deal does not happen, antitrust, the market, shareholder activism, somebody else. What the merger arbitrageur  and my finest achievement today is saying that word in front of you, that's a hard word to say  buys B, and sells A, and if it happens, they make a little money, and if it fails, they lose a lot of money.
I am dying to do this, I have not done it yet, I have talked about it for two years, I am about ready to try it. I want to ask one of my two older kids, they are a set of twins, they are 12 years old, ""Does this sound like a good idea to you?"" I'd have to hold their attention throughout this whole thing.
There's about a 98 percent chance they say, ""No. That sounds like a terrible idea to me, you can lose a lot, you can make a little. Who wants to do that?"" I'd be the proudest pop on Earth if either one of them kind of paused and said, ""how often do both of those two things happen, Dad?"" Because, that's the proper question.
It turns out, if you do this rather with zero skill, you do every merger that ever comes along. Maybe you can do better, maybe not, but you just do this every time, you've made a lot of money over time. You get killed occasionally, you are basically selling insurance, when the deals don't happen, you lose a lot of money.
Hedge funds have figured that out, there are lot of other things they figured out like this, that's the good part. The bad part is they do not, as a group (and keep in mind, this is self-serving, but we run things people would call hedge funds, not all of our business by any means, we think we are not doing this, we don't think we are the only ones giving clients the fair deal, I am talking about the industry as a whole) doesn't hedge enough.
I know that sounds stupid given the name, but if anyone likes geek numbers like correlation, for the last seven years, an index of hedge funds has been about 0.8 correlated with the S&P 500. That means if you tell me what happened in the S&P 500, I got a pretty good idea what's happening in the hedge funds.
The word ""hedging ""almost by definition refers to removing that risk. Trying to create returns that go up on average but at different times than stocks. You can get that again from Mr. Bogle for about 11 basis points, near a tenth of a percent.
They don't hedge enough and they charge a lot. I will never  you have a shot, Tyler, I don't have a shot  I will never get an economic law named after me. I gave that up when I went to try to make money. If I got one, I'd want it to be: There's no investment process so good that there's not a fee high enough that can't make it bad.*
[laughter]
""I will never get an economic law named after me. I gave that up when I went to try to make money. If I got one, I'd want it to be: There's no investment process so good that there's not a fee high enough that can't make it bad.""
ASNESS: I do think hedge funds don't hedge away a lot of the risk in return. You can get much cheaper elsewhere and then simply  on average, broad strokes, I'm insulting some people unfairly including myself  but they charge too much.
COWEN: Here's a historical question, but it can be about recent history. Who is the individual who has done the most to promote liberty who is undervalued in this regard?
[pause]
ASNESS: Has to be someone fairly terrible in my mind because it has to be a counterexample. I'm going to go with Joe Stalin.
COWEN: Please explain.
ASNESS: It's a pretty good example of what happens when you don't have it. Some of us might think it's a more relevant example than others. I'm not revealing anything that you might not know.
But I think counterexamples are probably more powerful than anything else. That counterexample of what happens when you take liberty away will be with us for a really long time. I don't think we're near there yet. I might be a raving lunatic but I'm not that much a raving lunatic.
I wouldn't have wished it. It wasn't worth the cost but he's helped the cause of liberty. Thank you, Joe.
COWEN: The contrarian answer. What's the side of Ayn Rand's philosophy that you feel is weakest?
ASNESS: Economics. I'm going to get yelled at by every libertarian friend I have on earth. I've never been a very big gold standard person. I respect it. I respect it, I have friends who are fanatic believers in it. I don't think it will be the disaster to a lot of people  but I don't think it cures all ills.
I have a lot of friends who I agree with who'll say ""we have too much regulation,"" I'll go ""check."" They go, ""We have a byzantine crazy tax code that often  not just that they're high  they create a lot of odd incentives for trade-offs that shouldn't exist."" I'll go ""check.""
""We should have hard money."" ""No check,"" and they think that would fix everything. I don't fully get it. Ayn Rand, the name of her philosophy was Objectivism and she just told you it was objectively right that gold is the standard of value. She's unkind to silver frankly.
[laughter]
ASNESS: You have succeeded in getting me making fun of Ayn Rand, that's very impressive. But when she ventures into economics like that and makes very bold, strong statements, I don't agree with them all.
COWEN: What's her most underappreciated side or aspect or angle?
ASNESS: Again, I'm going to try to flip it around. I don't think she was as anti-helping people as she sometimes comes off. If you read her talk about it she certainly  I disagree with her on this, by the way  she didn't consider charity a primary virtue. But she didn't have a problem with it whatsoever. She considered the individual sovereign and ""if that's important to you, do it.""
I think it's a larger virtue. Benevolence, charity is one of the things in her world  and my favorite thing, which you didn't ask about her, is ""you own your own life."" It's one line. I'm in love with that.
But the idea of a virtue being the desire to help other people  not someone forcing you to, which she was dead set against, but wanting to help other people  I disagree with her on. But I think people come off and think that she's snarlingly against it. I don't think she was for it enough, but I think she was rather passive and said it.
If you care about it, she talked about examples  giving up your own life to save someone else. If you value that person more than yourself, it's rational. Do it. So I don't think she was quite as nasty about that. Nastier than I think she should have been, but not quite as nasty.
And she could have used an editor. I admit that.
[laughter]
COWEN: Absolutely.
ASNESS: Come on. Come on. That speech  oh my Lord.
[laughter]
COWEN: For this conversation, I read all of these papers of yours, which is the tradition. I've read your Wikipedia page. I know you not well, but some modest amount. What is there in your life that's influenced you that I would have no idea about from what I've read by you and about you? What's the hidden influence on Cliff Asness that I don't see, maybe others don't see?
ASNESS: That's a hard one. I was probably wrong about this, and my parents are going to get mad at me. But we were by no means I'm not telling a poverty story. I love Marco Rubio, but if I hear one more time about the frigging bartender  his dad is a bartender. It's a wonderful story, but it's in every answer. But we grew up decidedly middle class, and my dad  he was a trial attorney. He had a job where some years he made a fair amount of money and some years he made no money.
My parents shared that way too much with me. I've told them that. I had a sense of impending doom as a child that I think was oddly a positive for achievement. It made me very focused on not being nervous about those kind of things, but doesn't make you a happy, relaxed person and is impossible to turn off after it's no longer useful.
My parents shared that way too much with me. I've told them that. I had a sense of impending doom as a child that I think was oddly a positive for achievement. It made me very focused on not being nervous about those kind of things, but doesn't make you a happy, relaxed person and is impossible to turn off after it's no longer useful.
COWEN: There's a literature by Ulrike Malmendier, you may know these papers, which try to argue that the risk premium in a given generation depends on exactly what economic conditions they grew up with. Do you think this is generally true, or just about you?
ASNESS: I've written probably a much more empirical paper on precisely this idea. I promise I'll get there. I'm going to get to the point.
There's this idea. There's something called the Fed model for valuing stocks that says when interest rates and inflation are low you should pay a higher PE for stocks.
In theory it's a very weak model because it deals with what are called nominal interest rates, not real interest rates. Unlike bonds, when inflation's low you expect earnings to grow slower.
Forget all the math. There's this puzzle that the world seems to follow the Fed model. They price stocks according to it. It's a very strong empirical regularity. When interest rates are low those PEs are higher and vice versa  though they do vary.
One thing we found  I wrote this as a Financial Analysts Journal article circa  I wrote one in 2001 and a followup in 2004  that how much more they demand in return, or how much cheaper they need stocks to be  when they're cheaper they return more  how much excess return they need on stocks versus bonds is a function very strongly of the last 20 years' relative volatility of stocks and bonds.
In English, I called 20 years a generation. I didn't monkey around with that too much. I checked. It works for 10 years, it works for 30 years. It's not cherrypicked. But if the last 20 years had experienced a wild ride on stocks versus bonds they demanded a very high return, going forward. I got there, if you didn't notice.
I've written on this. I believe it. You can't make a lot of money, by the way, trading on a 20-year phenomenon. Clients don't really enjoy the whole, well, I've been wrong for 19 years, but give me one more year.
I never give the short answer, but the short answer is I found the same thing myself. I think it's directly reflected in the numbers. I think people are somewhat a prisoner of their experience.
COWEN: Last question from me before we get to questions from the group. If policymakers could understand one thing better about financial markets that they don't understand now, what would you want that thing to be, and why?
ASNESS: I'd want them to understand that any form of near certainty without certainty-any time you convince the world that something is a certainty but it's not, it's the most dangerous time humanly possible.
I've looked back at the financial crisis and the key moments in it. A lot of arguments. I'm not even going to get into the partisan arguments. The right says government did it. The left says Wall Street did it.
Great shocks. You know what did it, if I had to pick one thing, that one primary cause of the financial crisis? The assumption that real estate prices can't go down. The government made this assumption, the people who say these terrible quantitative models were way off.
At the end of the day, somewhere in this giant model, in a thousand lines of computer code, there was, ""What's the worst case 10year return for real estate?"" If that worse case was not losing money, it's garbage in, garbage out. You can have the best model in the world. That's a problem.
When Lehman failed, we went into a huge spiral, because people were pretty much convinced that the government wouldn't let anyone fail.
When money markets, when the famous Reserve Fund broke the buck  this is money markets that are supposed to return you a dollar for a dollar. It's always been a fiction, by the way. You've been lied to for years.
Money markets own portfolios of short-term bonds that move in value. They allow them to round to (I could be off by a decimal place) to only two decimal places. That's not a lot. Two decimal places for short-term securities means most of the time, almost all the time it rounds to a dollar. Therefore, there's an illusion, but they're risky.
That is to me a very dangerous asset, because it tells people there's no risk when there's actually is risk. I'm not saying you have to go out a billion dollars. No one wants an NAV. What's your NAV? Pi. No one wants that.
But it's so short, it artificially looks stable. When you tell the world there's risk in something, and then bad things happen, it's not fun. It's still bad things. But, they tend to deal with it much better.
Many people have observed, the Internet tech bubble that I keep talking about, when that came down, the economic consequences, the threats to our system were far more benign. I think that's because no matter how crazy they might have gone, nobody thought they're utterly riskless. They didn't act as a group as if there was no possible problem.
Equity losses are expected. Bond losses are not expected. So I will say this. If you truly can take all the risk out, great. If you tell everyone it's risky and it's risky, great. I think the thing people don't appreciate is how dangerous things are that you think protect you, but only mostly protect you.
I think the thing people don't appreciate is how dangerous things are that you think protect you, but only mostly protect you.
COWEN: We're having a forum here Monday with Greg Ip.
ASNESS: That's a great example.
COWEN: It's on the illusion of safety. We're going to have a full session just on this.
ASNESS: Some people argue  I don't know what the data is  that football players would be safer if they didn't wear helmets, because they would know this was dangerous. They point to sports that are very violent, like rugby and whatnot.
That might be true. You can take this logic too far, right? Maybe we all drive more aggressively because we're wearing a seat belt, and we know we can hit the breaks and we won't go through the windshield. I'm not going to sit here and say that's a bad idea.
The effect still exists, by the way. You probably drive a little too aggressively, and you probably have an extra accident or two because of the seat belt. My logic doesn't mean it's always bad to take preventative action.
But, if you thought as we sometimes do in finance, like money market funds, that you could do anything in a car because you're wearing a seat belt, that's kind of the money market analogy I'm making. I think those are the most dangerous things.
COWEN: Thank you very much, Cliff, for those remarks.
AUDIENCE MEMBER: I hate to name drop, but I was out last week with Mr. Rubenstein of Carlyle. He was asked a question about retail distribution of alternative assets. He said, ""It doesn't matter to us at Carlyle Group, because we can raise money quite readily. But, it will probably go that way.""
Now AQR, you've lead with mutual funds. Do you think in the next few years, due to changes in regulation, that the alternatives industry will do more distribution to non-accredited investors? How will the industry handle that opportunity?
ASNESS: The short answer is, I think yes. Remember, I think these strategies can be used very usefully, but I don't think the hedge fund industry has broadly delivered them on fair terms to investors.
When you look at the mutual fund industry  and they're often called liquid alts, the hedge fund-like strategies that have started to appear in the mutual fund world  and, we do some of these.
I think they've largely been replicating some of the same problems. I think they're not fully, as a group, and we're in there, and I clearly like ours, but as a group I think they're not fully hedged and are probably still too expensive.
I do think that same intellectual battle will go on. But I think there are still reasonable strategies at the core doing reasonable things, and not everyone is in the same position as Mr. Rubenstein. A lot of people actually do want more assets. So, I do think that will get bigger.
AUDIENCE MEMBER: I wanted to ask you to reconcile momentum or any of these strategies related to the passive versus active debate. If you consider that all active strategies summed up effectively are passive  so any active strategy, including something like say momentum or value investing requires not just that someone not do it, but that they actually be on the opposite end of the trade.
So, who's on the opposite end of the trade? I mean, it is individual investors that are trying to stock pick, or are they other stupider professional managers?
ASNESS: We explicitly use that word.
[laughter]
AUDIENCE MEMBER: How confident can you be that there will continue to be the steady supply of stupider investors on the other side of the trade so that you continue to make money?
ASNESS: That's a great set of questions.
Backing up, you're not going to believe me, you're going to think I'm just copying you. But myself and a colleague, Antti Ilmanen  he's Finnish, he didn't just have odd parents  have been planning, we haven't written it yet, to write a paper with the literal title, ""Who is On the Other Side?"" Just a little shorter version of what you said, because we do think that is a very disciplining question.
I've written  I wrote something in the Financial Analysts Journal on 10 different things in finance that I thought were kind of interesting, short observations. One of them was your point precisely, where I said people think if they follow systematic strategies like we do, even low turnover  value happens to be what's called a low-turnover strategy  momentum changes its mind. You have to trade momentum more than value.
So, systematic and low turnover, they'll call passive. Mainly a fight about semantics, but I don't like that term because, to me, like I think for you, passive should be something we can all do. If we all try to do value, we can't all do it.
Value, even if it's systematic, simple to explain, works on average, still requires exactly what this man just said. If you're overweight cheap stocks, somebody has to be overweight expensive stocks.
I will say a lot of it gets back to the exact conversation I started out with Tyler. There are two possible reasons someone can choose to be on the other side of you. You have to start out right there, thinking about both of them.
One is what you're talking about works, my use of the word ""works,"" for risk reasons again. Same thing. Cheap stocks are inherently riskier. There is some scenario where they get killed in a depression, and that's risk.
You are willing to bear that risk. Someone else is not. They willingly and consciously, maybe implicitly  we do a lot of things in economics that just kind of happens even though we don't say it  but they in some sense willingly take a lower return because they don't want that risk.
Right there, might be true, might not be, but it's a perfectly logically valid story for who is on the other side.
The other is hope springs eternal for value. There are a number of people who simply see whatever has been going on.
Who's been beating earnings for the last few years? Whose products have been popular? That company probably should be worth more. They go too far. They overextrapolate. The behavioral story, the other version besides risk.
Both those stories, you're exactly right, do require someone on the other side but no means  I'm you asked  do I think these things could be used for everyone to outperform?
This is not Lake Wobegon. We can't all beat the index. It's actually a precise mathematical identity.
Having said that, I think there are risk premiums and there are behavioral biases that lead some to willingly or accidentally underperform. You have to have the story for each one.
Your other question, about why would it persist, Tyler asked a version of it, too. I think they can persist because they're pretty good but not extremely good.
We look at this, the value effect. It's been a good five years or so for the set of these three or four anomalies I've talked about  value, low risk, momentum, profitability.
It's been a bad five years for value. The pricing of cheap versus expensive is not egregiously weird, but it's about historically normal when they've on average probably looked a little too cheap if you're a behaviorist.
What happened? Of all these three or four I've talked about, that's the one that's been around the longest. It's very hard to arbitrage this away. Somebody is on the other side. Value goes through some horrible periods and hope springs eternal.
I do think it's a great question because you must always ask that question. I put it even starker sometimes. It's not a good title for a paper but, ""whose money are you taking?"" You know the old joke if you're at a poker table for five minutes and you haven't figured out who the sucker is, it's you.
[laughter]
ASNESS: Same answer. If you cannot say, ""whose money is being taken?"" Maybe, again, it's perfectly rational. They don't feel taken. They feel like their risk is being reduced. It's fair.
Why am I making this extra money? If you don't ask your question, you're not doing your job.
AUDIENCE MEMBER: Assuming it's behavioral and not risk, do you have a sense that the other side is individual stock pickers or other professionals that are stupid?
ASNESS: I think it's some of both. There is some evidence, for instance, I mentioned 
Remember, I stuck this in, that hedge funds seemed to have figured out and incorporate. This is just empirically  if you look at their returns, look at what strategies they're correlated to and not, they seem to show that they've figured out some of the momentum strategy.
They seem to buy more expensive, not cheap stocks. Maybe they buy the right ones, maybe they figure it out, but they are fighting the value effect. I think at least some of it is coming even from the very ""smart"" investors.
Other restrictions, like, remember, we talked low-risk investing for the Fisher Black reason, being effective because people are restricted from leverage. Professional mutual fund managers have that restriction left and right.
Whether they do it consciously or they're just led to it, they get pushed into higher beta stocks. They get pushed into taking more risk and probably overpay for them.
I like that example better than we're talking mom and pop's money, but that's probably in there, too, to some extent. But, keep in mind, I'm being nice. I'm advising mom and pop not to trade. They should go to Jack.
AUDIENCE MEMBER: I have a question specifically about the biotech and healthcare sector. It's traditionally outperformed the broader market over the past decade or so.
What are your thoughts both as a momentum trader and from a fundamental standpoint? Does that train continue or is it time to get out?
COWEN: Just to calibrate here, we have two more questions and 10 minutes. So, everyone, please time your answers and questions to make it all fit perfectly to the split second.
ASNESS: I'll do this one quickly then. [whispers] I have no idea.
These techniques, not only do they work on average over the long term, but you need a broad crosssection all the time. They're really, really bad at things like, ""What do you think of this sector?"" I don't know the answer, first of all.
This will be the third. I told you active management is too expensive, hedge funds are not a good idea. Now I'm going to tell you I don't know if we're overweight or underweight biotech. Everyone write that down.
I actually like telling people this. When I go on something, I don't do it too often, like TV, we coach them. Don't ask me about individual stocks. We're long and short thousands of things based on these quantitative measures, and we're doing that intentionally.
You could be cheap, good momentum, profitable, low beta, and the CEO could have a scandal tomorrow. These are statistical averages. You want to spread your bet. You want to make a lot of them.
A quantitative systematic manager like me shouldn't know a lot. I could go memorize all 5,000 positions. Biotech, I have no idea, but I think it's instructive why I have no idea.
If I have an idea, worse, if I have a very strong opinion, I'm just doing my stuff wrong. There are people who may or may not be good at that, but you do not come to a systematic quant manager  if your systematic manager says, ""here's what you do: put it all on biotech""  run.
AUDIENCE MEMBER: I'm wondering who is your favorite superhero? And has you studying economics changed how you felt about certain superheroes?
[laughter]
ASNESS: Oh my God. This may be sappy patriotic, but I've always liked Captain America. The whole he fought in World War II, suspended animation for 20 years, which, by the way, happens to anyone who falls into cold water.
[laughter]
ASNESS: That's just a throwaway. I could sing you the song if you'd like. ""When Captain America throws his  "" anyone old enough to remember that? ""All those that chose to oppose his shield must yield."" That's a great rhyme.
Even the most insane billionaire cannot afford a hundredth of what frigging Tony Stark or Bruce Wayne have. It's infuriating.
I've done well. I'm not the most insane out there. But if I wanted to go build a Batcave at my house, it would take approximately 600 times my wealth, and everyone would know about it.
[laughter]
I've done well...[b]ut if I wanted to go build a Batcave at my house, it would take approximately 600 times my wealth, and everyone would know about it.
ASNESS: It's a shockingly good question, which actually has been an annoyance of mine. I have a skyscraper that's also a missile silo.
[laughter]
COWEN: Your least favorite superhero if I may interject?
ASNESS: Before the movie, I would have said AntMan. But the movie was kind of funny.
There was a character named Hank Pym, one of the original Avengers. Avengers were, of course, as you all know, Thor, Hulk, Iron Man, Giant-Man, which is Hank Pym, and the Wasp. He is a loser.
Giant-Man was just a big guy. He wasn't even stronger than a regular guy! He was just big! Everyone beat up Giant-Man.
Then he used the same powers to shrink  and control ants, of course, because those go together. Then, he became an alcoholic, and then he hit his wife in the comic books. That's easy to hate. But I hated him even before the spousal abuse.
COWEN: Last question here.
AUDIENCE MEMBER: Question on advising people to get into the passive, low-cost, Vanguard kind of funds.
We've seen people heeding this advice and flowing in. Do you think that in and of itself  more people going to passive strategies  could open up more potential for active managers and more anomalies?
ASNESS: That's a great question. I'm going to fuse that question with one over here, not the superhero one. My favorite question I might add, but it cannot be fused with this question.
Instinctively, you want to say it's going to be easier to beat the market if fewer people are trying.
If you're a PhD student in finance, this is what you do at 3 AM in your bull session is, ""what happens if everyone indexed?"" No one who has gone through a PhD program in finance or probably economics has not done that. We really don't know. People will actually argue over this.
Then you try to get a little more realistic. What if almost everyone indexed? Feels very obvious that it would be easier to win.
On the other hand, this constraint that was brought up already, that the average can't beat the average, whose money are you taking? If everyone else is passive, how do you induce them to take a bet away from passive?
My sense  I think you could literally mathematically disprove this, so it's a sense of what would happen if we got close, not all the way  is it would easier and you could fool people if you were the one with some information. Information would be easier to get. The informationless trader, you could push away more by just bidding more for their stock. In the short term, you could capture some of their profits.
Still hard to make the math work. Because if you really push them away, they have to tilt away, and they're trying to be passive. If everyone else is trying to be fully market cap weighted, whom do you trade with? No one is willing to underweight.
You've actually brought up a paradox that people are still fighting about.
I think in a more realistic scenario  I do know this  more people chasing my strategy is not good for me.
But, in general, it's really hard to figure out. But a question everyone talks about, so I'm glad you ask, so I could fail at it also.
COWEN: Cliff, in one of your papers, you cite an old Slovenian proverb, which I quite like. It goes, ""Speak the truth, but leave immediately after.""
[laughter]
COWEN: I do think you'll be here for just a few more minutes if we have not been able to get to your question  but not for hours. Anyway, Cliff, we thank you heartily.
ASNESS: This is a lot of fun. Thank you all.
[applause]
COWEN: Just for all of you, our next event, ""Conversations with Tyler,"" will be January 26th. We are honored to have as our guest Kareem AbdulJabbar. We will cover a wide variety of topics. So please put that on your calendar.
ASNESS: You will have an answer to a trivia question. What do Cliff Asness and Kareem AbdulJabbar have in common? Because this will be the only thing.
[laughter]
COWEN: Supernormal returns, right?
* Cliff's economic law has been edited for clarity.
The Mercatus Center gratefully acknowledges the assistance of T.J. Whittle in curating most of the hyperlinks found in this transcript.
A podcast in which esteemed economist Tyler Cowen engages...
96 
1
",169
https://medium.com/@casey_rosengren/paypal-froze-our-funds-then-offered-us-a-business-loan-49a078310fb?source=tag_archive---------7-----------------------,"Paypal froze our funds, then offered us a business loan","About a year ago, we made the decision to use Paypal as the primary payment method for our company, Hacker Paradise. After about a month...",Casey Rosengren,3,"About a year ago, we made the decision to use Paypal as the primary payment method for our company, Hacker Paradise. After about a month, they froze all of the money we had in the account  around $30,000.
For context, our company runs trips around the world for developers, designers, and entrepreneurs who want to work and travel. Last year, we had vendors all over the world, in places like Bali, Barcelona, Thailand, and Tokyo, so it wasn't too surprising that our account got flagged.
We reached out to Paypal's startup evangelist, and with his help, were able to get $10,000 released. Since we were a risky account, they said they needed to hold on to the remaining $20,000 for up to six months.
We run a low-margin ops-heavy business, so we started scrambling to make up the gap in our cash flow. With personal funds and a healthy dose of luck, we managed to keep most our vendors happy.
A few months later, however, we noticed something surprising. Advertisements started popping up on our account for ""PayPal Working Capital"" business loans:
After exploring a bit, it became clear that we were being offered a business loan based on our Paypal volume. Unfamiliar? Basically, they offer you a loan based on sales history, which you then pay back over time as a percentage of any sales done on the account.
On one hand, Paypal was saying that our account was too risky for them to release the $20,000 they'd taken as collateral. On the other hand, they were saying our account was credible enough to offer us a loan.
We chose to approach this in the spirit of Hanlon's Razor: when in doubt, assume ignorance, not malice. So, we emailed a screenshot of the ad above to the startup evangelist, assuming it was some kind of clerical error.
After a few weeks, we received the following response from someone on the Paypal Working Capital team  the gold is in paragraphs 2 and 3:
To paraphrase: ""It may seem weird that we're holding collateral on your account and offering you a loan, but that was not a mistake! You're still eligible. Contact me if interested.""
For a new business, freezing $20,000 in funds could be catastrophic. In our case, we were able to bridge the gap with personal funds in order to stay out of the red. However, a young, cash-poor company could be left with no other option than to take Paypal up on their offer. Paypal causes a cash-flow crisis and then offers to fix it.
It's unclear whether they've changed their policy, but we have stopped seeing ads for loans. This may be because our account has a lot less activity on it nowadays... thank you, Stripe :)
For those interested, the CEO of Paypal announced in October that they've lent over $1 billion to small businesses through Paypal Working Capital loans.
This happened back in September, and our funds were just released, which is why we're publishing now (we didn't want to make waves while they were still holding $20k of our funds).
While the overall experience with Paypal was quite poor, we really appreciate the efforts of the startup evangelist we spoke with, George Kartakis. It was clear he was doing everything he could to advocate for us internally, but his hands were mostly tied when up against Paypal's bureaucracy.
We're extremely grateful to him for helping to negotiate down the amount of capital they kept as collateral. As a young, low-margin business, the amount he was able to get released made a huge difference.
",170
https://medium.com/dragonfly-research/a-visual-explanation-of-algorithmic-stablecoins-9a0c1f0f51a0?source=tag_archive---------5-----------------------,A Visual Explanation of Algorithmic Stablecoins,"Stablecoin usage has exploded in the last year. And yet, fewer and fewer people seem to understand how these stablecoins actually work.",Haseeb Qureshi,8,"Stablecoin usage has exploded in the last year. And yet, fewer and fewer people seem to understand how these stablecoins actually work.
For some reason, stablecoin creators are obsessed with making these designs seem impenetrably complex. Almost every white paper is mired in equations and newly invented jargon, as though their authors are trying to convince you: trust me, you're not smart enough to understand this.
I don't agree. At bottom, all stablecoin design is pretty simple. I'm going to show you a simple visual language to understand how all stablecoins work.
Think of each stablecoin protocol as a bank. They each hold assets and owe liabilities. Each of them captures value somehow and distributes that value to ""equity"" holders.
Consider a normal full-reserve bank.
On the left side are its real assets  the actual physical dollars it holds in reserve. On the right side are its liabilitiescall them ""digital dollars""which are claims on the assets in reserve.
In a full-reserve bank, each liability is matched 1:1 with assets in reserve. If someone with a digital dollar asks for the cash back, the holder is given the physical dollar and the corresponding digital liability is destroyed. This is how Tether, USDC, and every other fiat-backed stablecoin works.
The equity of the bank belongs to shareholders  investors in the bank  and they make money from the fees the bank charges. In Tether's case, the owners of Tether Ltd. are the shareholders, and their profits come from the Tether minting and redemption fees.
Every liability of a full-reserve bank should maintain a close peg to a dollar, since it's always redeemable for $1 in reserve. So long as the bank maintains cheap convertibility, arbitrageurs will ensure this effortlessly maintains its peg.
So that's a vanilla full-reserve bank. It's an obvious model, but it will help illustrate how crypto banks are different.
How would you create a crypto full-reserve bank whose liabilities are stable dollars?
Given crypto just reinvented money, the first thing you'd want to do is swap out the USD assets for crypto assets. But crypto is volatile, so 1:1 backing won't work if your liabilities are in dollars. If the value of crypto goes down, the bank will be left undercollateralized.
So just do the obvious thing: put down an extra cushion of crypto to give you a buffer in case the crypto goes down.
This is basically how MakerDAO works.
Dai's peg is currently stable.
Notice that the assets in reserve are significantly larger than the total liabilities (Dai). This keeps the whole system secure.
(I'm glossing over a bunch of details here. But for the purposes of comparing MakerDAO to other models, this is a good start.)
Now let's look at Synthetix.
Synthetix takes a different approach: instead of holding a diversified basket of cryptoassets, Synthetix issues its sUSD stablecoin against a pile of its own SNX token. This SNX is also the ""equity token""  in other words, the only asset Synthetix allows as a deposit is its own equity. Because SNX is so volatile, Synthetix demands 600% overcollateralization for each sUSD in circulation.
sUSD's peg is currently stable.
Both MakerDAO and Synthetix are analogous to traditional full reserve banks, except they are overcollateralized because their assets are in crypto. On some level, their pegs are secure because there is some mechanism to redeem the stablecoins into their underlying assets. (In both, there's also a system of interest rates that targets a desired price.)
However, there's another kind of stablecoin commonly known as ""algorithmic central banks.""
Algorithmic central bank stablecoins aren't redeemable at all, and don't have depositors in the traditional sense. This makes them less like traditional banks and more like central banks. (Central banks tend to use methods other than redeemability to keep prices stable.)
Each algorithmic central bank works in a slightly different way. To analyze an algorithmic central bank, we'll try to understand what it does in the two important scenarios: when the stablecoin is above the peg, and when the stablecoin is below the peg.
Structurally speaking, perhaps the simplest algorithmic central bank is Fei.
Fei launched recently to much notoriety, although it almost immediately broke its peg. Here's how Fei works in a nutshell:
FEI's peg is currently broken.
Fei functions much like a real central bank, defending its peg directly in the market. Note that Fei is not meaningfully overcollateralized and most of its assets are in crypto. This means that in a black swan event, Fei's assets could significantly drop below its liabilities and leave it unable to defend its peg.
While the above animation gives you the high-level intuition, Fei's real mechanics are quite involved. Fei uses Uniswap for all of its trading activity, and employs a technique called ""reweighting"" to perform its actual trades. It also uses ""direct incentives"" (effectively a type of capital control).
But the net effect is the same: the protocol participates in the open market to push the price toward the peg.
A similar algorithmic central bank is Celo protocol, which produces a stablecoin called Celo Dollar (cUSD). Celo Dollar uses CELO as its reserve collateral (the native asset of the Celo blockchain), along with a diversified portfolio of other cryptocurrencies.
Like FEI, the Celo protocol is continually willing to buy and sell Celo Dollars in the market, using a Uniswap-style market. The Celo reserve was initialized with significant assets in reserve, and the reserve aims to always stay overcollateralized. If Celo's assets ever dip below 200% of its liabilities, the system attempts to re-capitalize by collecting transaction fees on CELO transfers.
Thus, the main difference between Celo and Fei (besides its trading mechanics) are the assets it holds and its rules around collateralization.
Celo Dollar's peg is currently stable.
A third stablecoin in the same family is Terra's UST. It is collateralized by Luna, the native token of the Terra blockchain. Like FEI and Celo, the Terra protocol acts as a market maker for the stablecoin. If the stablecoin system runs out of assets, it restocks by inflating the native LUNA supply.
UST's peg is currently stable.
FEI, Celo, and Terra do not allow redemptions. Instead, they market make their own currency in the open market (that is, they are willing to buy or sell across a spread).
On the face of it, this seems quite different from redeemability! But it's actually a closer continuum than it seems. This is because a credible commitment to market making is economically identical to allowing mints and redemptions.
Imagine a stablecoin, collateralized by ETH. Call them STBL tokens. The protocol is always willing to market make the ETH/STBL pair. This means the protocol will be willing to sell 1 STBL for $1.01 ETH and buy 1 STBL for $0.99 ETH. If STBL is below the peg, it will keep swapping STBLs until its ETH runs out.
If STBL instead uses mints and redemptions, it might let anyone mint 1 STBL for $1.01 of ETH and redeem 1 STBL for $0.99 of ETH. If STBL is below the peg, it will keep redeeming STBLs for ETH until its ETH runs out.
This has the same net result!
In traditional central banking, being a market maker rather than allowing redemptions allows the central bank more discretion. But algorithmic market making is different, because smart contracts can make ironclad, self-enforcing commitments. In this light, market making and redeemability are two paths to accomplishing the same goal: providing liquidity and ensuring a tight peg.
We've now looked at ""central bank"" style stablecoins. But there is another type of algorithmic stablecoin that is significantly more exotic: Seigniorage Shares.
The classic ""Seigniorage Shares"" stablecoin is Basis Cash, based on an unlaunched predecessor called Basis. It is perhaps the quintessential algorithmic stablecoin, from which many other designs were later derived.
Here's how Basis Cash works (this one's long, so it's a video instead):
Basis Cash's peg is currently broken.
You can think of Basis Cash as working in two phases: when there are outstanding bonds, Basis Cash is in a contraction cycle. The money supply is not growing fast enough to pay off all the system debts. But if demand continues to increase, eventually all bonds will get paid off and the system will enter into an expansion cycle, wherein shareholders are once again getting rewarded with newly minted Basis Cash.
Newly minted Basis Cash are the ""seigniorage""the profit the central bank gets to make from issuing new currency.
Normal central banks keep the seigniorage on their own balance sheet for a rainy day. Basis Cash, on the other hand, pays all seigniorage to its shareholders the moment it receives any.
You can intuitively see that this makes Basis very ""collateral-efficient."" Basis literally has no assets on the balance sheet! This allows it to support a very large high stablecoin supply on 0 assets. But this also makes it susceptible to ""death spirals"" or confidence crises, as in fact happened to Basis Cash.
It's important to understand how Basis Cash works. Most later algorithmic stablecoins are descendants of the Basis design, including the final stablecoin we'll examine.
Empty Set Dollar (ESD) is a fair-launched stablecoin with a pseudonymous founding team. The original version of ESD, now known as ESD v1, was based closely on the Basis Cash design.
ESD v1's peg was broken and they have since pivoted to a new design.
ESD's innovation was to fuse the ""share"" token with the ""stablecoin"" token. This means the stablecoin, if staked, produced more stablecoins. As you might guess, this resulted in the stablecoin becoming highly volatile and drifting away from the peg, sometimes as high $2.00, until it finally collapsed to below $0.20.
So far, pure Seigniorage Shares coins have universally failed. The many Basis and ESD knock-offs like DSD all met the same fate. This tells us, at the very least, that stablecoin design really matters. These illustrations should help you reason through why Seigniorage Shares is so vulnerable to confidence crises.
In the early days of DeFi, many believed that decentralized stablecoins were fundamentally impossible. For now, it seems that these claims were premature. There is a large design space, and some designs are genuinely more robust than others.
But one thing is for sure: you shouldn't assume a decentralized stablecoin will be robust simply because a white paper insists it is. Think for yourself what it takes for that stablecoin to be stable. (And if you're confused, try drawing a picture. At least for me, it helps.)
Disclosure: Dragonfly holds positions in many of the assets discussed in this piece.
Original crypto research and analysis
1.5K 
18
",171
https://towardsdatascience.com/algorithmic-trading-bot-python-ab8f42c37145?source=tag_archive---------6-----------------------,Algorithmic Trading Bot: Python,The rise of commission free trading APIs along with cloud computing has made it possible for the average person to run their own...,Rob Salgado,8,"The rise of commission free trading APIs along with cloud computing has made it possible for the average person to run their own algorithmic trading strategies. All you need is a little python and more than a little luck. I'll show you how to run one on Google Cloud Platform (GCP) using Alpaca. As always, all the code can be found on my GitHub page.
The first thing you need is some data. There are a few free sources of data out there and of course sources that cost money. I'll be using the TD Ameritrade API which is free. The next thing you need is a trading platform where you can submit commission free trades through an API.
For that I'll be using Alpaca. Alpaca also allows paper trading (fake money) so we can test out our strategy in the wild without bankrupting our family . Then you just need a way to run your bot automatically and store/retrieve data. For that we'll use GCP because that's what I'm familiar with but any cloud platform (AWS, Azure, etc.) will work just as well.
Oh and of course you need a trading strategy. This post is about setting up the framework to run a trading strategy so the strategy itself here isn't important and not a focus. For demonstration purposes I will be using a momentum strategy that looks for the stocks over the past 125 days with the most momentum and trades every day.
You SHOULD NOT blindly use this strategy without backtesting it thoroughly. I really can't stress that enough. You SHOULD NOT take investment advice from me, you will most likely be sorry .
The first thing you need is a universe of stocks. I'll be using all the stocks listed in the NYSE. To get the symbols for those stocks, we'll scrape them from eoddata.com. Then we can request the data for each of those stock symbols from the TD Ameritrade API.
Once we have the data, we'll store it in a BigQuery (BQ) table so we can get it later for our strategy. This will all be run in a cloud function that we can then schedule to run every weekday after the markets close to get the latest closing price.
I store the API credentials in a text file on Cloud Storage so they are not hard coded. We just retrieve them from there with an API call. Then we get the date to use to check if the market is open. Then we scrape the NYSE stock symbols and pass them to the TD Ameritrade API to get the day's data.
Then we store that data in a BQ table through the API to use later for our bot.
I'm only using the closing price but the API returns a lot more data so it's a good idea to just store it all. I created a dataset called 'equity_data' and the table will be called 'daily_quote_data'. If the table doesn't exist (i.e. the first time you're doing this) the table will be created and then every day, the new data will get appended to that table.
In GCP you can create a Cloud Function with this script. To schedule this Cloud Function to run at a set time, simply choose 'Cloud Pub/Sub' for the trigger option and create a topic.
Then go over to Cloud Scheduler and set up the topic to run when you want it. Here we are setting it to run every weekday at 5pm eastern. The frequency is set in unix-cron format. The payload is just a message that will be sent and can be anything you want but it is required.
It's also a good idea to set the timeout of the cloud function to the max of 540s to...well avoid timeouts. This can be found under the advanced options section.
This will download the data going forward but we're also going to need back data for the trading bot. The endpoint I'm using here is the 'quotes' endpoint which does not provide historical data. To get historical price data you have to use the 'pricehistory' endpoint. I provided a file in the GitHub folder which for that called 'get_historical_data.py'. You can run that file locally and then download the dataframe into a csv and upload it to a BQ table.
At a basic level, the trading bot needs to be able to:
The entire cloud function is on the longer side so I'll summarize it here but the full code is on my GitHub. Like I said, the strategy isn't important here and I am using a simple momentum strategy that selects the ten stocks with the highest momentum over the past 125 of days.
First we download the historical data into a dataframe for the momentum strategy from the BQ API:
Then we get the current positions from the Alpaca API and our current portfolio value. They have an API wrapper which I'm using here. The credentials again are stored in a text file on cloud storage. Notice that the base url we are using is for paper trading. You can set any amount in your paper trading account, here I set it to $10K. Obviously if this is your first time running this you won't have any positions in Alpaca, so before you run the cloud function, just run the script locally to get your initial portfolio based on the momentum stocks you choose. Then send those tot he Alpaca API to buy them. I'm assuming here you already did that.
Now that we have the historical data and the amount we have to trade with, we can select the stocks based on our strategy. The first step is to identify the stocks with the highest momentum.
The momentum calculation is from the book Trading Evolved from Andreas F. Clenow which I would recommend. It's very easy to follow and has lot's of different code examples in it for different types of strategies.
The way it works is that it calculates a linear regression for the log of the closing price for each stock over the past 125 days (minimum number of days is 40). The next step is to make it easier to relate to. It takes the exponent of the slope of the regression line (tells you how much percent up or down it is by day) and then annualizes it (raise to the power of 252 which is the number of trading days in a year) and multiplies it by 100. That is then multiplied by the r squared value which will give weight to models that explain the variance well.
After we identified the top 10 stocks with the highest momentum score, we then need to decide how many shares of each we will buy. Portfolio allocation is a whole topic in and of itself so I won't get into it here as it's not important. To allocate here I am using the pyportfolioopt library.
We now have a df with the stocks we want to buy and the quantity. Now we need to figure out if we need to sell any stocks based on what is in our current portfolio. It's possible that:
We need to check for all those things and make any necessary sales or buys. First we'll check to see if there's any stocks in our current portfolio that we do not want anymore.
Now we have a dataframe with any stocks we want to sell and the quantity we need to sell. Next we'll check to see if the quantity of any stock we currently own has decreased. Again, there may technically be no changes here so we need to check if there are. This will give us a final dataframe with all the stocks we need to sell.
Now that we have the full list of stocks to sell (if there are any), we can send those to the alpaca API to carry out the order.
Finally, we need to see if there are any new stocks we currently own that have increased in quantity or if there are any new stocks we want to buy today that we didn't own yesterday.
If there are any we need to buy, we send those orders to the API.
It's also a good idea to log the portfolio once we're done. Alpaca only allows you to have a single paper trading account, so if you want to run multiple algorithms (which you should), you should create a log so you can track them on your own. We can create a strategy column to identify this strategy from others. Then we can simply add that to another BQ table.
The below SQL query will give you the daily totals with the percent change compared to the previous day for your portfolio.
That's the bot. You can now schedule it to run everyday in a cloud function. This should give you a good framework in which to run your own trading strategies.
",172
https://historyofyesterday.com/chernobyls-blown-up-reactor-4-just-woke-up-74bedd5fc92d?source=tag_archive---------2-----------------------,Chernobyl's Blown Up Reactor 4 Just Woke Up,Scientists don't understand why...,Andrei Tapalaga ,4,"The nuclear disaster that occurred in 1986 will forever be remembered, but the world will soon have a reminder of the event as the zone for some reason (yet unexplained by scientists)is becoming more radioactive. For those who may not be aware of the incident here is an article to get you up to speed.
Underneath reactor 4 there is still nuclear fuel that is active and which will take around 20,000 years for it to deplete. The uranium is too radioactive for anyone to live in the city and since the incident, the European Union had created a shield around the reactor which should not allow for the radioactive particles to come out.
""Chernobyl officials presumed any criticality risk would fade when the massive New Safe Confinement (NSC) was slid over the Shelter in November 2016.""
""The 1.5 billion structure was meant to seal off the Shelter so it could be stabilized and eventually dismantled.""
However, many other parts around Chernobyl have also been affected due to prolonged exposure, some more than others, and many of them have not been contained as they were not presenting any major radioactive activity until now. Neil Hyatt, a nuclear chemist from the University of Sheffield had mentioned that there is a possibility for the uranium fuel to reignite on its own.
Hyatt also offered a simple explanation on how this is possible, just like charcoal can reignite in a barbeque, so can nuclear materials that have once been ignited. He as well as a handful of nuclear chemists have mentioned previously the possibility of the uranium from Chernobyl to reignite, but the scientists from Ukraine that are responsible for managing the nuclear activity within the vicinity never really listened, until now.
Scientists from Ukraine have placed many sensors around reactor 4 that constantly monitor the level of radioactivity. Recently those sensors have detected a constant increase in the level of radioactivity. It seems that this radioactivity is coming from an unreachable chamber from underneath reactor 4 that has been blocked since the night of the explosion on the 26th of April, 1986.
The experts from Ukraine don't really understand why this is happening although they do have a hypothesis. Water is used to start the fission process within nuclear materials, this makes the nuclear material release energy that within a nuclear reactor can be maintained under control, but in this instance, the experts are afraid they will not be able to control it.
Another hypothesis is that since reactor 4 has been completely shielded, no water from the rain was able to reach the nuclear fuel. The water from rain may have been what kept the nuclear material under control. With no water, the nuclear fuel may be at risk of overheating, leading to another nuclear disaster.
There may be another reason for this constant increase in radioactivity, what has been mentioned above are only hypotheses, maybe something totally different is occurring under reactor 4 or within the nuclear material left inside. This is something that definitely should ring some alarm bells in order to prepare for the worst sort of situation and hopefully the world's smartest in the field of nuclear chemistry can come together to identify the problem and come up with a potential solution.
pubs.rsc.org
www.science.org
andrei-tapalaga.medium.com
From the times that the pyramids were raised to the end of...
8.9K 
55
8.9K claps
8.9K 
",173
https://medium.com/illumination-curated/the-unexpected-case-of-the-disappearing-flu-64fd1fa5e909?source=tag_archive---------2-----------------------,The Unexpected Case of the Disappearing Flu,Some endemic respiratory viruses on hiatus during the 2020-21 season,MG Sunde,7,"Some endemic respiratory viruses on hiatus during the 2020-21 season
During the months preceding the surge of SARS-CoV-2 infections this fall and winter, many public health officials expressed concern about the potential for a ""double-barreled"" respiratory virus season. In this scenario, healthcare facilities would be totally overwhelmed by: 1) patients afflicted by infections caused by endemic respiratory viruses (such as influenza) that occur during any normal year, and 2) a massive influx of coronavirus patients. Fortunately, such a catastrophe did not come to pass. The reason for this is an unprecedented reduction in flu prevalence for the 2020-21 season.
Before proceeding, it should be noted that not all acute respiratory infections during the so-called ""flu season"" actually result from influenza viruses. In fact, influenza viruses account for only a minority of such infections in many cases, with the causative pathogen commonly going unidentified. Other viruses causing the same general suite of symptoms include: rhinoviruses, coronaviruses, adenoviruses, respiratory syncytial viruses, among others.
Many public health experts have made statements regarding the reduction in influenza cases this season. These experts have attributed the phenomenon to such things as flu vaccinations, hand-washing, mask-wearing, and social distancing. However, these explanations are largely implausible for the following reasons:
4. If the wearing of masks was capable of almost entirely removing influenza from circulation, as has been observed, then this approach would also eliminate SARS-CoV-2. Indeed, SARS-CoV-2 virions, which range from ~50 to 200 nm, are similar in size to those of influenza (~80 to 120 nm), adenoviruses (~90 to 100 nm), and other endemic betacoronaviruses, such as HCoV-OC43 and HCoV-HKU1 (~118 to 140 nm), which share the same genus as the novel virus. Since SARS-CoV-2, influenza, and various other respiratory viruses are largely spread by fine-particle aerosols in indoor settings, an intervention that works for one should logically work for the other. But in spite of the stringency of interventions, SARS-CoV-2 cases have skyrocketed. At the same time, infections of endemic betacoronaviruses, which should respond to NPIs in the same manner as SARS-CoV-2, have also largely disappeared:
However, the presence of adenoviruses, which are also spread via aerosolized particles, have been consistently detected throughout the coronavirus pandemic. If mask usage were almost entirely removing influenza and coronaviruses from circulation, why not SARS-CoV-2 and adenoviruses, which share similar virion sizes and modes of transmission?
It is clear that attributing the current disappearance of multiple influenza and coronavirus types to NPIs is not logical, but what plausible explanations exist for this phenomenon? Is some previously unknown biological dynamic at play? Not exactly.
In his seminal work ""The Transmission of Epidemic Influenza"", Edgar Hope Simpson provides some discussion regarding such disappearances, which have been observed throughout history. Coined ""The Vanishing Trick"", he writes:
So perhaps a biological process, whereby viruses engage in some form of competition, or interactions, can better explain disappearances such as those currently being observed.
Subsequent research has borne out real world examples related to the phenomenon described by Simpson. According to a group of researchers at Yale, it is likely that a 2009 autumn rhinovirus epidemic interrupted the spread of influenza. The authors of that study write: ""one respiratory virus can block infection with another through stimulation of antiviral defenses in the airway mucosa"". Results from another study, conducted in mice, support those findings. Mice were infected with either a rhinovirus or a murine coronavirus, and it was found that both attenuated influenza disease. Moreover, it was observed that the murine coronavirus infection reduced early replication of the influenza virus. In another study, negative interactions between noninfluenza and influenza viruses were suggested. According to the authors: ""when multiple pathogens cocirculate this can lead to competitive or cooperative forms of pathogen-pathogen interactions. It is believed that such interactions occur among cold and flu viruses"". A recently published study examining the effects of interactions between an adenovirus and influenza in mice suggested that certain respiratory infections could impede ""other viruses' activities within the respiratory tract without attacking unrelated viruses directly"". Finally, in a paper entitled ""A systematic approach to virus-virus interactions"", the authors state: ""increasing evidence suggests that virus-virus interactions are common and may be critical to understanding viral pathogenesis"".
Since the current disappearance of various endemic influenza and coronaviruses cannot logically be attributed to NPIs (for the reasons previously discussed), biological phenomena (interactions between viruses), which are currently not well understood, present a far more plausible explanation.
Citations
Anderson, M. L., Dobkin, C., & Gorry, D. (2020). The Effect of Influenza Vaccination for the Elderly on Hospitalization and Mortality. Annals of Internal Medicine, 172(7), 445-452. https://doi.org/10.7326/M19-3075
Morawska, L., & Milton, D. K. (2020). It Is Time to Address Airborne Transmission of Coronavirus Disease 2019 (COVID-19). Clinical Infectious Diseases, 71(9), 2311-2313. https://doi.org/10.1093/cid/ciaa939
Nishiura, H., Oshitani, H., Kobayashi, T., Saito, T., Sunagawa, T., Matsui, T., ... Suzuki, M. (2020). Closed environments facilitate secondary transmission of coronavirus disease 2019 (COVID-19). MedRxiv, 2020.02.28.20029272. https://doi.org/10.1101/2020.02.28.20029272
Doshi, P. (2013). Influenza: marketing vaccine by marketing disease. BMJ : British Medical Journal, 346, f3037. https://doi.org/10.1136/bmj.f3037Chen, N., Zhou, M., Dong, X., Qu, J., Gong, F., Han, Y., ... Zhang, L. (2020).
Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study. The Lancet, 395(10223), 507-513. https://doi.org/10.1016/S0140-6736(20)30211-7
Mondelli, M. U., Colaneri, M., Seminari, E. M., Baldanti, F., & Bruno, R. (2021). Low risk of SARS-CoV-2 transmission by fomites in real-life conditions. The Lancet Infectious Diseases. https://doi.org/10.1016/S1473-3099(20)30678-2
Wu, A., Mihaylova, V. T., Landry, M. L., & Foxman, E. F. (2020). Interference between rhinovirus and influenza A virus: a clinical data analysis and experimental infection study. The Lancet Microbe, 1(6), e254-e262. https://doi.org/10.1016/S2666-5247(20)30114-2
Fennelly, K. P. (2020). Particle sizes of infectious aerosols: implications for infection control. The Lancet Respiratory Medicine, 8(9), 914-924. https://doi.org/10.1016/S2213-2600(20)30323-4
Zhu, N., Zhang, D., Wang, W., Li, X., Yang, B., Song, J., ... Tan, W. (2020). A Novel Coronavirus from Patients with Pneumonia in China, 2019. New England Journal of Medicine, 382(8), 727-733. https://doi.org/10.1056/NEJMoa2001017
Makela, M. J., Puhakka, T., Ruuskanen, O., Leinonen, M., Saikku, P., Kimpimaki, M., ... Arstila, P. (1998). Viruses and bacteria in the etiology of the common cold. Journal of Clinical Microbiology, 36(2), 539-542. https://doi.org/10.1128/JCM.36.2.539-542.1998
Killingley, B., & Nguyen-Van-Tam, J. (2013). Routes of influenza transmission. Influenza and Other Respiratory Viruses, 7 Suppl 2(Suppl 2), 42-51. https://doi.org/10.1111/irv.12080
Gonzalez, A. J., Ijezie, E. C., Balemba, O. B., & Miura, T. A. (2018). Attenuation of Influenza A Virus Disease Severity by Viral Coinfection in a Mouse Model. Journal of Virology, 92(23), e00881-18. https://doi.org/10.1128/JVI.00881-18
Demicheli, V., Jefferson, T., Ferroni, E., Rivetti, A., & Di Pietrantonj, C. (2018). Vaccines for preventing influenza in healthy adults. The Cochrane Database of Systematic Reviews, 2(2), CD001269-CD001269. https://doi.org/10.1002/14651858.CD001269.pub6
Thomas, R. E. (2014). Is influenza-like illness a useful concept and an appropriate test of influenza vaccine effectiveness? Vaccine, 32(19), 2143-2149. https://doi.org/10.1016/j.vaccine.2014.02.059
Payne, S. (2017). Family Coronaviridae. Viruses, 149-158. https://doi.org/10.1016/B978-0-12-803109-4.00017-9
Lynch 3rd, J. P., & Kajon, A. E. (2016). Adenovirus: Epidemiology, Global Spread of Novel Serotypes, and Advances in Treatment and Prevention. Seminars in Respiratory and Critical Care Medicine, 37(4), 586-602. https://doi.org/10.1055/s-0036-1584923
DaPalma, T., Doonan, B. P., Trager, N. M., & Kasman, L. M. (2010). A systematic approach to virus-virus interactions. Virus Research, 149(1), 1-9. https://doi.org/10.1016/j.virusres.2010.01.002
Tang, S., Mao, Y., Jones, R. M., Tan, Q., Ji, J. S., Li, N., ... Shi, X. (2020). Aerosol transmission of SARS-CoV-2? Evidence, prevention and control. Environment International, 144, 106039. https://doi.org/10.1016/j.envint.2020.106039
Tang, D.-C. C. (2020). Paradoxical modulation of influenza by intranasal administration of non-replicating adenovirus particles. PloS One, 15(11), e0241266-e0241266. https://doi.org/10.1371/journal.pone.0241266
Payne, S. (2017). Family Coronaviridae. Viruses, 149-158. https://doi.org/10.1016/B978-0-12-803109-4.00017-9
Vajda, J., Weber, D., Brekel, D., Hundt, B., & Muller, E. (2016). Size distribution analysis of influenza virus particles using size exclusion chromatography. Journal of Chromatography A, 1465, 117-125. https://doi.org/https://doi.org/10.1016/j.chroma.2016.08.056
Nickbakhsh, S., Mair, C., Matthews, L., Reeve, R., Johnson, P. C. D., Thorburn, F., ... Murcia, P. R. (2019). Virus-virus interactions impact the population dynamics of influenza and the common cold. Proceedings of the National Academy of Sciences, 116(52), 27142 LP  27150. https://doi.org/10.1073/pnas.1911083116
""The Transmission of Epidemic Influenza"". 1992. Edgar Hope Simpson.
The National Respiratory and Enteric Virus Surveillance System (NREVSS). https://www.cdc.gov/surveillance/nrevss/index.html
Adenovirus. Clinical Overview. https://www.cdc.gov/adenovirus/hcp/clinical-overview.html
ILLUMINATION-Curated hosts outstanding stories of advanced writers covering 100+ topics.
259 
10
",174
https://medium.com/@garrettgee/how-i-plan-to-design-my-body-d9bacd331598?source=tag_archive---------9-----------------------,The Diet & Workout Plan of a Full-Time Traveling Family  HIS,"Over a year ago, we sold everything for an adventure around the world! Eating clean and body building can be difficult, especially when..",Garrett Gee,7,"***NOTE*** I originally wrote this on January 1, 2015 ..and it has been a work in progress ever since. Each year, I make updates with new goals, progress pics, and lessons learned from the past year.***CLOSE NOTE***
INTRODUCTION : We are a full-time traveling family. Over a year ago, my wife and our 2 children sold everything and left our home in Provo, Utah for an adventure around the world. Before we left, and throughout our travels, health and fitness has always been a top priority for our family. Of course, eating clean and working out is difficult, especially when you travel full-time with kids! So here is our diet and workout plans that we travel with! Mine is different from my wife Jessica's, so we wrote them separate. His and Hers ;) Below is mine. Here is the link to hers : https://goo.gl/IVtuLR
[Jan 1, 2015] Here's the plan : I'd love to get more fit. Like the absolute most fit my body is safely capable of. I don't know what that fully means but I'm curious and motivated to find out. What does my body feel and look like at its very best?
[Jan 1, 2015] I currently work as a mobile app designer and spend a giant block of my days sitting behind a computer screen. Not ideal for a healthy body but I make special efforts to work it out. Ever since my early days in design school, it confused me why other designers (*stereotype alert*) seemed to care very little about the shape of their body. In my mind, our body is a ""blank canvas"" for us to shape and care for.
Perhaps the best project you will ever work on is you.
For me, there are generally 3 reasons for my health efforts:
 General health
 Athletic performance
 Body shape/aesthetics
Thanks to active hobbies and a progressively nutritious diet, my general health has been very good. As a collegiate soccer player, most of my workouts are based around performance. So really, for the first time ever, I'm giving more focus to the 3rd point : Body shape and aesthetics.
Obviously, achieving a bigger/better body is only going to get harder as I get older. I'm 25(ish;) now and feeling a ""now or never"" self-induced pressure. The nice part is, the sooner I'm able to achieve a higher level of fitness and health, the more of my life I'll be able to enjoy it.
So, here we go. 2015 is the year! This decision to change now will surely change my life forever! Some buddies are joining me and if you're feeling motivated, you can too. I've done my best research and work to prepare..
I've done crazy research in preparation for this. Side note : back in 2010 I actually got certified as a personal trainer through NASM. That gave me a good general foundation but that's about it. The body building industry is filled with debate. There are definitely numerous paths to similar end results. It seems every day there is a new trend, a new supplement, or a new ""ground breaking"" discovery. I spent most of my time focusing on the basic principles that seem to be proven and agreed upon across the board.
3 Proven Principles:
- Diet : Diet makes all the difference. It can fully boost or fully block all your hard work and development. Here are lists for good supplements and also other good foods to shop for. Also, here is a rough idea of what I'm eating on a daily basis : Meal Plan /// 4-5000 calories (based on my goal of clean healthy weight gain).
- Variation : No matter how high quality a workout is, if you never switch things up your body will eventually plateau. Variables like sets, weight, muscles, speed, intensity, lifts, and rest. Here is the list : The 7 Variables for Body Building
- Rest : This may be the most difficult part for me. Honestly, my body functions and feels good off little sleep. 4-5 hours and I feel good to go. I enjoy waking up early and having long busy days without a nap. But, I need to remember my goals and the importance of quality sleep. Here's the recipe : Muscle growth is stimulated when you workout. Muscle growth is fueled when you eat. Muscle growth happens, when you sleep!
No crazy shortcut supplements or fads. Just a consistent pattern of the right nutritious diet, hard work, enough sleep, and then even..more..hard..work.
I have constructed a specific MON-SAT workout plan that evolves every month. The variables and exercises change to stimulate progress and avoid plateauing. I have a dietary plan to properly fuel growth and recovery in the best ways using only natural and organic supplements.
Below is the template I designed for my monthly workout sheets.
I specifically designed my workout sheet so that each week's numbers would stack on top of each other. This makes it quick and easy for me to see what weight I lifted the week before, giving me a clear goal to beat. Having such a solid plan helps me be more efficient in the gym, pushes me to do more, and helps me appropriately space out days for rest between muscle groups.
Here is a link to view and download each month's workout sheet : https://goo.gl/oiH7Pw
A lot of the resources I studied emphasized the importance and benefits of ""publicly declaring your goals and plans."" So..I guess I've officially talked the talk :/ My resources also suggested posting my ""BEFORE"" pics. Not reeeally sure how I feel about some of these pics floating around the interwebs so we will take it slow and see how it goes. Unfortunately I definitely care about the opinions of others..so be nice! I will continue to add more as the year progresses..or until someone leaves a rude comment..
2015 : After 8 months of consistent hard work were rewarded with an adventure to Tahiti with my best friends/workout buds!! [video above]
[Jan 1, 2017] We started this fitness journey on Jan 1, 2015. At the time, I was playing soccer in school and working on my startup company. After selling my company and dropping out of school, we decided to sell everything and travel the world as a family. Although our lifestyle greatly changed, our dedication to fitness and health has remained constant. We prioritize every where we go to make sure we have a gym and healthy food available. Some places just don't have any sort of gym available but we still exercise first thing every morning using body weight and bands. We travel with 1 suitcase (of our 4 total) dedicated to a blender and supplements. We don't see it as exercise getting in the way of our travels. Rather, we find exercising in different countries a unique way to further experience cultures all around the world!
GOAL for 2017 : >180 lbs and <8% body fat.
This project is going to take me a good great long while. I'm hoping the rest of my life ; ) I'm definitely a rookie in many ways so please do send any advice, resources, encouragement or help my way.
Sincerely,
Garrett
",175
https://medium.com/digital-nomad-stories/the-cult-of-work-you-never-meant-to-join-cd965fb9ea1a?source=tag_archive---------8-----------------------,The Cult of Work You Never Meant to Join,Are our most valuable qualities being exploited at work? How our strengths get twisted into forming bad habits that just might kill us.,Jason Lengstorf,12,"Are our most valuable qualities being exploited at work? How our strengths get twisted into forming bad habits that  if we don't change fast  just might kill us.
You didn't mean to end up here. You didn't even see it coming.
It all started with a chance to earn a living doing something you loved. Your dream job. Creating things instead of rotting in a cubicle. You weren't just going to make a living  you were going to leave your mark on the world.
At first, you loved the work; it was challenging and fast-paced. Everyone around you was crazy smart.
You brainstormed in your off time. Took projects home with you. Put in extra hours on weekends. It never felt like overworking because it never felt like work.
You put in way more than 40 hours a week, but who was counting? This was fun.
But weeks passed into months and somehow you ended up here: Working 60 hours a week minimum, usually more. You greet your coworkers, bleary-eyed, half-joking about needing coffee to survive.
The work is still fun, but you don't feel the same passion anymore. Whole days slip by sometimes and you have no idea what happened; you certainly don't have much to show for it.
Your goals outside of work are on hold. You'd love to find out if the Belgians have anything to be cocky about waffle-wise, but you don't have time for a big trip right now. You know you need to get into an exercise routine, but something always comes up and you skip the gym.
""Later,"" you promise yourself, ""I'll get around to it soon.""
You're not exactly unhappy, but something's off. You can't put your finger on it. You've just always felt that there would be . . . more.
You're no longer a free member of society. You've been lured into the Overkill Cult.
The Overkill Cult is a cultural delusion that working 60+ hours each week  at the expense of everything else in our lives  is not only a necessary part of success, but that doing so is somehow honorable.
The insidious thing about the Overkill Cult is that it masquerades as all the things we like most about ourselves: dedication, ambition, follow-through, responsibility.
It tells us to push harder, stay later, sleep when we're dead. It tells us we're never going to get ahead if we don't show up first and go home last.
Cleverly, wickedly, the Overkill Cult persuades us to hang ourselves with our own strengths.
And if we don't break free, we're all going to die.
Balance is the first thing to go once the Overkill Cult has us in its grasp.
For me, it started with my health. I skipped the gym  too busy, I thought. I didn't have time to cook  too busy  so I ordered delivery.
My hobbies went next. Everything that wasn't work fell away  too busy, too busy  until I was on the computer constantly, working. In 2012, I was working 70-90 hours a week.
After that, I lost my social life. Friends knew I wouldn't show up  can't; too busy  so they stopped calling. Some days my only human interaction was ordering coffee.
Then  and this, sadly, is where I finally realized there was a problem  I lost my beard.
At the end of 2012, I landed the biggest project of my career at that point: a Black Friday sales site for a Fortune 100 company.
I was thrilled and terrified. A project like this had the potential to move my company to the next level, and I decided to do whatever it took to make this project the best I'd ever built.
The designers had great ideas, and I sat with them to make sure they were possible on our timeline. We came up with a slick, modern idea built on cutting-edge technology. The client loved it.
Then bureaucracy came into play. The legal department made changes. Brand adherence contradicted legal. Design went over schedule. Way over schedule.
By the time the design was approved, I had a third of the time we'd scheduled. And  since this was a Black Friday site  we couldn't push back the release date. It either launched on time or I was a failure. Period.
Not to be defeated, I powered through four straight days leading up to Black Friday, sleeping maybe six hours total. On Thanksgiving Day I skipped family get-togethers in favor of making the final push.
I was exhausted. Delirious. But, goddammit, I finished the project.
The client was thrilled. The site won a couple Addy Awards. I assume they made a metric fuckton in holiday sales.
Over the next few months, patches of my beard started to turn white. The whiskers became ultra-fine. Then they fell out altogether.
Shortly afterward, I lost my ability to grow a beard entirely  I was left with the unsavory choice between a clean-shaven ""giant fat baby"" look and a creepy mustache.
I had stressed myself out so badly that my body had forgotten how to grow a beard. And for what? So I could work 19-hour days and skip family holidays to meet crazy deadlines?
I was exhausted. My body was failing. I was overwhelmed and unhappy and isolated. I had a mustache, for chrissakes.
I had been guzzling the Overkill Cult's Kool-Aid.
Something had to change.
The telltale signs we've fallen prey to the Overkill Cult's influence are subtle:
We don't join overnight  this is death by a thousand cuts  and once we've joined, we'll probably deny it.
But we've joined. By the thousands, we've joined.
The Overkill Cult's siren song seems like a healthy sense of ambition. ""We have to work hard to get ahead."" It's something we've been told our entire lives.
We're doing what we think is best for the future.
But the Overkill Cult doesn't plan for survivors.
Though the symptoms of the Overkill Cult grow from good intentions, they're short-sighted habits that ultimately do more harm than good.
Let's look at each of the Overkill Cult's telltale signs, and how each of them is a long-term detriment disguised as a healthy work ethic.
Long hours often feel mandatory  it's just part of the culture. We think, ""My boss/coworkers/cat will judge me if I'm not working the same long hours as everyone else. I'll never get ahead if I don't go above and beyond.""
This is just what it takes to make it, right?
Wrong. Incredibly, terribly, spectacularly wrong.
Research has proven over and over again that it's not possible to be productive for more than 40 hours a week. At least not for sustained periods of time. Henry Ford introduced the 40-hour work week in 1914 because he saw  through research  that workers on five eight-hour shifts kept up the highest sustained levels of productivity.
Despite over 100 years of research supporting shorter work weeks, many companies still push for long hours, under the claims of a ""sprint"" or ""crunch time"" period.
The irony comes in when we look at productivity over time. After just two months of 60-hour weeks, productivity goes negative compared to what a 40 hour week would have produced.
Did you catch that?
By working 150% of the hours, you accomplish less in the long run.
Somehow, sleeplessness has become a strange badge of honor. We swap ""war stories"" of sleeping two hours a night with an odd, martyred pride shining dimly in our bloodshot eyes.
I never sleep because sleep is the cousin of death, we murmur drowsily. So many projects, so little time.
But this belief that burning the midnight oil somehow gets us ahead is utterly, tragically wrong.
You're the cognitive equivalent of a drunk driver after being awake for 18 hours. But the problem compounds: if you don't get enough sleep, that level of impairment comes faster the next day. After a few days of too little sleep, you're a drunken zombie.
We wouldn't go to work drunk, so why the hell do we go to work on four hours' sleep, when we're more impaired than if we were hammered?
To make matters worse, sleeping less than six hours a night may lead to an early death. The Overkill Cult is literally killing you.
When we're in the clutches of the Overkill Cult, we feel a stab of guilt when we're not working.
""I'd love to go to this holiday party, but I really shouldn't; this project won't finish itself.""
We fear that any time not spent working is wasted.
The irony is  yet again  science tells us exactly the opposite is true.
Overworking leads to higher stress levels and burnout, which have been linked to increased health risks.
Conversely, time away from work is proven to relieve stress and boost creativity, among numerous other benefits.
Besides, if we accept that the ideal is to sleep 8 hours a night and work 8 hours a day, that leaves us with 8 hours for non-work activities.
Taking time away from work gives us time to recharge. It puts distance between us and our projects, giving us time to remember why we like doing what we do.
We may have been duped into joining the Overkill Cult, but it's not too late to escape.
We've been conned using our own best qualities to develop habits that  even though it seems like they'd make us better  make us worse at our jobs, less satisfied with our work, and less happy in our day-to-day lives.
Leveraging the same strengths the Overkill Cult exploits, we can break free of its clutches and take back our happiness and passion.
After my beard died, I felt the full weight of burnout. I was burnt to a fucking crisp. I realized I could either leave my career altogether, or make some fundamental changes to my lifestyle.
For what it's worth, here are the promises I made to myself that helped me break away from the Overkill Cult.
Before anything else, I had to accept that it's only possible to do 6-8 hours of quality work each day.
Trying to work longer hours will not make me more productive. In fact, working longer hours actually results in me getting less done as time drags on.
I chose the latter, and implemented some radical (to me) strategies for controlling my time. I cut from an average of 70-90 hours a week in 2013 to an average 38 hours per week over the last year.
I expected to see less professional success in favor of better overall balance in my life  a sacrifice I was willing to make  instead I saw better productivity at work: my turn-around times went down and I was more consistently hitting my deadlines.
I was floored at the time, but in retrospect I'm not surprised at all.
Getting enough sleep is beneficial on every level. Yet it was always the first thing I'd sacrifice when life got busy.
Too little sleep wreaks havoc on my ability to think clearly, and that hurts me at work in a big, bad way.
After I cut my hours down, I started sleeping without an alarm. Since I'm not working crazy hours, I close my computer by six or seven in the evening, and by eleven I'm usually in bed, where I read for a bit before falling asleep. I wake up naturally between seven and eight-thirty.
This has changed my life. No bullshit.
Waking up to an alarm before I'm fully rested starts the day in a groggy, stressful way. Waking up naturally after getting as much sleep as my body needs leaves me much happier to be awake, and far more ready to start my day.
This was  and still is  the biggest challenge I faced in breaking away from the Overkill Cult. I love what I do, and I want to get my projects finished. It's easy to rationalize working more hours and skipping activities that keep me from working.
But now I know that taking breaks makes me more productive: time away from work lets my passion and excitement for the work renew itself; taking my mind off of a project allows my subconscious to roll around abstract ideas that result in better solutions; breaks from the job lower my stress levels and boost my creativity.
So I make sure to take time off, even if my gut (incorrectly) tells me it's a bad idea.
I take walks. I leave my phone in my pocket when I'm out with friends or eating my meals. I spend a fair amount of time on my hobbies, like writing and hunting for the world's best cheeseburger.
I'm happier today than I can ever remember being in my life. I feel excited to work on my projects, to pursue my hobbies, and to spend time with people I love.
I'm excited to be alive.
When my beard died in 2013, I feared it was only the first sign of an impending decline in my health that would ultimately kill me. It was a glimpse into my future, and I was terrified that if I didn't change, I was in for a life of isolation, ulcers, alopecia, and an eventual heart attack or stress-induced brain tumor.
By changing my lifestyle, I was able to turn things around. After just a year of balancing my work with the rest of my life, my beard grew back. I lost 30 pounds because I was actually going outside and making it to the gym. I felt more awake, and I became more positive.
When I left the Overkill Cult, everything in my life improved. Not one single thing got worse.
If you've been sucked into the Overkill Cult, know that you're not alone.
You may be facing cultural pressure to keep this crazy pace. You may be struggling with your identity as ""a hard worker"" and feeling that scaling back somehow makes you lazy or useless.
But I promise you  despite the doubts the Overkill Cult will force into your mind  there's a better way. Better for your career. Better for your health. Better for your relationships. Better for your happiness.
You ended up in the Overkill Cult because you're smart, ambitious, and dedicated. But you were misled by your good qualities and turned them into bad habits.
There's a better way, and you're smart enough to pull it off.
Dump the Kool-Aid in the sink. Take back your freedom. Find the happiness and success you were looking for when you started this career.
Close your computer. Go outside. And call your friends; they miss you.
If you're like me, you'd love to get away from the crazy hours and soul-sucking routines of the Overkill Cult, but you don't feel like it's possible.
I was wrong. I just had to trust myself to take the first step.
Wouldn't it be nice to get some balance back? To have extra time every day to dedicate to the things that are most important to you?
I want to help: I've compiled 5 Habits of the Unfuckwithably Productive, and I want to give it to you for free. These are time-tested habits that helped me escape the Overkill Cult; this is how I spend less than 40 hours a week on the computer  while making a living and traveling the world.
Click here to get the free guide.
Did this post make you smile, think, or nod in approval? Please consider clicking ""Recommend"" below  I'd really appreciate it.
Go to https://nomadlist.com/blog
1.4K 
8
",176
https://blog.doctorondemand.com/5-things-to-do-in-the-first-24-hours-of-a-cold-or-flu-f8e3dd9b6cf6?source=tag_archive---------7-----------------------,5 Things to do in the First 24 Hours of a Cold or Flu,Dr. Amy Cottrell,Doctor On Demand,3,"You've all been there. That tell-tale tickle in your throat, the drippy nose that starts a cascade of sneezes, that sinking feeling when you realize, ""Oh, no. I'm sick!"" This early stage of cold and flu symptoms is no time to wait and see how bad it will get. If you treat your symptoms early enough, you may be able to prevent the virus from spreading to others or becoming worse. The first 24 hours are critical.
Here's what I recommend to my patients at the first sign of symptoms such as fever, headache, cough, congestion, sore throat, body aches and chills, and/or exhaustion.
If you don't improve after 24 hours, you may want to consult a doctor. Multiple conditions (eg, flu, strep throat, bronchitis) share similar symptoms. But while your fever, cough, and sore throat may feel the same, each condition calls for a different treatment, and telling them apart can be confusing.
When it comes to the flu, starting antiviral medicine within the first 48 hours can lead to a shorter and milder illness  so it's important to act fast. Doctor On Demand lets you avoid the wait and see a doctor immediately by video, right from your smartphone, tablet, or computer. Early treatment can get you feeling better faster and avoid spreading the virus to others.
Dr. Amy Cottrell is board-certified by the American Board of Family Medicine and is an active member of the American Academy of Family Physicians. She graduated Summa Cum Laude from the University of Memphis with a degree in Biology. She then received her medical degree from the University of Tennessee Health Science Center and finished her residency in Family Medicine at the University of Arkansas for Medical Sciences in Little Rock.
The official blog of Doctor on Demand: solutions and advice...
141 
141 claps
",177
https://betterhumans.pub/the-sweet-spot-for-intermittent-fasting-9aae12a2158c?source=tag_archive---------5-----------------------,The Sweet Spot for Intermittent Fasting,Lower insulin means greater fat loss,P. D. Mangan,5,"Intermittent fasting  the practice of going without food for some (undefined) period of time  has many health benefits. It can help prevent heart disease, speed fat loss, and slow or reverse aging.
There are a number of physiological mechanisms involved. It reduces inflammation and oxidative stress, leads to increased numbers and quality of mitochondria, and increases autophagy, the cellular self-cleansing process.
Many of the beneficial effects are entwined with lower levels of insulin.
The function of insulin is to promote energy storage and the growth of the organism. When insulin is increased, fat is stored in fat cells, and other cells take up glucose from the blood.
Most importantly, when insulin is increased, lipids can't leave fat cells. Since fat loss is all about getting lipids out of fat cells to be burned, losing fat requires some attention to how diet, exercise, and fasting cause insulin to rise or fall.
Take a look at the following graph, taken from a paper by Volek et al. It shows that even small increases in insulin, within the normal range, virtually abolish lipolysis (the breakdown of fat).
This is where intermittent fasting comes in, as one of its effects is to lower insulin levels and thus increase lipolysis. The question is: how long do you need to fast before insulin comes down?
Eating causes insulin to rise. The amount of the rise is dependent on a number of factors, such as type and amount of food eaten and the insulin sensitivity of the person doing the eating.
High amounts of carbohydrates and lower insulin sensitivity cause a greater rise in insulin.
Insulin increases and stays higher for several hours after eating  that is, during the ""fed"" state. The absence of food during fasting lowers insulin.
What duration of fasting lowers insulin and to what extent?
Ted Naiman, M.D., whose great site is Burn Fat Not Sugar, posted the following graph recently (used with permission). The information comes from a paper, ""Progressive alterations in lipid and glucose metabolism during short-term fasting in young adult men"".(1)
Dr. Naiman suggested that ""the sweet spot for intermittent fasting"" occurs between 18 and 24 hours of fasting since this is the time period that sees the greatest drop in insulin and increase in lipolysis  the breakdown of fat.
Eyeballing the graph and comparing it to the one above from Volek shows that at an insulin level of about 40 pmol/L, lipolysis should be proceeding briskly.
There has been lots of guesswork as to how long intermittent fasts should last. One very obvious but usually non-verbalized reason is that most people don't want to fast for very long, and after a certain time most people get hungry. But they do want to lose fat.
A very common duration of intermittent fasting, one that I often do, is about 16 hours. A 16-hour fast involves skipping only one meal, typically breakfast, so it's easy enough. A shorter fast than this wouldn't involve missing even one meal, so we probably couldn't characterize it as a real intermittent fast.
But according to the information above, in terms of insulin level, the fast is only really getting going at 16 hours. Extending it further than this ramps the drop in insulin and the increase in lipolysis.
If you can extend your fast into the afternoon, you get more benefits, and a 24-hour fast would do even more.
One thing I would add to Dr. Naiman's ""sweet spot"" is that duration matters for insulin level. In other words, it's not just a matter of achieving a low insulin level, but how long you stay there. Therefore, longer fasts can be of great benefit  though inconvenient in terms of hunger and normal life.
One of the great benefits of intermittent fasting that I've written about often is that it promotes autophagy, the cellular self-cleaning process that breaks down and recycles damaged molecules and cellular organelles.
During the fed state, when insulin is increased, the rate of autophagy is low. During the fasted state, as insulin drops, autophagy increases dramatically, perhaps 5-fold.
Many of the anti-aging effects of calorie restriction and intermittent fasting come from the increase in autophagy.
A high rate of autophagy is characteristic of young organisms; with aging, autophagy decreases, and this allows cellular damage to accumulate. By fasting intermittently, autophagy rates can be reset to that of a younger person.
As we see above, one of the ways this happens is through lowering insulin.
Exercise improves insulin sensitivity, so for any given fasting stimulus, a person who is in shape will see insulin levels drop and lipolysis commence faster than someone not in shape.
Low-carbohydrate diets cause less of a rise in insulin, so probably someone on a low-carb diet who then fasts will also see a quicker drop of insulin into the range at which it promotes lipolysis and autophagy.
Autophagy is suppressed in the presence of insulin resistance and hyperinsulinemia. (2) This may be one of the main mechanisms behind the deleterious effects of diabetes and obesity.
It's worth noting that the study cited by Dr. Naiman used healthy young men as subjects, and they presumably had good insulin sensitivity.
As noted, by increasing autophagy, intermittent fasting causes the breakdown and clearance of damaged cell parts.
Aging just is the accumulation of damage. As we age, we can no longer recycle damage as readily; cell constituent turnover decreases.
We saw in the interview with Dr. Michael Fossel that activating telomerase has the effect of increasing cell turnover to youthful levels by lengthening telomeres.
Dr. Fossel roundly rejected my suggestion that, until telomerase therapy comes along and/or it's shown to truly reverse aging, intermittent fasting is the best way to slow aging.
But, he's wrong. If the accumulation of damage characterizes aging, and if the aging organism is characterized as being less able to clear damage, then increasing the rate of clearance is the best way to slow aging.
One of the best ways to increase clearance of damage, and hence to slow aging, is through intermittent fasting.
The Better Humans publication is a part of a network of personal development tools. For daily inspiration and insight, subscribe to our newsletter, and for your most important goals, find a personal coach.
",178
https://medium.com/@RNesto/the-6-types-of-icu-nurse-b73a39afe72d?source=tag_archive---------6-----------------------,The 6 Types of ICU Nurse,"(If you like this post, follow me on Twitter: @icuwriter. Thanks for reading!)",Ernesto Barbieri,7,"(If you like this post, follow me on Twitter: @icuwriter. Thanks for reading!)
You went to nursing school at a time when nurses still wore those funny, starchy white hats, back when sexual harassment and even physical abuse were expected hazards of the profession. You have seen more crazy shit than most people can expect to experience in several lifetimes ... and you do not hesitate to let people, particularly new nurses, know it.
You've seen more dead bodies than a grizzled homicide detective, a county coroner, and all our active military combined. Your very first day on the job, a sundowning patient snatched the engagement ring right off your finger and swallowed it whole. You once caught a severely hypernatremic patient drinking urine from his own Foley bag. The last time you were the least bit fazed by a patient action, M.A.S.H. was still must-watch Sunday-night TV.
You see things in the pupillary response that even an experienced neurologist would miss. You can determine, merely from the color of a patient's great toe, the precise extent of his hypoxemia. In a single hour of your shift, you can predict that a patient will crump, defuse a potentially explosive situation between bickering coworkers, educate a patient on colostomy hygiene, advise an endocrinologist on his choice of tube feeds, and still find time to hold the hand of an actively dying cancer patient.
God, your back hurts. You've been on ""light duty"" since 1957. Even the attendings are in awe of your vast body of nursing knowledge, and will cheerfully defer life-or-death decisions to your judgment. You don't just deserve respect; you command it. You are, in summary, awesome.
And very cranky.
Strengths:
-Can locate a vein in parts of the human body that Superman's X-Ray vision could not even detect-Refers to attending physician by first name, and, occasionally, ""Sweetie"" or ""Sonny""
Weaknesses:
-Has no discernible tolerance for small talk, clinical ineptitude, or hijinks of any kind-Electronic charting a constant uphill struggle-Probable latex allergy-Kyphosis
Hey man, you're just passing through. This isn't your world. You're only here for your requisite one year of ICU service, and then it's hello, anesthesia school, and on to the BIG BUCKS. This is your movie; we're all just extras in it. Get in, get out. That's the ticket.
But while you're here, you simply can't resist ""showing up"" your coworkers with incessant talk of prolonged QT intervals, wedge pressures, and the efficacy of norepinephrine in an ""alkaline environment."" While the rest of the unit is bathing patients, you are off in some corner, attempting to engage a disinterested respiratory therapist in a loving analysis of blood gas results, as if that were the most natural thing to do at four in the morning. You openly joust for the most complex, unstable patients  a proned, ballooned open heart on four pressors and CRRT who just tested positive for C. diff  that's your bread and butter. You dream about that shit.
Despite your constant talk of leaving this place, you will find that on your last day  more so than any other nurse on this list  you will be saddest to leave, and may even openly weep on the ride home.
Strengths:
-Often drives a nicer car than the doctors-Secretly has the cardiac arrest decision tree tattooed across back-Will acquire and interpret your patient's 12-lead, unprompted and unordered
Weaknesses:
-Perpetually studying for the CCRN-Fanciful delusions of one day going to med school-General insufferable douchiness
Oh, aren't you adorable.
Look at you, with your makeup freshly applied, showing up to night shift all bright-eyed and bushy-tailed, bringing your preceptor coffee. You have a pep in your step that immediately betrays your status as a recent grad; you are like the power-walking embodiment of a shiny new RN license.
Your constant smile is infectious, and maddening. It may seem like your coworkers resent you, and indeed they might, but that's only because you remind them of themselves, back when they had that bounce.
Strengths:
-Very useful in ""code brown"" situations-Excellent folder of linens-Room left clean and well-stocked-Always down to help you bathe
Weaknesses:
-Rambling, virtually incoherent change-of-shift reports-Utterly confounded by pulmonary artery catheters-Unclear how or if boards were passed
Remember San Francisco? Remember working as an assistant to the VP of that sports marketing company? Remember your sparse, clean, but not-quite-upscale bungalow in the Outer Sunset? The one with the patio out back, where you had a little herb garden? Sure you do. You think about that herb garden a lot. All the time, really.
But something was always missing, wasn't it? A vague, foreboding sense that you were wasting your life, that you weren't doing ""real work."" A nagging need to HELP PEOPLE; to be spiritually useful, somehow. This is what drew you to nursing, what called you to the field.
Still, you feel vaguely naked and vulnerable at work, as if having accidentally showed up to your old office job in your pajamas. Your scrubs still feel a bit like a Halloween costume, and perhaps always will. You can be seen most days wandering around the unit with a bag of labs in your hand, a look of confusion draped across your face, as if to say, ""What do I do with these? Where do I go from here?""
And yet ... something keeps bringing you back. Maybe it's the lanyard you wear around your neck, the one that says REGISTERED NURSE in big red letters. Maybe it's the serenity that results from doing work that is truly important, and doing it well. Maybe it's that 60-year-old mother of four who you helped die with dignity, or coaching her family through what was likely the worst crisis of their lives. These small victories are very few and far between, but they always seem to come along at just the right moments, helping to push you along, at least for another few shifts.
Strengths:
-Lots of ""interesting"" stories about previous life in San Francisco, Austin, Seattle, etc.
Weaknesses:
-Stories are largely fabricated, if not outright untrue-Is one nasty liquid bowel movement from getting in car, speeding out of parking garage, and driving off into the sunset, never to be seen or heard from again
Hear that sound? That rocking cacophony of blurting pumps and monitors? The screech of wheels against a checkered floor as the crash cart is rolled into a room? The sound of ribs snapping from that first crunch of chest compressions? You fucking love that sound. It's the sound of doing battle with a soon-to-be-defanged Grim Reaper.
For you, this might as well be the sound of children playing in a sandbox. A full choir of heavenly angels, singing praises down onto the unit. You will waste no opportunity to insert yourself into any shit show or blood bath, so long as death is a realistic possibility. Double-lung transplant, fresh from the O.R., going into V-tach arrest? BRING IT. Cardiothoracic surgeons at the bedside, cracking open the chest? PARADISE.
Strengths:
-Considers CPR a ""great triceps workout""-Can subsist for weeks at a time on Sugar-Free Red Bull and Zebra Cakes from the vending machine
Weaknesses:
-Says things like: ""I like to live on the edge"" in job interviews-Will attempt to cardiovert a walking, talking patient-Runs a ""code brown"" like it's a Code Red
You, sir or madam, are deeply scary. No one really knows what you're doing here, or where you came from, whether you're part of some weird, covert government experiment. Years of working in a high-stress, high-volume profession have deformed you into a person indistinguishable from the rest of the human species. You have odd, unsettling hobbies outside of work  things like animal husbandry, or blogging about how the moon is a hologram projected into the sky by agents of the Illuminati. You are a poor historian, prone to trite, silly fabrications, like that you're a third-degree blackbelt, or had stigmata as a child. You once claimed that you'd never consumed  or even heard of!  an avocado. You said this while dipping nachos into a big bowl of guacamole. No one dared refute you.
Strangely, you know your shit when it comes to patient care. Even the War Horse would concede this. All of which serves to make you that much more intimidating to newer nurses. You are not so much a bully as a monstrous, malevolent, all-knowing presence on the unit. People actually schedule their shifts so as to not coincide with yours. Your continued employment appears to be the result of you having something incriminating on the higher-ups at the hospital. Also, you have a near-encyclopedic knowledge of the methodologies of various serial killers, and you sometimes enjoy sneaking these dark factoids into your shift reports. You are bat-shit crazy. Even the unit director doesn't dare cross you.
Strengths:
-A fertile wellspring of unit gossip and speculation into your private life-Can be paired with annoying, unlikable new hire (or student nurse)-Will eventually become a charge nurse
Weaknesses:
-Borderline personality disorder-Shows up to work in Rudolph the Reindeer scrubs ... in July-Will eventually become a charge nurse
(Follow me on Instagram and/or Twitter: @icuwriter)
",179
https://medium.com/hackernoon/biohack-your-intelligence-now-or-become-obsolete-97cdd15e395f?source=tag_archive---------3-----------------------,How to biohack your intelligence  with everything from sex to modafinil to MDMA,Other deep-dive articles by Serge:,Serge Faguet,47,"hackernoon.com
hackernoon.com
Author's note: I have been thinking a lot since I wrote this article. I deliberately made it very aggressive because I wanted people to talk about it and to pay attention. But some of the aggression went too far and is not aligned with my values.
I want there to be a great future for those of us who (like myself) want to become posthumans. I want to encourage all humans to explore enhancing their health, intelligence and productivity. There is a real risk of being left behind if you do not do that. I also want all of humanity to share in an amazing, grand future, whether they choose to be trans/posthumans or not.
If we do this right, we will have essentially limitless resources so everyone can benefit. Human and posthuman grand futures are compatible.
To that end I edited the article and removed some of the language I feel does not reflect how I see the world. To be clear I am not in any way going back on my aggressive beliefs or goals. I just realized that I was wrong to think that these goals must be in opposition to the goals of others. There is plenty of awesome future for everyone.
I will write another article focused on this topic later on.
I had some free time over the holidays and wrote this article to showcase, on the basis of a personal story, many highly actionable, science-based, approaches and tools that can be used to significantly enhance intelligence.
In my case these include legal/illegal drugs; using sex as a biohacking tool; drinking ketone esters; using beta blockers or testosterone to gain advantage in negotiations; eating only once a day; and a lot more.
Editor's Note: This story contains some R-rated approaches to bio-hacking. We published it because we want readers to be informed of what's actually happening in the technology industry. Proceed at your own risk.
I'm a cliche Silicon Valley techie Russian, Stanford, YCombinator, started a couple large/successful companies, working in artificial intelligence now.
My previous article detailed how, as a 32-year old with no medical problems, I spent ~$200k on enhancing my health. Thousands of tests, medical teams, dozens of prescription drugs.
I openly posted all my data. It shows many health benefits  3-4x reduction in body fat, very high athletic performance (VO2Max ~70), negligible inflammatory processes, >80% increase in testosterone, and improvements to many biomarkers of aging.
Biohacking works.
The article became extremely popular and reached millions of readers. Many of you loved it. Many of you felt anger and fear. Aggressive bioenhancement of human abilities has long been a sci-fi dream.
And (if you read the previous article) here is concrete evidence and a lot of data that show it is already working.
I think that what we are doing with biohacking is the beginning of humanity's split into different species. Enhanced posthumans who will not look anything like the humans of today. Unenhanced humans who choose not to do this for their own reasons.
The reason this cataclysmic shift is coming: intelligence can already be enhanced.
Let's use the definition the wonderful Prof. Max Tegmark from MIT, uses in his new and highly-recommended book, Life 3.0.
    Intelligence = ability to accomplish complex goals    
Intelligence is applied and multi-dimensional. Intelligence is much more than just IQ or skill at mathematics.
The above definition is critical to understanding this article  re-read it a couple times.
There are many applications of intelligence. But if we drill down, there is a set of intellectual abilities essential to nearly all complex human goals. These could be broken into:
""Intelligence"" will refer to these ""universally useful intellectual abilities"" going forward. We can talk about those of us who have them as ""smart"" and those who do not as ""stupid."" This will piss some of you off, but it really is time we stop pretending we are all equally smart. That sounds nice and PC, but the claim that everyone has roughly the same ability to achieve complex goals is patently false.
Intelligence:
If the last part isn't obvious, test your ability to accomplish complex goals after not sleeping at all, while you have the flu, right after having an intense argument or after drinking 10 shots of espresso.
The above is exactly what the world looks like today. We just need to replace ""marathon running"" with ""intelligence.""
Here is why:
Income is already driving biological inequality, and more of it with every year. From the stats above, we can speculate that men born in 1985 and in the top 1% of income already have a life expectancy of at least 95-100; for women this number is even higher. Even before biohacking. Or the massive progress in biotechnologies anticipated in the coming decades.
To be certain of living to 100 those of us who are well-off and into basic health just need to pay attention to the data.
These trends feel unstoppable, and may even accelerate.
Key point: you can't be intelligent if you don't know what you want
If we read all the tactics below without considering goals, we can be left with the impression that Serge is basically saying ""mine is the One True Path, everyone needs to copy me, and have sex with models in swing clubs while sipping clomid-ketone smoothies!!""
I optimize my intelligence towards my specific 50-year goal. To build one of the platform companies that give us The Singularity. To help make us immortal posthuman gods that cast off the limits of our biology, and spread across the Universe. To have limitless abundance.
We don't all have the same goal. But we should accept that the above is indeed my goal, and that I find it incredibly meaningful. Much more so than wasting my life on buying yachts and jets, having children, or giving to charities.
The framework below is designed to enhance our intelligence to get us whatever we want. But you need to know precisely what that is, and I can't help you figure that out.
I want to be a god. Note the ""a god"" (as in one of many cool, ultrapowerful and ultraintelligent posthuman beings) not ""The God.""
Key point: if you can't focus, you are really, really fucking stupid
Always start with AI (energy, focus, willpower etc). This is the engine we use to power everything else.
This means:
We will dig into all these tools a bit later in the article.
Every time we successfully get into deep flow, we adapt. It becomes easier next time. Every time we procrastinate  flow becomes harder.
A sign we are smart is if we manage to get 5-8 hours of totally focused, truly deep, uninterrupted work with no procrastination.
And still have time and energy for gym, friends, travel, music, sports, sleep, meditation etc.  all of which we are able to be deeply engaged in.
This is feasible. In December, I managed this on 23 days, took 3 days off completely, and got disrupted down to 0-3 hours of deep work for 5 days for various reasons (that can be pruned further).
This level of deep work is easy to maintain, but extremely hard to get to. We are very distractible. The world is very distracting.
A clear sign we are stupid is if we do not read new books, take poor care of our health, rarely have deep conversations with friends, do not learn new skills. Because we are ""too busy.""
Although at the end of the day it is hard to write down precisely what valuable progress we have achieved with our busy-ness.
In this case, we are not actually ""busy."" We are ""stupid."" The good news is that we can become smarter.
This is the part when you write outraged comments about how your job/children prevent you from deep work. Suggestion: find one hour of deep work a day. Use it to learn a new skill  graphic design, coding, whatever. Change your job. Hire a nanny to help with children. Invest the time you freed up into more deep work. This is a gradual upwards cycle of increasing intelligence.
Those of us who feel that the above is too long/hard and want something easier will simply remain stupid.
High Applied Intelligence lets us be captains of our lives.
Key point: daily focus + biohacking = extreme intelligence
If we do a great job out of AI, we will have an enormous resource of energy and focused time. That (we can speculate) only <1% of us will ever achieve.
The question is what to invest it into. Key candidates:
Last month I spent several days reading the (exceptional) Pre-Suasion by Robert Cialdini (THE guru of persuasion). Went slow. Took notes. Applied ideas to a public talk on biohacking the following week. Distilled the core concepts in a long recommendation-review for friends. This book became a mental model for human behavior. Many of its takeaways help make this article extremely persuasive.
This is one of many things deep work lets us do  truly understand, internalize and apply complex new ideas.
In the last year I read ~40 books + took ~8 online classes (list of recommendations here)+ learned to code; started a new company (Mirror Emoji Keyboard) + raised the highest-valuation Q1 seed round in Silicon Valley + launched a highly-rated product that may well create material financial wealth; wrote biohacking articles that reached millions of readers and helped meet amazing like-minded leaders in Silicon Valley; slept 8.5 hours on average (proofpic in sleep section!) + worked out every week and meditated every other day; made amazing new friends + went on awesome drug trips + had great sex + lived all over the world. Feel truly happy. And excited about continuing this growth trajectory every year for decades.
But let's compare with 5-10 years ago. I was unhappy, unproductive, fat (27% BF). Carried forward in life by some combination of luck, total denial, and extreme ambition. I was pretty stupid. And I'd never admit it back then to others, or to myself, but my life fucking sucked. Suicidal thoughts appeared a couple times. Luckily never in a serious way.
The point is: biohacking can significantly enhance our lives. Let's use all the medical technology we can get to enhance our intelligence. Start now. Reinvest all of it over years and decades into building intellectual, financial, social and physiological wealth. Utilize the new technology that is coming. We will get to achieve everything we want. Lead the future. Be healthy and happy.
If you want a health-focused discussion, read the previous article. This one will mention some of the same things. Stuff that makes us healthy is mostly the same stuff that makes us intelligent. But this article is written specifically with the purpose of helping all of us enhance our intelligence.
(There is another, deeper, more interesting reason why I spent all the time writing this. It will become obvious by the end).
Do not mindlessly copy this. I do most of these things with a team of MDs with decades of medical experience led by Peter Attia, one of the top health-optimization doctors in the world.
If you read some of his articles on heart disease prevention, cholesterol, how ketosis enhances athletic performance, and why whether you are fat is not truly about calories you will appreciate the depth of thinking that goes into our decisions. Peter and his team really go after any of their recommendation with the same level of tenacity and dedication.
In particular, we do thousands of tests and know that our interventions do not carry much risk for me. But all of us have different bodies. For example I have kidney function with eGFR of >160 (in the top 1% of 30-year-olds), whereas you might not. This means your risks may be higher. Be cautious.
AGAIN: I am just a guy from the Internet. Not your doctor. I am not responsible for your health and am not telling you to follow my advice blindly.
Key point: if you sleep less than 8 hours or go to sleep at inconsistent times, you are fucking yourself, making yourself stupid, and helping yourself get Alzheimers.
If anyone wants all the science, look into this book  it references hundreds of studies many of which the author (the director of the UC Berkeley Sleep Lab and one of the world's leading sleep neuroscientists) performed himself. Here we will just list the highlights.
Even minor sleep deprivation (sleeping 6-7 hours); circadian shift (changing sleep time by 1-2 hours a day from one night to the next); or reduction in Deep NREM or REM sleep reduce our intelligence in the following ways:
Plus on top of the Things That Make Us Stupid above, we have: worsened insulin resistance, cancer, cardiovascular disease, car crash risk, athletic performance etc.
I don't have time to link all the studies but it is in the book. Besides pretty sure most of us will know the above to be true from personal experience. I for one feel like a moron in the afternoon after undersleeping.
In other words, sleep is a major opportunity for intelligence enhancement. It impacts many other things. And for most of us, sleep quality is poor.
First a bit of important theory:
What this means is that if we spend 7 hours in bed we sleep ~6 hours and cut out the last stages of sleep, degrading their unique functions by >80%. If we change our bedtime by 2 hours from one day to the next, we destroy the early stages of sleep and degrade their unique functions by >80%.
KEY POINT: you think sleeping 6 hours or going to sleep 2 hours later degrade sleep marginally, but actually they do so very severely.
So sleeping better means spending more time asleep, in the right sleep phases, at the right and consistent time of the day.
The first thing to do is measure. Peter, other leading health-optimizers and I recommend the Oura ring. Use the code ""sergef"" to get $100 off their new version; I do not earn a referral fee from this (they kindly offered, I declined). The reason we prefer this particular device is that it gives far more accurate data than all the wristbands. Skin thickness, skin color, and contact tightness are all more favorable on fingers in terms of blood flow analysis.
Here are the key things we want:
To sleep better:
Here's the thing: like most suggestions in this article the stuff above has compounding benefits. Each night of bad sleep permanently damages us and we can never fully recover that damage. Part of the damage is to the apparatus of sleep itself, which over time makes us stupid and ages and kills us.
Many of us do not want to make the changes to our social lives, dating etc. for the sake of sleep.
It is a matter of priorities. You want to go to the club, or you want to not have Alzheimers.
Key point: stressed out, negative, emotional people basically have long-term brain damage
Constant stress truly hurts everything: it makes cognition worse[1 2], drains energy, and even cuts adult neurogenesis and neuroplasticity in our brains [1 2]. Makes memory crap [1]. Interferes with hormonal systems [1]. It ages and kills us in many ways[1 2]. Here are another 50+ studies on how chronic stress fucks us up.
In other words, constant stress makes us stupid. Anything we can do to reduce it is a big win. Here are the specific tools I found useful for this:
Escitalopram is extremely safe even in large doses [1] especially for me because I have genes that are associated with significantly higher positives and lower negatives of this specific drug.
Old antidepressants (e.g. MAOI) are dangerous. There are also studies out there that claim even the latest, best antidepressants are bad for you [1]. My medical team is skeptical of those studies. The biggest reason is ""sick cohort bias."" These ""researchers"" take a bunch of depressed people that are prescribed antidepressants, compare them with people NOT on antidepressants (while claiming to magically compensate for the fact that the latter group are obviously healthier people). Conclude that the AD group has a small difference in some kind of risk, and PR this to gullible media. This is bullshit, not science.
My doctors take SSRIs themselves and make no money from prescribing them to me. So I trust their conclusions.
We can be completely non-religious and do not need to believe in any mystical bullshit that meditation is (sadly) surrounded with. There is a large amount of serious scientific evidence that suggests meditation is valuable for everything from neurogenesis to cognition, mood, attention, and disease risk [here is a collection of ~50 studies on this].
Once we are good at meditation, it provides very concrete applied hacks we can use. Here are some I use every day:
And it feels like in meditation I only just scratched the surface although I meditated (incorrectly) for 5 years. It is already easy for me to control my emotions. My goal this year is to get to a point where negative emotions don't even appear. This is feasible.
Additionally, I saw material enhancements for stress resistance from things described in other sections. Most of all better sleep, sex, MDMA, removal of news/social media, and hormonal enhancements.
Last important point on stress: our propensity for it is a long-term thing.
Stress and fear increase the size and power of the amygdala (the part of our brain where fear is generated), which in turn makes it easier for us to be stressed and afraid.
If we are constantly fearful, stressed, depressed, negative, skeptical of others  we are ""sick"" in the medical sense that our brain structure is doing something detrimental to us due to long-term damage. And it is detrimental  lots of scientific evidence for that (refer to all the studies listed above). We can get better, but it takes time and effort.
Actively work on keeping stress levels low. It doesn't just make us happier and healthier. It makes us significantly smarter.
[Oct 2019: I edited this section to reflect my updated thoughts on the subject]
Apology to the female half of the audience. This section is male-oriented. That is what I know and optimize for.
Key points: Sex = good for you. Humans = not monogamous. When not in a relationship, I just hire fashion models to have sex with in order to save time on dating and focus on other priorities. Great sex = biochemistry.
I think of sex as something similar to exercise, meditation, or food. Another physiological need to be addressed in a time-efficient way; another tool to enhance health (talking about safe sex obviously) and intelligence. There are many reasons why sex is useful for intelligence:
Getting great sex takes too much time and energy:
My solution at this point in life is simple:
Basically: I get to have sex with women I find attractive, when I feel like it. All the psychological and physiological value of sex, with very little time or emotional cost. Helps me have more appreciation for the long-term relationships I have because I satisfy sexual urges and feel free without a sense of resentment at my girlfriend for restricting me. And no risks because everyone's happy.
Society tells us that ""the right kind"" of sex needs X months/dates of knowing each other to be good. And of course it must ""not be based on looks or money"" (a theory easily disproved by the existence of makeup and Lamborghinis).
In reality awesome sex just needs the right biochemical buttons pressed. That is all.
A lot of you do the stuff I do quietly. Or fantasize about doing it. Just do it openly. You will look confident and empowered. The world does not get to tell you not to do what you want.
BTW, the right long-term relationships can be amazing and valuable. I want these to be honest and based on a genuine connection. Not on ""I am really horny, I'm going to go waste my time to pick up women I actually have zero long-term interest in.""
Finally, if you think it is ""misogyny"" to pursue your own desires and encourage everyone, regardless of gender, to do the same, you may want to look up the definition of ""misogyny"".
Key point: your hormones are probably screwed up. Fix them, it improves mood, energy and health. And generally makes life awesome.
For reasons related to modern life (stress, poor sleep, pollution etc.) most of us will have suboptimal hormones.
This is hard to fix without expensive professional help and testing. But it is really worth exploring. The general idea is:
I had ""below average but clinically normal"" levels of thyroid hormones and of testosterone. Boosted both via targeted interventions described in my previous post.
This improved mood and boosted energy in quite a material way. Plus testosterone is quite important for Social Intelligence (explained in a later section). It increases confident, aggressive, dominant behavior as well as willingness to take risks. The associated body language and behaviors make it easier to get people to listen to what you say and do what you want.
Key point: eating once a day makes you smarter and healthier.
I eat only one time a day  late afternoon / early evening, and fast for 24 hours. I do this nearly every day. This:
Here are ~70 supporting studies. There is very wide scientific consensus that IF is great for you.
Basically it makes us smarter, saves valuable time and makes us healthier. Think of how big of an advantage it is to have an additional hour of sharp focus a day for 30 years. And we will live longer!
It is also natural. All that stupid shit about eating breakfast and eating 5 times a day ignores the very obvious question: do you actually think you evolved to have an around-the-clock buffet? do you think the hunter-gatherers ate 5 times a day?
This is really a no-brainer. A bit hard at first, but the body and mind adapt quickly.
Key point: your body and mind work much better if you do not eat sugar, processed foods and grains. Because guess fucking what? Evolution designed you to mostly eat fat!
A simplified explanation of ketosis is that we are switching our bodies into burning fat rather than glucose. This requires eating nearly all our calories from fat, and can be measured quite precisely via finger blood sticks.
The fundamental argument why ketosis is good is as follows:
There is a great deal of evidence that suggests ketosis is advantageous. Here's another 70+ studies if you want to read about it.
The problem is that ketosis is very hard in the modern world. Much harder than intermittent fasting. The reason is that sugars and carbohydrates are everywhere + that they are very addictive. And in order to be in ketosis, we really have to eat close to zero sugars and our carbs have to be limited to a small amount of fruit + what we will get from vegetables.
It looks like there is interesting technology on the horizon  HVMN, a biohacking company funded by Marc Andreessen, has developed the first commercial-grade ketone ester that really does raise ketones rapidly and significantly (I got 0.5=>3.4 in 15 minutes for those in the know), and reduces blood glucose. The older ketone salt products don't get nearly as good of a result.
When I took their ketone ester (note: I am not earning any referral fees or the like from them although I am friends with the founder), I felt a rapid and lasting inflow of energy/focus. Another particularly acute effect was needing to breathe less  in normal activity, in the sauna, and in an interval run.
Right now this is expensive (~$3000/month to stay in keto all day) and unpleasant (the stuff tastes... really bad although Geoff, the CEO, is saying this will be fixed shortly). Hopefully both will get better. I really like the effects I perceived so far, as well as the science behind ketone health benefits. Will probably take this every day on top of my on-and-off keto diet.
Not much to add to what I wrote in my previous article  do interval training (not bullshit long cardio or marathons), do heavy hip hinge exercises, sit less. This likely contributes to intelligence via hormonal systems, sleep, stress control etc.; and even if it did not you would want to do it for the health benefits.
Key point: technology, notifications and news media are vampires that suck our time and energy.
Many things require deep, focused work. Switching contexts is expensive. If we are distracted from writing code by a 5-minute phone call we do not lose 5 minutes, we might lose hours of excellent work.
There is more. Every time a notification, a phone call, a YouTube video distract us, our neural networks get rewired. We become more distractible. Less able to focus. More addicted to these things.
We become more stupid.
Modern tech and media are deliberately designed to make us addicted. Couple quotes from top Facebook executives about this:
This is not limited to social applications. The news media, from Buzzfeed to the New York Times, are overhyping threats, optimizing clickbait-y headlines, and generally doing everything they can to make us care about things that are actually totally fucking irrelevant to our lives.
That's right  the news media that claims to keep us informed and complains about Facebook designing addictive technologies actually thrives on hijacking our minds, sucks our time and energy. And directly makes us more stressed, negative, stupid people.
Do you really want to spend your days and limited attention resources worrying about what Kim Jong-Un will do? Why. The fuck. Do you. Care?
(If you think worrying about Kim Jong-Un in fact does provide value, I challenge you to post in the comments a list of specific decisions that reading the political news helped you make in 2017, and what concrete, valuable outcomes resulted from these decisions).
Here are some ideas on how we can approach this challenge:
Some of you might feel this is extreme and not worth the time investment. Track how many hours you waste away on procrastination, social media, news articles etc. in a typical month. For me that number used to be several hours a day and is now approaching zero. Our investment in controlling our infospace pays for itself many times over. And it is easy once habituated.
This section is also the reason I choose not to have children. None of us will disagree that children are extremely distracting, disrupt sleep for years; are generally a massive cost of time, focus and energy; and have material risks of not working out, for reasons outside our control.
I just don't see the ROI in children given my goals. They won't ever return my time, focus or energy back. There is no point in passing on our genes once we can live forever ourselves (and there are good reasons to think some of us will do so). We can have other meaningful long-term projects. And if we ever feel lonely we can take MDMA with friends or boost our hormones and neurotransmitters. True happiness is in our biochemistry state. We can have it without intermediate steps.
Bad news for the high-IQ introverts amongst us. Donald Trump is smarter than we are.
He repeatedly survived through bankruptcies that would have destroyed most of us if we were in his place. Played the media like a fiddle. Became president of the US. And he isn't a lucky one-hit wonder. He has been achieving for decades things that the majority of us would very much like to achieve.
This is not an endorsement of him. I do not like what he does. But his ""ability to perform complex tasks"" (i.e. intelligence) is high. And it is based in SI. Body language. An understanding of human emotional buttons. That the human brain equates attention and credibility. Even if you really dislike him it makes sense to learn some of his techniques.
(I entered into a $20k bet on Trump winning the presidency 15 months before the election. For those of us with high social intelligence, it was always obvious Trump would win).
We should recognize that Social Intelligence is far, far, far more powerful than IQ. The reason Social Intelligence is so powerful is that it scales. An understanding of human emotional buttons enables us to get others to like and support us with their skills  no matter who they are.
A high IQ and ability to debate with formal logic is useful too of course. Planes don't fly on emotions. But if we, after a lifetime of observing ourselves and other humans, think we can use logic/facts/IQ to persuade/lead/connect with others, we are truly fucking stupid.
There is a lot of evidence that Social Intelligence and social status have many second-order effects on intelligence: neurogenesis, happiness, intelligence, brain volume, desire to compete, lower stress etc. Here is a whole book full of supporting evidence by Robert Sapolsky, a leading neurologist at Stanford. And lest we think this is just because of human social inequality, a great deal of the research is replicated even for monkey social intelligence and social status (read the book).
So  how do we boost it?
The prerequisite is to truly internalize that humans are ""irrational."" But the irrationality is highly predictable. We have to understand and believe this, at our deepest core.
Favorite example of ""irrationality"" (I am well aware of that priming studies are controversial):
When we hear French music in a wine store our preferences are materially shifted to French wines. Even if we don't ever become aware of the music.
The reason things work this way is actually highly logical.
Somewhere in our mind there are neural nets for France, French Music, French Wine, French Flag image. They are linked to each other. That is how we know these are related in some way. Triggering the French Music net triggers all linked nets, including the nets for France and French Wine. So the incoming signal to the French Wine net is a bit stronger than into the Italian Wine net at this moment in time. That changes our decision probabilities. Without us ever noticing.
All of us are programmable, hackable machines.
So  how can biohacking help our SI?
Recently I gave a talk about biohacking to an audience of ~200 successful leaders in San Francisco. When I asked how many of them took the drug MDMA (Schedule 1 illegal in the US), >50% of hands went up.
Think about that for a second.
I am not telling you to take it. But in my subjective experience, MDMA boosted Social Intelligence more than anything else. Its effects were permanent and extremely beneficial.
The scientific research says that risks of MDMA (Ecstasy) appear to be far lower than those of alcohol and tobacco [here are several studies; this is obvious to anyone who ever looked at the science]. The FDA is about to approve it for treatment of PTSD and says it is a ""breakthrough"" [1].
The setting I take it in is long, chill house parties with friends/family, great music and ambience, known supply with precise measurement.
For those of us who haven't tried it: the effects are:
The interesting thing is that for me all the effects above stayed permanently, albeit they are not as strong while actually on MDMA.
The biggest benefit is that I became extremely comfortable with being open about who I am and what I feel. In this article I publicly admit to taking illegal drugs and paying for sex. It is trivially easy for me to say what my biggest insecurity used to be (height  5'8 and spent years agonizing over it) or the most embarrassing lie in my life (lied about getting into Stanford GSB before I actually did; it got to the Dean of Admissions; I'm still not sure how I persuaded him to let me in despite this). All my friends and family read these articles BTW.
It is extremely liberating to not care at all about what other people think of us. And just be who we are.
And here's the thing: we all LOVE those of us who are so open and honest. It makes us interesting. Memorable. Trustworthy. Relatable. Confident. It makes it easy to get into deep conversations, which builds relationships. It is soooooo much easier to get what we want in life when we have no fears and just directly ask for it. And so few of us use this extremely powerful hack.
Alcohol (88,000 deaths/yr in the US), tobacco (480,000 deaths) and opioid painkillers (15,000 deaths) are legal, but MDMA (50 deaths) is not. Clearly many of us understand this and do not respect this law (>50% in my San Francisco sample openly said they used MDMA, while just about nobody smoked tobacco).
Unscientific and irrational drug laws undermine the moral authority of law as a whole.
A recent study at Oxford reviewed all English studies on Modafinil over 25 years, and concluded that Modafinil significantly enhances the attention, executive function and learning (i.e. intelligence, in its most clear form imaginable) of healthy non-sleep-deprived humans when performing complex tasks, and with no side effects or mood changes at all. [1].
I take 100-200mg every day in the morning (as early as possible) and plan to always do so. Anecdotally some people find their sleep is disrupted.
The country that first decides to give free Modafinil to all its citizens will reap massive competitive benefits in the global economy.
I used to take Adderall XR (daily for many years). Adderall is very effective and quite safe (here's a huge review of the science, and it clearly suggests Adderall can give you superhuman concentration with not much real risk).
I switched to Modafinil because I found that Adderall made me more anxious + made it too easy to focus on unimportant things.
I still take it occasionally when I need to do something boring but important. For example in my last company I read/edited key legal documents on Adderall. And the mechanisms I built in gave negotiating advantages years later. When reviewing 200 pages of dense legalese, a $1000/hr investor lawyer gets obliterated by a founder on Adderall.
But overall I do not recommend it. Too powerful.
Key point: many things in this article help you grow new neurons. that helps you learn. if you do not constantly learn new and challenging things, your intelligence degrades. oh, and LSD.
Learning itself is a skill that needs to be developed. For a general overview of the neurobiology of learning I highly recommend completing the Learning How To Learn class on Coursera. It is easy and very valuable for all of us  those in high school, and those who are CEOs alike.
Learning is dependent on neuroplasticity and neurogenesis  i.e. the ability of our brain to grow new neurons and rewire synaptic connections between existing neurons. These are driven by something called BDNF, which is highly modifiable.
We already mentioned that a number of things enhance these: lithium, SSRIs, sleep, meditation, stress reduction, sex, fasting. Here are some more:
Here's another 50+ supporting studies.
The last bit about LSD deserves a separate mention. I recently began microdosing LSD. Interesting experience. No hallucinations, but the mind wanders, focuses intensely on various sensory inputs, and links ideas in novel and unpredictable ways. I find it impossible to do focused work. But it seems to be an incredible way to deeply enjoy things like music, art or even taking a bath. It also seems to be a very powerful way to shift the mind into a diffuse mode of thought after a period of intense focus, which is essential for learning (watch the class referenced above for details). LSD even in large doses is extremely safe. I will keep using them from time to time when I want a day off. NOTE: do not mix LSD and lithium.
If you are the kind of person who looks down on those who take illegal drugs, you should know that Steve Jobs, Bill Gates, Richard Feynman, Thomas Edison and many other top businesspeople, scientists and leaders used illegal drugs and often spoke of them as crucial to their success.
And of course to improve our learning skills we need to keep learning. We can teach ourselves to code or complete classes on neuroscience or genetics or quantum physics or French or playing the piano, rather than spend our precious time and neurons on the failings of politicians, the habits of celebrities, or the details of terrorist attacks. Even if we don't ever use quantum physics, learning it will make us smarter and more capable of learning any other concepts, as well as link them to quantum physics in novel ways.
This article is already long enough. I take lots of supplements  50+ pills a day (that's an actual picture above). They are basically super-safe things (e.g. concentrated garlic) that can have plausible benefits. More details in the health article.
Key point: pollution and especially smoking damages your brain.
It is well known that many pollutants damage intellect directly (lead, mercury); have broadly negative neurological effects (volatile organic compounds such as styrene); and disrupt hormone signaling (xenoestrogens are linked to long-term testosterone decline observed around the world).
I try to avoid all of these as much as possible. On a practical level this means:
I know I used the word ""stupid"" many times in this article but it all pales in comparison with the stupidity of smoking.
Many of these interventions are likely to carry significant placebo effects. Placebo is awesome. If we have genuine belief in the efficacy of an intervention, that belief itself generates part of the desired effect. This is one of the most proven observations in medical science.
Optimism is healthy and good for us in and of itself.
I made this point many times. But this stuff compounds. Think about how smart you will become if you have decades of focused learning, awesome sleep, great health, practicing social skills, building wealth and power. An upwards spiral that others will never catch up with. And because gains in society are exponentially concentrated at the very top, gains from maximizing your intelligence are large.
And deep work habits also train us to have ""deep downtime."" To have deeply engaging conversations with our friends and family, where we show our vulnerabilities and get to the core of who we are. Rather than staring at our phones and distractedly discussing the weather (as most of us do).
This whole approach isn't just healthy and useful to create wealth and influence. It is also a fucking awesome way to live.
It is not the ability to easily multiply large numbers, but the ability to consistently win, that makes AlphaGo superintelligent in the (very narrow) domain of playing Go.
Likewise we are intelligent if we accomplish complex goals. Stupid if we do not. That intelligence is based on our biochemistry.
This article is about executing extremely well. You should also pick the right goal and tie it together on different timeframes. I have very general objectives by the decade for the next 40 years => fairly concrete OKRs (Objectives & Key Results) for 2018 => Quarterly OKRs => they inform my weekly and daily priorities. In other words I can link my action item today ""find coach that helps me train my voice to be slower"" to annual OKR of ""enhance persuasion skills"" to ""become immortal post-human god sometime after 2060.""
This article has many suggestions that can make those of us who adopt them more intelligent, happier and more influential. And I genuinely enjoy helping others.
But there is a deeper purpose.
I want to live in a post-human future that is aligned with values I align with: knowledge, science, technology, freedom, progress, power, abundance, pure meritocracy, optimism. And where tribalism, religion, tradition, nation-states, irrational emotions, conservatism and socialism have much less power over the world.
I know that only those of you who hold a very similar technocratic worldview will internalize and obsessively adapt these hacks. Those of you who believe in different values will say this is unproven. Too radical. Too weird. Too un-human. Too far off in the future to care about. Too complicated.
These tools will help the group that adopts them gain more influence. And thus help further advance a situation where those of us with values similar to mine influence the agenda for mankind. This will create a much better world than what we have today.
So for me the lines along which we will split on this approach is itself a feature of the approach.
This article  crafted with deliberate attention-grabbing concepts like sex, illegal drugs and fear  is designed to help bring about the grand future I believe in.
*******************************************************************
hackernoon.com
hackernoon.com
#BlackLivesMatter
44K 
163
Some rights reserved

",180
https://medium.com/@gidmk/8-common-arguments-against-vaccines-5d45ad9c1e29?source=tag_archive---------5-----------------------,8 Common Arguments Against Vaccines,And why they don't make any sense at all,Gideon M-K; Health Nerd,8,"Being an argumentative sort of person, every once in a while* I find myself getting into a spot of witty repartee. It usually happens on twitter, the land of the 140-character he-said/she-said/THE-GOVERNMENT-IS-OUT-TO-GET-YOU banter, but occasionally I find myself getting into arguments in other places online, or even in real life.
Often these arguments are over the subject of vaccines. Because whilst vaccines have been accepted by public health organisations the world over as the most important medical innovation of the 20th century, and one of the most lifesaving interventions that we've ever come up with, there is a small minority of people who are convinced that vaccines are bad for their child's health.
A small, very vocal, minority.
Now, I'm not here to criticize parents. It's very important to note that most parents want the best for their kids. They are trying to look out for their children, and occasionally in this pursuit they get misled. And make no mistake, the people who sell vaccine fear are professionals in the art of deception. They know exactly how to convince a worried parent that the most dangerous thing in the world for their child is the vaccine, rather than, say, the measles.
It's not the parents who are spreading vaccine denial. They are victims of professionals. If you are a parent who is worried about vaccination: don't stress. You are a good parent. You have just been lied to. Have a read of this article, and maybe go have a chat with your doctor about why immunization is important and why it's a good thing for your kids.
Whenever you talk vaccines, the anti-vax professionals come up with the same arguments time and again. Here are my top 8, and why they make no sense whatsoever:
8Vaccines Cause Autism. I'm not really going to go into this, because it has been refuted time and again. Virtually every study involving a) humans, b) more than 10 participants, and c) researchers who haven't been convicted of fraud, has shown that there is no link between vaccines and autism. It was a valid concern in the early 90s, but we have 30 years of evidence showing that autism is in no way linked to vaccines.
VACCINES DO NOT CAUSE AUTISM ALL REPUTABLE STUDIES HAVE SHOWN THIS FOR DECADES
7There Hasn't Been Much Research. This is always a bit of a weird one, because people are usually claiming that on the one hand there hasn't been enough research done on vaccines to prove them safe, but on the other they know the truth because they've done their research and it shows vaccines to be basically poison.
It's a strange argument to make, but it comes up all the time.
This is simply a lie told by vaccine-deniers to make parents scared. Vaccines are one of the most well-researched interventions of all time. We have data from literally millions of children across the world demonstrating their safety. There has been more research on vaccines than almost any other medical intervention.
The research has been done. Time and again. Vaccines are safe and effective.
6Vaccines Are Enormously Profitable. This is also a weird one, because...so what? So are any number of things. The international flour market is gigantic, but that doesn't make every bread advert a missive from the devil. Flour millers have actually been influential in protecting babies worldwide by fortifying their products with macronutrients and preventing neural tube defects.
It's also untrue. Pharma companies make far more money from so-called 'blockbuster' drugs than vaccines  for example, Astrazeneca's Nexium, despite being no more effective than cheaper options for gastrointestinal problems, has made them more than $50 billion. The yearly earnings have been somewhere between 2 and 5 times as much as the flu vaccine. In fact, if you look at the top 20 earners for pharma companies, not one of them is a vaccine.
5Vaccines Cost Loads. Perhaps more importantly than this, however: vaccines don't cost much at all. Take the whooping cough vaccine. A full 3 doses costs around 100 USD. That seems like quite a bit, until you remember that a single case of whooping cough can easily top $10,000 if it requires significant treatment.
Vaccines are actually cost-saving. What this means is that for every dollar you spend on vaccines, you get about seven dollars back because you stop people from getting sick and dying from their illnesses. Generally speaking, it would be much more profitable for the medical industry to not vaccinate, because disease tends to be really expensive.
4The CDC Is Lying. This is one of my favourite red herrings, because it is just so easy to disprove. Whenever someone brings up the CDC, my response is...so what? Let's say the CDC is evil, awful, in the pocket of Big Pharma. It's not  the people who work at the CDC are dedicated, honest, and usually incredibly good at public health  but for the sake of argument, let's say the CDC is corrupt.
Who cares?
People who focus on the CDC ignore one glaring truth: the US isn't the only country in the world. If the CDC is corrupt, what about every other public health organisation in the world that recommends vaccines. Australia. France. The UK. Japan. China. The list goes on. Forget about the CDC. Have a look at the Australian Department of Health on vaccines. Or the Japanese immunization schedule. Or one of hundreds of other countries that all choose to vaccinate. Either there's a global conspiracy including countries that are literally at war with one another  a bit unlikely  or immunization is a good thing no matter what you think of the CDC.
3The US Is Special. This is another one that I love, because it's so easily disprovable. No, the US doesn't give a uniquely high number of immunizations. Much of the OECD has a virtually identical vaccine schedule to the US, bar a few minor differences. The US also has significantly less punitive laws in terms of vaccination than other countries  for example, in France you can go to jail for failing to vaccinate your kids.
So no. The US isn't special. It's just another country, trying to stop nasty diseases like polio, diptheria and measles from killing children.
2Vaccine Manufacturers Can't Be Sued For Making Kids Sick. This is actually a very simple lie. You can sue whoever you want, even in the US. What the 1986 National Childhood Vaccine Injury Act actually does is make it much easier to get compensation for children who have suffered vaccine injuries. If you can demonstrate that you had a vaccine and suffered a recognized issue  let's say anaphylaxis  there is a reasonably simple method of gaining access to compensation in the US.
Elsewhere in the world, for example Australia, often all you can do is sue in civil courts. And even if you've suffered genuine harm from vaccination, proving this in a court of law is next to impossible, meaning that people who do suffer injuries are almost never compensated.
It's also worth noting that saying ""vaccine manufacturers can't be sued"" is again a uniquely American piece of nonsense. There are hundreds of other countries. Most of them allow anyone to try and sue anyone. And yet, the UK court system isn't flooded with cases of vaccine manufacturers being successfully sued.
I wonder why?
1Vaccine Injury Is Common/People Are Getting Sicker. Last but not least, the most common one of the bunch. Forget the CDC, forget the pharmaceutical companies, this is the real evil.
Every year, people are getting sicker. And it's all down to vaccines.
There are two parts to this story. Firstly, we aren't getting sicker. Not even a little bit. Life expectancy is marching steadily upward, with some people predicting that we will be living past 100 in this century. Not only that, but infant and child mortality is at record lows, and is only heading swiftly down. This isn't just true for wealthy countries mind you  the entire world is getting stubbornly healthier.
Secondly, vaccine injury is an amazingly well-researched field. We know the rate of injuries associated with vaccines all too well. It's roughly 1 serious problem for every million vaccinations given. This is a number that has been replicated worldwide, from Japan to Thailand to Australia to Finland and yes, to the US.
Contrast this with the rate of serious problems from, say, measles, which is around 1 problem for every 4 people infected, and you can see why vaccines are so important!
There's not really much more to say. These are common arguments, mostly just based on simple lies. Vaccines are safe and effective, not because pharmaceutical companies say it's so or because the CDC has proclaimed it, but because thousands of dedicated researchers the world over have spent decades checking to make sure that they are.
So go and get your kids vaccinated. It's good for society, it will save us all money, but most of all it might save their life.
Vaccines rock.
It's as simple as that.
You can now listen to more Health Nerd on the Sensationalist Science podcast:
If you enjoyed and/or like vaccines, send some claps my way with the button below! You can also follow me here or on twitter, or read one of my other articles about why herd immunity is pretty cool, how we stop babies dying from whooping cough (hint: it's vaccines), or what you can do to keep kids safe from disease.
*For 'while', read 'hour'
",181
https://betterhumans.pub/how-to-set-up-your-iphone-for-productivity-focus-and-your-own-longevity-bb27a68cc3d8?source=tag_archive---------0-----------------------,"A Reasonably Detailed Guide to Optimizing Your iPhone for Productivity, Focus and Your Own Health","The very, very complete guide to productivity, focus, and your own longevity",Coach Tony,75,"The iPhone could be an incredible tool, but most people use their phone as a life-shortening distraction device.
However, if you take the time to follow the steps in this article you will be more productive, more focused, and  I'm not joking at all  live longer.
Practically every iPhone setup decision has tradeoffs. I will give you optimal defaults and then trust you to make an adult decision about whether that default is right for you.
In addition, because this is a long post, I've written it in a way to make it easier to skim. Here's how to read the post:
As a bonus, because I know you got excited when you saw this was a seventy minute read, I've gone all out on getting pedantic about productivity and even included three appendixes to give an overview of the behavior design principles, to break out the potential financial budget for implementing this advice, and then a real-world example from my own phone.
Also, for convenience, here is a clickable table of contents. (The links below work if you're reading in a browser, but not if you're reading in the app.)
Open the Apple Settings App, then go to the Notifications Section. You're going to need to get good at opening the Settings app, so learn to find this icon:
Go app by app, turning off all notifications.
By the end, the vast majority of your apps should have a notifications setting that looks like this, i.e with no notifications:
There are only a very few reasons to leave notifications on for a particular app. Here are those reasons:
I led with this advice to turn off notifications because it's the most powerful. Also, you're never going to finish reading this post if you leave your notifications turned on.
These are the productivity reasons that should make you wary of notifications.
#1: Notifications are uncontrolled interruptions from your real goals. They prevent you from ever getting into a flow state. You should be in control of what you do and when  not your phone. I'm going to refer to this over and over as ""your phone is a tool, not a boss."" See Appendix A at the end of this article for more.
#2: The brain science behind learning requires sustained focus to trigger myelin growth around active neural pathways. That's what brain plasticity is about. However, if you go around interrupting that process, you'll never get the myelin growth that locks in whatever you were learning. Essentially, notifications lead to a stunted life.
#3: Those red dots cause anxiety, and anxiety causes health problems like heart disease. It's not hyperbole that I talked about life expectancy in the title of this post. Not specific to red dots, but mild anxiety was shown to increase mortality by 20% over a ten year period.
'Slot machine apps' is a pejorative phrase to refer to apps that use variable rate rewards to try to trigger mindless and addictive behavior. That's how the app tries to become your boss  although maybe boss isn't even a strong enough word. These are virtual drugs and due to societal oversight, your dealer (Facebook, Twitter, Instagram, Snapchat) is allowed to pose as a respectable member of society.
Thankfully, you can configure all of your social media to eliminate the addictive elements.
This last trick comes from Tristan Harris.
Here's what I mean. When your addictions are in the first screen of a folder, they're still visibly calling out to you. Still bad:
Instead, move your apps to the second screen of that folder, like so (the first screen has just one app, the second screen has the rest):
This second-screen-of-folder-on-second-screen strategy requires that at least one app be visible. When you reach this decision point for social media apps, you obviously should choose LinkedIn. It's the least addictive.
Extra credit for people that are actual productivity nuts: just delete all your social media apps.
This is the same strategy as #2, just for messaging apps. Messaging apps also have a built-in variable rate reward  that's what makes them slot machines.
The productivity secret for inbox management is to decide when you want to check your messages. Then, process them all in one big batch. Batch processing puts you in control. Unfortunately, most people live life reactively, constantly checking their inboxes for messages to react to. For you to reach your full potential, you need to switch to a batch processing mindset for all of your inboxes.
These messaging apps should never interrupt you. This goes back to the brain science justification in #1. You want to block off your day so that you have some contiguous time dedicated to being smart and creative (no interruptions) and then other blocks dedicated to rapid task processing (email, etc.). Cal Newport calls this Deep Work (in a very good book).
Productivity nuts: consider deleting all of these apps. People who set a schedule for when and where they check their inboxes often realize they can do all of their emailing from a proper computer. For Slack users, private messages and channel notifications are meant to be asynchronous  that means you don't need immediate alerts.
You open an app intending to get work done, and then that app prompts you to leave a review. This is an unwanted interruption, and your job is to remove as many interruptions as possible.
So disable these unwanted Review Requests.
Is it crazy how we think of computers as productivity devices but then allow so many unproductive features? I think it's crazy. This isn't just some app-developer hack, it's actually a built-in feature provided by Apple. That's how blind Apple is to the damage caused by interrupting your work flow.
What's happening with review requests is that when you use free apps, you are actually entering a partnership with the app developer where you are working on their behalf, often by clicking on advertisements, or in the case of app reviews, by acting as marketing.
Apps with more positive reviews get ranked higher by Apple. As a consequence, app developers tend to interrupt you with review requests just as you're doing something productive.
Do you care about the morality of opting out of this partnership? A savvy user can often leach off of the hard work of app developers and the money of investors without giving anything back. I've been doing that with the MoviePass service  seeing twice as many movies as I paid for, all subsidized by some venture capital investors. (Unfortunately, that gravy train seems to be ending.)
When you or I do take advantage, we're basically stealing. This sort of stealing is not illegal, but is it bad for your health? The impact off morality on longevity is muddy. As close as I could find was research on religious vs. non-religious people within the same country.
Religious Americans are reported to have more robust immune systems, lower blood pressure, and better recovery times from operations, (although these claims have been disputed).
If you're worried about the effect of morality on your longevity, here a few workarounds. Manually go to the App Store, look through your recently updated apps, and add reviews to each of them. Share your favorite apps with your friends (as I'm doing in this post). Or, opt for the pro or paid versions. I'm finding that I almost always prefer to pay for an app.
Most people should have their phone permanently on Do Not Disturb.
Do Not Disturb is not as severe as you might think, thanks to a sub-feature to ""Allow Calls From Favorites."" As a result, you can still allow certain people to interrupt you or wake you up.
The trick to getting the Do Not Disturb feature to work all day is to turn it on from the same time to the same time, such as from 9am to 9am. I tested it and that works (I was worried it would effectively turn itself on and off again in the same minute).
If, however, you do want strangers to be able to contact you (for example, if you work in sales), then just set Do Not Disturb for your sleep and leisure time.
The justification here is similar to the steps above. Limited interruptions is smart for a number of reasons, including the science of brain plasticity, the health impact of anxiety, and the productivity gains of optimizing for deep work (all covered in more depth in Appendix A below).
What the above Do Not Disturb setting allows you to do is to take a 'whitelist' strategy to interruptions. So rather than banning telemarketers one-by-one ('blacklisting'), you pre-select the very limited number of people who you would allow to interrupt your day. For me, that's my immediate family, my dog walker, and my dog's vet.
The absolute best wallpaper is an all black background. Choosing black destroys the idea that your phone is some shiny toy that you need to be looking at all the time.
Plus, with OLED screens (most new iPhones), black actually saves battery (as much as 60%). If you chronically run out of battery or are a true productivity nut, then black is the best option for you.
If you can't stomach making a thousand dollar phone look that ugly, choose the black with rainbow stripe option that's right next to the black option in the Stills. That's what I've done for all the screen shots in this article.
There are other options:
If you're shy, choose a wallpaper that will help as a conversation starter.
If you find inspirational images inspiring:
One problem with inspirational images is that words often make your phone feel cluttered. It's better to have an image, like a mountain or a person working out, than it is to have a quote or motivational phrase. For example, here's a reasonable set of affirmations that just don't show up well behind various iPhone icons and widgets.
If you are going to have affirmation or motivational text, a quick hack is to make the background yourself in Instagram Stories. Instagram will let you save the Story to your phone's Photos.
In this case, try to stick to just a single word, and place that word low enough that it shows up below your Do-Not-Disturb message. If you take this approach, consider two things:
Here's an example Jonathan Howard sent me when he was reviewing a draft of this article:
If you still haven't found a suitable strategy, pick an image with a dominant color that tells a color story to prompt one of the below emotions.
Don't pick orange (cheap) or yellow (warning). If you're not sure? Pick Red. All of these color choices will drain your battery more than a black background, but you may find the emotional gain to be worthwhile.
The reality for me is that I alternate between a black background and a meaningful picture. There's some possible science supporting the value of small changes to your work environment to create a boost in productivity. Unfortunately, I can't find a citation, although I'm 90% certain I read this in David Rock's Your Brain at Work. The theory is basically that shaking up your environment a small amount puts your brain on alert (but not so much that you're anxious).
The Raise to Wake feature lets you quickly see notifications on your lock screen just by lifting your phone.
This is a bad idea. You don't want to accidentally see notifications on your lock screen when you just happen to be moving your phone around. You want only see notifications intentionally.
Winners check their notifications on their own schedule.
This is yet another setting to make sure you're the boss and your phone is your tool.
The Screen Time widget is new from Apple and it helps show you where your time is going. Ideally, the Social Media category will be non-existent. Unsurprisingly, I'm not going to be able to find a screen shot where that is true.
To install the Screen Time widget:
You're going to use this widget as a reality check against your own biased memory.
This article is going to recommend a few more widgets later, and then recommend that you build the habit of checking the Today View by swiping right from your home screen (conceptually, it lives to the left of your home screen).
It does seem to be roughly true that what gets measured gets done. There are a number of variants of that quote, but my favorite is ""What gets measured gets done, what gets measured and fed back gets done well, what gets rewarded gets repeated.""
A goal for many of the steps in this article is for you to use your phone less, and to use social media apps much, much less.
This widget is how you'll know if you're succeeding. I consider it the feedback part of the above quote. Then hopefully the reward is an intrinsic satisfaction in your own life and productivity.
In addition to installing the widget, you should set yourself a goal for social media usage. Imagine you had a child and were setting a limit for how much television they could watch each day. Is one hour reasonable? Probably. Is six hours reasonable? No.
Now, instead of this child, consider instead that you are setting limits for yourself, and that social media has replaced your television watching time. How much ""leisure"" time each day do you think is appropriate for yourself? If you're not sure, choose thirty minutes. That's enough time to scan your Facebook and Instagram, drop tons of likes on your friends, tweet once, get the gist of the news, and consume a huge dose of We Rate Dogs.
Sometimes it's helpful to block yourself from certain websites. I have zero pride preventing me from treating myself like a toddler in need of parental controls. The reality is that we all could use some strict blocks to prevent our worst habits.
On the iPhone, the feature to block specific websites is hidden inside of Apple's Limit Adult Websites feature.
I'm not trying to make any point at all about your adult website usage. I just want to help you find the feature (and it's the most deeply buried feature in this article).
Turning on this feature allows you to then add specific websites, which don't have to be adult websites at all.
What you should consider is whether you have any habitual behaviors around checking specific websites and then use this feature to break those habits. For example, I used to live in San Francisco and so had a habit of checking the website for the daily paper. That's the one site I block because I don't want to have that habit anymore.
Additionally, I was a huge Grantland reader before it was shut down by ESPN. I still type that URL into my browser just out of pure muscle memory.
If I were designing content restrictions for productivity, I'd have one called Google-only, which would allow you to Google any term and then click any result. But you'd be blocked from going directly to any sites or clicking deeper into any site.
However, since I'm not the boss at Apple, the solution above is the best available approach and is probably ideal for most people. I'll give a more hardcore solution next.
None of you are going to do this... but I tried a month with no access to a web browser. If you are up for this, I definitely want to hear from you.
The theory is that the browser is one of the addictive slot machines that draws your attention and wastes your time.
So I used parental controls to disable Safari.
In practice, I would very occasionally need a web browser, so I'd download the Chrome app, do my browsing, and then delete the Chrome app.
If this method of reclaiming your phone at all appeals to you, here's the secret:
When I tested this, I used Chrome as my occasional browser because the path for removing access again was shorter. I trusted myself more to delete the Chrome app than to remember to find the Safari restriction option that's hidden behind five taps.
There are four ways to organize your apps: by function, by color, by random chance, and alphabetically.
You're going to organize your screens by function, and you're also going to organize apps into folders by function. The home screen is for tools only. The second screen is apps organized into folders. The third screen is for junk, namely Apple apps you aren't allowed to remove.
However, on each screen and within each folder you have to make additional decisions about organizing. You should choose alphabetically.
The phrase ""your phone is a tool, not your boss"" is implying that you're the boss. But it's more subtle than that.
We want to set your phone up so that your rational brain is the boss, and your emotional, addictive, worst-decisions brain is asleep or blocked.
The best explanation for this is in the book, Thinking, Fast and Slow (or just read the NYT book review for a good overview). The author lays out a model for the brain as having two systems.
The Fast system is our default. It's effortless, instinctual, and functional for staying alive, but also the source of most of our worst impulses. It's the system that likes slot machine apps.
The Slow system is what we think of as our rational brain. It's analytical, but requires effort and intention to access.
When I train meditation in my Heavy Mental program, I train a verbal way of moving the thoughts that come up during a meditation into our Slow system. That way we can analyze the thought, and then drop it. The entire trick is that activating your language center always activates your Slow system.
Here, we're doing something very similar. When you go for an app, I want you to have the actual name of the app in mind. That way it's easier for you to be acting rationally and intentionally. That's the main reason to adopt an alphabetical organizing structure.
The second good reason is that alphabetical is less brittle. Organizing by function is hard because sometimes apps have more than one function. Organizing by app name is intellectually trivial in comparison.
For the vast majority of people, the ideal phone setup is to embrace Google Cloud services (mail, calendar, photos, maps) and pair them with Apple hardware.
If you're on some other setup, like Apple email or Outlook, then stick with that. It's not worth switching.
You can often configure the Apple apps to connect to the Google services. But it's always better to just use the Google-specific app. In this case, I'm going to talk about Gmail.
Don't use Apple's Mail product. Google's actual Gmail app just works more smoothly, especially search.
And don't bother with any app that promises any sort of ""smart"" filtering or sorting of your email. Relying on somebody else's algorithms is hugely overrated.
I hosted a Q&A with Marshall Hughes, who is our most prolific Inbox Zero coach. Yes, that's a type of coach  a lot of our coaches zero (do you like my pun?) in on specific behavior changes.
In the Q&A, everyone wanted to ask Marshall about email automation tools. And Marshall was adamant that every experience he had with clients using automation tools turned out badly. Clients who chose automation were bailing on essential inbox habits. They'd given up on the most important habit of all, which is to say no to email by constantly unsubscribing and manually filtering.
So, paired with the settings above, you should be working on your email habits. That means primarily unsubscribing and blocking aggressively. I get a lot of extra email too that I consider FYI  for example, I like having a history of all of the newsletters we send out, but I don't need to read each one as they come in to my inbox. I filter those into folders and only check those folders occasionally.
Sticking with the Google Cloud theme, use the native Google Calendar app and ignore Apple's Calendar app.
The two most important subtleties are trying to shorten meetings and building a Today screen that's worthy of a daily habit.
The Today habit is just you starting your day by getting a summary of what's planned (and later, what the weather is like). If you can build this habit, then you can use it to trigger new habits.
In the Tiny Habits method, checking your calendar would be called an anchor habit. Above in the Screen Time section, I've already attached a new, not-quite-natural habit of checking your Screen Time widget. The reason I think that will work is because I trust that you will naturally want to check the calendar and weather, thus triggering the new and less natural habit of checking additional widgets.
On the short meetings front, this is literally a chance to save yourself hours each day by making the meetings you go to shorter and more focused. The key to focus is to have a clear goal, and push directly toward that goal.
I have one friend, a CEO, who wants the time on his calendar to be as precious as the time on a U.S. president's schedule (he told me this during a different administration). If a meeting only needs seven minutes, then just give it seven minutes. A different friend, also a CEO, set a company policy that if a meeting invite didn't include goals, an agenda, and pre-meeting preparation, you could skip it. The acronym there is GAP  no GAP, no need to attend.
Last, in my screenshot there's a massive amount of time blocked off called ""Reserved."" I'm unapologetic about scheduling time on my calendar for deep work. You can't squeeze deep work into fifteen minute gaps  you need to carve out contiguous blocks of time. For most office workers, I use what Cal Newport calls a ""bi-modal day."" That's where half of my day is for deep work and the other is for shallow work, i.e. meetings, email, phone calls.
Apple Maps has gotten better, but it's still not as good as Google Maps. You're only going to use Apple Maps when you use Siri (it's Siri's permanent default).
Every other time, you're going to use Google Maps.
This is yet another example of preferring the Google Cloud. And the custom settings for Home and Work are just small time-savers. There's not a huge additional productivity explanation.
This will let you type faster through swiping. The world record for typing on a phone is set through the swipe method: just swipe your finger over the letters of the word you're trying to type. The keyboard will figure out what you mean.
At first this will feel a little uncomfortable, but it will quickly become second nature.
Gboard, from Google, also has a bunch more features too like GIF and emoji search.
I found that I was so unhappy with the Apple keyboard peck-typing that I'd avoid using it altogether. I'm a fast laptop typist, so I'd always postpone writing until I got to my desk.
Now, with swiping, I'm just a little bit faster and that's the difference between not typing anything and being willing to type a longer message.
Where that ties into productivity is what a lot of people call the ""touch it once"" principle. Especially with email, you want to avoid reading the same email twice. So if I happen to read a message that needs a response, I want to give that response right away.
This is the last of the settings to use the Google Cloud over Apple's built-in options.
The main benefit of Google Photos is that the search is amazing. They use machine learning to categorize all of your photos so that you can later search them. For example, without any work I can find all my selfies by just searching for the word me. And I often pull up pictures of my dogs by searching for dog. I have even had someone pull up photos of a specific handcrafted greenland kayak paddle.
For photos, take the following steps.
I end up storing my photos in four places: Google photos, iCloud, laptop and Dropbox sync. That's because I'm paranoid. I should probably take them out of Dropbox.
Of those, Google Photos is far and away the best experience for looking at my photos, thanks to the machine learning behind Google's search. This is an example of where a messy-by-design organization structure beats rigid one. Search is more reliable and faster than you trying to manually categorize every photo.
If you already love your note taking app and to-do list app, then fine, stick with those. This is a recommendation where habits beat tools. Don't switch if you already have strong habits.
However, if you don't use a note taking app or to-do list app, or are unhappy with what you do use, let me give you a guiding theory that will lead you to Evernote: go messy and trust search.
Put your to-do list in Evernote, either by creating one long note that you edit every day or a new note for each day. Then put all your other notes in Evernote too. Don't bother particularly with categorization. Instead, just trust that you'll be able to find whatever you want later through Evernote's search.
What you end up with is a messy but long-term functional system. The other approach, constantly switching apps, systems, and categorization schemes always breaks. Always.
One power of a messy to-do list is that not everything has to be a check list item. You can mix in quick drafts and longer notes to yourself. Or, as I've recommended in my Interstitial Journaling technique, to live mindfully, you should literally intermix short journal entries and to-do list items.
The problem with most productivity systems is that they break. As a result, a lot of productivity nuts spend a lot of their time creating new productivity systems over and over again. This, obviously, is not productive.
For that reason, where possible, I suggest that you choose messy systems over rigid systems. The ultimate messy system that I know you're all familiar with is the paper notebook. A paper notebook gives you incredible flexibility: you can take notes however you want, write drafts, sketch outlines, draw pictures, write to-do lists, etc. A to-do list app just doesn't allow for that.
The downside to paper notebooks is that it's impossible (or at least very hard) to find an old note.
All of that is the argument for merging your to-do lists with your notes, and then putting them all into a single cloud-backed note taking system with good search features.
Although Evernote has advanced features that may or (probably) may not be a pleasure to use, the basics work well and reliably. I recommend starting with free and then upgrading ($60/year) if you bump into a limit on bandwidth or offline availability.
I learned to meditate from Headspace. That's a good option. But guided meditation is something that you'll graduate from quickly. Most people I know who meditate don't still need a daily guide.
Once you graduate to meditating on your own, Calm is the much better option because of their built-in timer and tracking. I was an advisor in the early days of Calm specifically because of my experience building habit tracking apps.
Meditation is a productivity and performance practice. The explanation for why is a little long, so I'm mostly just going to point you to longer articles we've published in Better Humans. However, here's the quick pitch for why the Calm app is about performance and not just ""calm.""
A lot of people talk about meditation as a relaxing or spiritual practice. That's fine for them. But you're reading a productivity article, so I'm focused on what meditation does for your productivity.
The core concept comes from the world of deliberate practice, which is when you identify the components of skills that are important to your success, and then practice those components individually. I want you to approach meditation as a practice session for a skill that you're going to use to help your productivity.
You should read our full article on Deliberate Practice to get a feel for how to design practice for all important life skills.
With meditation, you're practicing a two-step process that you will use outside of the meditation. The first step is becoming aware of where your mind wandered, acknowledging the thought and then putting the thought down. Call that Awareness. The second step is bringing your focus back to your point of focus (usually your breath). Call that Focus.
This Awareness-Focus loop is what you are practicing during a meditation session. A lot of people feel bad if their mind wanders during meditation. But you should actually feel good. The more often your mind wanders, the more times you get to practice this Awareness-Focus loop. I tell people what they are doing is mental pushups. The more wandering they do, the more pushups they get in.
Now that you've practiced, here's a way to then apply the Awareness-Focus loop in ways to be more productive by beating procrastination.
There are basically two philosophies for how to use a goal tracker. Both involve picking a set of small goals or habits and marking them off in the app each time you do them.
In the Quantified Self philosophy of goal trackers, you are tracking your goals simply out of curiosity because you want to get more information about yourself. (The word Goal in Goal Tracker doesn't even come into play.)
The second philosophy is focused on a goal-oriented behavior-change mindset where you are using the goal tracker for motivation and accountability in order to get yourself to adopt a new behavior and become a better person.
I simply do not understand the Quantified Self philosophy  don't they want to improve? Every time I go to a Quantified Self event, I feel like I'm surrounded by aliens who love data but not growth.
""I got all this data about myself."" > ""And then what?"" > ""Nothing.""
It just doesn't make sense.
So my goal tracker recommendation is for a goal tracker I built. It is the foundation of the Coach.me community. Most users and most coaches started out by forming habits through this goal tracking app.
Install the Coach.me app and consider tracking a few of the 101 most tracked habits in 2018.
Obviously, I'm talking from my own book here. This article is the product of prior work that started with this goal tracker and then morphed into coaching and personal development publishing. There are many links out there to projects we built at Coach.me or articles we published at Better Humans.
So, with the caveat that I'm deeply biased, my position is that I've seen all the other goal trackers and generally they all use similar formulas and then differentiate just a tiny bit on what graphs they show you.
However, we put every ounce of our design effort into something different: your psychology and motivation. That includes how we trigger reminders, how we try to avoid triggering guilt, how we introduce variable rate rewards for your own good, and how we handle when a goal is too challenging for you.
If you've read this far in the article, I think you will have picked up on my philosophy, which is that I want to see results.
Trying to remember hundreds of passwords is a waste of time. Using the same password for all your accounts is the fast track to getting hacked.
There are a number of good third-party password managers that are much easier to use than Apple's built in Keychain. If you are already using one, then stick with it. This is a principle (covered also in Appendix A) that habits are more important than tools. The benefits of your habits around your current password manager outweigh the feature benefits of switching to a different manager.
I use 1Password but that's not what I'm going to recommend to you. I signed up with them a long time ago when they had a pay-once option. But now they've moved to a subscription model that's quite a bit more expensive than other better options.
So, if you are looking to use a password manager for the first time, then install LastPass. It's the probably best for most people and definitely the least expensive at $24/year.
Download LastPass here. You're going to need to set it up in three places: as an app on your phone, as an app on your computer, and as an extension in your computer's web browser.
On your iPhone, you'll also need to configure LastPass to fill passwords in Safari. Select your password manager from iOS Settings > Passwords & Accounts > Autofill Passwords.
The theory behind the value of a password manager comes down to pragmatic security and reduced cognitive load.
Your parents used to memorize people's phone numbers. Now nobody does that anymore. It should be the same with passwords  you have better things to remember. That's a cognitive load reduction.
Plus, password managers can save time. A common trap is to half-way embrace unique passwords for each app or site, but then find yourself constantly forgetting and having to go through a lost-password routine. This is wasted time.
On the security front, most people who don't use a password manager end up reusing passwords. So when a hacker gets your password to one site, they get it for all sites. Password managers aren't immune to getting hacked either, but at least if you use a popular one you're likely to hear about it when the hack happens.
After accuracy, the must-have feature for calculators is a history. Otherwise you're going to make a typo and not notice.
Unfortunately, Apple's calculator does not ship with a history feature. So:
Notice the second line of numbers at the top? That's a history. Having that history saves you time and reduces your errors because you can spot when you've fat-fingered an entry. Reduce your anxiety and live longer.
""Even people displaying minor symptoms of psychological distress were found to have a 20 percent increased risk of dying over the 10-year study period."" (This is the same citation I used in the section about turning off notifications.)
Technically, this is quadruply redundant.
You can swipe down to get the camera from your control center, tap the camera on the lock screen, or swipe left from your lock screen.
That last, swipe-left from the lock screen is really convenient. Practice that. But also add the camera to your toolbar.
The camera is a great tool for happiness and gratitude. I'm not talking about preening in front of the camera all day. I'm talking about capturing the most interesting moments of your day for posterity and to share with others.
Using your camera regularly also helps you develop an eye for detail.
The general theme of this article is to set your phone up to be more present in the world. Looking for pictures to take is one way to be more engaged with your surroundings. Stopping a meal so that you can capture your food, however, is not the path to living in the moment.
If you are wanting to post pictures to Instagram and Facebook, you can consider the placement of the Camera app to be a replacement habit that allows you to schedule your social media usage. Take a picture in the Camera app and then post it later, during your allotted leisure window.
What does 70% chance of rain mean? Sometimes it means there's a 70% chance of rain over 100% of your area. But it can also mean there is 100% certainty of rain, but only over part of your 70% of your area.
That's confusing. And so the only sane way to check the weather is to compare it to the Doppler radar. These radars visualize the rain and the direction that the rain is heading. That way you can eyeball for yourself if the rain is actually going to affect you.
Download the NOAA Radar app.
While you're setting up weather, you should place this Doppler app on your home screen, and then go to your Today screen and turn on Apple's Weather widget.
A big part of productivity is planning. You've heard a million people complain about how inaccurate weather forecasting is. So here's the solution: be your own forecaster.
The idea behind Pomodoro is to be fully focused for a set period of time, usually twenty-five minutes, and then have a five minute break.
This is a way to train yourself to avoid procrastination. You end up constantly pushing yourself a little bit harder to make it to the end of your work period, knowing that you'll get a short reward after.
The rules of Pomodoro aren't complicated, but it's still nice to have a dedicated app. There are two good ones, but I prefer BeFocused Pro for $2.99 because it can easily categorize your Pomodoros. For example, I'm currently in a Pomodoro that I marked for the Writing category.
When do you use Pomodoro? It's useful for when you are doing individual work, like checking your email or working on a project. You wouldn't use this method during a meeting.
This is one of a handful of strategies in this article for beating procrastination. My intention is that you use all of them at once. For example, the Meditation section is mostly focused on meditation as training that allows you to catch and overcome the feelings that lead to procrastination.
Pomodoro comes at procrastination from a different perspective. It makes for a smaller, more achievable goal. A lot of people get down on themselves if they can't go an entire 8-hour work day without procrastinating. Pomodoro helps you set a more achievable goal, say 25 minutes. And if that's too long, make your Pomodoro even shorter. You can always start small and build up to your final goal.
(The third main strategy for beating procrastination is next  using Brain.fm for background noise.)
A lot of people have trained themselves to listen to music while they work. But almost all research says that performance is poorer in the presence of a background sound.
One obvious benefit, though, of music is social. You put on your headphones and people know not to bother you. I often wear headphones with no sound just to indicate to my coworkers that I'm busy.
The research, however, on music as a background noise is that it's tricky  there are occasional benefits to productivity but also many, many pitfalls. There is another approach: an emerging field of auditory science used to boost focus and reduce mind-wandering.
Brain.fm is the best of these brain music options.
Since I come from the world of coaching, I spend most of my time helping people apply behavior changes. As a result, I often end up in a place where I think I see certain advice working, but I don't necessarily understand or trust the scientific explanation for why that advice works. That's the case with Brain.fm.
My experience with Brain.fm is that it's amazing and works exactly as advertised. Sometimes, without sound, my brain will have a tendency to wander. With the Brain.fm focus music, it some how shuts down that wandering during any dead spots in my work (like if I'm waiting for an app to load). As a result, I have more sustained periods of focus.
However, I find their explanation of the science to be inscrutable. It sounds exactly like the type of pop-culture brain science that lots of people spout. This doesn't bother me, as long as it works.
The music is designed to have effects on neurophysiology via unique acoustic features woven into the music (Brain.fm holds patents for key aspects of this process). Examples include modulations optimized to evoke entrainment of neural oscillations, filtering to exclude distracting sound events, or smooth movement in virtual space to direct attention or avoid habituation.
However, Brain.fm have run studies funded by the National Science Foundation that back up my experience, which is that Brain.fm is better than silence and silence is better than music.
I need to emphasize that this is a corporate-run study that magically ended up with a self-serving result. So, more than the science, I just want you to take my word for it enough to try it out for yourself (remember, there's a free trial).
Your podcast app should be on your home screen and you should train yourself to listen to podcasts during your commute, while cleaning, and during light cardio.
If you listen to podcasts on the bus or subway, here's an important, little-known fact. It's preferable to just listen! You don't also have to be playing games and scrolling Instagram. Be a single-tasker.
The below are where most people should start when it comes to productivity and health podcasts.
Of course, Tim Ferriss is on this list. But I rounded him out so that you get a diverse set of ideas and approaches.
Do not approach your podcast subscriptions as if you need to listen to every episode. Instead, pick and choose the most recent episodes that feel relevant to you.
I've never heard anyone who shares my reasoning for why and how to listen to productivity and health podcasts. Most people just think having more information is inherently good. That's not my reason.
Information is never enough for making important changes. You need to get emotionally hooked, amped up and committed. The podcast format gives you a chance to connect with advice at an emotional level and really feel the social proof. That matters.
Second, most advice only works for some people some of the time. I've written before that you should approach personal development advice as if it only has a 10% success rate. The obvious implication is that you'll always be needing to try lots of approaches until you find the one that works for you. Given that observation, it feels entirely natural to me that you would listen to both Jocko Willink and Gretchen Rubin.
Do not bother with cliff notes. There's more value in being a slow reader, where the stories in the book can work on your emotions, and where your brain can roam freely to make connections between the words and your own world. So skip book summary apps like Blinklist, and embrace reading on the Kindle.
So, yes, install the Kindle app. This would be a good app for your home screen. Try to replace mindless social media usage with deep learning via either reading or podcasts.
But you're probably not done.
Do you like to read before bed? Do not bring your phone to bed. That kills your sleep, bad sleep kills your health, and eventually your bad health is going to kill you. For the ""Human Longevity"" promise of this article, buy a Kindle Paperwhite and put that next to your bed. If you're an iPhone owner, you can afford this second device.
Do you want a book recommendation to go with this section? I've got one you're not going to hear anywhere else. Go buy the sci-fi book Dune and read it in the context of personal development.
Also, two behavior design notes:
I'm absolutely positioning the Kindle to be a replacement habit for Facebook and Twitter. How much smarter would you be if you replaced half of your social media usage with reading?
Second, I have a not-very-well supported theory that's paired with the book Thinking, Fast and Slow. The behavior design implication of that book is that you need to speak to two systems of the brain. Speaking to the rational, Slow System is easy. Just lay out the facts.
Speaking to the emotional Fast System is much harder, namely because it's so hard to see or introspect on what's going on in there. But if you accept that difficulty (and this is the part of my theory that feels like pop brain science), then you realize that you need to start looking for ways to rewire your emotional core.
Then, having accepted that rewiring your emotions is part of most behavior design, I've started to notice things  like that most self-improvement advice is not very rational. That's by design. A self-improvement book is mostly emotional rewiring. That is exactly why you need to read the entire book rather than cheating with a summarized version.
I've tried and like Firefox Focus and Google Chrome, but there's a problem. Either you'll end up cutting and pasting URLs that auto-opened in Safari or you'll end up having to manage individual app's preferences about how to handle a URL click.
Skip those complications. Safari is good.
There's a secret, super rad option in Safari called Reader mode. This mode strips out all of the in-article ads, clutter and junk. I find that it does a great job and saves me from fat-fingering ads that have been placed inside the body of the article.
Here's the before and after on an article in Reader mode.
To turn on reader mode once, there's a little four line icon at the top left of Safari. I'd managed to find that on my own.
What I hadn't realized was that if you long press on that icon, you'll get an option to turn on Reader mode permanently for that site. This is amazing and completely changed my experience of reading articles on my phone.
There are two things that you're setting up here.
One is that you'll flat out save time by not seeing or accidentally tapping any ads. That's a small productivity gain each day.
The second is related to being in charge of your phone. You don't want to see ads because you don't want your phone telling you what to buy and when. Advertising on your phone breaks the tool-not-boss rule.
You've hidden all of your shallow social media experiences in a folder on your second screen. Now, build a replacement habit for those dead times in your day when you would be tempted to be on Twitter or Facebook.
Pick the media that actually makes you smarter and then put apps for that on your home screen.
My apps are Medium, Kindle and Podcast. Maybe you include the Washington Post (although that's probably an anxiety producer that doesn't actually need to be checked all day).
These are your deep learning apps and you just need to make sure they are easier to find than your old, shallow, addictive apps.
Replacement habits are a very common strategy in behavior design. The underlying brain science is that it's easier to create a new habit than to delete an old habit.
In fact, you don't ever really delete old habits. You might stop using the neural pathway for your old habit, but the neurons are still there, waiting for a moment of weakness. Eventually those cells will die out. But there isn't actually a way for you to train them to death.
That's why we use replacement habits so often. You can train a new, strong habit that supersedes your old habit. In this article, we've come at social media usage in a bunch of ways, all of which work together. We've tried to short circuit your existing habits by moving the apps, we've added accountability through Screen Time, and in this section, we've finally introduced the replacement habit that you'll do instead.
Skip this step if you already have a way you track steps. Lots of you have Fitbits or other ways to do this.
If you aren't already using a pedometer, your iPhone will automatically track steps for you in the Health App. However, you don't want to have to open that app in order to see your step count.
So, you're going to need to install an app that comes with a Today screen widget. My recommendation is Pedometer++.
You're building up reasons now to check your Today screen daily. That's good.
Also, I went looking for some research to include here on the benefits of 10,000 steps. Unsurprisingly, the pleasing roundness of that number owes more to marketing than to any particular science (9,901 steps is practically just as good).
There is quality research on the health benefits of even tiny amounts of walking (much less than 10,000 steps): trading two minutes of sitting per hour for two minutes of walking per hour reduced mortality by 33%.
That's not a justification for doing a full more-is-better 10,000 steps though. The science for doing more walking is mixed, and requires piecing together your own projections. For instance, this 2004 Arizona State paper classifies people who walk 10,000 steps as active and people who walk 12,500 as highly active. But they leave it up to you to cross reference other studies on the health benefits of being in either activity category.
So, my recommendation is to wait on the science and trust your gut instead. For most people, walking feels good. It's a chance to use your body, to build up pride in a consistent amount of activity, to listen to podcasts, to see your town or city. Those should be reason enough.
You've heard losing weight is calories in, calories out (CICO). And then you might have also heard this is wrong or at least misguided.
The official position in the Better Humans publication is that CICO is just the wrong framing. Rather, instead, you need to think: to lose weight, I need to burn fat.
Our position is that weight loss is all about putting your body into fat-burning situations.
Calorie restriction, it turns out, is not guaranteed to lead to a fat-burning situation. It can instead lead to a lower metabolism.
What does lead to fat loss then? Low carb diets and fasting. And the most common form of fasting is time-restricted eating, where you fit all your eating into an 8 hour window and then fast for the next 16 hours. People refer to this as 16:8.
Additionally, my experience watching people diet (I ran a 15,000 person diet study and have had more than 100,000 dieters come through Coach.me), is that time restricted eating is an easier behavior change. Basically, it's easier to train your body to skip breakfast than it is to give up carbs, not for a month, but for the rest of your actual life.
Of course, you can combine both approaches. But if you had to pick one, I'd start with time restricted eating.
If you absolutely want to go Low Carb, download MyFitnessPal and upgrade ($49.99/year) so that you can track macros. You want to keep your carbs below about 50g. If you don't pay for the upgrade, you'll only be able to see calories and I'm telling you that's not effective.
But really, where I want you to start is just to skip all of that food restriction stuff and start with time restricted eating. As an example, I try to stop eating before 8pm and then don't start again until noon the next day.
To track all of this, use an app called Zero and put it on your home screen.
The best explanation why fasting leads to burning fat and why simple calorie restriction leads to lower metabolism comes from Dr. Jason Fung.
Consider that your body has two primary sources for fuel. One is glycogen, i.e. the carbs you eat. And the other is stored fat. Your body uses insulin to switch from one source to the other.
Specifically, when you are in a fed state, i.e. you ate recently, your body releases insulin, which inhibits your body from burning fat. Dr. Fung's uses a train track visualization below and I always try to keep in mind which track I'm intending to be on.
Night shift ""shifts"" the colors of your display away from the blue spectrum and toward the warmer (redder) spectrum. That's supposed to help you sleep better.
The standard advice seems to be to avoid screen time and blue light starting two hours before your bed time.
However, my experience with sleep coaching is that people are often going to bed much later than they should, often because of a phone addiction. Starting Night Shift four hours earlier gives you an opportunity to both go to sleep more easily and also to shift your bed time up. If you find yourself going to bed earlier, then just get up earlier. Congratulations, you've become a morning person.
The case against screens is strong. Here a quote from one study:
We found that the use of these devices before bedtime prolongs the time it takes to fall asleep, delays the circadian clock, suppresses levels of the sleep-promoting hormone melatonin, reduces the amount and delays the timing of REM sleep, and reduces alertness the following morning.
All bad.
The science for the red shift as a solution to all of the above negative effects is much more iffy. In fact, Night Shift is a pretty silly feature. The studies say blue light is bad. But at the time the Night Shift feature was built, nobody had done proper testing of red light. It turns out red light is also bad.
So, the reason I like this feature is because it's a prompt to start working on your evening routine to head to bed. Basically, it's just a color coded reminder. That's it.
The Medical ID feature makes key medical information available to strangers when your phone is locked.
If you are incapacitated during a medical emergency, a stranger can go to your power-off screen (long press right button and volume up on modern iPhones). That's where your Medical ID info will be.
I see Medical ID as having three practical benefits to you, in order of likelihood.
Product designers consistently choose female voices for services like Siri and Alexa because culturally we're more comfortable with bossing around women.
Put aside that that's gross for one second.
You're a productivity nut. You're going places. Don't you think at some point you're going to have to get comfortable bossing around men?
This is going to get weird and darkly introspective.
Originally, I'd looked into changing Siri's voice because somebody had told me that men are more likely to be cruel to a female-voiced AI. You know what I mean about getting frustrated when Siri or Alexa makes a mistake. Alexa, especially, likes to butt in and is just begging to be told to shut up. I'm constantly having to tell Alexa, ""Alexa, stop."" but how close am I to cracking and yelling ""Shut up, bitch?""
I don't treat any woman in my life that way and I don't want to start with a robot.
I couldn't find any research at all to back up this idea of gendered cruelty to robots, although I did find published anecdotes:
When Jeremy barked orders at his personal assistant, she didn't flinch, but I did. Something about the sound of his sharp, commanding tone  directed not at me, but still, at a woman  repulsed me. In the few weeks we had been dating, he had never spoken to me this way. But could he? Hearing Jeremy make ungrateful demands didn't make him seem powerful or important. He sounded entitled and difficult, like someone who enjoyed commanding for the sake of commanding. He would ask her to do things he could easily do himself, almost as if to prove that he could. Surely, it would take less time to reach out and hit the light switch by the door than to bark ""ALEXA. LIGHTS ON"" every time he entered the apartment.
Not finding more established research, I backtracked to to the person who'd originally told me this. Her sources were multiple backchannel discussions from people who work on these AI products. People, not just men, say the rudest, cruelest things to female AIs, much ruder and cruder than they do to male AIs. If I can get one of these AI product people on the record, I'll add it to this post.
Our relationships to robots is so weird and interesting and scary  there's deep cultural conditioning, new robot etiquettes, power dynamics, etc.
At heart, I'm worried about developing patterns with my female AIs that I don't have with my female friends, peers and colleagues. And what if those AI patterns transfer over to my human interactions?
You might think the opposite, why can't I practice treating my female AIs with respect and grace? I don't have a good answer. I just know that this dynamic is worth examining in yourself.
Was this too weird of a tangent? This is honestly the type of shit I think about all day.
The default is something like, ""Tony Stubblebine's iPhone."" That exposes your privacy when you have your hotspot turned on, and advertises to the entire world that you don't know how to customize your phone.
There are three strategies for choosing a new name:
Here's how to set this yourself:
Positive self-talk is actually surprisingly effective. I always thought it was too woo-woo, but then I tested it out with some people at work. They all said it was more effective than anything we'd ever done, including journaling, meditation, sleep, priority setting, and morning routines. People have a lot of negative self-talk that they don't like, but which they haven't taken the time to train theirselves out of.
If you're interested in positive self-talk, I got my start with this By the Book episode, where they read and tested a book about changing your self-talk.
Of course, training yourself to be more positive isn't as simple as changing the name of your phone, but it's a start.
If you turn off advertising tracking, then the ads you see won't be specifically targeted to you and what advertisers know about you. The point here is that getting less targeted ads is good. You want to spend money on your own terms.
This is a variant on ""your phone is a tool, not your boss"". When you want to spend money, you want to use your phone for research and then make a purchase based on that research. You do not want the other way around, where your phone is telling you or brainwashing you what to buy. You are the boss.
You're still going to get advertising in some places, but almost all of the advice I've given here includes paying for the ad-free version of the apps you use.
Paying for ad-free apps probably saves you money, as you're less likely to buy something you don't want.
When you stop using your phone, it'll auto-lock to prevent some stranger from grabbing your phone and digging through your private info. That's basically a good feature, but most often the result is that you end up locking yourself out.
Most people keep their cell phones on their person  so keeping your phone locked is not a huge security risk. We're only talking about five minutes  that's the maximum auto-lock setting. If you check your phone on the way into your gym, walk to your locker, put your phone in your locker, change into your workout clothes, lock your locker and walk away, your phone has probably already locked itself. (Plus, most of you actually take your phone out to the gym with you!)
So the strategy here is to save yourself the few seconds it takes to unlock your phone by extending the auto-lock time.
My take on the value of saving time here is that those few seconds of waiting for a phone to wake up is when you are at risk for getting distracted. So, worst case this setting saves you a few minutes of time (research says you check your phone 25 times per day). But best case, it saves you from 30 minutes of goofing off.
Sometimes you need to share your hotspot password with a close friend or loved one. A random three-word phrase is something you can say out loud that they won't have trouble spelling.
This is also a more general trick for creating quality passwords that are easy for you to say or remember. Think about your router's password for example and how you're always having to share it with guests. Sharing a three-word phrase is so much easier than a string of random letters and numbers.
The debate about the security of this strategy for passwords ranges from very positive to just slightly positive.
I spent a summer driving all over the United States, totaling 9,000 miles while working from a campervan. I know a thing or two about internet access from weird spots. In almost all places, using my phone as a hotspot beat out trying to find local wifi. My phone was faster and more convenient. This setting is about making your hotspot even easier to use, especially when you are connecting new devices to it or sharing it with traveling companions. That can actually be a real productivity win.
Swipe down from the top-right of your screen and you'll have access your Control Center which contains toggles for wifi, bluetooth, flashlight and more.
Apple wants to give you the option to turn this behavior off when you're inside of a different app. That's a mistake.
The main reason to have always-on access to the control center is so that you can toggle wifi on and off. Direct, manual toggling of the source of your internet is an important way to control the speed of your app experiences.
There have been dozens of times where I've wondered why an app was slow and then realized the culprit was either the wifi or the cell signal. As someone who travels a lot, I often find that I get the best internet speeds when I'm willing to take direct control of switching between wifi and cell service. Sometimes one of those is much stronger than the other.
When you are using your phone as a tool, then fast internet is a direct link to productivity. That's the main reason to have access to the Control Center.
This setting used to clog up your bandwidth and kill your phone battery. But we're in a new era of fast bandwidth and better batteries. So turn it on.
The upside is that you'll have one less thing to manage, and all of your apps will stay updated automatically.
No special discussion here other than referencing saving your cognitive budget. When automation works well, you're saving your brain cycles to do something better. That's the case here.
GarageBand was 1.8GB when I went to check my installed apps. On a new phone, I'm not planning to do a lot of managing my storage, but 1.8GB is enough to get my attention.
Don't worry, you can always reinstall these apps.
This is the sort of thing that's better to do now, while your focused attention is on setting up your iPhone. You're saving yourself time down the road when you run out of storage at an inopportune time.
Some people take right away to talking to Siri, and other people don't.
The problem is the mental version of muscle memory. If you've never talked to Siri, then it's hard to muster the new behavior in those occasional times where it would be useful.
Here's what I suggest: go through the script below a few times and see if anything sticks. You can turn on Siri by long pressing the button on the right of your iPhone. In parentheses, I've listed the phrase that will reverse the alarm or reminder that you just set.
Now do something to make calling people easier. Set a nickname for a loved one, like your mom.
What we just did here is a tactic called deliberate practice. When most people read about an iPhone trick, they will try to store the trick away in their memory, hoping that they will remember that trick down the road.
I want you to consider a much more effective approach. If you ever think you want to use a new trick or behavior later on, try practicing it first. The practice makes later recall so much more likely.
This is really a life philosophy to build in practice time for all of your life and work skills. For a very deep dive, see Jason Shen's Complete Guide to Deliberate Practice. This is the same article I recommended in the Meditation step.
There are basically two keyboard shortcuts that everyone should have. One is for your email address and the other is for your home address.
Once you get used to the power of shortcuts, you'll probably start adding more.
Go to Settings > General > Keyboard > Text Replacement
Now when you type either of those shortcuts, you'll get an option to autocomplete to your full text replacement.
This is a pure and simple productivity hack. No discussion necessary.
There are at least (at least) five places where your phone will pull your home address.
If you went through the Google Maps section, then you added a home address there.
If you went through the Text Replacement section, then you set a shortcut for your address.
There are still three more places to set:
Boom. Now you've saved yourself some typing down the road. That's productivity.
Yes, you should have backups even though practically every app you install stores your data in the cloud. But backups do make it much easier to set up a new phone when you upgrade (or lose it).
I strongly suggest iCloud backups  they're cheap. I'm on the $10.99/year package for 20GB and creeping toward the next level which is just another $0.99/month for another 30GB.
There should be options there to manage your iCloud storage and which apps get backed up. You should back up most of them given how cheap storage is.
I used to resist paying for backups, but the math on the time savings alone is great. You'll pay a few tens of dollars to save yourself hours of work when you upgrade your phone. And then you'll save yourself a huge headache if you actually do lose your phone completely.
Below are the principles that come up over and over again in this article. Where possible, I've tried to cite science.
However, many of these principles come through my work at Coach.me. We started as a gamified habit tracker, and through that experience, we helped a few million people form a few tens of millions of new habits. The vast majority of those people were motivated by productivity or health.
And then, also through Coach.me, I ran a brain training group that worked individually with 2,000 people and, separately, started a behavior design coaching certification that's produced coaches who've worked more than with 20,000 clients.
So, the principles that don't have citations are, well, just, like, my opinion, man.
No need to lead with science here  this starts as dead simple philosophy. What's the point of life? Abdicating your life to be a servant of your phone is a shitty approach.
Moreover, a lot of research points out that your phone is not a very good boss. See Tristan Harris' How Technology is Hijacking Your Mind or Linda Stone's Email Apnea.
There are a handful of exercises in this article that have very small surface-level productivity gains. Does it really matter if you save one second every time you open your phone?
I say yes. In my experience training people to break procrastination or enter flow states, those pauses are prime times for you to get distracted. In those cases, a two second pause can turn into a 30 minute break.
If any programmers are reading this, what is the honest answer to how long it takes to run your automated tests? Is it the literal time that the tests run, or the time of the break you take while the tests are running?
From my piece on Superhuman Cognitive Stamina:
Every decision, big or small, every moment of mental focus, and every act of comprehension is part of your day's cognitive stamina. Let's call that your cognitive budget.
Many people look at this as the Steve Jobs turtleneck story. What did he wear to work every day? A black turtleneck. So while you were spending your cognitive budget getting dressed, he was saving his up for when he got to work.
So, on your phone, you want to look for places to eliminate decisions ,because you'll use those decisions somewhere else.
This NYT piece is the right primer on the research that says willpower is limited. This other NYT piece is the right primer on Carol Dweck's work that points to the opposite conclusion, that self-belief determines if willpower is a limited resource. Science is so frustrating  two seemingly opposing views. I believe willpower is limited, but trainable.
When you spend time in the productivity world, you start to realize how much time people spend playing around with productivity systems.
Part of this is that a lot of productivity nuts are pursuing productivity as a hobby  essentially, they are role-playing a world where they are more successful. So for them, it's fun to constantly tweak systems and try new tools.
However, the other reason people spend so much time changing tools and systems is that rigid tools and systems break. When they break, it's almost always easier to just start over from scratch with a new system.
So, where possible, try to use a messy system. Those will tend to be less fragile. In this article, the most common messy systems were backed by a strong search feature.
A strong habit is a case where you can do something automatically and effortlessly. That's so valuable. It's very rare that a new tool has a feature that's more valuable than your old habit.
Many productivity reviews say the opposite. They say you should definitely try the newest and latest thing.
They're wrong. If you have a habit based around an old tool, then you should be unapologetic about sticking with that tool.
If any programmers are reading this, a good tool to examine would be your text editor. I alternate between Sublime and Vim. I want to be a Sublime user, but my muscle memory around Vim shortcuts is so good that I still find I'm much more productive in Vim which was publicly released in 1991 and which I've been using since 1996.
I'm strongly influenced by Cal Newport's book, Deep Work and Mihaly Csikszentmihalyi's book, Flow.
Cal comes from the perspective of deep work often being your most productive and valuable work. Mihaly's research was actually just focused on happiness. Deep work is when people report the deepest levels of satisfaction.
Both of them recognize how difficult it is to do deep work or deep learning. Those moments don't come by accident, and when they do, they are easily disrupted.
A lot of the advice in this article is meant to make it just a little bit easier for you to focus, to get into flow, and to achieve deep thinking and deep accomplishments. The main strategies are to eliminate interruptions, replace shallow behaviors with deeper ones (reading vs. tweeting), and schedule leisure activities and shallow activities so that they don't interfere with your deep work.
By default, your phone is set up to create anxiety. I referenced a study about how mild anxiety leads to increased mortality twice in this article.
You paid too much for your iPhone to let it kill you  that's an outrageous side effect. So instead, eliminate the anxiety.
I gave a few examples of how your phone can change your diet and activity patterns in order to help you live longer. Use Coach.me, Zero & the Pedometer.
The bare-bones budget for the advice in this article is as follows:
Bare-bones budget: $59/year
The mid-level budget for the advice in this article adds:
Mid-level budget: $122.99 up front and then $189/year
The max-level budget for the advice in this article is
Max-level budget: $262.99 up front and then $1,172.99/year.
#1. Turn OFF (Almost) All Notifications.Zero badges anywhere. No notifications for messaging, email, Slack. My most notable deviation for notifications is for Coach.me.
#2. Hide social media slot machinesI don't have the Facebook app installed although I do check the web sometimes. Facebook bullied me into having the Facebook Messenger app installed because they won't show me messages on the mobile website.
#3. Hide messaging slot machinesI do see text messages on my home screen from a small handful of people, namely my family and my dog walker. I look at everything else during scheduled inbox time.
#4. Disable App Review RequestsDone.
#5. Turn on Do Not DisturbDone, but just for leisure and sleep time, i.e., 6pm to 9am.
#6. Be strategic about your wallpaperI went with the built in black-with-rainbow.
#7. Turn off Raise to WakeDone.
#8. Add the Screen Time widgetDone.
#9. Add Content RestrictionsJust sfgate.com for me, which was a website that made more sense to visit when I lived somewhere else.
#10. (Optional) Use Restrictions to turn off SafariI did this once in the past. I would do this again temporarily to break any new web browsing habits.
#11. Organize your Apps and Folders alphabetically
This is how I did it. You probably won't have a TV folder like I do  most of those apps are just ways to watch Warriors basketball games.
#12. Choose GmailYes. I get to Inbox Zero every day, practice ""Touch it Once"" where I never read an email more than once, and unsubscribe/mute/filter so that I have as little email in my inbox as possible. I don't use any built-in smart filters.
#13. Choose Google CalendarYes.
#14. Replace Apple Maps with Google MapsYes. I only have a Home set, since I work from home.
#15. Install the Gboard keyboard for faster typingYes, with black background and Apple keyboard removed.
#16. Switch to Google PhotosDone.
#17. Use Evernote for all note taking, to-do lists, everythingYes. It's in my toolbar.
#18. The Case for Calm as your go-to meditation appDone. It's on my home screen.
#19. Install the right goal tracker for youYes, Coach.me, and more than 11,000 check-ins to my habits. I personally track:
#20. Store all your passwords in a password manager, probably LastPassI use 1Password, but if I were starting from scratch I'd use LastPass.
#21. Use Numerical as your default calculatorYes.
#22. Put the Camera app in your toolbarYes.
#23. Use this Doppler Radar appYes.
#24. Use this Pomodoro appYes.
#25. Use Brain.fm for background noise
Yes.
#26. Subscribe to these podcastsSubscribed.
#27. Install the Kindle app but never read it in bedYes. I charge my phone in the kitchen and use my Kindle Paperwhite in bed.
#28. Use Safari this wayYes. The option to automatically turn on readability-mode for certain sites is one of my favorite features.
#29. Organize your home screen for deep learning over shallow learningYes.
#30. Track steps this wayI also get steps from a Garmin watch, but I find I'm more likely to have my phone on me than I am to be wearing my watch. So my phone has the more accurate count.
#31. Prefer Time Restricted Eating Over Calorie CountingYes. Love the Zero app and I generally try not to eat before 12. I also own a KetoMojo blood tester ($60) to measure when I'm in a fat burning mode.
#32. Schedule Night ShiftYes, mine is set for 8pm.
#33. Set up Medical IDYes, mainly so that I can share my emergency contact. I don't actually know my blood type.
#34. Change Siri to a manI'm using the Australian Man voice.
#35. Change your phone's nameYes, ""I Am Focused""
#36. Turn off advertising trackingYes.
#37. Set auto-lock to the maximum time5 minutes.
#38. Set your personal hotspot password to a three word phraseI'm not telling you what it is.
#39. Turn on control center everywhereYes. I also have quick access to my timer and alarm from here, although I'm most likely to set those from Siri.
#40. Turn on Background App RefreshYes.
#41. Delete Garage BandFor me, just Garage Band.
#42. Develop verbal memory for talking to SiriYes, for all the ones in the list plus calling and face-timing people.
#43. Set up these text replacement shortcutsI use three: my address, my personal email address, and the URL coach.me.
#44. Set your addressYes, in all five places.
#45. Backup this wayYes, I use iCloud, with multiple redundancies since my photos get backed up to other places and all of my important individual services are backed by cloud storage.
Do you have anything to add or correct in this article? Let me know and I'll make an update.
But while [Keychain is] broadly useful in OS X, as more developers have adopted it and there's Keychain Access for direct lookups and retrieval, in iOS you have to drill down to Settings > Safari > Passwords to view, edit, or (swipe all the way to the bottom) add passwords. Further, you can't invoke Keychain in Apple's non-Web login dialogs, making it useless for common purposes. And while you can make up a password when you need one, it's awkward to get to and can only be retrieved easily on a corresponding Web page.
The Better Humans publication is a part of a network of personal development tools. For daily inspiration and insight, subscribe to our newsletter, and for your most important goals, find a personal coach.
Founded @coachdotme, @bttrHumans, @bttrMarketing. Helped @medium @calm. Subscribe to my newsletter, Better Humans Daily: https://coachtony.medium.com/subscribe
",182
https://medium.com/@nathancheng/the-cure-for-type-2-diabetes-is-known-but-few-are-aware-5dd3064328b6?source=tag_archive---------9-----------------------,"The cure for type 2 diabetes is known, but few are aware","I recently posted to Facebook about a cure for diabetes and suggested someone try it. Just six days later, I received the following message...",Nathan Cheng,7,"I recently posted to Facebook about a cure for diabetes and suggested someone try it. Just six days later, I received the following message from a friend:
I just wanted to drop you a line and thank you for that post... My lab results at the beginning of the month were 230. After just this last week it's down to 155. I think I'll be in normal range within a month. Really miraculous... It's really been a game changer for me already and I wanted you to know how much I appreciated the info and how much of a difference I think it will make in my life.
Four months later, the friend posted this to Facebook:
I started on this regiment when Nathan posted about it [four months ago]. My blood glucose level at that time, while taking two daily glucose meds, was 235. Two weeks ago, my [fasting] glucose level, WITHOUT the meds, was 68.
If you google ""diabetes cure"" you are directed to websites like WebMD and the Mayo Clinic where you find information on diet, exercise, medication, and insulin therapy, but nothing about the cure. This lack of information may have to do with the fact that Americans spend $322 billion a year to treat diabetes, $60 billion a year on weight-loss programs, and $124 billion a year on snack foods. This is about 3% of the US economy! Because so many peoples' livelihoods are supported by diabetes and its main cause, obesity, the viral effect of people getting cured and telling others is greatly diminished.
Because of this understandable stifling of the message, if you are like my Facebook friend and have already experienced the type 2 diabetes cure for yourself  there are thousands of you out there  it is important for you to share your success stories as far and wide as possible. You can simply share this post!
Now, here is the cure. Just follow these three simple rules. If you are taking insulin or other medications, you must coordinate this with your doctor, as dosages will need to be adjusted (downward) so that you don't die from an overdose.
1. Absolutely no snacking between meals.
This seems hard to do, but really it's not if you know one secret: Replace snacking with something far more satisfying  fat. That's right, the government is wrong to recommend a low fat diet. Fat is what makes you feel full until your next meal. Take away the fat, take away the full. Don't go to an extreme, but do lean strongly toward a high-fat low-carb diet.
One easy way to increase your fat content and quit snacking is to begin your meal by eating an avocado. I and others I know have used this trick to easily quit snacking. Avocados protect you from one of the reasons some dietary research wrongly claims that high-fat diets are bad for you: the danger of gorging yourself on delicious, fatty foods. With plain avocados, there is little danger of gorging. Another danger is clogging your arteries and giving yourself heart disease. But it's been amply shown that the blame for that falls squarely on trans fats, like margarine. If you see any product with the words ""partially hydrogenated"" or ""hydrogenated"" in the list of ingredients, put it back, it's a trans fat. On the other hand, any fat that comes directly from an animal or plant is not a trans fat and can be safely consumed.
Hint: There may also be benefits to eating the avocado first, as research has shown that the fat literally coats the inside of your intestine, physically blocks other food from being absorbed, and effectively reduces the size of your meal.
Hint: Using more olive oil in your cooking can also increase healthy fat content during meals and help you eliminate snacking, just like the avocado. Butter also works, and it is very delicious, just don't overdo it.
One way to cheat: Coffee and tea, optionally with a bit of cream  but never with sugar or other sweeteners  can be used as a snack if you are having an especially hard time lasting til your next meal.
2. Limit meal times to 1 hour or less, with at least 12 hours between the end of dinner and beginning of breakfast.
You should have no more than three of these ""feeding times"" per day. The reason limiting the number and duration of your meal times is so important has to do with staying out of the vicious cycle of increasing insulin resistance. To get smart on insulin resistance  the cause of both type 2 diabetes and obesity  read Dr. Jason Fung's book, The Obesity Code: Unlocking the Secrets of Weight Loss, or watch his free lecture on YouTube.
Hint: If you feel like snacking, just wait a bit and make the snack part of your next meal.
Hint: You never have to count calories again. When you eat, eat until you're full. I find myself putting down my phone and focusing on the eating. With calorie guilt gone, you can eat what you want and savor it slowly. After years of torturous calorie restriction, eating is really enjoyable again!
The above two rules are the only dietary rules you need to maintain ideal weight for the rest of your life, assuming you apply common sense and avoid extremes. The diet works by building in regular periods of insulin relief, keeping your body from becoming resistant to insulin. Following these two rules, you will maintain your weight and health by never entering the vicious cycle of increasing insulin resistance.
HOWEVER, if today you are not your ideal weight (a BMI of about 22), you are already in this vicious cycle and you need to break out of it. In order to break out and quickly get down to your ideal weight, you need one more rule, the rule of using special tricks:
3. Use additional tricks to accelerate your escape from diabetes and obesity.
Use any combination of the tricks below to accelerate your weight loss and return to good health. If you use all five wisely, you can get to your ideal weight in 6-12 months or less  even if that means losing 100 pounds or more. Yes, think about your weight 10, 20, 30 years ago. Another friend of mine started on this journey last year weighing 270 pounds. He's in his mid-thirties and about to reach his college wrestling weight class of 197 pounds and just ran his fastest 2 miles ever. He got to this point by following the two rules above and just 3 of the 5 tricks below.
Trick (important): Add a teaspoon of vinegar to your first bite of food at every meal. This lowers your blood sugar by 30%. Most people use apple cider vinegar. If you can't stomach it, a friend who monitors her blood sugar to control gestational diabetes tells me that 5 gherkin pickles also do the trick.
Trick (important): Cut down on sweets, and if you can, cut them out entirely for a couple months. I still eat ice cream about once a week, and know people who are losing weight on this diet while eating ice cream almost every day. But this probably won't be the case for everyone. Better to severely restrict sweets for the first few months, and then gradually reintroduce.
Trick (not as important): Exercise more. Regularly giving your muscles a workout has been shown to increase insulin sensitivity. I say exercise ""more"" because if you're not getting any exercise at all, you're not even trying and probably haven't read this far anyway.
Trick (not as important): Eliminate breakfast. Instead of breakfast, increase the fat, protein, and vegetables you eat at lunch and dinner. I've eliminated breakfast while increasing my overall daily calories from 1600 to 2500+, and still lost six pounds in the first two weeks.
Trick (most important): Go for longer periods of time without eating (yes, yes, fasting). Consume water only for days or weeks at a time. Your fat will literally dissolve away, and with it your type 2 diabetes and other ailments. The definitive books here are Dr. Joel Fuhrman's, Fasting and Eating for Health: A Medical Doctor's Program for Conquering Disease, and Dr. Jason Fung's Complete Guide to Fasting. I highly recommend them both; if you're skeptical, read the testimonial comments on Amazon. I and at least 20 of my friends have tried fasts lasting days to weeks. It works, and it is amazing.
Don't let anyone discourage you! Your doctor may be skeptical and resist your efforts to cure yourself, but persevere! Worst case, put your doctor in touch with Dr. Jason Fung, a nephrologist who grew tired of simply controlling pain for his end stage kidney patients at the end of lives ravaged by diabetes, and decided to do something to help them thrive with the energy of a healthy life well-lived. Now follow the simple rules plainly and freely explained above and help yourself!
If you have positive experience with the methods described here, please leave comments below, hit the ""recommend"" button, post on Facebook and Twitter, and forward this post to everyone you know! If you don't have experience yet, but are thinking about it or have other questions, please reach out and I will help.
",183
https://medium.com/curology/pimple-face-mapping-what-pimples-on-specific-areas-of-your-face-means-84795c018cf6?source=tag_archive---------4-----------------------,Face mapping: how to deal with acne like a true detective,How to deal with acne like a true detective,Curology Team,5,"Figuring out what causes acne can be a tough case to crack, but preventing breakouts just takes a little detective work. It's a not-so-smooth criminal that tends to return to the scene of the crime (i.e., your face), if you'll join us on this true crime metaphor. Face mapping is key to identifying the culprit: while many factors can lead to breakouts, the location on your face can be a revealing clue. Think of pimple face mapping as doing a forensics report at the scene of the crime. Here are some clues to the meaning of pimple locations, and what you can do to solve the case of the reappearing zit.
There are a few possible reasons you'd get pimples in the same place over and over again. Acne can come back in the same spot from certain behaviors, such as sitting chin-in-hand while reading or working on your computer, touching your phone to your face when you're talking on the phone, or putting certain makeup or products on the same area on a regular basis. If you have a habit of picking or squeezing your zits, that will also cause recurring breakouts in that area, because the pimple won't heal properly and may spread into the surrounding area. Often, a pimple recurs in the exact same spot because there is still inflammation deeper in the skin, which makes the spot vulnerable to the usual acne influencers (i.e., hormones, stress, diet, local irritants, etc.). Using a medicated treatment like your custom Curology superbottle can help with that!
Pimples popping up on your lower cheek, jawline, and chin is known as a ""hormonal pattern."" Acne in these areas is often caused by your skin's oil glands overreacting to hormonal responses, which can be triggered by factors such as stress, eating too much sugar or dairy, or the (perfectly normal) fluctuations that happen during women's menstrual cycles. Also, women who have a hormonal imbalance due to a condition such as polycystic ovary syndrome (PCOS) may develop acne in a hormonal pattern.
Just like on your jawline and chin area, acne on your neck can be caused by your skin's oil glands overreacting to hormonal responses.
Learn more about what causes acne, including what foods and other factors can trigger an acne-triggering hormone reaction.
Blackheads are small clogged pores (aka open comedones) that turn black because the trapped oil and skin cells are exposed to the air. People tend to get blackheads on their nose because the skin on and around your nose (aka the T-zone) has a high concentration of oil glands.
You might try a pore strip once or twice a week, such as Biore Deep Cleansing Pore Strips or Boscia Pore Purifying Charcoal Strips. (We only recommend pore strips for your nosethey can be too harsh for other parts of your face).
Since your nose might get oilier than other parts of your face, you might not need to apply moisturizer as much there. The ingredients in your custom Curology superbottle can help keep it in balance, too.
Our cheeks are one of the more common places to break out, and this can happen for a variety of reasons: touching your cheeks, holding your phone against your cheek when you're talking on the phone, friction or bacteria from the pillow you sleep on (or sleeping on your hand), or wearing makeup on your cheeks, to name a few. However, in most people, there is not a specific reason for acne occurring on the cheeks  it just happens (sorry)!
Forehead pimples could be caused by hair products, shampoos, and conditioners, so if you're breaking out there, check your hair products for pore-clogging ingredients such as sodium lauryl sulfate, sodium laureth sulfate, and coconut oil (aka cocos nucifera oil in many ingredients lists). You can also look up a product on CosDNA.com to get an instant analysis of its ingredients, complete with a rating on the acne-causing and skin-irritating scales. Check out our easy guide: How to know if any makeup or skincare products might cause acne breakouts.
Another possible cause of forehead bumps is pityrosporum, a type of fungus. Pityrosporum (aka malassezia) is a regular guest on our skin, but too much can be related to seborrheic dermatitis (i.e., dandruff). Dandruff, FYI, can happen on your face as well as your scalp. 
If you've got signs of pityrosporum or seborrheic dermatitis clogging your pores with skin flakes, there are some easily accessible treatments. A zinc pyrithione soap or shampoo can help acne along the forehead/hairline, and an over-the-counter ketoconazole shampoo (1%) will help with a flaky, greasy scalp.
Check your toothpaste for sodium lauryl sulfate and/or sodium laureth sulfate. Some people break out more when their skin comes into contact with toothpaste foam because it's got those pore-clogging ingredients. Some people might be irritated by whitening agents or fluoride, too. If you're breaking out around your lips, try switching to an SLS-free and fluoride-free toothpaste for a couple of months  it might make a difference! Check out brands such as Tom's of Maine, Sensodyne, and Jason Natural Cosmetics for SLS-free toothpaste options.
Whatever the cause of your acne, your custom Curology superbottle can help you vanquish those villainous whiteheads and blackheads. It's elementary for our dermatology providers. (How's the detective metaphor working for you?) We'll even take the case pro bono for the first month when you sign up for a free trial at curology.com.
Skincare with substance.
100 
4
",184
https://medium.com/hackernoon/im-32-and-spent-200k-on-biohacking-became-calmer-thinner-extroverted-healthier-happier-2a2e846ae113?source=tag_archive---------3-----------------------,"I'm 32 and spent $200k on biohacking. Became calmer, thinner, extroverted, healthier & happier.",Other deep-dive articles by Serge:,Serge Faguet,29,"hackernoon.com
hackernoon.com
This post is about how to use modern science and personalized medicine to make yourself healthier, more productive and happier. Every day.
LEGAL DISCLAIMER: IT IS VERY IMPORTANT THAT YOU THINK CAREFULLY, EVALUATE YOUR OWN HEALTH, AND CONSIDER PROFESSIONAL ADVICE BEFORE YOU THINK ABOUT DOING ANY OF THE THINGS I DO. WE ARE ALL VERY DIFFERENT AND WHAT WORKS FOR ME MAY BE DANGEROUS OR USELESS FOR YOU. I AM NOT A DOCTOR. THIS IS NOT MEDICAL ADVICE. THIS IS JUST MY PERSONAL STORY.
Serge Faguet, Russian, 32, Cornell undergrad / Stanford MBA-dropout, started some tech companies (TokBox  leading B2B video communication infrastructure company bought by Telefonica; Ostrovok  largest online travel co in Russia, profitable with ~$500m gross turnover in 2017; new stealth-at-this-point AI co in Silicon Valley; been through Ycombinator; worked at Google very briefly).
Before all this I had a lot of challenges with anxieties, insecurities, introversion, weight, focus, anger management, bad moods, procrastination, and other things that many of us struggle with. Now all of them are effectively gone. I mean, I wrote these two huge posts in one weekend and didn't procrastinate for a minute.
It seems obvious that whatever our goals are in life, certain things can help. I (and many others) want to be more energetic, have better willpower, not have bad moods, have the confidence to talk with the cute girl in the line at the supermarket.
All these things depend on transient mental states. And they in turn depend on your biochemistry (amongst other things). If you ever meditated, took drugs, had a bad night of sleep, or been sick you know this to be true: with these biochemical fluctuations your personality changes, and behaviors can become much easier or harder.
My goal is simple: manipulate my biochemistry to get more of the things I want, and less of the things I do not want.
Good mood, confidence, focus, energy, willpower, stress resilience, brainpower, calm, health, longevity, removal of social anxieties/inhibitions. All the time. With minimal investment of time and minimal risk.
I attack 6 major blocks:
In this post I will give just an overview of things I feel are most important in each area, and then in future posts will dig in at length.
Important note: all these go together. You cannot sleep well while eating like shit, not exercising, and stressing out. And you cannot exercise/eat well/not stress out if you sleep badly.
In all these areas I use roughly the following framework to decide regarding specific tactics:
I am actually very conservative and paranoid about these things. For example, I take months to research and talk with people before I decide to take a new compound, start with low doses, and consult with several independent experts. Be careful and don't screw yourself up with this kind of thing.
Get plenty of restorative deep and REM sleep on a regular basis.
This book has more info if you'd like to really dig in http://biohackingbook.com/ (you want the free chapter on sleep, I don't like the other ones as much)
Sleep is essential to willpower, which is essential to everything else here.
Keep in mind that things in other sections will help a lot. In a future post I will dig into a lot more detail and describe my personal journey to better sleep.
Eat good stuff. Don't eat bad stuff. Habituate.
minimize insulin levels, maximize glucose disposal, optimize various growth factors (IGF-1, mTOR etc.) towards things we want (e.g. muscle) and away from what we don't want (e.g. cancer).
Nutrition compliance is generally incredibly hard and may take years to develop. Don't try to do this all at once, it is too hard. Just try to make small steps to improve. I still occasionally eat bread or pasta or have a glass of wine.
In a future post, I will describe how I gradually, slowly and very painfully improved in this regard.
Maximize metabolic benefits (hormones, insulin sensitivity). Minimize time spent and risk of injury.
I do the following:
I found gym to be one of the easiest routines to set up. Having personal trainers helped a lot. Now it is such an ingrained habit that I don't need them anymore.
Maximize stress resilience, social support, sense of purpose. Eliminate bad moods.
I don't talk much about sense of purpose because I never lacked it. I don't know how to help discover that.
Mental health is the most complex of all the things listed here, and is helped by all other sections. So there will be a lot of extra posts on this.
Discover your bugs and fix them if possible. Everyone has bugs.
I wrote a very detailed separate post that includes a huge number of tests I have done, how I interpret them etc. Read it here, I promise it is very interesting, and it lists hundreds of metrics I consider. It is here: https://medium.com/@sergefaguet/hundreds-of-biomarkers-i-test-in-pursuit-of-focus-health-energy-confidence-and-happiness-dd01572c518b
Interpretation is hard. This is a new field, the data is very complex, and if you really want to get value out of this, you have to spend a lot of time and effort. It is just the way it is.
Also if you look at the testing post, you will see that:
Depends on the specific intervention.
Above are some of the supplements (not including pharmaceuticals) I currently take. A top-notch medical research team I work with went through a large number of studies related to most of these and felt the ones at the top are quite likely useful, the middle probably useful, and no opinion on the ones at the bottom. None of these supplements were thought at all likely to be harmful.
Keep in mind: pharmaceuticals act very differently on different people. Some of these may be useless of harmful for you, even if they benefit me. I have gradually phased these in over years of self-experimentation and deep understanding of how they work.
LEGAL DISCLAIMER: I AM NOT RECOMMENDING YOU TAKE ANY MEDICINE WITHOUT PROFESSIONAL SUPERVISION. PRESCRIPTION DRUGS SHOULD ONLY BE TAKEN BY PRESCRIPTION FROM A REPUTABLE DOCTOR. I AM NOT A DOCTOR.
On that note. I think given the way healthcare is organized today, this kind of personalized medicine could significantly increase social inequality beyond where it is now.
People like me will be able to pay for this out of pocket and use off-label prescriptions from private doctors who focus on upgrading and prevention rather than merely healing. Downstream the extra mood, energy, focus, health, willpower and social skills  enhanced over decades  will accrue further and further advantages to people who upgrade themselves, which will lead to a cycle of further concentration of wealth.
I don't have any suggestions on this, just pointing out that this will be a far bigger problem than many other social-justice issues that get attention today.
Thanks to everyone who has read this, I hope it has been useful! Also again reminding  read this other post if you did not already https://medium.com/@sergefaguet/hundreds-of-biomarkers-i-test-in-pursuit-of-focus-health-energy-confidence-and-happiness-dd01572c518b
I am also happy to speak at high-quality events/podcasts, talk with journalists about all this  feel free to reach out! Best is to send me a message on Facebook Messenger (https://www.facebook.com/sergef) or Instagram (https://www.instagram.com/sergefaguet/).
This post is an in-depth review of hundreds of biomarkers I test with real data and commentary  insulin resistance, hormones, ketones, microbiome, toxins, athletic capabilities, bodyfat, mercury, allergies and a lot more. The idea is to illustrate the biomarkers I check and ways I interpret them. I have no sense of privacy at all so feel free to use in whatever way you like.
This is meant to be read together with my main post on biohacking: https://medium.com/@sergefaguet/im-32-and-spent-200k-on-biohacking-became-calmer-thinner-extroverted-healthier-happier-2a2e846ae113
My interpretation at the end of this is that I am extremely healthy, much younger than my chronological age, perform much better than the vast majority of humans on many dimensions, and will live longer/healthier than anyone who has ever lived as of 2017.
Bold claims  partially designed to persuade you to read this long post :) Let's jump into it.
Cholesterols:
Excellent on inflammation markers  way better than suggested optima, CRP below detection limit. There is a lot of evidence that suggests inflammation is key in all major aging diseases (cancer, heart disease, neurodegenerative disease) so happy to see this. Everyone should know their inflammation markers.
Also great on endothelial function which is key for preventing heart disease.
Here should mention some key genetics things you should be aware of, especially your ApoE (key to Alzheimers risk) and MTHFR (which is what screws up my B12 metabolism).
My TSH is actually not great, the lab is wrong to consider that optimal. Optimal is under 2, so my thyroid is working a bit harder than it should. Wrong lab range is partially because reference ranges are based on sick people.
You should know your Vitamin D and homocysteine. My Vitamin D started at around ~24 and I supplemented it up. My homocysteine started at ~12 and I supplemented it down into optimal territory with a lot of B-vitamins. Both markers have significant associations with key diseases
(credit www.marksdailyapple.com  a great blog about health!)
The screen above shows data compiled from a bunch of studies suggesting that relatively high levels of Vitamin D are protective against a number of diseases. It is very unlikely you will get to these levels without supplementing unless you are a lifeguard on a beach in Israel.
Here the most important thing is that I have exceptional kidney function (eGFR and Cystatin-C)  in the top 5-10% of people in my age group. Especially important for me because all my supplements add workload on kidneys. eGFR is one of the key markers evaluated to calculate people's ""true biological age.""
This all mostly has to do with insulin sensitivity which is one of the most key pathways in the body in terms of staving off aging and disease. My baseline glucose, insulin and HbA1c are actually not that great, but they don't matter as much.
What matters the most is the OGTT (""Postprandial Evaluation"" in the image above)  a test where you measure markers, then ingest a lot of fast sugar, then measure again couple more times after defined periods. This test shows the dynamics of how efficiently your body gets rid of sugar. In my case, it is extremely good  basically drinking 75mg of a very sugary solution has barely elevated my sugar, and then a small amount of insulin removed a huge amount of glucose out of my blood, taking me down to 46mg/dl. Most people would feel quite hypoglycemic here but I don't because my sugar is constantly low and my body is used to it.
Alpha-hydroxybutyrate is raised because of my ketogenic diet.
Ferritin is fairly low because I eat well and give blood to do all these tests! This is quite key, especially for men. Lower iron is hypothesized to be one of the reasons why women live longer.
OK, so more on insulin/glucose pathways. Sorry about the Russian in the images. Above is a test called RQ (respiratory quotient) of how much oxygen/co2 you process at rest (you basically put on a gas exchange mask that is also used for VO2max tests below, and sit calmly). What it tells me is that my body runs 85-100% entirely on fat, a little on protein, and not at all on carbs. Which is exactly what you want. I start using the carbs in my body as a dominant energy source only after I get to >150 heartbeats/minute. This enhances athletic performance since glucose storage in the body is limited.
(credit Peter Attia @ http://eatingacademy.com  great blog by one of the most knowledgeable people about preventative medicine in the world, in particular about insulin resistance and ketogenic diets)
If you read up about how cancer cells preferentially eat sugar, how ketogenic diets are more metabolically efficient and generate less waste byproducts per unit of energy etc., you will see that keeping your insulin/glucose pathways extremely optimized is an interesting opportunity for material life and healthspan extension. Not to mention body fat % reduction. This also deserves a whole separate post.
On that note, here is my body fat graph. This was done in 33 separate measurements by the same personal trainer with the same equipment via measuring ~15 different sites for subcutaneous fat with calipers. This is error-prone. DEXA scans are better, but we didn't have easy equipment access on a constant basis.
I think the absolute values are suspect, and bioimpedance suggests I am in the 10-11% range right now rather than ~7%. But the directional trend is very clear and encouraging. Also cool that I actually barely lost any weight, so I added a lot of muscle which has major health benefits (if you want a great detailed book on how muscle enhances your health, read https://www.amazon.com/Body-Science-Research-Program-Results-ebook/dp/B001NLL38S/).
I don't good have before-and-after pics but I definitely used to be chubby, as all my friends and employees are delighted to keep reminding me :)
Also for those that are interested in ketogenic diets, you measure them via glucometers + ketometers similar to mine above. Be warned that keto blood strips are quite expensive. And that actually getting to a level of ketosis like mine above (after a ~16hr fast) will take a long time and a lot of work.
Also you can use CGM  an implant that tracks your glucose in real-time and sends data to your iphone. This can help figure out which exact foods, activities etc. alter your glucose and are harmful. The above image is from a night a couple years ago when I was stressed, stayed up late, and ate a lot of chocolate while sitting in a bathtub. Note how big the late-night jump is relative to the increase with good food around 6pm. This graph very clearly and unambiguously tells me ""hey dude, you are fucking yourself by eating this sugary garbage. Like you just fucked yourself right now  look, let me show you in real-time how you are damaging your health!"" Motivational stuff.
CGMs are very cool, but expensive (the Dexcom G5 I used was >$4,000) and a huge hassle. I do think this is the future and plan to use them a lot though. Imagine real-time monitoring of testosterone connected to a pump that keeps it at optimal levels depending on time of day or activity! This is already feasible (albeit not with all molecules), someone just needs to engineer it.
OK, back to more blood testing! Some notes:
As I mention in my main framework post, I boost thyroid hormones, IGF-1 and testosterone.
Testosterone deserves a special mention. In short, I have average-ish testosterone but low LH/FSH (which serve as inputs to testosterone). What controls FSH/LH is binding of estrogen at the pituitary gland. And because estrogen is a testosterone downstream product, there is a feedback loop.
So what I did is take an ""estrogen blocker"" which competitively occupies the same sites estrogen occupies at the pituitary, without inhibiting FSH/LH. The result can clearly be seen above  in a very short period of time (2 months), free testosterone jumped a lot. This has a noticeable effect on mood and energy. Also very noticeable effect on wanting to have sex all the time:)
This is a basic blood panel. Nothing too interesting, you basically just do these to make sure nothing is screwed up.
I have high EPA/DHA and Omega 3s in general + low Omega 6 and trans fats. Result of great diet + supplementation. This is basically what you want too.
OK, so in the graph above you can see I had elevated mercury and got rid of it. The hypothesis is that it is due to a dental amalgam. I fixed it by stopping eating any large predatory fish + enhancing liver detox capabilities via B-vitamins and the like.
Mercury is likely to interfere with a lot of neurobiological processes and contribute to many issues such as mood disorders. Good riddance.
This is a CIMT test. It basically shows the thickness of your neck arteries, which appears to be very predictive of cardio risks a long time in advance. Another factor that suggests that my lifestyle approach is working. The cool thing is that this compares you with people in your own age demographic, so I know that unless something changes, I should have low cardio risk even when I am 85. Cool test, non-invasive, highly recommended.
VO2Max is a measure of how your body performs in terms of gas exchange, and as such evaluates the capabilities of your cardiovascular and pulmonary systems. You measure it by exercising while wearing respiratory gas analysis equipment as in the photo of me above. As the quotes from New York Times above highlight, it appears to significantly predict lifespan and ""fitness age.""
My VO2Max has risen a lot over time and is very high, not far from Olympic medalist levels although I have just about zero athletic accomplishments. I mostly credit high-intensity interval training and weights training, but I don't really know. More recently in a period when I stopped doing HIIT, I saw this metric worsen. Another good indicator that I'm doing something right though.
The above is from a test called ""GI Effects"" by Genova Diagnostics. It tests your poop (which is very fun to mail) for bacteria, parasites, metabolites and a lot of other interesting things.
I think this is a very fascinating test, but I have no idea how to really use it yet beyond ""do not take antibiotics, do take probiotics & prebiotics"". Microbiome is a super exciting new frontier of health, but we don't know whether certain microbes cause health states or simply correlate with them. If you want to learn more, take this great class on Coursera which I did and learned a lot from https://www.coursera.org/learn/microbiome
https://drive.google.com/file/d/0BzeKcwaaUlObZlhEWFotenk4d0U/view?usp=sharing
Anyway, my latest entire test is linked to above. Now the entire internet knows exactly what bacterial strains live in my poop as of 2017. Enjoy :)
This is an interesting test (Genova Diagnostic Toxic Effects) that suggests somewhere I was getting significant exposure to VOCs (which are definitely bad for you). The actions I took were to enhance office/home ventilation, not eat/drink from plastic containers, not stand next to people who smoke.
On that last note, one time I took this test after smoking some pure-weed joints (which I do very rarely), and the toxins went off the charts high. If you can't stop smoking, at least vape. Inhaling burned weed or tobacco is incredibly bad for you, I was shocked by the levels of toxic chemicals I saw in myself after just one evening of several joints.
This is from two tests of allergies/sensitivities by Cyrex. The reason I took it is that if you eat products you are sensitive to, you may increase inflammatory activity in the body which we do not want.
My takeaways are that I shouldn't eat tuna or scallops, and that I am allergic to certain molds. Also that I appear to have a clearly-acquired gelatin sensitivity because of all the capsules I take. Fascinating.
Other interesting tests/metrics:
There are many other tests I have done, but this is already an incredibly long post.
The key takeaways are that:
Also here is a framework I like using. Below are all-age causes of death in the US. My markers suggest that I am at extremely low risk for a large number of them, even in my age group. So if I maintain a similar set of behaviors (and actually I keep improving every year) I should have low risks in my age group when I am 80 as well unless something screws up.
With the above framework I also identified that my key risk is cancer  which is why I am so focused on low sugar and insulin (which appear to feed cancer preferentially, https://en.wikipedia.org/wiki/Warburg_hypothesis) and my immune system which is somewhat weaker due to CMV, so very interested in iP stem cell therapies that could replenish naive lymphocytes.
I expect that even without major medical progress, I will be in excellent mental/physical condition at 100 and will live longer than any human who has ever lived as of 2017. With tech progress, it seems likely that immortality is reachable.
I could be wrong, but I suspect that today there are fewer than 1000 people out of 7 billion who know as much about their own health as I do. Because of how onerous, expensive, and non-intuitive this is.
But this should become much more mainstream. It really does give you a significant advantage in life across very many areas  thanks to this, I often feel ""superhuman"" because of how much control I have over my health and behavior relative to what I see in other people. And humans tend to adopt things that enhance their social competitiveness.
I am happy to speak at high-quality events/podcasts, talk with journalists about personalized patient-driven medicine etc.  feel free to reach out! Best way to reach me is to send me a message on Facebook Messenger (https://www.facebook.com/sergef) or Instagram (https://www.instagram.com/sergefaguet/).
*******************************************************************
hackernoon.com
hackernoon.com
#BlackLivesMatter
16.6K 
116
Some rights reserved

",185
https://medium.com/@learngirl/iron-supplements-explained-61c1ce570313?source=tag_archive---------5-----------------------,I need more iron,"There is a  lot of iron research in here, but I've tried to keep it as simple and visual as possible.",zawadi,18,"and it seems I'm not the only one.
""Iron deficiency is the most common and widespread nutritional disorder in the world"" WHO
During my search for answers on why I'm not getting enough and what the difference is between supplements, I was frustrated at the lack of clear answers. Hopefully my research will help others struggling to work out the same thing. I'm a designer not a doctor, but that might make me the perfect person to explain things simply.
First a couple of important terms:
A good start starting point is considering how we absorb it.
Chances are that you're already aware of the consequences of low iron. The main ones being physical and mental fatigue that comes from not having enough iron, the crucial ingredient for dna, dopamine, white blood cells and hemoglobin for oxygen transport. It likely affects kids development and can have particularly negative consequences if you're pregnant.
Iron overload however can be worse.
Iron can be a reactive little element when it isn't properly bound, and can release free radicals which cause oxidative stress to tissues. For this reason our body tries to keep iron locked up in a stable form, however if we increase iron too quickly or over time saturate our normal ways of dealing with it then we end up with iron overload. Excess iron is stored in our organs, so long periods of high iron can cause nasty things like liver and pancreas damage, heart disease and reproductive problems. The people most at risk of iron overload are people with the genetic disorder hemochromatosis. This disorder causes the body to absorb too much iron from the diet and it is fairly common if you're of Northern European ancestry  like 1 in every 200-300 people. Without this disease you're not that likely to overload from food, but it can happen through accidental overdosing on iron supplements. Children are especially at risk of this.
Increased risk of infection
There is another thing to be aware of with iron. As much as we need iron to grow, so do microorganisms. People with a lot of free iron floating around are generally more susceptible to bacterial infections and other related diseases like diabetes, Alzeheimer's and multiple sclerosis.
Both iron deficiency and overload can make you tired (as well as lots of other things) so self diagnosis is definitely not recommended. The test is a simple blood test. The lab will indicate the healthy range, but here's what the different measurements mean.
Only a fraction of the ferritin in your body is in the blood, but measuring this ferritin gives a good indication of your overall iron storage. Ferritin is the first iron to deplete so it's the early warning sign of iron deficiency.
There are tester kits (1, 2) on the market where you can check your own ferritin levels.
The amount of iron attached to transferrin is an indicator of how much iron is in your body. As well as measuring the serum iron, usually another test is done to check that transferrin is working properly (TIBC or direct transferrin test). Using these two tests you're given Transferrin Saturation(TS)%.
This measures the amount of hemoglobin in the blood which is needed to transport oxygen around the body. Low Hb is the final stage of iron deficiency and is an indicator for anemia. Low iron isn't the only thing that causes anemia, which is why all the tests are needed for the full picture.
There are two factors that make up how much iron you're getting:
It's not so hard finding the iron content of things but determining bioavailability is a challenge.
With all that uncertainty I guess it's not surprising that there are so few figures on the bioavailability of iron in food. One figure bandied about is the bioavailability of heme iron which is often given as 10-35% . Note that heme makes up only part of the iron in meat and cooking and freezing can further reduce the heme.
Plants contain only non-heme iron but aren't any easier to figure out. In general they have a lower bioavailability than meat at somewhere between 1-15%. A difference of up to 15 times is quite a lot, and it's due to the large effects of enhancers, inhibitors and cooking.
The plant food types that have high bioavailaiblity are generally the ones that contain high amounts of natural acids. Ascorbic acid (also known as vitamin C) is the most effective enhancer, which helps iron by protecting the iron from inhibitors, keeping it soluble and in the ferrous state. Other natural acids like citric, malic, tartaric and lactic acid also enhance iron but to a lesser extent. How much acid is needed is often given as 20mg ascorbic acid for 3mg of iron, but that's provided there aren't too many inhibitors. With high amounts of inhibitors then double that is needed.
The acids don't help heme iron get absorbed, however meat tissue enhances all types of iron. This 'meat factor' isn't fully understood, but its enhancing effect is less than ascorbic. One study found the effects of 30g muscle tissue to be equivalent to 25 mg ascorbic. These two enhancers aren't additive, so if iron has been fully enhanced by acid, meat can't enhance it further.
Reference doc for enhancers
Inhibitors form a compound with iron that stops it getting absorbed. Phytate is the main inhibitor of plant food which is found mostly in legumes and cereal grains. A bit annoying seeing as legumes are especially high in iron. Preparations like soaking, sprouting and fermenting can greatly reduce the phytate content and therefore increase their bioavailability.
Polyphenols (used to be called vegetable tannins) are another category of inhibitors. They're found in lots of tasty things like teas, coffee, red wine and cocoa. There are different types of polyphenols with different inhibitory strength, for example tea polyphenols inhibit more than coffee or wine ones.
Certain proteins like whey and casein found in milk, and egg proteins inhibit non-heme iron. The protein in soy beans has also been found to inhibit iron.
The last inhibitor usually cited is calcium, which unlike the others affects both heme and non-heme iron. There is debate around how much calcium affects iron directly, and how much it affects it by blocking phytates from releasing or due to the proteins it's often taken with. So at this stage it's a maybe.
Reference doc for inhibitors
Looking for ways to get all your iron needs from food is ideal, but if it's not happening for you or you need a quick boost then supplements are a proven option. Comparing the bioavailability of supplements is a lot more difficult than I anticipated. In addition to all the problems with measuring bioavailability listed above, supplements don't require any clinical trials to test their effectiveness. There is so little relevant data especially on the newer iron types.
Rather than skip this important comparison, I've put the data I found into a best approximation. So treat this as such and check out my source documents if you'd like to see what clinical trials were used. Throughout I'm always comparing the elemental weight of iron in each supplment. This is the weight of Fe, not the weight of the whole compound.
Ferrous sulfate (also spelt sulphate) is the most common iron supplement, and for good reason. It's very soluble so it's fast working, it has good bioavailability and is the cheapest form. When overall bioavailability has been measured for ferrous sulphate it seems around the 20% mark, but going up towards 40% for anemic people and with enhancers, and towards zero for people with sufficient iron and inhibitors present. Most studies use relative bioavailability (RBV) as a measurement in order to cancel out some of the fluctuations, in which case sulfate is the reference and is set at 100%.
The usual complaint with sulfate is due to it being quite reactive. Like all the ferrous salts it's mostly fine at low doses, but at high amounts it may cause stomach irritation. Taking it with food can lessen the irritation but this isn't recommended because of the wide range of inhibitors that affect it. Instead smaller doses more often, or a smaller dose with enhancers seems to be the way to go. Another thing to keep in mind is its higher toxicity. This is generally fine for adults, but a fair amount of children each year are poisoned by accidental iron overdose.
Reference doc for ferrous sulfate
Ferrous gluconate is a soluble iron salt that seems to be less bioavailable than ferrous sulfate but not by much  like around 90% of sulfate levels. When comparing the same dose, there weren't any reported differences in side effects between gluconate and sulfate. Supplement doses tend to be smaller in gluconate though, which could be why it is viewed as gentler on the stomach. Especially in formulations like Floradix which people talk about being gentle on the stomach, the amount of gluconate is only 10mg per serving. Like the other iron salts, gluconate is prone to the effects of enhancers and inhibitors so care needs to be taken with what it's consumed with.
Reference doc for ferrous gluconate
Ferrous fumarate is the least soluble out of these ferrous salts. This means it's a little more reliant on gastric acid to dissolve and takes a little longer to absorb. Ferrous fumarate is often referred to as having less bioavailability than sulfate and gentler on the stomach. This makes sense due to its lower solubility, but I haven't found the studies to back that up. Most rate its bioavailability and side effects the same as sulfate. Inhibitors and enhancers affect it, but one thing to note is that unlike sulfate, it isn't enhanced by Na2EDTA. Na2EDTA is often added to processed foods to stop oxidation, so that's one source of enhancers that fumarate misses out on.
Reference doc for ferrous fumarate
There aren't many studies on this iron salt despite it being around in the 50's, so I left it off the chart but it's worth a mention. It tends to get used in combination supplements rather than on it's own which might have to do to with its fairly low solubility. One study put its bioavailability relative to sulfate as 92% in tablet, and 122% in liquid. So pretty good and maybe more bioavailable than sulfate in liquid form. The problem is the lack of information on it. People report it as being gentler on the stomach compared to sulfate which makes sense for a less soluble form, but I haven't seen any data to back that up.
FeBC is a type of iron amino acid chelate with a structure of one molecule of iron bound by two molecules of glycine. There are other types of iron amino acid chelates but FeBC is the main one, often going by the brand name of Ferrochel. It's a very soluble and stable form of iron, so compared to sulfate it's less likely to cause irritation during digestion. It is still affected by inhibitors, but this is estimated at about half as much as sulfate. Most studies conclude its bioavailability is the same as sulfate either at the same dose or at half the dose. Not sure why the lower does the same job, but it implies that it could have a higher bioavailability. It needs more studies to confirm this.
Reference doc for FeBC
Ferritin is what we and nearly all organisms use to store iron. It's a protein with an ""iron core"" of up to 4500 ferric iron molecules inside. Although ferritin can come from both plant and animal sources, bovine ferritin is what is used most in studies. The number of ferric molecules inside ferritin can vary, and animal sources tend to have higher concentrations compared to plant sources. It was originally thought that all the ferritin we consume is absorbed in the normal non-heme way, where the protein opens up during digestion and the ferric iron is released. Many studies have now shown that as well as this method of absorption it also has its own pathway where it can get absorbed into intestinal cells whole. Which of these two pathways ferritin takes seems to depend on the conditions of the stomach such as gastric acids and stomach pH. If the ferritin stays whole through digestion then inhibitors don't affect it. Unlike all other non-heme iron, ascorbic acid seems to have a negative effect on it unless it is enteric coated.
There is a bit of a divide with studies measuring the bioavailability of ferritin. Older studies that use extrinsic labeling put the bioavailability quite low. Newer studies use intrinsic labeling which claim to be more accurate, and show that ferritin has the same bioavailability as ferrous sulfate. So ferritin seems like it could be a pretty good source of iron, but there are few commercial supplements of it. Infact I could only find one expensive product that has little information about it. How this supplement compares to ferritin in existing studies is unknown. In terms of side effects, there are few studies on it but it is generally considered to have low side effects which makes sense due to its stable structure and naturally slow release.
Reference doc for ferritin
IPS (sometimes called ITF282) is a form of ferric iron bound to milk protein. This protein casing protects it from being solubilised in the stomach, and instead it opens up in the intestines. This translates into good tolerability, and studies tend to put it at around half the side effects of sulfate. In terms of its bioavailability, it's generally put at a little less than ferrous sulfate. Interestingly, a couple of studies show it overtaking sulfate in effectiveness after prolonged use. It is speculated that this could be because it causes less irritation, which is known to inhibit future absorption. There's not much information about how it's affected by inhibitors and enhancers, but it would make sense that it would be affected less while in the stomach, and when the ferric iron has been released it would be acted on in the usual way.
Reference doc for IPS
There are different types of PIC, but generally this complex is made from a carbohydrate and ferric iron. Like the other larger synthetic complexes, the idea is that the carbohydrate stabilizes the iron while it's in the stomach so it is less likely to react and cause irritation. Once in the small intestines it releases the ferric iron so it can be converted to ferrous iron and absorbed. That's the idea, but it's hard to say how well it does this due to a lack of good studies on it. It appears that its bioavailability is a bit less than ferrous sulfate and that it has less negative side effects. There's not much information on the enhancers and inhibitors, but it seems like they affect it a bit less than sulfate.
Reference doc for PIC
Carbonyl is a highly purified form of iron. The difference between it and other elemental iron powders isn't the iron but rather the manufacturing process and in particular the size of iron particles. Carbonyl is the most bioavailable of all the iron powders, but when compared against ferrous sulfate it is generally considered around 75% as bioavailable. Carbonyl needs stomach gastric acids to solubilize it before it can be absorbed which makes it quite susceptible to the negative effects of food. Wheat especially seems to decimate its absorption. This requirement of gastric acids means that it is naturally slowly released, which makes it very safe and also translates into low gastro side effects.
In studies for children it seems to do a bit better than adults, which is probably due to the smaller dose and/or differences in gastric acids. As well as the safety aspect, another feature that could make it appealing as a choice for children is the small pill size and its availability in a chewable form.
Reference doc for carbonyl
HIP is classified as a medical food rather than a supplement, but I think it's worth lumping it with the rest for comparison. The heme iron in HIP generally comes from bovine hemoglobin, although it is possible to get heme iron from plant sources like soy. Unlike all the other supplements, it gets absorbed through the heme pathway which means that it stays whole through digestion and isn't affected by most of the enhancers and inhibitors. Its only enhancer is meat tissue, and it is possibly inhibited by calcium. Because HIP isn't that affected by inhibitors, it's easy to get wildly different results when comparing it against other supplements depending on what food it's taken with. From looking at the small selection of studies on it, I'd put it about twice as bioavailable as ferrous sulfate but that's just a best guess. So HIP seems to be an effective supplement with the flexibility of being able to be taken with food and with low levels of side effects. The negatives are that it's expensive and there is a lack of studies on the commercial HIP products. They may perform differently to the HIP in the studies.
Reference doc for HIP
I really wanted to add intravenous iron to the chart but it wouldn't quite squish into the same comparison categories. IV isn't a type of iron but rather the method of injecting iron directly into the blood stream. This has the bonus of bypassing the whole gastric system, so in terms of gastric side effects it's about zero. Bioavailability is really high because it's absorbed straight into the bloodstream and therefore it also can't react with any food.
Sounds pretty awesome except like everything there are some big downsides. One, as mentioned previously is the risk of having too much iron floating around in your body. This risk doesn't seem too high if you're low on iron and without a high risk of infection, but we don't fully know what effects there could be from big spikes in our bodies iron levels. As a precaution it is not recommended during the first trimester of pregnancy.
The other big downside is the price to adminster and the cost of the iron formulae. The price of administering IV iron is coming down due to advances in IV iron formulas. Older types were suitable only for multiple small dose injections or else large dose infusions that needed a lengthy hospital visit. Newer formulas can be given by one quick high dose injection from your GP. The other side of price is the cost of the iron formula, which is surprisingly high. The newer formulas that are easier to administer seem to be priced in the $US 80-300 mark for 1,000mg of iron. It doesn't seem like an efficient way of tackling iron deficiency except for urgent cases or patients that aren't responding to other methods. That could change though if prices come down.
Reference doc for IV iron
Lactoferrin is an interesting protein and worth a mention because it has been shown to increase iron levels. It's primarily found in human milk but there is also a similar version of it in cows' milk. It can bind strongly to two molecules of ferric iron and has a direct pathway into our intestinal cells in a similar way that heme and ferritin does. It also has extremely low side effects, so you'd think it would be a good iron supplement but there is more to it. Seeing as it strongly attracts iron, if we take it in its depleted form then rather than give us iron it can scavenge iron from our bodies. This can be a good thing because it can take free iron away from bacteria. In fact this is one of the great things about lactoferrin, it has been shown to have lots of anti-microbial and anti-inflammatory properties.
The debate around lactoferrin is whether the increase in iron levels that we see (about the same increase as ferrous sulfate) is due to lactoferrin giving iron or its ability to reduce inflammation. It seems like it's more to do with its anti-bacterial properties and generally it's recommended not to be taken by itself for iron deficiency but possibly with other iron. Most lactoferrin supplements are in the depleted iron form, so potentially if you could get it in its iron bound form it could increase iron but more studies are required. For those susceptible to iron overload, it could be worth looking into.
Reference doc for lactoferrin
Slow release tablets generally refer to an enteric coating which keeps the tablet together in the acidic conditions of the stomach and then breaks down in the non-acidic intestines. Sounds good in theory but not so impressive in studies. The main slow release iron that has been tested is ferrous sulfate. The general consensus is that it may be gentler on the stomach, but this is due to not all of the iron getting released in time. So basically you're getting less iron and may as well just take a smaller dose of normal ferrous sulfate and save yourself some money. The UK health board recommend that the NHS don't prescribe it and the following in-vitro study put the bioavailability of Ferrograd C at 71% of normal ferrous sulfate. That's pretty low considering Ferrograd C as well as being enteric coated has the enhancer ascorbic acid too.
Cooking in cast iron pans has been proven in several studies (1)(2) to increase iron levels for those that eat from it. Rather than comparing it against other supplements they compare it against other types of pans so it's hard to know how it stacks up to ferrous sulfate, but the increases in Hb and ferritin make it seem equivalent to a low dose supplement. It is especially effective when acidic foods are cooked in it. Foods with little acidity like legumes it doesn't do much for.
A similar idea to cast iron pans is iron-rich water like the type Spatone sell. The only studies I can find on it are the three that Spatone have paid for. All three have small sample sizes and don't compare it against another type of iron. I've read enough trials to know that this isn't normal practice. A Spatone sachet contains a tiny 5mg of ferrous sulfate at a high cost. I have no doubt that they're gentle on the stomach considering the small amount of iron in them, I'm just unconvinced that they're any different than taking small amounts of ferrous sulfate which is a very soluble form of iron anyway. Until a proper trial is done, money might be better spent making your own iron water with one of these fishies.
If you're wondering what type of iron is added to food as fortification, the answer is it could be one of many like ferrous sulfate, ferrous fumarate, ferric pyrophosphate, and electrolytic iron powders. While bioavailability and side effects are most important for supplements, iron for fortification has the priority of not discoloring or causing rancidity to the food. For this reason fortification iron tends to be less soluble unless it's going into dry goods with a quick turnover. Less soluble iron usually means less bioavailability however, and often cheap elemental iron powders are used which the WHO don't recommend due to their low bioavailability. If you want to check out the iron that the WHO recommend for different products see their list in the link above.
Surprisingly this key question seems to have the least amount of research on it. Putting global iron deficiency down to things like plant based diets, pregnancy, menstruation and childrens growth seems simplistic. We've been doing those things a long time, have we always been iron deficient? Here are a couple of my thoughts.
The food we eat may have less iron then in the past. Articles like this one talk about soil depletion and the use of fast growing varieties that have less time to acquire minerals and vitamins. Also the way we eat food is a likely contributor. Processing, cooking and storing food destroys good iron enhancing acids. Even more reason to add more fresh and raw foods to the diet.
It's well known that certain chronic diseases trigger inflammation which reduce our iron absorption. We do this as a defence mechanism in order to stop harmful micro-organisms from using the iron themselves. Inflammation can happen without chronic disease too. One third of people with irritable bowel syndrome (IBS) suffer from anemia. So inflammation of various kinds can stop us absorbing iron. Many studies show the link between stress and IBS and other forms of inflammation. So all the links are there, but somehow stress is seldom discussed in relation to iron deficiency. Maybe it comes down to the gapping hole in the health system when it comes to dealing with the mind, but that doesn't mean it's not a valid area to look into for your own answers.
The first line of this story was true at the start of my research but I'm not low in iron anymore. That doesn't mean I have all the answers, just that I've found what works for me. If I had to sum up my thoughts i'd say:
A personal project by zawadi. If you know any research that would make this more accurate I'd be happy to hear about it.
",186
https://betterhumans.pub/coffee-time-out-34eafb198c73?source=tag_archive---------7-----------------------,What I Learned From Quitting Coffee After 15 Years Of Daily Consumption,"I used to drink coffee on a daily basis since I was a teenager. Two weeks ago, I stopped cold turkey. Here's how I did it.",Angelo Belardi,8,"I used to drink coffee on a daily basis since I was a teenager.
Every now and then I toyed with the idea of drinking less coffee, but I never made an attempt at giving it up. Tolerance develops quickly when you drink it regularly and I hadn't felt the waking effect in a long time. I drank coffee for no particular reason other than having come to like its taste.
Two weeks ago I stopped cold turkey. Here's the backstory, how I did it, and what I noticed since making the change.
Coffee had been my constant companion since I was a teenager. At home, in school, at the university. I was hooked on the stuff early on, partly because my parents used to drink the black energy liquid one cup after the other when I grew up.
It was normal to drink coffee. For breakfast, after and during lunch or dinner, or in-between whenever one felt like it. We would walk over to the machine, place a cup under the dispenser and wait for the rhythmic noises in anticipation of the delicious odor that would soon fill the air. Coffee was my family's hot beverage of choice. It was the default option.
In high school, we had coffee machines at the canteen and I would pour myself a cup whenever I had the chance to do so.
During the first years at the university, I regularly drank coffee from vending machines around the campus, but it was during my Master's that I increased my consumption even more. I picked up the idea that 'science is fueled by caffeine' and made sure to be part of it.
After starting my PhD I introduced a bunch of changes to my life and built several habits. In the course of this, I restricted myself to drink only four cups of coffee per day.
This held up for about three years. There have been exceptions of course: One day during a road trip in Arizona with friends, I bought a 1.5 liter bottle of unsweetened Starbucks espresso in a grocery store. While driving that day, I kept sipping from that bottle all day long. When we arrived at a Motel in the evening, I started to feel a little uneasy and sick and had to stop. That was the only time I can remember feeling that I had too much coffee.
Generally, though, my 'four cups' restriction held up until recently. As I worked more and more from home, my coffee consumption increased to about five or six large mugs of instant coffee per day.
What eventually sparked the idea to give up coffee for a while was something unexpected.
It was only after writing a story about stepping out of one's comfort zone that I decided I could do that: I could make myself a little uncomfortable and stop drinking coffee. For the sake of a higher goal, one further down the path of my life, living healthy and avoiding consuming stuff that might not be good for me.
Thus, while writing about the idea in my diary on Monday morning (26th of March), I drank up the mug of coffee that stood beside my notebook on the table, knowing that this would be my last one. At least for a while.
No more coffee. Cold turkey. Just as I did it when I stopped smoking years ago. Whenever I thought about drinking a coffee, I simply drank a glass of water or made myself a cup of tea instead.
I had surprisingly little difficulty giving up coffee. I didn't feel more tired or less alert during my days. I didn't feel much craving for coffee. Sure, the first few days I thought every once in a while ""now I'd be drinking a coffee normally"", or ""wouldn't a cup of coffee be great now?"" But there was no strong craving, contrary to what I had expected when I began. I may just have been lucky or it might have helped that I still drink tea with caffeine and a lot of water.
Maybe I expected it to be much more difficult based on my own experience with giving up smoking about nine years ago. During the first days and weeks of doing that, I certainly felt a strong craving for nicotine. Months and even years later it would still surface at times, though luckily it eventually vanished.
Whether that is just due to the difference in the substances  nicotine is much more addictive and thus also more inclined to lead to physical withdrawal symptoms  or also linked to how I approach behavior changes these days, I couldn't say.
It certainly helped that I was a lot around my girlfriend these days, who rarely drinks anything else than water. I had her complete support all the time and no one luring me with the smell of a freshly brewed coffee or asking me to come for a coffee break with them.
I expect that it might get difficult when I'm in the office or around people who drink coffee, but by now I imagine I can instead just drink a cup of tea while they sip their coffee, and nonetheless take part in 'coffee' breaks and still have the social benefits that come with those.
What also didn't happen were any improvements in health that were observable. Yet. I didn't really expect anything like that, though, because a lot of these changes are subtle and will play out in the long run.
Two weeks later I can begin to assess the changes I did see.
I expect removing coffee from my drinks will have a positive effect on my overall health in the long run, particularly for the cardiovascular system.
But what changes did I notice so far?
While I still consume caffeine in the form of green and black teas, my caffeine consumption went down drastically.
According to a report from the Mayo Clinic, black tea has about one third of the caffeine content of brewed coffee, green tea about one fourth or fifth. All the other teas I drank over the time don't have any caffeine in them.
The numbers can only be used as a rough guide, given that roast and brewing temperature, time, and amount of coffee powder and whatnot all influence how strong a coffee ends up. Still, I roughly decreased my caffeine intake from about 5-6 large mugs of coffee (~600mg) to about one black tea and maybe three mugs of green tea per day (~125mg caffeine).
While it was never the goal to quit caffeine altogether, it was clear that this would happen and that it will probably have a positive long time effect on my health.
I would usually drink coffee either in the form of a short espresso with a spoonful of sugar, or in huge mugs of coffee with a splash of milk.
I still have milk in my breakfast muesli and a little splash in my teas, but that's it. I'd say this makes about 2dl less milk per day.
Besides coffee, I don't usually sweeten any foods or drinks with sugar. Not even my tea. Consequently, my refined sugar intake  at least the one I can control and isn't hidden in processed foods  dropped basically to zero.
I decided that I still liked to have a hot beverage every now and then, especially since it's still rather cold here at the moment, the winter hasn't really worn off completely yet even though spring has officially begun.
Thus I began to drink more tea than I have in years. Apart from the habit of drinking a cup of green tea with my breakfast and the occasional binge-drinking of herbal teas when I have a cold or a cough, I haven't drunken much tea over the years.
I don't see myself as a caffeine addict anymore. That means that I don't see myself as an addict to any substances anymore. I used to smoke, I used to binge drink alcohol, and for all these years I used to drink coffee and think of myself as someone who needs that stuff to be alert and focused at work or while studying.
By now I know that I don't need any of these, that I can live happily without having a constant need for any specific substance. That's a huge improvement in my own understanding of the control I have over myself, over my behavior and body. While I had learned this to a certain degree before e.g. by establishing positive habits  it feels still empowering to realize that there are no substances anymore which have such a power over me.
Coffee isn't cheap. For the capsules to the coffee machine at the office I used to buy two packs of 30 capsules for about 17 CHF (that's about 17 USD) every three weeks. I spent less when using the instant coffee variation, but way more when buying a cup at a coffee shop every now and then. Thus, while it might not be the worst money waste, I think I can easily save the equivalent to five annual Medium membership subscriptions by switching from coffee to tea and water.
She's all in favor of me not drinking coffee anymore. Still, she was rather surprised that I simply did that out of the blue one morning and then stuck to it for days.
Until now she's the only person who knows about it. I assume it will get tougher when I visit my parents in whose kitchen the scent of freshly brewed coffee constantly hangs in the air.
I proved to myself that I can do it. I can give up something I liked for a long time even though I knew it wasn't good for me. If nothing else, this fuels my confidence in being able to change deliberately, to accomplish something I had not thought I would be able to do. Something I had done for years and years without questioning.
I could change. Just like that. On an ordinary Monday morning.
And that's what I want to end this story on. We don't have to continue doing things just for the sake of it. On any ordinary day, we have the power to question the status quo in our lives and do something about it. We can reduce negative behaviors and increase those which are positive. Every day. There's no need for an extraordinary event to change. No New Year's Eve or huge life event.
We all have the power to constantly change. To change our behavior. To change our lives. For the better. Every day.
Do not wait; the time will never be ""just right."" Start where you stand, and work with whatever tools you may have at your command, and better tools will be found as you go along.  Napoleon Hill
The Better Humans publication is a part of a network of personal development tools. For daily inspiration and insight, subscribe to our newsletter, and for your most important goals, find a personal coach.
Researching psychologist with a passion for habits and mindfulness.
",187
https://byrslf.co/the-forgotten-art-of-untucking-the-tail-57d8ef619e4e?source=tag_archive---------1-----------------------,The forgotten art of untucking the tail,A tiny detail we've lost since the hunter-gatherer times and how to fix it,Ivana Demmel,6,"A tiny detail we've lost since the hunter-gatherer times and how to fix it
I've been spending most of my time lately learning about biomechanics, healthy alignment and how movement affects our bodies on cellular level. The research from the last few years shows that the way we move affect us not only mechanically, as it was previously thought, but also causes biochemical changes in our cells, changing us from inside out. This process of the body adapting to and being shaped by movement is called mechanotransduction.
We seem to be living in the world that encourages sitting with the tail tucked under.
The furniture we use sitting all day is optimised for comfort and convenience, but usually not ergonomically adapted for pelvic floor health. Who would even think about that?
There is a well spread myth in our society that pelvic floor issues are a normal consequence of ageing or child birth. But we would rarely think of looking for a cause a bit deeper, in the way we have been living and moving before that.
Did you know that the anatomic function of our remnant of a tail is, amongst others, to control opening and closing of the pelvic outlet? If you look at a dog or a another tailed animal you can see that they keep their tail up happily wagging when they are in a good mood. And this is the default. When you see a dog wearing his tail down between his back legs, it's usually a sign that the dog is on the fight or flight mode and his pelvic floor is tense as a part of his whole body reacting to danger.
Even though we now have only a tiny part of what was once a real mammalian tail, it behaves the same. Untucking the tailbone opens the pelvic outlet, tucking it closes it  tightening the pelvic floor. The dog with his tail down between his legs is an equivalent of you sitting on your sacrum, the back supported by a chair or a couch. If you spend multiple hours a day in this position, your pelvic floor doesn't really have a chance to release and allow the muscle fibres to regain their natural length at resting state. So gradually it shortens.
In his book Pelvic Power, dance educator Eric Franklin compares natural movement of the pelvic floor to a kite. As you stand up the pelvic floor slightly lifts and narrows, similarly to a kite picking the wind and taking off. Reversely, as you sit down (with untucked tail, using your sit bones) the pelvic floor widens and releases to its full length, like a kite descending down, opening and landing. But what if we never allow it to release?
Tight muscle does NOT equal a strong muscle.
Biomechanist Katy Bowman compares an optimally functioning pelvic floor to a trampoline. It supports the weight of all our pelvic organs and allows any extra load to just bounce off its healthy, elastic fibres, tensing and releasing naturally. Keeping the muscles in the shortened position all the time doesn't let them perform their function optimally. And further on, as our bodies adapt to the way we use them, we gradually end up with shortened muscles.
This is why pelvic floor exercises (Kegels) can only be a short term fix. Practising contractions of a muscle isolated from the rest of the body and without allowing it to ever fully release you get a muscle that is tight and locked short. A tight muscle does NOT equal a strong muscle. A strong muscle is able to both fully contract and fully release.
So why am I telling you all this now? Reading Katy Bowman's last book  Move Your DNA, I've started to incorporate more variety movement into my life other than and independently from exercising. And especially, I've started to squat more, when playing with my toddler, picking things off the floor and going to the toilet.
Learning how to untuck my tail has made me finally realise why I still had to push my baby out and not bear it gently down, letting the gravity do the job, in spite of all the birth preparations and beautiful relaxation techniques I mastered beforehand.
To ""breathe the baby out"" you have to first untuck your tail.
Anatomically, in order for the baby to come out the pelvic floor has to release, but you can't possibly release the pelvic floor with the tailbone tucked under. You can only use the strength of your muscles (transverse abdominals and diaphragm as they instruct you to hold your breath and ""puuuuussssh!"") against your pelvic floor. And that's a sure way to end up with a nice tear and possibly a pelvic floor disfunction. I was lucky that this mad pushing in my case ended without any serious damage. The body just took over at the last moment and jumped on a dining chair in a strange half-squat with untucked pelvis, fully opening the pelvic outlet. The tucked squats and kneeling with rounded back would have probably taken me to an assisted delivery.
""Modern birthing science has placed a large burden on secreted hormones (like relaxin) to prepare the body for needed mobility."" Katy Bowman says. Yes, relaxin is useful in letting the body open up for the birth, but it's not enough, unless we have strong, yielding muscles that can fully contract, but also fully release.
And there is one more thing that we might be missing nowadays.
In order to have bodies able to smoothly perform all their biological functions, we need to build a whole-body endurance.
A great way to develop endurance as well as a strong pelvic musculature and a responsive pelvic floor is to walk a lot. You can gradually build up the mileage and frequency of walking in the day. Up the hill, down the hill. Not just on flat pavements but on a variety of terrains and ideally in a shoe that allows your feet to experience and respond to them.
It's about building up towards more movement, but also more variety of movement. Gradually adding more and more little movements into your life will allow the body to adapt to them, building capacity to make them easy.
So, the bottom line is: we really need to get off our tails and sit on the sit bones instead and walk more, squat more, move more. It's all about really using the body. Reintroducing a frequent and varied movement in our day-to-day life we can gradually reclaim the lost ranges of movement and functional strength.
Reference list:
Don't instruct, share.
430 
24
430 claps
430 
",188
https://medium.com/@dmahugh/hip-replacement-isn-t-what-it-used-to-be-12-weeks-ago-25ba106edc8c?source=tag_archive---------7-----------------------,Hip Replacement Isn't What It Used To Be 12 Weeks Ago,"Twelve weeks ago, I had a total hip replacement on my right side. When I wrote up a few impressions one week later, I said:",Doug Mahugh,10,"Twelve weeks ago, I had a total hip replacement on my right side. When I wrote up a few impressions one week later, I said:
I'm only a week into this adventure, and I also have another hip to do in a few months, so it would be silly to sum things up at this point. I have much more to learn and experience, and I'll write more as the process goes forward.
That time has come. I've recovered from the first surgery, and on Monday of this week I had the same type of hip replacement surgery on the left side. I've learned and experienced a lot in the last 12 weeks, so I'm going to share some impressions now from a slightly more informed perspective.
To be clear, I'm no expert on this topic. I'm just a guy who has had both hips replaced. My recovery from the first one went very well  my surgeon and physical therapist said I recovered faster than most patients, and apparently I spent more time on the details than the average patient does, but I'll never know whether the two are related or I'm just lucky.
As before, I wrote this up mostly for friends who are considering hip replacement or have a good chance of it in their future. Mine have come a little earlier than average, so I seem to know more people anticipating this procedure than people who've already done it. If your hips are rock-solid and you're well under 50, this article isn't intended for you and I certainly wouldn't be offended if you don't bother to read it.
It's nice to have been through this once, so now I know generally what to expect. To recap, here's how it went the first time around ...
After a few days, the more intensive PT (physical therapy) began. It's mostly about learning to walk without a limp, and your muscles need to get stronger to make up for various changes. You need to accomplish these goals while adhering to a set of hip restrictions such as not crossing your legs or bending more than 90 degrees at the waist for a few weeks (or longer in some cases).
I should point out that there are many different ways to do a hip replacement, and I've had the posterior approach both times, so I've been working through the specific issues involved in recovering from that procedure.
There is also an anterior approach where the surgeon comes in through the front of the leg, and if you're interested in that debate you can find online many passionate opinions from orthopedic surgeons and patients regarding the differences. At the risk of generalizing about a topic I don't fully understand, it seems that fans of the anterior approach talk about how short the recovery time is, and surgeons who prefer the posterior approach like the extra visibility they have when installing the new hip from that angle. My surgeon Dr. Bruckner only does posterior replacements, and that's good enough for me: I chose a surgeon, not a surgery, and I'm comfortable with him making those decisions, given his track record.
After three weeks, I was to the point where most of my PT time was just walking, sometimes with Nordic walking poles and sometimes without. I completed the Microsoft 5K with Nordic poles on Day 30, which felt good as a milestone.
After that, it was just a matter of getting clearance from the surgeon to lift all hip restrictions at the 6-week mark, and life returned to normal. Here's a graphical representation of key milestones during those six weeks as recorded in my journal:
For hip #2, I had the same hospital stay (two days), but I've already had a good night's sleep on Day 3 instead of Day 4. It's early, but I feel like this recovery will go faster, both because I already know the drill and also due to a couple of the changes covered below.
First, a few things that didn't change. Many details of my two hip surgeries stayed exactly the same.
Most notably, I had same orthopedic surgeon both times, Dr. James Bruckner of Proliance. He's so popular that you have to wait to get in to see him (I had to wait over two months for the initial consultation), but I highly recommend him. As I said after my first surgery, ""I like Dr. Bruckner's style and that of every member of his team I've dealt with: they're sharp, alert, consistently positive without being pollyannish, and extremely well-organized."" It was exactly the same experience the second time.
Some of the Overlake Hospital staff members I met after my first surgery were also there for my second one. For example, It was comforting to start the day with a familiar face and personality: John the pre-op RN with curly silver hair, long goatee, and a big purple-handled comb in the back pocket of his scrubs. He got me prepped, including details such as the dots on my feet where they would check my pulse after surgery and a big blanket with an attached tube gently blowing warm air on me  studies have found that you're less likely to get an infection when you're nice and warm. John has a quick smile and the calm, steady demeanor of a guy who's seen it all.
Once I got back to my room after the surgery, I ran into a few more familiar faces.
Tom, the tall daytime RN with a Type 1 diabetes bracelet tattooed on his wrist, gently pushed me both times to take enough Oxycodone to stay ahead of the pain curve. He may have accurately surmised that I have a tendency to not take pain management seriously (pain meds are for wimps!), and in fact on the first night of hip #2 I definitely got ""behind the curve"" for a while, but Tom helped me get back on track in the morning.
Patrick, a tech (nurse's assistant) who works night shift, is a model of efficiency. He has mastered the subtle art of moving quickly at all times without ever seeming brusque or impatient. The perfect guy to show up at 2AM when you really need to pee  he's gone before you realize you're done.
Nick ""the vampire"" as I called him (and I'm pretty sure I'm not the only one) has a very specialized role each morning. He visits all of the post-surgery patients in the orthopedic ward at 4:30AM to take blood samples, so that when Dr. Bruckner and his PA make their daily rounds at 6:00AM they will have the latest blood test info for every patient. I've had blood drawn countless times during all the appointments related to these surgeries the last few months, and Nick can draw two vials and tape a cotton swab over the hole before most people can find a good vein. He works 12 hour shifts, 7 days on and 7 days off, and Wednesday is the end of his week. I told him to enjoy his 7 days off as he rushed out of my room yesterday morning, and he laughed and said ""it won't be a full 7 full days, they always call.""
It was great to see these guys. They all felt like old friends after working with them for a few days less than three months ago.
Many details were different this time around, and the biggest difference was the change in anesthesia protocol. For hip #1, my anesthesiologist Dr. James Larson administered a general anesthesia with a morphine-based epidural pain killer. That was the approach Bruckner's team was recommending for hip replacements at that time.
But in the weeks since then, they've changed their approach. For hip #2 anesthesiologist Kevin Fujinaga recommended a bupivacaine spinal instead of general anesthesia. He said they've found that most patients recover more quickly that way, with less nausea and other symptoms.
I hesitated at first  I've heard of the sawing and pounding that goes into a posterior hip replacement (hell, they start by deliberately dislocating the old hip), and I sure didn't want to be awake for any of that! Kevin explained that I would be ""deeply sedated"" instead of unconscious, a distinction without a difference in most cases.
I said sure, let's go for it, and as it turned out everything was fine. The only thing I remember hearing during the surgery was a snippet of conversation about the Seahawks-49ers game the day before, and it seems likely that was in the recovery room.
There are several differences between the bupivicaine experience and the general anesthesia experience, some positive and some slightly negative, but overall I much preferred this approach. If I ever need a similar surgery in the future, I'd want the bupivicaine spinal.
It was clear as soon as I came to in the recovery room that this time was different. I was alert and clear-headed, whereas after the general/morphine approach I was very foggy for a long time. Here are a few other differences that became clear that first day:
When I saw Kevin the day after surgery, I told him I much preferred the bupivicaine spinal, and he said ""for the right person, this approach seems to work great."" The physical therapists on my floor had similar comments as well. Apparently some people prefer the morphine. But I'm glad to have a little less pain control (which can be remedied for the most part with other meds) rather than spend two days feeling stoned and groggy.
A few other differences this time around worth noting:
A funny story about that last detail: on the final day of surgery #2, I passed the nurse's station and had an immediate visceral reaction to a woman standing there. It took a second to remember how I knew her. She was the tech who had removed my catheter, and the sight of her scares me. :)
Surgical procedures are evolving quickly, and I wasn't expecting to see so much change in just 12 weeks, especially considering it was the same team at the same hospital. But it presents an opportunity to recover even faster and get even stronger, and I'm planning to take advantage of that opportunity. Let's do it, Jamie!
UPDATE: after the final surgeon follow-up six weeks later, I wrote up a few final thoughts in Cleared For Takeoff.
",189
https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca?source=tag_archive---------0-----------------------,Coronavirus: Why You Must Act Now,Politicians and Business Leaders: What Should You Do and When?,Tomas Pueyo,27,"Updated on 3/19/2020. This article has received over 40 million views in the last week. Over 40 translations at the bottom of the article. Here's a list of epidemiologists and experts who have publicly shared or endorsed this article.
Following articles:
To receive the next articles, sign up here.
With everything that's happening about the Coronavirus, it might be very hard to make a decision of what to do today. Should you wait for more information? Do something today? What?
Here's what I'm going to cover in this article, with lots of charts, data and models with plenty of sources:
When you're done reading the article, this is what you'll take away:
The coronavirus is coming to you. It's coming at an exponential speed: gradually, and then suddenly.It's a matter of days. Maybe a week or two.When it does, your healthcare system will be overwhelmed.Your fellow citizens will be treated in the hallways. Exhausted healthcare workers will break down. Some will die.They will have to decide which patient gets the oxygen and which one dies. The only way to prevent this is social distancing today. Not tomorrow. Today.That means keeping as many people home as possible, starting now.
As a politician, community leader or business leader, you have the power and the responsibility to prevent this.
You might have fears today: What if I overreact? Will people laugh at me? Will they be angry at me? Will I look stupid? Won't it be better to wait for others to take steps first? Will I hurt the economy too much?
But in 2-4 weeks, when the entire world is in lockdown, when the few precious days of social distancing you will have enabled will have saved lives, people won't criticize you anymore: They will thank you for making the right decision.
Ok, let's do this.
The total number of cases grew exponentially until China contained it. But then, it leaked outside, and now it's a pandemic that nobody can stop.
As of today, this is mostly due to Italy, Iran and South Korea:
There are so many cases in South Korea, Italy and Iran that it's hard to see the rest of the countries, but let's zoom in on that corner at the bottom right.
There are dozens of countries with exponential growth rates. As of today, most of them are Western.
If you keep up with that type of growth rate for just a week, this is what you get:
If you want to understand what will happen, or how to prevent it, you need to look at the cases that have already gone through this: China, Eastern countries with SARS experience, and Italy.
This is one of the most important charts.
It shows in orange bars the daily official number of cases in the Hubei province: How many people were diagnosed that day.
The grey bars show the true daily coronavirus cases. The Chinese CDC found these by asking patients during the diagnostic when their symptoms started.
Crucially, these true cases weren't known at the time. We can only figure them out looking backwards: The authorities don't know that somebody just started having symptoms. They know when somebody goes to the doctor and gets diagnosed.
What this means is that the orange bars show you what authorities knew, and the grey ones what was really happening.
On January 21st, the number of new diagnosed cases (orange) is exploding: there are around 100 new cases. In reality, there were 1,500 new cases that day, growing exponentially. But the authorities didn't know that. What they knew was that suddenly there were 100 new cases of this new illness.
Two days later, authorities shut down Wuhan. At that point, the number of diagnosed daily new cases was ~400. Note that number: they made a decision to close the city with just 400 new cases in a day. In reality, there were 2,500 new cases that day, but they didn't know that.
The day after, another 15 cities in Hubei shut down.
Up until Jan 23rd, when Wuhan closes, you can look at the grey graph: it's growing exponentially. True cases were exploding. As soon as Wuhan shuts down, cases slow down. On Jan 24th, when another 15 cities shut down, the number of true cases (again, grey) grinds to a halt. Two days later, the maximum number of true cases was reached, and it has gone down ever since.
Note that the orange (official) cases were still growing exponentially: For 12 more days, it looked like this thing was still exploding. But it wasn't. It's just that the cases were getting stronger symptoms and going to the doctor more, and the system to identify them was stronger.
This concept of official and true cases is important. Let's keep it in mind for later.
The rest of regions in China were well coordinated by the central government, so they took immediate and drastic measures. This is the result:
Every flat line is a Chinese region with coronavirus cases. Each one had the potential to become exponential, but thanks to the measures happening just at the end of January, all of them stopped the virus before it could spread.
Meanwhile, South Korea, Italy and Iran had a full month to learn, but didn't. They started the same exponential growth of Hubei and passed every other Chinese region before the end of February.
South Korea cases have exploded, but have you wondered why Japan, Taiwan, Singapore, Thailand or Hong Kong haven't?
Many of them were hit by SARS in 2003, and all of them learned from it. They learned how viral and lethal it could be, so they knew to take it seriously. That's why all of their graphs, despite starting to grow much earlier, still don't look like exponentials.
So far, we have stories of coronavirus exploding, governments realizing the threat, and containing them. For the rest of the countries, however, it's a completely different story.
Before I jump to them, a note about South Korea: The country is probably an outlier. The coronavirus was contained for the first 30 cases. Patient 31 was a super-spreader who passed it to thousands of other people. Because the virus spreads before people show symptoms, by the time the authorities realized the issue, the virus was out there. They're now paying the consequences of that one instance. Their containment efforts show, however: Italy has already passed it in numbers of cases, and Iran will pass it tomorrow (3/10/2020).
You've already seen the growth in Western countries, and how bad forecasts of just one week look like. Now imagine that containment doesn't happen like in Wuhan or in other Eastern countries, and you get a colossal epidemic.
Let's look at a few cases, such as Washington State, the San Francisco Bay Area, Paris and Madrid.
Washington State is the US's Wuhan.The number of cases there is growing exponentially. It's currently at 140.
But something interesting happened early on. The death rate was through the roof. At some point, the state had 3 cases and one death.
We know from other places that the death rate of the coronavirus is anything between 0.5% and 5% (more on that later). How could the death rate be 33%?
It turned out that the virus had been spreading undetected for weeks. It's not like there were only 3 cases. It's that authorities only knew about 3, and one of them was dead because the more serious the condition, the more likely somebody is to be tested.
This is a bit like the orange and grey bars in China: Here they only knew about the orange bars (official cases) and they looked good: just 3. But in reality, there were hundreds, maybe thousands of true cases.
This is an issue: You only know the official cases, not the true ones. But you need to know the true ones. How can you estimate the true ones? It turns out, there's a couple of ways. And I have a model for both, so you can play with the numbers too (direct link to copy of the model).
First, through deaths. If you have deaths in your region, you can use that to guess the number of true current cases. We know approximately how long it takes for that person to go from catching the virus to dying on average (17.3 days). That means the person who died on 2/29 in Washington State probably got infected around 2/12.
Then, you know the mortality rate. For this scenario, I'm using 1% (we'll discuss later the details). That means that, around 2/12, there were already around ~100 cases in the area (of which only one ended up in death 17.3 days later).
Now, use the average doubling time for the coronavirus (time it takes to double cases, on average). It's 6.2. That means that, in the 17 days it took this person to die, the cases had to multiply by ~8 (=2^(17/6)). That means that, if you are not diagnosing all cases, one death today means 800 true cases today.
Washington state has today 22 deaths. With that quick calculation, you get ~16,000 true coronavirus cases today. As many as the official cases in Italy and Iran combined.
If we look into the detail, we realize that 19 of these deaths were from one cluster, which might not have spread the virus widely. So if we consider those 19 deaths as one, the total deaths in the state is four. Updating the model with that number, we still get ~3,000 cases today.
This approach from Trevor Bedford looks at the viruses themselves and their mutations to assess the current case count.
The conclusion is that there are likely ~1,100 cases in Washington state right now.
None of these approaches are perfect, but they all point to the same message: We don't know the number of true cases, but it's much higher than the official one. It's not in the hundreds. It's in the thousands, maybe more.
San Francisco Bay Area
Until 3/8, the Bay Area didn't have any death. That made it hard to know how many true cases there were. Officially, there were 86 cases. But the US is vastly undertesting because it doesn't have enough kits. The country decided to create their own test kit, which turned out not to work.
These were the number of tests carried out in different countries by March 3rd:
Turkey, with no cases of coronavirus, had 10 times the testing per inhabitant than the US. The situation is not much better today, with ~8,000 tests performed in the US, which means ~4,000 people have been tested.
Here, you can just use a share of official cases to true cases. How to decide which one? For the Bay Area, they were testing everybody who had traveled or was in contact with a traveler, which means that they knew most of the travel-related cases, but none of the community spread cases. By having a sense of community spread vs. travel spread, you can know how many true cases there are.
I looked at that ratio for South Korea, which has great data. By the time they had 86 cases, the % of them from community spread was 86% (86 and 86% are a coincidence).
With that number, you can calculate the number of true cases. If the Bay Area has 86 cases today, it is likely that the true number is ~600.
France claims 1,400 cases today and 30 deaths. Using the two methods above, you can have a range of cases: between 24,000 and 140,000.
The true number of coronavirus cases in France today is likely to be between 24,000 and 140,000.
Let me repeat that: the number of true cases in France is likely to be between one and two orders or magnitude higher than it is officially reported.
Don't believe me? Let's look at the Wuhan graph again.
If you stack up the orange bars until 1/22, you get 444 cases. Now add up all the grey bars. They add up to ~12,000 cases. So when Wuhan thought it had 444 cases, it had 27 times more. If France thinks it has 1,400 cases, it might well have tens of thousands
The same math applies to Paris. With ~30 cases inside the city, the true number of cases is likely to be in the hundreds, maybe thousands. With 300 cases in the Ile-de-France region, the total cases in the region might already exceed tens of thousands.
Spain has very similar numbers as France (1,200 cases vs. 1,400, and both have 30 deaths). That means the same rules are valid: Spain has probably upwards of 20k true cases already.
In the Comunidad de Madrid region, with 600 official cases and 17 deaths, the true number of cases is likely between 10,000 and 60,000.
If you read these data and tell yourself: ""Impossible, this can't be true"", just think this: With this number of cases, Wuhan was already in lockdown.
With the number of cases we see today in countries like the US, Spain, France, Iran, Germany, Japan, Netherlands, Denmark, Sweden or Switzerland, Wuhan was already in lockdown.
And if you're telling yourself: ""Well, Hubei is just one region"", let me remind you that it has nearly 60 million people, bigger than Spain and about the size of France.
So the coronavirus is already here. It's hidden, and it's growing exponentially.
What will happen in our countries when it hits? It's easy to know, because we already have several places where it's happening. The best examples are Hubei and Italy.
The World Health Organization (WHO) quotes 3.4% as the fatality rate (% people who contract the coronavirus and then die). This number is out of context so let me explain it.
It really depends on the country and the moment: between 0.6% in South Korea and 4.4% in Iran. So what is it? We can use a trick to figure it out.
The two ways you can calculate the fatality rate is Deaths/Total Cases and Death/Closed Cases. The first one is likely to be an underestimate, because lots of open cases can still end up in death. The second is an overestimate, because it's likely that deaths are closed quicker than recoveries.
What I did was look at how both evolve over time. Both of these numbers will converge to the same result once all cases are closed, so if you project past trends to the future, you can make a guess on what the final fatality rate will be.
This is what you see in the data. China's fatality rate is now between 3.6% and 6.1%. If you project that in the future, it looks like it converges towards ~3.8%-4%. This is double the current estimate, and 30 times worse than the flu.
It is made up of two completely different realities though: Hubei and the rest of China.
Hubei's fatality rate will probably converge towards 4.8%. Meanwhile, for the rest of China, it will likely converge to ~0.9%:
I also charted the numbers for Iran, Italy and South Korea, the only countries with enough deaths to make this somewhat relevant.
Iran's and Italy's Deaths / Total Cases are both converging towards the 3%-4% range. My guess is their numbers will end up around that figure too.
South Korea is the most interesting example, because these 2 numbers are completely disconnected: deaths / total cases is only 0.6%, but deaths / closed cases is a whopping 48%. My take on it is that a few unique things are happening there. First, they're testing everybody (with so many open cases, the death rate seems low), and leaving the cases open for longer (so they close cases quickly when the patient is dead). Second, they have a lot of hospital beds (see chart 17.b). There might also be other reasons we don't know. What is relevant is that deaths/cases has hovered around 0.5% since the beginning, suggesting it will stay there, likely heavily influenced by the healthcare system and crisis management.
The last relevant example is the Diamond Princess cruise: with 706 cases, 6 deaths and 100 recoveries, the fatality rate will be between 1% and 6.5%.
Note that the age distribution in each country will also have an impact: Since mortality is much higher for older people, countries with an aging population like Japan will be harder hit on average than younger countries like Nigeria. There are also weather factors, especially humidity and temperature, but it's still unclear how this will impact transmission and fatality rates.
This is what you can conclude:
Put in another way: Countries that act fast can reduce the number of deaths by a factor of ten. And that's just counting the fatality rate. Acting fast also drastically reduces the cases, making this even more of a no-brainer.
Countries that act fast reduce the number of deaths at least by 10x.
So what does a country need to be prepared?
Around 20% of cases require hospitalization, 5% of cases require the Intensive Care Unit (ICU), and around 2.5% require very intensive help, with items such as ventilators or ECMO (extra-corporeal oxygenation).
The problem is that items such as ventilators and ECMO can't be produced or bought easily. A few years ago, the US had a total of 250 ECMO machines, for example.
So if you suddenly have 100,000 people infected, many of them will want to go get tested. Around 20,000 will require hospitalization, 5,000 will need the ICU, and 1,000 will need machines that we don't have enough of today. And that's just with 100,000 cases.
That is without taking into account issues such as masks. A country like the US has only 1% of the masks it needs to cover the needs of its healthcare workers (12M N95, 30M surgical vs. 3.5B needed). If a lot of cases appear at once, there will be masks for only 2 weeks.
Countries like Japan, South Korea, Hong Kong or Singapore, as well as Chinese regions outside of Hubei, have been prepared and given the care that patients need.
But the rest of Western countries are rather going in the direction of Hubei and Italy. So what is happening there?
The stories that happened in Hubei and those in Italy are starting to become eerily similar. Hubei built two hospitals in ten days, but even then, it was completely overwhelmed.
Both complained that patients inundated their hospitals. They had to be taken care of anywhere: in hallways, in waiting rooms...
bergamo.corriere.it
Healthcare workers spend hours in a single piece of protective gear, because there's not enough of them. As a result, they can't leave the infected areas for hours. When they do, they crumble, dehydrated and exhausted. Shifts don't exist anymore. People are driven back from retirement to cover needs. People who have no idea about nursing are trained overnight to fulfill critical roles. Everybody is on call, always.
That is, until they become sick. Which happens a lot, because they're in constant exposure to the virus, without enough protective gear. When that happens, they need to be in quarantine for 14 days, during which they can't help. Best case scenario, 2 weeks are lost. Worst case, they're dead.
The worst is in the ICUs, when patients need to share ventilators or ECMOs. These are in fact impossible to share, so the healthcare workers must determine what patient will use it. That really means, which one lives and which one dies.
www.brusselstimes.com
""After a few days, we have to choose. [...] Not everyone can be intubated. We decide based on age and state of health."" Christian Salaroli, Italian MD.
All of this is what drives a system to have a fatality rate of ~4% instead of ~0.5%. If you want your city or your country to be part of the 4%, don't do anything today.
This is a pandemic now. It can't be eliminated. But what we can do is reduce its impact.
Some countries have been exemplary at this. The best one is Taiwan, which is extremely connected with China and yet still has as of today fewer than 50 cases. This recent paper explain all the measures they took early on, which were focused on containment.
jamanetwork.com
They have been able to contain it, but most countries lacked this expertise and didn't. Now, they're playing a different game: mitigation. They need to make this virus as inoffensive as possible.
If we reduce the infections as much as possible, our healthcare system will be able to handle cases much better, driving the fatality rate down. And, if we spread this over time, we will reach a point where the rest of society can be vaccinated, eliminating the risk altogether. So our goal is not to eliminate coronavirus contagions. It's to postpone them.
The more we postpone cases, the better the healthcare system can function, the lower the mortality rate, and the higher the share of the population that will be vaccinated before it gets infected.
How do we flatten the curve?
There is one very simple thing that we can do and that works: social distancing.
If you go back to the Wuhan graph, you will remember that as soon as there was a lockdown, cases went down. That's because people didn't interact with each other, and the virus didn't spread.
The current scientific consensus is that this virus can be spread within 2 meters (6 feet) if somebody coughs. Otherwise, the droplets fall to the ground and don't infect you.
The worst infection then becomes through surfaces: The virus survives for up to 9 days on different surfaces such as metal, ceramics and plastics. That means things like doorknobs, tables, or elevator buttons can be terrible infection vectors.
The only way to truly reduce that is with social distancing: Keeping people home as much as possible, for as long as possible until this recedes.
This has already been proven in the past. Namely, in the 1918 flu pandemic.
You can see how Philadelphia didn't act quickly, and had a massive peak in death rates. Compare that with St Louis, which did.
Then look at Denver, which enacted measures and then loosened them. They had a double peak, with the 2nd one higher than the first.
If you generalize, this is what you find:
This chart shows, for the 1918 flu in the US, how many more deaths there were per city depending on how fast measures were taken. For example, a city like St Louis took measures 6 days before Pittsburgh, and had less than half the deaths per citizen. On average, taking measures 20 days earlier halved the death rate.
Italy has finally figured this out. They first locked down Lombardy on Sunday, and one day later, on Monday, they realized their mistake and decided they had to lock down the entire country.
Hopefully, we will see results in the coming days. However, it will take one to two weeks to see. Remember the Wuhan graph: there was a delay of 12 days between the moment when the lockdown was announced and the moment when official cases (orange) started going down.
The question politicians are asking themselves today is not whether they should do something, but rather what's the appropriate action to take.
There are several stages to control an epidemic, starting with anticipation and ending with eradication. But it's too late for most options today. With this level of cases, the only options politicians have in front of them are containment, mitigation or suppression.
Containment
Containment is making sure all the cases are identified, controlled, and isolated. It's what Singapore, South Korea or Taiwan are doing so well: They very quickly limit people coming in, identify the sick, immediately isolate them, use heavy protective gear to protect their health workers, track all their contacts, quarantine them... This works extremely well when you're prepared and you do it early on, and don't need to grind your economy to a halt to make it happen.
I've already touted Taiwan's approach. But China's is good too. The lengths at which it went to contain the virus are mind-boggling. For example, they had up to 1,800 teams of 5 people each tracking every infected person, everybody they got interacted with, then everybody those people interacted with, and isolating the bunch. That's how they were able to contain the virus across a billion-people country.
This is not what Western countries have done. And now it's too late. The recent US announcement that most travel from Europe was banned is a containment measure for a country that has, as of today, 3 times the cases that Hubei had when it shut down, growing exponentially. How can we know if it's enough? It turns out, we can know by looking at the Wuhan travel ban.
This chart shows the impact that the Wuhan travel ban had delaying the epidemic. The bubble sizes show the number of daily cases. The top line shows the cases if nothing is done. The two other lines show the impact if 40% and 90% of travel is eliminated. This is a model created by epidemiologists, because we can't know for sure.
If you don't see much difference, you're right. It's very hard to see any change in the development of the epidemic.
Researchers estimate that, all in all, the Wuhan travel ban only delayed the spread in China by 3-5 days.
Now what did researchers think the impact of reducing transmission would be?
The top bloc is the same as the one you've seen before. The two other blocks show decreasing transmission rates. If the transmission rate goes down by 25% (through Social Distancing), it flattens the curve and delays the peak by a whole 14 weeks. Lower the transition rate by 50%, and you can't see the epidemic even starting within a quarter.
The US administration's ban on European travel is good: It has probably bought us a few hours, maybe a day or two. But not more. It is not enough. It's containment when what's needed is mitigation.
Once there are hundreds or thousands of cases growing in the population, preventing more from coming, tracking the existing ones and isolating their contacts isn't enough anymore. The next level is mitigation or suppression.
Mitigation or Suppression
The new article, Coronavirus: The Hammer and the Dance, covers mitigation vs. suppression at length.
Mitigation requires a heavy amount of testing, contact tracing, quarantines and isolations to flatten the curve without stopping the outbreak
Suppression tries to go one step further and quench the outbreak. It requires heavy social distancing. People need to stop hanging out to drop the transmission rate (R), from the R=~2-3 that the virus follows without measures, to below 1, so that it eventually dies out.
These measures require closing companies, shops, mass transit, schools, enforcing lockdowns... The worse your situation, the worse the social distancing. The earlier you impose heavy measures, the less time you need to keep them, the easier it is to identify brewing cases, and the fewer people get infected.
This is what Wuhan had to do. This is what Italy was forced to accept. And then France, Spain, and many other countries. Because when the virus is rampant, the only measure is to lock down all the infected areas to stop spreading it at once.
With thousands of official cases  and tens of thousands of true ones  this is what countries like the US, the UK, Germany, Netherlands or Switzerland need to do.
But they're not doing it.
Some businesses are working from home, which is fantastic.Some mass events are being stopped.Some affected areas are quarantining themselves.
All these measures will slow down the virus. They will lower the transmission rate from 2.5 to 2.2, maybe 2. Hopefully more. But they aren't enough to get us below 1 for a sustained period of time to stop the epidemic.
So the question becomes: What are the tradeoffs we could be making to lower the R? This is the menu that Italy has put in front of all of us:
Then two days later, they added: No, in fact, you need to close all businesses that aren't crucial. So now we're closing all commercial activities, offices, cafes and shops. Only transportation, pharmacies, groceries will remain open.""
How to pick the right measures for your country? Read Coronavirus: The Hammer and the Dance. One approach is to gradually increase measures. Unfortunately, that gives precious time for the virus to spread. If you want to be safe, do it Wuhan style. People might complain now, but they'll thank you later.
If you're a business leader and you want to know what you should do, the best resource for you is Staying Home Club.
stayinghome.club
It is a list of social distancing policies that have been enacted by US tech companiesso far, 328.
They range from allowed to required Work From Home, and restricted visits, travel, or events.
There are more things that every company must determine, such as what to do with hourly workers, whether to keep the office open or not, how to conduct interviews, what to do with the cafeterias... If you want to know how my company, Course Hero, handled some of these, along with a model announcement to your employees, here is the one my company used (view only version here).
It is very possible that so far you've agreed with everything I've said, and were just wondering since the beginning when to make each decision. Put in another way, what triggers should we have for each measure.
To solve this, I've created a model (direct link to copy).
docs.google.com
It enables you to assess the likely number of cases in your area, the probability that your employees are already infected, how that evolves over time, and how that should tell you whether to remain open.
It tells us things like:
The model uses labels such as ""company"" and ""employee"", but the same model can be used for anything else: schools, mass transit... So if you have only 50 employees in Paris, but all of them are going to take the train, coming across thousands of other people, suddenly the likelihood that at least one of them will get infected is much higher and you should close your office immediately.
If you're still hesitating because nobody is showing symptoms, just realize 26% of contagions happen before there are symptoms.
This math is selfish. It looks at every company's risk individually, taking as much risk as we want until the inevitable hammer of the coronavirus closes our offices.
But if you're part of a league of business leaders or politicians, your calculations are not for just one company, but for the whole. The math becomes: What's the likelihood that any of our companies is infected? If you're a group of 50 companies of 250 employees on average, in the SF Bay Area, there's a 35% chance that at least one of the companies has an employee infected, and 97% chance that will be true next week. I added a tab in the model to play with that.
It might feel scary to make a decision today, but you shouldn't think about it this way.
This theoretical model shows different communities: one doesn't take social distancing measures, one takes them on Day n of an outbreak, the other one on Day n+1. All the numbers are completely fictitious (I chose them to resemble what happened in Hubei, with ~6k daily new cases at the worst). They're just there to illustrate how important a single day can be in something that grows exponentially. You can see that the one-day delay peaks later and higher, but then daily cases converge to zero.
But what about cumulative cases?
In this theoretical model that resembles loosely Hubei, waiting one more day creates 40% more cases! So, maybe, if the Hubei authorities had declared the lockdown on 1/22 instead of 1/23, they might have reduced the number of cases by a staggering 20k.
And remember, these are just cases. Mortality would be much higher, because not only would there be directly 40% more deaths. There would also be a much higher collapse of the healthcare system, leading to a mortality rate up to 10x higher as we saw before. So a one-day difference in social distancing measures can end exploding the number of deaths in your community by multiplying more cases and higher fatality rate.
This is an exponential threat. Every day counts. When you're delaying by a single day a decision, you're not contributing to a few cases maybe. There are probably hundreds or thousands of cases in your community already. Every day that there isn't social distancing, these cases grow exponentially.
This is probably the one time in the last decade that sharing an article might save lives. They need to understand this to avert a catastrophe. The moment to act is now.
If you want to receive more articles from me, subscribe to my newsletter.
I will start adding links to translations here. I can't verify any except for Spanish, French and Italian, so if translations look bad, please let me know.
EuropeanFrenchSpanishItalianGerman (alternative version)Dutch (alternative version)Portuguese (and alternative version and another one) CatalanNorwegianFinnishSwedishEstonianLithuanianPolish (and alternative version)CzechSlovakianHungarianRomanian (partial translation)SerbianAlbanianSlovenianBulgarian (alternative version)MacedonianGreekTurkishRussianUkrainianGeorgian
East & South AsianTraditional ChineseSimplified ChineseJapaneseHindi (alternative)MalayalamThai (alternative version)VietnameseMongolianIndonesianBengali
Middle East, Central Asian and AfricanArabic (alternative outside of Medium)Persian (and alternative version in pdf)Dari (pdf)Hebrew (alternative translation outside of Medium, for Hebrew formatting)Somali
Special call out for Argentina, diving into its numbers, as an early warning
",190
https://medium.com/edge-of-innovation/how-safe-are-graphene-based-face-masks-b88740547e8c?source=tag_archive---------4-----------------------,Manufacturers have been using nanotechnology-derived graphene in face masks  now there are safety concerns,"There are warnings of potential ""early pulmonary toxicity"" associated with face masks using graphene",Andrew Maynard,11,"Face masks should protect you, not place you in greater danger. However, last Friday Radio Canada revealed that residents of Quebec and Ottawa were being advised not to use specific types of graphene-containing masks as they could potentially be harmful.
The offending material in the masks is graphene  a form of carbon that consists of nanoscopically thin flakes of hexagonally-arranged carbon atoms. It's a material that has a number of potentially beneficial properties, including the ability to kill bacteria and viruses when they're exposed to it.
Yet despite its many potential uses, the scientific jury is still out when it comes to how safe the material is.
UPDATE August 13, 2021: Health Canada is permitting the sale of the four Shandong Shengquan New Materials Co. Ltd. mask models following testing. No other graphene face masks are currently permitted for sale in Canada. More information here.
As with all materials, the potential health risks associated with graphene depend on whether it can get into the body, where it goes if it can, what it does when it gets there, and how much of it is needed to cause enough damage to be of concern.
Unfortunately, even though these are pretty basic questions, there aren't many answers forthcoming when it comes to the substance's use in face masks.
(Added March 26, 2021) Current concerns around the use of graphene in face masks stem from a memo sent by Health Canada to Canadian Provincial and Territorial Ministries of Health on March 25. This memo hasn't, to my knowledge, been made public yet, although it does mention plans to release a public statement.
In the memo, Health Canada recommends users ""stop purchasing and using face masks containing nanoform graphene""  a statement that covers a growing array of commercially available face masks.
Backing this up, it states
""Health Canada has conducted a preliminary risk assessment which identified a potential for early pulmonary toxicity associated with the inhalation of nanoform graphene. To date, Health Canada has not received data to support the safety and efficacy of face masks containing nanoform graphene.
""As such, and in the absence of manufacturer's evidence to support the safe and effective use of nanoform graphene coated masks, Health Canada considers the risk of these medical devices to be unacceptable.""
Beyond this, there are no details yet of the data that went into that preliminary risk assessment.
If you find this article interesting or useful, consider signing up for Andrew Maynard's newsletter at andrewmaynard.net
Early concerns around graphene were sparked by previous research on another form of carbon  carbon nanotubes. It turns out that some forms of these fiber-like materials can cause serious harm if inhaled. And following on from research here, a natural next-question to ask is whether carbon nanotubes' close cousin graphene comes with similar concerns.
Because graphene lacks many of the physical and chemical aspects of carbon nanotubes that make them harmful (such as being long, thin, and hard for the body to get rid of), the indications are that the material is safer than its nanotube cousins. But safer doesn't mean safe. And current research indicates that this is not a material that should be used where it could potentially be inhaled, without a good amount of safety testing first.
In recent years there have been a number of comprehensive reviews on the potential toxicity of graphene, including this 2018 paper by Bengt Fadeel and colleagues, and this one by Vanesa Sanches and colleagues. Both are solid reviews by highly respected research teams. And both indicate that, while the toxicity of graphene is complex and may be low in some cases, it isn't negligible.
When it comes to inhaling graphene, the current state of the science indicates that if the material can get into the lower parts of the lungs (the respirable or alveolar region) it can lead to an inflammatory response at high enough concentrations.
There is some evidence that adverse responses are relatively short-lived, and that graphene particles can be broken down and disposed of by the lungs' defenses.
This is good news as it means that there are less likely to be long-term health impacts from inhaling the material.
There's also evidence that graphene, unlike some forms of thin, straight carbon nanotubes, does not migrate to the outside layers of the lungs where it could potentially do a lot more damage.
Again, this is encouraging as it suggests that graphene is unlikely to lead to serious long-term health impacts like mesothelioma.
However, research also shows that this is not a benign material. Despite being made of carbon  and it's tempting to think of carbon as being safe, just because we're familiar with it  there is some evidence that the jagged edges of some graphene particles can harm cells, leading to local damage as the body responds to any damage the material causes.
There are also concerns, although they are less well explored in the literature, that some forms of graphene may be carriers for nanometer-sized metal particles that can be quite destructive in the lungs. This is certainly the case with some carbon nanotubes, as the metallic catalyst particles used to manufacture them become embedded in the material, and contribute to its toxicity.
The long and short of this is that, while there are still plenty of gaps in our knowledge around how much graphene it's safe to inhale, inhaling small graphene particles probably isn't a great idea unless there's been comprehensive testing to show otherwise.
And this brings us to graphene-containing face masks.
As a general rule of thumb, engineered nanomaterials should not be used in products where they might inadvertently be inhaled and reach the sensitive lower regions of the lungs. But do graphene-containing face masks shed graphene-containing particles that are small enough to be inhaled and deposit in sensitive regions of the lungs?
Here, I must confess I've hit a dead-end in my search for evidence for or against the release of graphene-containing particles in the face masks mentioned by Radio Canada. But this in itself is a red flag.
Given all that we know about the pulmonary toxicity of engineered nanoparticles, and the uncertainty over the inhalation risks of graphene, surely someone should have asked this question when developing graphene-containing masks.
When airborne nanoparticles are inhaled and penetrate to the lower regions of the lungs (the alveolar region), they can elicit a response that's more closely associated with the number or surface area of the particles than their mass. And because of this, very small quantities of material have the potential to cause a lot of harm  much more than you might imagine from the mass of material alone.
And one consequence of this is that the smaller or thinner the particles are, the more harm they have the potential to create.
Graphene is typically made up of plate-like particles that are just a few atoms thick, and hundreds to thousands of nanometers wide (a nanometer being one billionth of a meter). If these platelets were released into the air from face masks as a wearer inhaled, many of them would reach the alveolar region of the lungs.
Of course, we don't know if they are released or not. I haven't seen any data on this, and they may be so firmly attached to the mask material that they stay put. And from what we know of the physics of nanoparticles, individual platelets are unlikely to be dislodged as the forces keeping them in place would simply be too strong.
But there's a reasonable chance that clumps of platelets could be released  especially if the mask producer hasn't thought the design through adequately. In this case, any released airborne particles up to around 5-10 m in diameter could potentially present a health hazard.
And this is where more information is desperately needed  especially as there are a growing number of graphene-based masks being sold around the world.
If Radio Canada is correct that Health Canada has warned against ""the potential for 'early pulmonary toxicity'"" associated with a particular brand of graphene-containing face masks, this would suggest that there is a plausible potential for graphene-containing particles to be released and inhaled when someone's wearing these masks. And if so, serious questions need to be asked about the potential health risks, and the extent of the problem.
Here, it's important to stress that we don't yet know if graphene particles are being released and, if they are, whether they are being released in sufficient quantities to cause health effects. And there are indications that, if there are health risks, these may be relatively short-term  simply because graphene particles may be effectively degraded by the lungs' defenses.
At the same time, it seems highly irresponsible to include a material with unknown inhalation risks in a product that is intimately associated with inhalation. Especially when there are a growing number of face masks available that claim to use graphene.
Radio Canada claims that the graphene face masks people are advised not to use are produced by the Quebec-based manufacturer Metallifer. However, it appears that these masks originate from the Chinese holding company Jinan Shengquan Group Share Holding Co., Ltd.
Within the Shengquan Group, the Shandong Shengquan New Materials Co., Ltd. makes a range of face masks and respirators that use graphene. And a quick search on Amazon indicates that a large number of companies seem to be selling face masks containing Shandong's flagship technology ""biomass graphene.""
According to information on nbgenerator.com, Shandong's biomass graphene is ""derived from natural straws as raw material, which use the pyrolysis method based on group deposition carbon deposition"". The website also refers to the Chinese patent ZL 2015 1 0819312.x.
This patent provides a little more insight into the material, but sadly not a lot. What it does indicate however is that the product contains trace amounts of various catalytic metals, including iron and nickel  possibly in the form of nanoparticles. And going back to what's known about the inhalation toxicity of other forms of carbon, the presence of catalytic metals can be a problem.
Interestingly, the US Centers for Disease Control and Prevention National Personal Protective Technology Laboratory (NPPTL) ran tests on a Shandong biomass graphene respirator back in June 2020. The respirator performed well in tests that are designed to evaluate its ability to prevent exposure to airborne particles in the air outside it. But these don't look explicitly at particles that might have been released from within the mask.
The good news here is that the high filtration rates measured (over 97% effective) suggest that there was little internal shedding of fine particles. However, the tests do not explicitly show that potentially harmful graphene particles were not released.
And Shandong isn't the only producer of graphene-based face masks. Over this past year, a number of researchers have explored adding the material to masks  this Hong Kong-based research team is just one example. And more companies have started to use the technology. In fact, a quick search on Amazon reveals a long list of products and manufacturers, all claiming to offer better protection because they contain graphene.
Despite a lack of clear evidence on health risks associated with graphene-containing face masks (although Health Canada may have data that haven't been released yet), I must confess that I'm concerned by what I see unfolding.
I've been at the forefront of researching nanomaterial risks and developing approaches to safe and responsible use for over 20 years. And over this time, it's become clear that the safe and responsible use of any new products that potentially lead to nanomaterials getting into the human body needs to be taken seriously.
Fortunately, many products of nanotechnology are relatively safe  or can be rendered safe with some forethought. But we know enough  and have done for years  to have a good sense of what questions we should be asking anytime there's a product where nanoscale particles might be released and inhaled.
These are basic no-brainer questions: Can the material get into the body? If it does, can it behave in ways that could cause harm? If so, what sort of harm, and how is it caused? And how much material is needed to cause concern?
Some of these questions are tricky to answer when it comes to nanomaterials like graphene as we don't always know what it is about the material that messes with our biology, and what the consequences are. But this is where research and a good dose of caution kick in under the universal rule of ""better safe than sorry.""
The irony here is that hundreds of millions of dollars have been poured into studying the risks of engineered nanomaterials over the past couple of decades. Yet when it comes to real-world products and real-world risks, no-one seems to be asking the questions that count, or providing answers!
Shandong is not the only manufacturer of graphene face masks. There are millions of graphene face masks and respirators being sold and used around the world. And while the unfolding news focuses on Quebec and one particular type of face mask, this is casting uncertainty over the safety of any graphene-containing masks that are being sold.
And this uncertainty will persist until manufacturers and regulators provide data indicating that they have tested the products for the release and subsequent inhalation of fine graphene particles, and shown the risks to be negligible.
If these data don't exist, this is irresponsible innovation on a grand scale  even if the risks turn out to be negligible. It demonstrates a level of naivety and disdain for past risk research that threatens to undermine trust and confidence in mask use. And it runs the additional risk of raising anxieties within those who have been using face masks responsibly, and are now wondering if they risked their health as a result.
And if the risks are not negligible, we have a problem on our hands that extends far beyond Quebec!
I sincerely hope that any risks from using graphene in face masks will be negligible, and that data to show this will come to light quickly.
But when it comes to the risks of using new technologies, hope alone is not good enough. Neither is naively using a new material while ignoring the potential risks.
Acknowledgements: I'm indebted to Jim Thomas, Research Director with the ETC Group, for bringing this issue to my attention and doing some of the initial digging into the company and technology involved.
In researching the Shandong graphene face mask, the following graphic came up on nbgenerator.com that seemed to indicate the mask has FDA approval:
I've yet to find anything that fits this on the FDA website. Shandong have a number of 501(k) premarket notifications with the FDA, but none of these mention the use of graphene in face masks. Until further evidence comes to light, my best guess is that this is FDA confirmation of approval to sell a product that is substantially equivalent to an existing approved product  although not one that uses or mentions the use of graphene  or a local certificate of production.
There are, interestingly, a number of other manufacturers of graphene masks that claim FDA approval (for example, MamaMoor, Medicevo and NQX. However, it's frustratingly difficult to find out what exactly FDA approval means here. Or, for that matter, when FDA consider product equivalency, whether they consider the potential for a mask to shed respirable nanoparticles.
In other words, even where the FDA is concerned, there seem to be more questions than answers.
This is an emerging story. It was updated March 28 for clarity, and to include additional information on the March 25 memo from Health Canada, and on April 2 to include the Health Canada advisory on not using graphene-containing masks.
",191
https://historyofyesterday.com/eben-byers-the-man-who-drank-radioactive-water-until-his-jaw-fell-off-d2634336b504?source=tag_archive---------3-----------------------,Eben Byers: The Man Who Drank Radioactive Water Until His Jaw Fell Off,The tragic death of Eben Byers and the ugly history behind Radithor,Andrei Tapalaga ,4,"Disclaimer: Some of the content within this article might emotionally affect readers so viewer's discretion is advised
The early part of the 20th century was still quite an experimental phase for the pre-modern medicine era. Medics would prescribe very strong drugs such as cocaine and heroin for various illnesses. Some people would medicate themselves with potions that we categorize today as poisonous to the human body.
However, the substance that stands out the most for actually being prescribed by medics was Radithor or in other words radium (highly radioactive) combined with water. Ironically enough, when the product was released it was advertised as ""a cure for the living dead.""
The potion was manufactured by the Bailey Radium Laboratories in New Jersey. This company was founded by William J. A. Bailey who was not a certified doctor as he dropped out of the Harvard Medical School, however, he kept telling people that he graduated so everyone sort of believed him.
Bailey and his team of chemists had done a bit of research on radium and found some ""hypothetical"" benefits for the human organism. This was at a time when radioactivity wasn't a very well-researched subject and especially what it can do to the human body. They knew that in high doses it could kill a person, but a very small dose, well distilled with water could be beneficial.
The product was launched in 1918 to the whole of the United States. The benefits from Radithor were very vague as the description only mentioned that ""it will enhance the vital processes of the body."" This did not stop people from buying it and medics from prescribing it and as per usual the placebo effect took place as everyone was feeling much better only after a few doses.
Eben Byers was born in 12 April 1880. His wealthy background permitted him a good education, even going as far as graduating from Yale University. He was an athletic young man who won many gold championships in the early 1900s. Once his father deemed him old enough, he made him the president of Girard Iron Company.
In 1927, during one of his matches, he tripped and injured his arm. The wound wasn't very bad, but it provoked a lot of pain, something Byers was not used to. Therefore his medic prescribed Radithor for faster healing besides all its other hypothetical benefits.
Byers followed his medic's prescription to only take a small spoon a day. Over a short period of time, Byers was also hit by the placebo effect as he was feeling great, so great that he started taking a whole bottle a day, then weeks later two, and after a year even three bottles of a day.
In 1931 he got a surprise as his jaw literally fell down. The high consumption of radioactive water made his tissue and bones disintegrate from the inside. He didn't feel much pain as all of his nerves were also melted by the substance in time.
The medics tried to remove all the putrifying tissue from his face and surgically build him a new jaw that would make him not look so disfigured. From 1927 when he first started taking Radithor until 1931 when he stopped, Byers consumed over 1,400 bottles. With this much, his vital tissue and organs were also disintegrating inside his body which led to his death in 1932, when he was only 51 years of age.
Bailey's company was shut down by the authorities, however, he tried to sell the same item under a different name.
In 1965, Byers' body was exhumed to be studied and his remains after 30 years we're still extremely radioactive.
From the times that the pyramids were raised to the end of...
841 
4
841 claps
841 
",192
https://medium.com/personal-growth/myifguide-38037b3b1ec4?source=tag_archive---------0-----------------------,My Intermittent Fasting Lifestyle: How I Dropped 50 Pounds,A Quick Start Guide by Sumaya Kazi,Sumaya Kazi,10,"Updates [November 2021]:
Hi there  My name is Sumaya and thanks to Intermittent Fasting (or IF for short), in 7.5 months I've dropped 50 pounds, 10.5% in body fat and 40 inches around my body. These results are entirely as a result of IF as I was unable to exercise for the first several months due to a fractured foot.
After college, I spent at least 5+ years in the overweight category (no thanks to bad habits, traveling and eating out) before spending another 5+ years in the obese category (no thanks to startup stress, late nights and even more work travel). I'm now officially in the normal weight category (according to my BMI).
I tried everything from Jenny Craig/Weight Watchers to going to the gym 4-5x a week to weekly meal prep. While I did see some results, I eventually couldn't keep up with it and then yo-yo'd. IF has been the simplest and most manageable way I have found to improve my health (and stick with it).
This post is intended for my friends (and friends of friends) who have been following my health journey on Instagram, Facebook, Snapchat, or have asked how to get started. Because the interest has been in the thousands (so amazing!), I've decided to share this information more publicly.
In the interest of time, I've created this super quick start guide. In the near future, I'll be sharing more details, providing tips, and answering the most commonly asked questions I get.
The parts I've highlighted in italics below are areas I plan to elaborate on in future posts. Again, this is meant to be a quick start guide. If you have specific questions, please do comment below or message me on Instagram or Facebook, or Snapchat.
If you find this guide useful, please tap the  button below!
Fasting has been around for thousands of years and commonly practiced in major religions and utilized by medical practitioners.
There are many studies documenting the benefits of fasting which include lowering the risk of type 2 diabetes, reducing cholesterol levels, enhancing the body's resistance to oxidative stress (which is connected to aging and many chronic diseases), reducing inflammation (a key driver in many common diseases) and... weight loss! The weight loss benefit is how I first got introduced to Intermittent Fasting (or IF for short).
Like most other approaches to weight loss, the overall goal is to reduce calories without malnutrition. However, compared to the other approaches, Intermittent Fasting is focused on *when* to eat and when not to.
There are many different kinds of IF but for this quick start guide, I will be sharing the style I've adopted and has worked for me.
I know many of you are itching to get started right away, but IF is not for everyone. It is not recommended for pregnant women, women who are breastfeeding, people with diabetes, or other people who need to closely regulate their blood sugar. In addition, there has not been research on participants who are underweight, very old, or very young (<18 yrs. old) and these populations could be at higher risk for experiencing negative consequences of fasting (Longo and Mattson , 2014). So please:
The above two are the most helpful and what my friends and I use daily. The below are additional things I've done to measure success (but not something you have to do or slow you down from starting IF). I love data, so I choose to do the following as well:
I've adopted what's called a 4:3 style of IF. This simply means, I eat 4 days during the week and fast 3 non-consecutive days of the week.
This is what my 4:3 weekly schedule looks like:
SUNDAY: Eat Day // I eat as I normally would throughout the day and I begin my fast at 9 pm PT (this means I stop eating or drinking anything with calories).
MONDAY: Fast Day // On fast days I only drink coffee, tea, non-caloric drinks and water (flavored sparkling water like La Croix has been awesome during fasting, my favorite is the Peach-Pear flavor). I add a little half & half in my coffee, and those are the only calories I consume on fast days. If/when I feel hunger pains on fast days, I drink a bottle/can of sparkling water and that helps me get through the day.
TUESDAY: Eat Day // I break my fast at 9 am PT. I eat my full day of meals. The total calories I eat is my TDEE (total daily energy expenditure) range. I use this tool to calculate my TDEE: https://tdeecalculator.net/index.php I start my fast again at 9 pm PT.
WEDNESDAY: Fast Day // Same as Monday.
THURSDAY: Eat Day // Same as Tuesday.
FRIDAY: Fast Day // Same as Monday & Wednesday.
SATURDAY: Eat Day // Same as Tuesday & Thursday.
SUNDAY: Eat Day // Same as Tuesday, Thursday & Saturday.
In summary: MWF = Fast Days, TuThSaSu = Eat Days. Repeat every week and you will see and feel the difference.
For the visual types, this is what my schedule looks like:
I start my fast at 9 PM and end the day after next at 9 AM, but these time frames can also be changed. Some of my friends prefer 7 PM/7 AM, 8 AM/8 PM, etc. to better accommodate their work/family schedule.
I have found that the 4:3 style fasts have worked well for my friends and me based on our work/life schedule and our personal health goals. I like that full day fasts feel like an on/off switch  I don't think about eating on fast days and on eat days if I happen to overeat, I don't feel guilty about it (since I'm eating at a deficit during the week). I find it more manageable to cut calories over a week (using the 4:3 style of IF) instead of every day (trying to eating less daily).
The first couple weeks can feel challenging since your body is trying to adjust but once you get through it, fasting will feel more like a habit.
Watch: 5 Tips to Make Your First Week With Intermittent Fasting Successful
Here are a few tips that helped me:
On Fast Days:
On Eat Days:
Intermittent Fasting can be challenging especially in the beginning which is why having support early on is important. I'm fortunate that my sister, brother and good friends all fast with me now (which makes the lifestyle easier).
I encourage you to introducing IF to a friend, family member or even colleague so that you can experiment with it together! Additionally, there are great support groups on Facebook that I recommend. I'm an admin and active in these groups:
WeFast Intermittent Fasting & Metabolic Performance Community This group includes doctors, specialists in the field, and people who fast for many different reasons including weight loss, biohacking, and living a longer life.
Intermittent Fasting For Women This is a fast-growing group that provides great support for women around the world who are exploring fasting. You can learn from over 80,000 members and counting!
That wraps up my quick start guide. I hope this helps you in the right direction!
Based on requests I've already started receiving I plan on elaborating more on items above in italics, answering frequently asked questions, sharing tips and discussing topics like the following:
Follow me on Instagram
Follow me on Snapchat
Intermittent Fasting has changed my life. If it can help just one more person with their struggles and goals, I'm happy to spend the time to answer questions and support you the best I can (and hopefully one day you'll pay it forward too)!
If you'd like to continue following my IF journey, you can do so by following me on Facebook, Snapchat or Instagram stories.
If you want to receive future posts from me about Intermittent Fasting, feel free to add your email address here: http://bit.ly/IFupdates
A favor:
Wishing you the greatest success! 
Your friend (or friend of friend),
Sumaya
 
** Big thanks to Jaya, Tina, Chandni (also IFers) and Lora for their feedback on this guide! 
Sharing our ideas and experiences.
12.8K 
86
",193
https://medium.com/hackernoon/i-stopped-drinking-for-30-days-heres-what-happened-7e24135d42fe?source=tag_archive---------8-----------------------,I Stopped Drinking for 30 Days. Here's What Happened.,Instant Workaholism Is No Substitute for Happy Hour.,Bjorgvin Benediktsson,9,"Instant Workaholism Is No Substitute for Happy Hour.
From April 10th to May 10th I stopped drinking entirely. I wrote the bulk of this article the day I could start drinking, but in the interest of experimentation I decided to wait to publish it.
You know, just in case something went terribly wrong and I ruined my life by going back to beer. Turns out, I'm still here.
The day I could start drinking again was an interesting one. Because I enjoy a nice, cold craft beer so much I would've thought that I'd be giddy with excitement, like a toddler waking up on Christmas day.
Alas, I felt more indifferent about it than I thought I would.
But let's start at the beginning.
The biggest reason I decided to do my 30-day challenge was a simple one:
I wanted to know if I could do it.
As someone who regularly has a beer at lunch, likes to celebrate a workday with a couple of drinks at the bar and parties on the weekends I wanted to know just how strong of a hold alcohol had on my life.
It turns out, my ""alcoholism"" has nothing on my willpower.
I wouldn't classify the challenge as ""easy, "" but it was far from difficult. There was never a time I was so desperate to finish the challenge that I banished alcohol from the house or didn't meet people at happy hour because it was at a bar.
To demonstrate, this was the cooler sitting in my shed the entire duration of this challenge.
Curiosity was the biggest reason for trying the challenge. I wanted to see what, if anything, would happen if I completely gave up alcohol for a month.
However, I can't say the two things are 100% positively correlated, but it does have an interesting twist to it.
One tends to substitute addictions instead of getting rid of them. In my case, workaholism took over. Since I wasn't socializing and having a beer to calm me down and zone me out, there was no reason NOT to work from 5-10 PM.
An extra 3-5 hours of work every day wasn't bad per say, but it wasn't positive either. As All-Father Oinn (yes, I'm Icelandic. Yes, I'm quoting Norse Gods) says, and I'm paraphrasing,
""Everything is great in moderation.""
Moderation and balance are the keys to being a successful, well-rounded person. Therefore, isolated workaholism isn't the solution for filling the time you used to use for happy hour.
Even though I was more productive, it didn't necessarily contribute to my overall well-being. I was just being productive for productivity's sake instead of using that time for personal development in other ways. My financial success that month had more to do with a product creation that happened before my challenge started than my sobriety. However, the launch of that product happened during my 30-day challenge so I can't rule out a certain level of clear-headedness that might have added to its success.
Here are some further thoughts I had during and after my experiment.
Unsurprisingly, hangovers zap your energy like Anna ""Rogue"" Marie zaps your mutant powers. If you drink too much (especially after you turn 30), you're grumpy and unproductive for most of the day after. The morning benefits of sobriety are great. Your sleep quality is better, you wake up clear-headed, and your workday is more exciting.
Of course, I work for myself and get to decide what I do every day, so I'm pretty lucky in that regard. If you hate your job, maybe it's better to just do it in a zombie state. However, if happy hour is the only thing you look forward to during your workday maybe the problem isn't your drinking, but rather your career choice?
Without a hangover, my morning routine is way more enjoyable. You can read all about it here, but needless to say, it's easier to get a good head-start on the morning when you want to get out of bed.
I felt calmer and in touch with my emotions when I was completely sober. Also, because I was never hungover, it was easier to stick to my meditation routine.
Meditation teaches you to respond instead of react to external issues. As I was completely sober and meditating more than usual, I was calmer and more aware of every situation that needed my response. Instead of knee-jerk reactions of annoyance, I would be more methodical in my answers.
If you don't drink, you don't want to hang out at bars, but you still don't want to change your entire routine and alienate all your friends.
This was a difficult one because bars lose their appeal when you don't want 99% of what they have on the menu. They also get old fast once the crowd starts getting drunk and loud.
However, your friends are the reason you're at the bar in the first place, not the alcohol. That said, you still feel awkward because of the unwritten social contract of having a drink in front of you.
Solution?
Order a non-alcoholic cocktail. Some bartenders in Tucson took their non-alcoholic cocktail making skills seriously, and I thank them for it.
Cameron over at Congress even told me,
""Just tell everybody it's a Tom Collins if you're asked.""
People tend to act weird if you say you're not drinking but they won't question you if you tell them a name of an alcoholic drink.
The main point here is that you shouldn't stop your routine just because you don't drink. You will feel self-conscious and weird about not having a drink in your hands. It's a psychological thing so make sure you have something to do with your hands.
During this challenge, I realized that I also just drink too fast in general, regardless of the drink's alcohol content. I thought I drank quickly because I wanted to get drunk faster but turns out I drink a glass of soda water just as fast.
I attribute this to my hyperactive, Type-A disorder of wanting to fill every pause in the conversation with a sip of what I'm drinking, instead of some underlying, alcoholic speed demon.
It's a funny feeling to be sober when everybody else around you is trashed. People stop paying attention to their surroundings as they get drunker. They block off the world except for what's going on around their 3-feet radius. This makes people watching hilarious because everybody seems oblivious to the fact that they are in a room filled with a bunch of other people.
So unless you stare intently at somebody and get their attention, it's easy to turn your boring night out into an anthropological observation.
When I mentioned my challenge to people, they seemed supportive and interested. They said things like ""it's good to do that"" or ""I should try that sometime"" but what I heard was,
""That's cool and all, but there's no way I'm doing that.""
There was always that underlying tone in their voice that suggested that yes, maybe in a perfect world that would be interesting, but there's no way they're making that a priority. I don't blame them. After doing the challenge, I learned that I'm not interested in being 100% sober all the time either.
People are self-absorbed most of the time, especially when they've been drinking. I don't mean that in a mean way. It's simply a very honest observation. Everybody's looking out for themselves and they should. It's a natural extension of their survival instinct.
Because of this, people don't notice you're not drinking unless you tell them.
My friends are used to me drinking all the time. They don't even have a reason to question what I have in my cup, so they get very surprised when I tell them I haven't been drinking at all. Not then, nor at the party yesterday where we hung out together too.
I probably go out about 4-5 times per week.
That includes happy hour, weekend outings and brunch. If you also count buying alcohol from the grocery store, it starts to build up.
Let's assume I go to dinner two times per week and I have two drinks at dinner. That's about $12-15 on average each time (only counting drinks). I would go to happy hour about three times on average, usually drinking about three beers over the course of the night. That would end up being ~$20 each time with tax and tip.
For brunch, we're pretty good at finding good bottomless mimosas or cheap drinks. However, that's still about $10-15 each Sunday.
Then, if I buy one nice 12 pack of beer at the grocery store, I'd be spending around $20 on that.
This is an estimate, and it's probably conservative, to be honest. But it comes out to $125 per week or $6,500 per year. That's a pretty good chunk of money and well past 10% of the average household income. Realizing you're spending that much on alcohol hurts in more ways than one.
Just like it's easier to be productive when you're not hungover, it's easier to persuade yourself to work out. Although I don't need much persuasion to exercise in general, whether I'm drinking or not, I went to the gym more often.
Instead of 2-3 times per week I would now be going 3-5 times instead. 30 days might not be enough time to gauge any measurable body results, but I feel fitter and healthier. And without the additional carb load from 3-6 beers every day I don't feel as fat. I can't say that I've lost any weight, but I do feel stronger. I can probably thank my strength training for transforming my fat to muscle in that case.
In addition to working out more, I also cook better dinners. Instead of having a couple of drinks at happy hour and then being too lazy to cook I spend more time in the kitchen and eat healthier food.
Overall, it was an interesting challenge. Now I know I can easily do it and can stop worrying about whether I'm slowly succumbing to some demon disease I don't have.
The hardest parts of the challenge were the happy hour times between 5 and 7 pm. There's just something so rewarding about a job-well-done drink that it was hard to give that up.
Weirdly enough, the desire for a drink dissipated fairly quickly after dinnertime because at that point I had no desire to drink because of the possibility of being hungover in the morning. That's why the 5-7 pm time frame is such a sweet spot. You can relax and enjoy a cold drink without any huge professional consequences.
The biggest change was my increased workaholism. I realized that I work too much. The challenge made me more productive, but it didn't necessarily make my life any better.
Although I did have one of my best months as an entrepreneur, was it ALL the consequence of not drinking? No, but being clear headed every morning and working most weekends helped.
Overall, I learned a lot about willpower, productivity and substituting habits. I don't think I'm cut out for sobriety because I like my happy hour too much. But it made me realize that waking up early and being productive makes me happy as well.
In my case, balancing the two, in moderation, seems like a good barometer for success.
Or as Oinn would say,
""Drink your mead, but in moderation, Talk sense or be silent:No man is called discourteous who goes To bed at an early hour""
#BlackLivesMatter
349 
9
",194
https://betterhumans.pub/how-to-to-lose-10-pounds-of-fat-a-month-even-if-you-have-a-slow-metabolism-3fc12cf548dc?source=tag_archive---------8-----------------------,How to lose 10+ pounds of fat a month- even if you have a slow metabolism,"If I had to list three things everyone wants, I'd say:",John Fawkes,8,"If I had to list three things everyone wants, I'd say:
I can't help you with the first two items on that list- but fortunately, I specialize in helping people lose weight. As it happens, you can slim down, flatten your belly, and fit into your old jeans- and it doesn't have to take years. In fact, it is entirely possible to lose more than ten pounds a month if you know what you're doing.
Just take a look at Xi Han, who lost 45 pounds in 5 months, and gained muscle in the process. Or Samantha, who beat Type 2 Diabetes by following a flexible diet and a high-volume exercise program. And then there's Jesse Shand, who lost over 350 pounds with the help of the Bodybuilding.com community.
At the end of the day, fat loss is just a matter of burning more calories than you consume. Since a pound of fat contains around 3800 calories, losing ten pounds a month requires a caloric deficit of around 1200 calories a day.
To lose fat quickly and safely, and avoid rebound weight gain, you just need to do four things:
This doesn't require a starvation diet. In this article, I'm going to show you exactly how to cut calories without starving yourself, burn more calories without spending hours a day in the gym, and prevent yo-yo dieting before it happens.
Count calories and macros. You'll need to track calories and macros (grams of fats, protein and carbs) religiously, at least for the first month. First, use this calculator to determine your body fat percentage- and don't suck in your gut when you measure your waist. Then, use this calorie calculator (be sure to select the lean mass formula) to determine your Total Daily Energy Expenditure.
Your daily calorie targets are 1000 less than TDEE on days you lift weights, and 1200 under TDEE on days you either don't work out, or only do cardio. On all days, eat at least 30 grams of protein with every meal and 10 grams with every snack.
Eliminate sugar and liquid calories. In addition to the obvious culprits, watch for hidden sugars in things like fruit, sauces/condiments, and ""healthy"" foods such as protein bars.
Drink a lot of water. Staying hydrated is a great idea in general, but dehydration also sends thirst signals to your brain that can get misinterpreted as hunger. Drink at least a gallon of water a day if you're male, or 3/4 gallon if you're female, and have a glass before every meal.
Practice intermittent fasting. Skip breakfast and compress your daily eating into a smaller window of time. If you're a man, fast for at least 16 hours and eat in an 8-hour window every day- noon to 8 PM works best for most people. If you're a woman, fast for at least 14 hours and eat for 10. In either case, this means you're having two small meals and one smaller low-calorie snack each day.
If you're flexible about timing, intermittent fasting is also a useful tool for improving meal quality. When I spent a year backpacking around the world, I would often fast in an ad-hoc manner in order to avoid eating when only crappy food was available, which in turn allowed me to eat more delicious healthy food at other meals.
Eat slowly and stop eating as soon as you're not hungry. There's a delay of up to a half hour between when you eat something and when it makes you less hungry- and that's a half hour in which you can end up eating food your body doesn't need. To make sure that doesn't happen, spend at least ten minutes eating every snack and a half hour on every meal.
Finish your meals in thirds, starting with the veggies, then eating your protein, before finally getting to the carbs. Take 5-minute pauses between each third of the meal to give your brain time to catch up to your stomach. As soon as you no longer feel hungry, stop eating- and don't confuse ""not hungry"" with ""full.""
Drink sugar water once a day. This brain hack works by suppressing production of the hunger hormone ghrelin and is a small exception to the no sugar rule above. It was discovered by the late Dr. Seth Roberts, and tested and verified by Drs. Stephen Dubner and Steven Levitt, of Freakonomics fame. Done properly, it has a dramatic appetite-suppressant effect, making your diet much easier to follow.
Once a day, during your fasting period, drink one to three glasses of ice water with a tablespoon of sugar dissolved in each glass. This sugar water should have little or no taste. You should also consume nothing else other than water for an hour before and after doing this- it's important to separate the calorie consumption from any sensation of flavor (usually easiest to do in the evening). Remember to include this sugar in your calorie count.
Make workouts short, intense, and frequent. Work out 5-6 days a week, for 25-45 minutes at a time. I recommend separating your resistance training and cardio workouts.
For weight training, you have two options. 1) 3 days of full-body weight training workouts like these, or 2) 4-5 days a week of body part split workouts like this leg workout. If you prefer to train at home, get a set of resistance bands and learn how to use them, along with a yoga mat, and maybe also a pull-up bar and pair of adjustable weight dumbbells.
For cardio, you also have two options. First, you could do 45 minutes of steady-state cardio, meaning go just as fast as you can sustain for 45 minutes. Second, you could do a much shorter but more intense hurricane sprint workout, which I explain in this article.
Better yet, you can perform a very short bodyweight workout first thing in the morning, as soon as you wake up every day. Doing this in addition to your regular workouts will give you the equivalent of one extra gym workout a week.
Burn more energy as heat. Expose yourself to cold temperatures regularly to make your body burn off more energy as heat. Drink ice water, take cold showers, keep your bedroom cool, or hold an ice pack to your upper back. Eating spicy foods also makes you produce more heat- having a slice of cold cut meat sprinkled with cayenne pepper right before a cold shower can significantly accelerate fat burning.
Get moving after every meal. After eating, either take a short walk or spend a couple minutes doing air squats and pushups. This activates the GLUT-4 receptors in your skeletal muscles so that more of the food you just ate goes to your muscles, rather than fat stores.
Sleep at least 7 hours a night. Eliminate all sources of light from your bedroom. Get to bed at least 8 hours before you expect to wake up, and spend the last hour before that relaxing with the lights turned down. When you sleep better, you'll have more energy, and thus move more and burn more calories. Your body will regulate it's appetite better. Your hormonal profile will improve, allowing you to build more muscle and burn more fat, independent of diet and exercise.
No alcohol. Alcohol contains more calories than carbohydrates, disrupts your sleep, and raises estrogen levels so that your body stores more fat. Cut it out entirely until you reach your goal weight.
Remind yourself of your mission. Take a set of ""before"" photos, print them off, and post them somewhere you'll see them every day- such as your bathroom mirror.
Surround yourself with encouraging people. Your social environment has a huge impact on your success, so make sure the people you talk to are encouraging you to stay fit. Ideally, you should have a few friends who are losing weight or have done so in the past. Note that friends who want to lose weight but have never done so may not be supportive- look for successful people who will bring you up with them.
If you don't have friends who can fill this role for you, start taking a fitness class, or talk to people online in places like the Bodybuilding.com forums or reddit. Or better yet, hire a personal trainer, or an online trainer like me.
Keep public logs of meals and workouts. Get a workout app such as JEFIT, GymHero or Bodyspace and record all of your workouts with it. Snap a photo of everything you eat before you eat it. Upload the photos to a dedicated Facebook folder or Instagram account, and post your latest workout stats to Facebook once a week.
Schedule workouts and meal prep times. Keep your workouts in your calendar. Additionally, schedule two blocks of time each week to prepare meals in bulk, so that you have healthy food in the fridge ready to eat at all times. Set reminder alerts to go off an hour before each workout and meal prep, and treat this as work- only a real emergency should stop you from getting it done.
Yes, it will. It will work as long as you actually do it.
Your body doesn't violate the laws of physics. If you ""can't"" lose weight, the reality is that you're eating more calories and/or burning fewer than you think you are.
Don't have time to work out? Do 20-minute bodyweight workouts at home, and interval sprints in the street outside your home. You can find the time. You can make the time. Many people are in amazing shape despite working 60+ hours a week, and you can be too.
Worried about hunger? There are a lot of simple tactics you can use to curb your appetite. Drink caffeine in the morning to kill your appetite until lunch, eat a lot of vegetables and protein, drink a lot of water, and make sure to have your sugar water at night. The first few days can be tough, but the hunger goes away a lot faster than you'd think.
Rapid weight loss isn't easy, but neither does it have to be a grueling exercise in self-torture. With commitment and discipline, you can get healthy and build the body you've always wanted- and you can see a difference within weeks, rather than years.
The Better Humans publication is a part of a network of personal development tools. For daily inspiration and insight, subscribe to our newsletter, and for your most important goals, find a personal coach.
",195
https://medium.com/trykecultivator/if-youre-buying-your-weed-solely-based-on-the-thc-level-you-re-doing-it-wrong-ec5422d4ddfd?source=tag_archive---------9-----------------------,"If You're Buying Your Weed Solely Based on the THC Level, You're Doing it Wrong.",The entourage effect of the terpene and cannabinoid content plays a far greater role in determining your experience,"Mike ""DJ"" Pizzo",7,"Picture this. You are preparing an elegant dinner for yourself and your significant other. You head into a liquor store looking for the perfect alcoholic beverage to complement the flavors of your meal. Do you walk up to the counter and ask the shopkeeper for the bottle with the highest alcohol percentage? Most likely not, as Everclear doesn't really have the most appetizing taste.
But with the advent of state legal marijuana, many dispensaries in the United States report that medical patients or recreational users tend to do just that when selecting their strains. ""Which one has the highest THC content,"" is a question often heard by the ears of budtenders, suggesting that anything with a lower count isn't worth their dollar. But that couldn't be further from the truth. After all, what is the reason that sometimes one drinks a beer, another time one enjoys a glass of wine, and yet another night one savors a smooth whiskey on the rocks?
The problem with selecting cannabis strains based solely on the THC content is much like stomaching the worst swill at the bar just because it has a high alcoholic percentage. By doing this, the consumer is robbing themselves of not only the rich scents and flavors of the strain, but also missing out on the beneficial effects that can be delivered through a strain's specific terpene profile.
Terpenes are essential oils that determine all of these factors and make each strain unique. Together with the cannabinoid content  compounds such THC or CBD  we get what's often referred to as an ""entourage effect,"" which ultimately has the final say in what kind of benefits you can receive from a particular strain. In fact, it has almost gotten to the point where one can zero in on what exactly they would like a strain to do for them, and by studying the effects of the individual terpenes, with a little trial-and-error, they can find the perfect strain to suit their needs.
But because of the limited scientific research done inside the United States on marijuana, many of these discoveries have been made ""in the field"" by the underground cannabis users.
""It's almost as if the rest of the scientific community is in the present, but as far as marijuana is concerned, we are sitting at the forefront of the scientific revolution,"" says Adam Laikin, Director of Marketing for Tryke companies.
Darin Carpenter, Director of Cultivation at Tryke, feels passionately about the science of terpenes. We spoke to him to get a better idea about the fascinating discoveries happening in this space.
Mike Pizzo: When did people really start looking at terpenes as a variable in the cultivation process for marijuana?
Darin Carpenter: I think generally people were looking at terpenes  whether they knew it or not  way back in the underground days, because terpenes are what give the cannabis its smell. Some people prefer very strong, pungent types of smells. Others prefer the sweet types of smells. Other growers didn't want any type of smell. But it was all based on the concentration and makeup of the terpenes.
More recently, I think, the terpenes became more of a major subject of interest, once states started mandating analytical labs to test the different potencies of various compounds. This caused people to start truly questioning the effects and combinations of terpenes and cannabinoids.
What are some good examples of how the entourage effect works or how cultivators zero in on what is going to work together in a strain?
It's synergy; multiple elements that work together to amplify an effect. When the cannabinoids are paired with terpenes and certain concentrations of them, that's what generally provides the particular effect. What science understands now is that the combination of those terpenes actually has the ultimate say in the type of effect and intensity that one experiences when consuming cannabis.
Cannabinoids do have a very specific effect, but the terpenes work to amplify that effect and move it to a more particular subset, whether it's anti-anxiety, paired with pain remediation, or helping people with insomnia, etc. CBD, for instance, is known to be a neural protectant. So that's the effect of it, helping people with seizures, etc. When people are consuming a CBD plant with one profile of terpenes, they might get a different effect than consuming a different CBD plant with a different terpene profile.
What are some of the more popular terpenes, or are they well known enough to be popular?
Well, for instance, myrcene gives off the sweet smell, more indicative of indica plants. Then you have limonene, which has more of a citrus odor, which leans heavier into sativas. There are over 200 terpenes out there, it's just the molecular composition that changes its odor and effect.
I think there is a lot more experimentation that needs to be done. Certain labs are actually increasing the number of terpenes and cannabinoids that they are actually looking for, in an effort to understand the entourage effect of the molecular profile of that particular genetic and how it effects individuals. You can break the terpenes down to a core group, but there might be some other compounds that actually are enhancing that core group for the total enhancement. There is a lot more work that needs to be done.
Probably because there are so many combinations out there, right?
Right. So, for instance, Medusa, some people just don't like Medusa. It doesn't work for them. But for me, it might be the thing that takes my pain away. It might be the thing that helps me with anxiety, or symptom I am trying to control. One strain might work for you, that may not work for me the same way. One may make you feel relaxed and comfortable, while I might get a feeling of paranoia.
Everyone is harping on, ""I have to have the highest THC concentration."" To me, it's not about that. It's the effect and finding the strain that works for you, particularly. Yes, THC gives you that psychoactive effect, but some of the better strains out there aren't higher THC. It's the combination of the flavors and the terpenes that are paired with the THC and the cannabinoids that make it so powerful. People see THC as a value proposition, like ""I am spending $25 an eighth and this one's got 30% THC. Why would I spend $35 or $45 on something that's got 15%?"" If they tried it, they might see that it's not all the THC. They actually might get a better effect, taste and experience because of the terpene/cannabinoid profile that works best for them.
Federal law has made it really difficult to study cannabis in the United States, so there is not a lot of scientific research done on this here, right?
Yeah, not a lot in the United States. Israel and England is where most of the research is being done.
That actually brings up another point, that a lot of the ""testing"" is going on in the field, where people are going ""This worked for me, this didn't,"" etc. And through word-of-mouth, those findings are kind of going ""viral"" in the cannabis community, is that right?
Yeah. A lot of the guys in Mendocino and Santa Rosa, the Emerald Triangle area, they were kind of growing and breeding what works for them and what speaks to them. That's really kind of how it's been, but nobody that I am aware of has really started a targeted breeding program to develop strains to create a library with a wide range of particular effects, quality and yield.
So do you think people that are purchasing based only on a super high THC levels are doing so out of naivety?
It's not always about the concentration of the psychoactive ingredients in the flower. It is the combination of the terpenes and cannabinoids that provide that effect. So if people are looking for something, they don't need to go for that super high-potency strain. Yes, it may work for them, but there are other options that may or may not.
For me, I hate expensive beers. If you take some super craft, micro brew with a heavy amount of hops and for me it just tastes too bitter. It doesn't quench my thirst. I prefer other types of flavors when enjoying a beer. I think when educating the patients, we need to explain that you will still get the desired effect in most cases, and because of that, you should enjoy something that has the flavor profile smell and look that speaks to you and controls their individual symptoms.
How and where can people figure out what the effects of each terpene are and which strain is right for them?
Leafly and Weedmaps have that capability. They have a menu where you can search for strains that provide whatever you are looking for and where to get them.
If you enjoyed this article, please click the  icon below. This will help to share the article with other readers on Medium.
With concierge customer service and the best product on the market, Tryke Companies and Reef Dispensaries define the standard for medical marijuana patients in each Las Vegas, Reno and Arizona. Visit us at ReefDispensaries.com.
Photography by CJ Mele
The publication of Tryke Companies and Reef Dispensaries
1.7K 
34
",196
https://medium.com/@justjudyanne/2016-is-not-killing-people-984b583a0ecc?source=tag_archive---------3-----------------------,2016 Is Not Killing People,I have a very unpopular opinion. 2016 is not to blame for many of our favorite icons dying too soon. Substance abuse is.,Judy Anne,3,"I have a very unpopular opinion. It is not 2016 that is to blame for many of our favorite icons dying and falling ill at ages that are too young. It is drug and alcohol abuse.
People exclaimed over and over as one well known artist after another died too soon, ""2016 is the worst! Please stop now!"" But one important thing quietly slipped by this year with barely a notice that is related to many of the deaths we have experienced this year. A study was released by the CDC stating that the US Life Expectancy has decreased for the first time since 1993. And no one knows why. There are a few thoughts to the cause such as obesity, opioid abuse and suicide. The important thing to pay attention to is, if you are in you 40's or 50's, you will probably not live as long as your parents or grandparents did and it may be due to a life threatening addiction.
Let's look at the two most recent health events suffered by famous people, George Michael and Carrie Fisher. George Michael died of heart failure at 53 and Carrie Fisher suffered a massive heart attack at 60. According to those close to him, George Michael, an enthusiastic user of cannabis for decades, moved on to dabble with a deadly selection of new narcotics in the past few years (some will remember his 2008 arrest for smoking crack cocaine). The singer, who dropped out of public view after releasing an album last spring, was said to be smoking crack cocaine again recently. 'He spends all day in a dark room with a pipe,' a friend said 'It's his addictive personality  everyone is scared to death about him.' Carrie Fisher is so well known for her drug & alcohol abuse she is often referred to as ""noted former cocaine user Carrie Fisher"" in articles about others who have done the drug.
Here is the thing, cocaine is the perfect heart attack drug, even if you only use it a few times a year. Cocaine users have higher blood pressure, stiffer arteries and thicker heart muscle walls  all which lead to heart attacks. 2016 did not cause mysterious illnesses to befall our favorite artists. A disease from which I suffer and is as deadly as any cancer is killing my generation at an alarming rate at no one wants to admit it.
Prince was larger than life, but died from a very human disease that is surrounded by stigma and almost impossible to get treatment for. I wonder if he would still be alive if we were able to talk about drug abuse as easily as we do about diabetes?
It is time to stop being superstitious and pretending a calendar year is killing people. Alcoholism and drug abuse are killing people. Next year it will kill more. Next year it might be your favorite recording artist, or maybe your best friend, or your brother, or maybe even you.
",197
https://kantantechnology.com/how-i-added-five-more-ultra-productive-hours-to-my-day-and-how-you-can-too-cf2749b6fd1e?source=tag_archive---------7-----------------------,How I Learned to Sleep Only Three Hours Per Night (and Why You Should Too),I am here to challenge everything you thought you knew about sleep.,Cody Monson,6,"I've always needed a lot of sleep. A week before doing this experiment I tried buying myself more time by sleeping just six hours per night.
It wasn't sustainable. In fact, I was so groggy and sleep deprived it had the unintended result of lower overall productivity.
So, naturally I was skeptical when I heard you could sleep just three hours per night with no adverse effects.
But I had nothing to lose. In fact, IF THIS WORKED, I could gain the extra time I've been seeking for years. So I HAD to try it out.
Here are my experiences and lessons from the most incredible sleep experiment I've ever tried.
If you've ever used a sleep tracker you'll notice sleep is split into two categories  light sleep and deep sleep.
During light sleep you are settling in, moving fairly frequently, muscles relaxing, body cooling, and lightly dreaming. This represents 60% of sleep.
Deep sleep contains two phases  deep and Rapid Eye Movement (REM).
During deep sleep you are hardly moving, you aren't dreaming, and your body is repairing cells, consolidating memories, and building up muscle tissue.
During REM sleep your brain is active again and you are dreaming intensely. The science isn't clear on what exactly REM sleep does, but it's suggested that it plays an important role in memory storage and mood balancing. Things you wouldn't want to mess with.
Combined, deep and REM account for 40% of sleep.
So how does one sleep only three hours at night and still get enough deep sleep?
Polyphasic Sleep (as opposed to Monophasic sleep, or sleeping once a day) is the idea of breaking your sleep into smaller chunks, or naps, to maximize your waking hours.
The hope is to drop light sleep and achieve deep sleep each time you close your eyes.
Polyphasic Sleep has many forms.
The most extreme one is the Uberman method. It consists of six 20-minute naps for two total hours of sleep a day.
A less extreme form and the one I've chosen for myself is dubbed the Everyman method. It consists of one ""core"" sleep lasting between 3-4 hours, and three 20-minute naps throughout the day for a total of four to five hours.
I have chosen the Everyman method as it provides more flexibility. Also, to be honest the idea of never sleeping for more than 20 minutes intimidates me. Perhaps that'll be my next experiment.
So what are the benefits? Why should you deprive yourself of a glorious eight hours of sleep?
Think of the last time you had a five hour block of fully-focused, uninterrupted time. Can you imagine the possibilities if you had that every day?
No one else is waking up at 3am. Which means no one is emailing, Slacking, or texting at 3am.
I use my daily uninterrupted time to (finally) take coding seriously, start this blog, do more research, and enhance my writing abilities.
I discovered that doing everyday things like reading the news, watching Youtube, or playing games are not worth sleeping less over.
Thus, many temptations to let my mind wander are naturally eliminated, making these morning hours ultra-productive.
Not more, but better.
My first few days were rough. I don't often nap during the day, and it was difficult for me to learn to unwind.
But this issue didn't persist for long because missing a single nap feels like missing a whole night of sleep. Enough missed and poor naps and I was shocked into a proper schedule.
It's amazing how great I feel if I get a good core and three solid naps. I sleep from 10pm-2am, 540am-6am, 1140am-12pm, and 440pm-5pm. You can move those times to fit your work and social life.
I no longer get post-lunch, food-induced comas. Sleeping right before lunch eliminates this issue. Trading 20 minutes before lunch for 2 hours of lost productivity after lunch is a killer deal.
Although I get really tired four times a day (it sets in about 30 minutes before core and 5 minutes before naps), I am attentive and sharp for many more hours than before.
Topping up four times a day rather than one has a way of keeping you fresh.
I have many interests. I enjoy reading, staying current on events in politics and tech, designing and building things, watching Netflix and VidAngel, executing on ideas, writing, outdoor recreation, listening to music, learning new skills, videography, and photography, to name but a few.
But as much as I enjoy these hobbies, I refuse to compromise family time or work performance for them.
Now I spend more time with family, get more work done, and explore more of my interests.
I like to experiment. I love discovering new tools and methodologies for improving my life. This blog is going to be about the lessons I learn through my research and experimentation.
I constantly seek to disrupt my habits and patterns in hopes of elevating or shifting my paradigms. It's the best way to take personal growth into my own hands.
Polyphasic sleep is one of those game changers.
In this case, the big WHAT IF was so appealing that I started the night I heard about it. Sure the initial sleep depravation was rough, but the productivity gains were immediate.
Regardless of how long I continue the Everyman method of sleep, I know I am better off for having done it.
If you decide to try Polyphasic sleep in any of its forms, please, let me know. And if you have lessons of your own, please share those too!
Enjoy your extra time :)
Edit Jan 2018  Updated article and title to 3 hours per night. I started with four but moved to 3 after discovering it was more sustainable for me.
*I am not a scientist. As far as I know, there haven't been any controlled studies on polyphasic sleep and the long-term effects it may have. Consult with a doctor if you have questions on health concerns. Be safe!
Paradigm-busting ideas and relentless experimentation.
1.5K 
26
1.5K claps
1.5K 
",198
https://itsyourturnblog.com/i-was-without-food-for-11-days-here-is-what-happened-7560e8625dbf?source=tag_archive---------0-----------------------,"I fasted for 11 days, here is what happened.",I had been feeling a desire to fast for months but resisted out of fear. The fear of fasting itself was crippling. I found this state of...,EagleWorks,11,"I had been feeling a desire to fast for months but resisted out of fear. The fear of fasting itself was crippling. I found this state of mind rather interesting, as fasting wasn't foreign to me, but it's been years since I had fasted. In my teens and early 20's, fasting was regular. It wasn't uncommon for me to do 1 to 3 days of dry fasting (no food, no water) regularly. However, I entered a season of life that became so busy so quickly and slamming the brakes was often not in my jurisdiction. Eventually, after months of procrastinating, I started my fast, not without researching how to make it less deadly for me as I had intended to do a prolonged fast. I was going to aim for seven days, at least. Perhaps 14 days? It was going to be more than what I had ever done before three days of fasting. I think I got scared of what it would feel like having not fasted for over four years. I feared headaches, the weakness that would force me to break the fast prematurely. I feared feeling so weak and brain dull that I won't be able to open my eyes to read, which was exaggerated. I didn't just want to lie in bed all day.
I decided to do a water fast. I was going to drink a gallon of water daily. It was a lot, but I was determined to deceive my digestive juices by diluting it excessively, and it worked. I also chose something I had never done before; ISOLATION. In this ever increasingly distracting world, I opted to shut myself in and exclude the outside world. I didn't have the privilege of going to a remote place with no wifi. I did it locally; in my home, in my room. With my phone switched off in a separate part of the house and my email set to vacation mode, I embarked on my water fast with no specific goal date in mind other than to be over three days. The goals for fasting and isolation was to quieten my mind, connect to a deeper part of me, increase my spiritual awareness as I had some important decisions to make that I couldn't take lightly. The time I chose would have set me up for failure if I wasn't resolute as it was the festive season of the year. I had been invited to parties, all of which I had to turn down to focus on.
Documented below is my experience and what you should expect when embarking on such. Your experience might be different. Here is my encounter on the ten days without food. I broke the fast on the 11th day due to the ""female visitor"" that came knocking, other than that; I could have kept going, I believe.
No headaches: I felt day 1-2 would be hardest. In the past, day two has been hardest for me. I was diligent in drinking lots of water, and it did help. I was surprised not to be hungry and be without headaches on day 2. Overall, I did not have any blinding headaches that I had initially feared.
Flawless skin: I couldn't stop touching my face. No face ritual. I was in isolation and didn't need to use any products. I noticed soft cheeks and flawless skin somewhere around day 5. My face was so smooth like a baby's butt. I felt like I had regenerated skin. I also noticed a glow on my forehead. I was lighter in complexion. I compulsively touched my face all day for the reason that it was outrageously soft.
Weight loss: I wasn't fasting to lose weight, but it was an unintended consequence of my chosen state. I lost 12 lbs in 7 days and a total of 16 pounds in 10 days. It was the most weight I had lost in a short time. I also ""felt"" my gut shrinking. After a few days of no food, I think my stomach shut down as it progressively shrunk daily and stopped rumbling.
Bodily healing: For the past two or so years, I had observed numbness and tingling in my right hand. This numbness got worse to create cramps in my hands so painful that it would wake me up from sleep. As a Physician who knew the anatomy, physiology, and pathogenesis of the disease, this was scary. I diagnosed myself with diabetes, though I had no other signs and no family history. But wait! It was unilateral, so I figured it's not systemic but structural. Considering that it was my right hand, and I am a writer, I diagnosed myself to have Carpal Tunnel Syndrome. I did all the exercises to determine that this was the case physically, all negative! I observed and couldn't decide what triggered it. I, however, knew that writing or typing made it worse. The condition got worse to the point where I could not hold my pen for more than 30 secs before it got numbed so bad, it hurt. And yes, I would not see a doctor because you know...I don't like seeing doctors. On days 5, I had been journaling for hours, and it suddenly dawned on me that my right hand wasn't numb. It was the first time in over two years to experience this. I got healed! I haven't had numbness, tingling, or pain in my right hand ever since. It was my greatest reward from the fast.
Immense energy: I noticed a short burst of energy from day 3. By day 6, my energy level had peaked so much that I felt like dancing and running. I suddenly felt powerful like a lion with a glorious mane on a throne.
Less sleep: By day 4, I began to sleep 4 hrs or less naturally. There were days I slept 2 hrs in 24 hrs just naturally. I was surprised at how little sleep I needed. The body is ridiculously brilliant. I realized then that the human body could function on 4 hrs of sleep or less if given the right things and not made to expend energy on the wrong things, like digesting food all the time.
Cold extremities: I took notice of this on day 3. Trying not to include medical jargon here, but it took me a while to understand what was happening to me. I initially thought my environment was cold. The heater was running as this took place in the winter month. I had my nordic winter socks on. I covered myself in a thick duvet and full dresses, yet my hands and feet would not warm up. I got progressively cold. By day 5, it finally hit me! The physiology of it was that since I was not eating and actively utilizing the energy cycle in the body, my body was most likely using stored energy and fat to carry out vital functions in my body. It pulled blood from non-vital parts like my extremities to focus on critical organs like the brain, heart, kidney, etc. My body was hibernating and needed to survive. I was COLD for the rest of the fast. It was no fun.
Painful memories: On day 1, my mind wandered a lot. It was like a drama scene; it kept debating, analyzing, and questioning. It won't shut up. With subsequent days, it got quieter. On day 9, a memory of injustice previously done to me stayed with me. Its presence was strong. It felt like bile on my tongue. I tried to shake it off with no luck and realized it was brought to my attention for a reason. I practiced letting go till it dispersed, and all was at peace.
Acute mind: On day 2, someone had given me a password for something a while ago that I didn't use and needed to document it. I remembered it so accurately like I had a photographic memory. On day 9, I had slept a little over 3 hours and woke up feeling so alert. I journaled, ""I am wide awake and have energy. Nine days without food! This is neat! My mind is sharp!"".
Constipation? My last meal was Chinese takeout. I had no solid poop at all after that. I was concerned the food residue was stuck in my gut. I had oily liquid come out in its place. I wondered, ""I am not drinking oil, why am I pooping fatty pale stool?"" I only knew this based on my knowledge of medicine. This also plagued me as I was worried that something might be wrong with my liver. ""Do I have fatty liver disease? But I have no risk factors? What conditions cause oily liquid from the gut?"" I racked my brain. I felt I should have solid poop since I ate a Chinese takeout meal seven days ago. It wasn't healthy, and I hated the thought that it was perhaps still in my gut. I thought I was constipated and decided to drink magnesium stuff to purge my system. I did, and my stomach was CLEAN. I popped water and more fatty liquid material. No solid mass was ejected. It was all in my mind. The reason for the oily liquid was that my body was breaking down stored fat to survive. My gut was most likely laden with fat cells swimming around, trying to promote and enforce energy so I could stay alive. And no, I did not have fatty liver disease  more on this below.
Back pain: This was VERY, VERY strange. I noticed pain on the midback on day 6; It wasn't superficial. The pain lingered for hours. It felt like it was DEEP in my muscle or bone. The pain point was definite. On day 8, the back pain moved just below my right scapula. It was deep. The intensity of the pain increased to the point that I was gasping for breath. I was in isolation; therefore, there was no one to ask. I had no knowledge of anyone who had done what I was doing. My medical experience couldn't give me answers. Fasting isn't in the curriculum in medical school. Out of desperation and confusion about what was going on in my body, I launched into the online world for answers. Some resources I found had spiritual explanations to it pointing that if one has been betrayed (back-stabbed) in the past, it was a sign of spiritual healing. Some have physical reasons for it, indicating that during a fast since the body is not spending the time to digest food, the cells have been freed up to do more healing work.
Therefore, cells would go around the body, healing parts of it that need healing, especially if it started a process of healing in a body part that wasn't finished. I remembered. A year ago, I was rear-ended in a 4-car accident on the highway that jerked me forward and back. The pain point under my scapula was a place of trauma. I had physical therapy and massages and had been fine, but perhaps there was more healing for my body to do? I did not verify any of this information, but both made sense to me. However, the pain did not make sense; it was severe and stopped on day 9. 3 days and healing was complete.
Better vision: When I woke up, I not only felt instantly alert, but my vision was super clear. Though I wear glasses to correct a structural defect, I could feel my eyes at the back of my head. No joke! The light in my room felt brighter and clearer. Everything looked picture perfect. It almost felt psychic. I saw what I needed to see, not just what I wanted to see.
Deep knowledge: I was in constant meditation as I fasted; this gave me a higher sense of spirituality. Picture, places, visions kept flashing before me. There was a sense of deep knowing that stayed with me. On day 5, I was suddenly gifted with a clear strategy, high inclination and definite sense of belonging for my life. I suddenly knew what to do and how. I had a higher level of consciousness and profound sense of identity; who I am and whose I was. It felt like a map was laid clearly before me. Everything made sense.
Change of heart: I noticed my heart posture soften. I felt love, compassion, empathy, gained a better understanding of previous life events. Things that mattered were brought to the front of your mind. A sense of humility washed over me. Certain people were highlighted in mind for different things, and I journaled them all; someone to call, talk to, intuition into happenings in their life they might be struggling with, clarify an item with, feeling their emotions. In these cases, when I contacted them after I got out of isolation, it was 100% accurate. I also noticed I had a great appreciation for myself. There was a time of intense delight and revelations about myself that made me bubble from within.
Isolation: I did not miss answering emails or social networks. The world continued without me. For busy folks, if you choose to unplug, you can. The world won't fall apart if you aren't there. It would continue. I would admonish you to take the time you need. It was hard to be isolated. I did check my phone occasionally...but if a text came in, I did not respond. It was also hard because I did it at home and those who knew I was home determined to get me out. I had to stand my grounds and enforce my position.
Test: Because of all the physiological changes that I experienced during the fast, I was incredibly curious about my body. I wanted to know how it did on a cellular level. I decided to order a comprehensive blood test online. Gratefully, I didn't need a doctor's order to get my blood test nor a doctor to interpret it for me. I ordered a complete blood count, basic metabolic profile, full hormone profile, cholesterol profile, liver profile (you bet! I needed to know if something was wrong with my liver). I ordered every test that could give me an insight into how every system of my body was doing on a cellular level. It cost me almost $500 out of pocket from a local lab around me.
It was a lot of blood...I joked with the phlebotomist after he took blood for the 9th vial (there were 13 vials before me), ""You just took a pint of blood from me. I am afraid I would pass out at this rate as I just lost a pint and been fasting"". It was the most blood I have had to give for a test, and it made sense as I had self-ordered loads of test. Results were out after five days, and all was perfect, but my BUN, urea, and creatinine values were low. These made sense to me as these were conditions pervasive in a body in starvation mode. Since I had been fasting, my body had been breaking down protein with no replenishments, hence the skewed results. And yes, my liver was perfect!
Looking back, I did not think the blood test gave me a good sense of how my body did since there were no pre-fast lab results to compare the post-fast numbers to. It would have been better to have a pre- and post-fast test.
I think fasting has excellent benefits to the body, soul, and spirit. You don't need to do prolonged fasting as I did. An intermittent one is something worth trying. Tell me, what has your fasting experience been like?
***Disclaimer: Do not take this as medical advice. I do various experiments on myself and just happened to be a Physician who is very curious and deeply introspective, and can speak to the clinical side based on my own body. However, I would suggest that if you have medical concerns, talk with your doctor who is familiar with your medical history first.***
Seth Godin's altMBA and Akimbo workshops alumni unofficial...
5.9K 
61
5.9K claps
5.9K 
",199
https://medium.com/@tomaspueyo/coronavirus-the-hammer-and-the-dance-be9337092b56?source=tag_archive---------1-----------------------,Coronavirus: The Hammer and the Dance,"What the Next 18 Months Can Look Like, if Leaders Buy Us Time",Tomas Pueyo,29,"This article follows Coronavirus: Why You Must Act Now, with over 40 million views and 30 translations. Translations into 37 languages available at the bottom. Running list of endorsements here. Over 10 million views so far.
Following articles:
To receive the next articles, sign up here.
Summary of the article: Strong coronavirus measures today should only last a few weeks, there shouldn't be a big peak of infections afterwards, and it can all be done for a reasonable cost to society, saving millions of lives along the way. If we don't take these measures, tens of millions will be infected, many will die, along with anybody else that requires intensive care, because the healthcare system will have collapsed.
Within a week, countries around the world have gone from: ""This coronavirus thing is not a big deal"" to declaring the state of emergency. Yet many countries are still not doing much. Why?
Every country is asking the same question: How should we respond? The answer is not obvious to them.
Some countries, like France, Spain or Philippines, have since ordered heavy lockdowns. Others, like the US, UK, or Switzerland, have dragged their feet, hesitantly venturing into social distancing measures.
Here's what we're going to cover today, again with lots of charts, data and models with plenty of sources:
When you're done reading the article, this is what you'll take away:
Our healthcare system is already collapsing.Countries have two options: either they fight it hard now, or they will suffer a massive epidemic.If they choose the epidemic, hundreds of thousands will die. In some countries, millions.And that might not even eliminate further waves of infections.If we fight hard now, we will curb the deaths.We will relieve our healthcare system.We will prepare better.We will learn.The world has never learned as fast about anything, ever.And we need it, because we know so little about this virus.All of this will achieve something critical: Buy Us Time.
If we choose to fight hard, the fight will be sudden, then gradual.We will be locked in for weeks, not months.Then, we will get more and more freedoms back.It might not be back to normal immediately. But it will be close, and eventually back to normal.And we can do all that while considering the rest of the economy too.
Ok, let's do this.
Last week, I showed this curve:
It showed coronavirus cases across the world outside of China. We could only discern Italy, Iran and South Korea. So I had to zoom in on the bottom right corner to see the emerging countries. My entire point is that they would soon be joining these 3 cases.
Let's see what has happened since.
As predicted, the number of cases has exploded in dozens of countries. Here, I was forced to show only countries with over 1,000 cases. A few things to note:
Do you notice something weird about this list of countries? Outside of China and Iran, which have suffered massive, undeniable outbreaks, and Brazil and Malaysia, every single country in this list is among the wealthiest in the world.
Do you think this virus targets rich countries? Or is it more likely that rich countries are better able to identify the virus?
It's unlikely that poorer countries aren't touched. Warm and humid weather probably helps, but doesn't prevent an outbreak by itself  otherwise Singapore, Malaysia or Brazil wouldn't be suffering outbreaks.
The most likely interpretations are that the coronavirus either took longer to reach these countries because they're less connected, or it's already there but these countries haven't been able to invest enough on testing to know.
Either way, if this is true, it means that most countries won't escape the coronavirus. It's a matter of time before they see outbreaks and need to take measures.
What measures can different countries take?
Since the article last week, the conversation has changed and many countries have taken measures. Here are some of the most illustrative examples:
In one extreme, we have Spain and France. This is the timeline of measures for Spain:
On Thursday, 3/12, the President dismissed suggestions that the Spanish authorities had been underestimating the health threat.On Friday, they declared the State of Emergency.On Saturday, measures were taken:
On Monday, land borders were shut.
Some people see this as a great list of measures. Others put their hands up in the air and cry of despair. This difference is what this article will try to reconcile.
France's timeline of measures is similar, except they took more time to apply them, and they are more aggressive now. For example, rent, taxes and utilities are suspended for small businesses.
The US and UK, like countries such as Switzerland, have dragged their feet in implementing measures. Here's the timeline for the US:
Lots of states and cities are taking the initiative and mandating much stricter measures.
The UK has seen a similar set of measures: lots of recommendations, but very few mandates.
These two groups of countries illustrate the two extreme approaches to fight the coronavirus: mitigation and suppression. Let's understand what they mean.
Before we do that, let's see what doing nothing would entail for a country like the US:
If we do nothing: Everybody gets infected, the healthcare system gets overwhelmed, the mortality explodes, and ~10 million people die (blue bars). For the back-of-the-envelope numbers: if ~75% of Americans get infected and 4% die, that's 10 million deaths, or around 25 times the number of US deaths in World War II.
You might wonder: ""That sounds like a lot. I've heard much less than that!""
So what's the catch? With all these numbers, it's easy to get confused. But there's only two numbers that matter: What share of people will catch the virus and fall sick, and what share of them will die. If only 25% are sick (because the others have the virus but don't have symptoms so aren't counted as cases), and the fatality rate is 0.6% instead of 4%, you end up with 500k deaths in the US.
If we don't do anything, the number of deaths from the coronavirus will probably land between these two numbers. The chasm between these extremes is mostly driven by the fatality rate, so understanding it better is crucial. What really causes the coronavirus deaths?
This is the same graph as before, but now looking at hospitalized people instead of infected and dead:
The light blue area is the number of people who would need to go to the hospital, and the darker blue represents those who need to go to the intensive care unit (ICU). You can see that number would peak at above 3 million.
Now compare that to the number of ICU beds we have in the US (50k today, we could double that repurposing other space). That's the red dotted line.
No, that's not an error.
That red dotted line is the capacity we have of ICU beds. Everyone above that line would be in critical condition but wouldn't be able to access the care they need, and would likely die.
Instead of ICU beds you can also look at ventilators, but the result is broadly the same, since there are fewer than 100k ventilators in the US.
This is why people died in droves in Hubei and are now dying in droves in Italy and Iran. The Hubei fatality rate ended up better than it could have been because they built 2 hospitals nearly overnight. Italy and Iran can't do the same; few, if any, other countries can. We'll see what ends up happening there.
So why is the fatality rate close to 4%?
If 5% of your cases require intensive care and you can't provide it, most of those people die. As simple as that.
Additionally, recent data suggests that US cases are more severe than in China.
I wish that was all, but it isn't.
These numbers only show people dying from coronavirus. But what happens if all your healthcare system is collapsed by coronavirus patients? Others also die from other ailments.
What happens if you have a heart attack but the ambulance takes 50 minutes to come instead of 8 (too many coronavirus cases) and once you arrive, there's no ICU and no doctor available? You die.
There are 4 million admissions to the ICU in the US every year, and 500k (~13%) of them die. Without ICU beds, that share would likely go much closer to 80%. Even if only 50% died, in a year-long epidemic you go from 500k deaths a year to 2M, so you're adding 1.5M deaths, just with collateral damage.
If the coronavirus is left to spread, the US healthcare system will collapse, and the deaths will be in the millions, maybe more than 10 million.
The same thinking is true for most countries. The number of ICU beds and ventilators and healthcare workers are usually similar to the US or lower in most countries. Unbridled coronavirus means healthcare system collapse, and that means mass death.
Unbridled coronavirus means healthcare systems collapse, and that means mass death.
By now, I hope it's pretty clear we should act. The two options that we have are mitigation and suppression. Both of them propose to ""flatten the curve"", but they go about it very differently.
Mitigation goes like this: ""It's impossible to prevent the coronavirus now, so let's just have it run its course, while trying to reduce the peak of infections. Let's just flatten the curve a little bit to make it more manageable for the healthcare system.""
This chart appears in a very important paper published over the weekend from the Imperial College London. Apparently, it pushed the UK and US governments to change course.
It's a very similar graph as the previous one. Not the same, but conceptually equivalent. Here, the ""Do Nothing"" situation is the black curve. Each one of the other curves are what would happen if we implemented tougher and tougher social distancing measures. The blue one shows the toughest social distancing measures: isolating infected people, quarantining people who might be infected, and secluding old people. This blue line is broadly the current UK coronavirus strategy, although for now they're just suggesting it, not mandating it.
Here, again, the red line is the capacity for ICUs, this time in the UK. Again, that line is very close to the bottom. All that area of the curve on top of that red line represents coronavirus patients who would mostly die because of the lack of ICU resources.
Not only that, but by flattening the curve, the ICUs will collapse for months, increasing collateral damage.
You should be shocked. When you hear: ""We're going to do some mitigation"" what they're really saying is: ""We will knowingly overwhelm the healthcare system, driving the fatality rate up by a factor of 10x at least.""
You would imagine this is bad enough. But we're not done yet. Because one of the key assumptions of this strategy is what's called ""Herd Immunity"".
The idea is that all the people who are infected and then recover are now immune to the virus. This is at the core of this strategy: ""Look, I know it's going to be hard for some time, but once we're done and a few million people die, the rest of us will be immune to it, so this virus will stop spreading and we'll say goodbye to the coronavirus. Better do it at once and be done with it, because our alternative is to do social distancing for up to a year and risk having this peak happen later anyways.""
Except this assumes one thing: the virus doesn't change too much. If it doesn't change much, then lots of people do get immunity, and at some point the epidemic dies down
How likely is this virus to mutate? It seems it already has.
This graph represents the different mutations of the virus. You can see that the initial strains started in purple in China and then spread. Each time you see a branching on the left graph, that is a mutation leading to a slightly different variant of the virus.
This should not be surprising: RNA-based viruses like the coronavirus or the flu tend to mutate around 100 times faster than DNA-based onesalthough the coronavirus mutates more slowly than influenza viruses.
Not only that, but the best way for this virus to mutate is to have millions of opportunities to do so, which is exactly what a mitigation strategy would provide: hundreds of millions of people infected.
That's why you have to get a flu shot every year. Because there are so many flu strains, with new ones always evolving, the flu shot can never protect against all strains.
Put in another way: the mitigation strategy not only assumes millions of deaths for a country like the US or the UK. It also gambles on the fact that the virus won't mutate too much  which we know it does. And it will give it the opportunity to mutate. So once we're done with a few million deaths, we could be ready for a few million more  every year. This corona virus could become a recurring fact of life, like the flu, but many times deadlier.
The best way for this virus to mutate is to have millions of opportunities to do so, which is exactly what a mitigation strategy would provide.
So if neither doing nothing nor mitigation will work, what's the alternative? It's called suppression.
The Mitigation Strategy doesn't try to contain the epidemic, just flatten the curve a bit. Meanwhile, the Suppression Strategy tries to apply heavy measures to quickly get the epidemic under control. Specifically:
What does that look like?
Under a suppression strategy, after the first wave is done, the death toll is in the thousands, and not in the millions.
Why? Because not only do we cut the exponential growth of cases, we also cut the fatality rate since the healthcare system is not completely overwhelmed. Here, I used a fatality rate of 0.9%, around what we're seeing in South Korea today, which has been most effective at following Suppression Strategy.
Said like this, it sounds like a no-brainer. Everybody should follow the Suppression Strategy.
So why do some governments hesitate?
They fear three things:
Here is how the Imperial College team modeled suppressions. The green and yellow lines are different scenarios of Suppression. You can see that doesn't look good: We still get huge peaks, so why bother?
We'll get to these questions in a moment, but there's something more important before.
This is completely missing the point.
Presented like these, the two options of Mitigation and Suppression, side by side, don't look very appealing. Either a lot of people die soon and we don't hurt the economy today, or we hurt the economy today, just to postpone the deaths.
This ignores the value of time.
In our previous post, we explained the value of time in saving lives. Every day, every hour we waited to take measures, this exponential threat continued spreading. We saw how a single day could reduce the total cases by 40% and the death toll by even more.
But time is even more valuable than that.
We're about to face the biggest wave of pressure on the healthcare system ever seen in history. We are completely unprepared, facing an enemy we don't know. That is not a good position for war.
What if you were about to face your worst enemy, of which you knew very little, and you had two options: Either you run towards it, or you escape to buy yourself a bit of time to prepare. Which one would you choose?
This is what we need to do today. The world has awakened. Every single day we delay the coronavirus, we can get better prepared. The next sections detail what that time would buy us:
With effective suppression, the number of true cases would plummet overnight, as we saw in Hubei last week.
As of today, there are 0 daily new cases of coronavirus in the entire 60 million-big region of Hubei.
The diagnostics would keep going up for a couple of weeks, but then they would start going down. With fewer cases, the fatality rate starts dropping too. And the collateral damage is also reduced: fewer people would die from non-coronavirus-related causes because the healthcare system is simply overwhelmed.
Suppression would get us:
Right now, the UK and the US have no idea about their true cases. We don't know how many there are. We just know the official number is not right, and the true one is in the tens of thousands of cases. This has happened because we're not testing, and we're not tracing.
The measures from this section (testing and tracing) single-handedly curbed the growth of the coronavirus in South Korea and got the epidemic under control, without a strong imposition of social distancing measures.
The US (and presumably the UK) are about to go to war without armor.
We have masks for just two weeks, few personal protective equipments (""PPE""), not enough ventilators, not enough ICU beds, not enough ECMOs (blood oxygenation machines)... This is why the fatality rate would be so high in a mitigation strategy.
But if we buy ourselves some time, we can turn this around:
Put in another way: we don't need years to get our armor, we need weeks. Let's do everything we can to get our production humming now. Countries are mobilized. People are being inventive, such as using 3D printing for ventilator parts. We can do it. We just need more time. Would you wait a few weeks to get yourself some armor before facing a mortal enemy?
This is not the only capacity we need. We will need health workers as soon as possible. Where will we get them? We need to train people to assist nurses, and we need to get medical workers out of retirement. Many countries have already started, but this takes time. We can do this in a few weeks, but not if everything collapses.
The public is scared. The coronavirus is new. There's so much we don't know how to do yet! People haven't learned to stop hand-shaking. They still hug. They don't open doors with their elbow. They don't wash their hands after touching a door knob. They don't disinfect tables before sitting.
Once we have enough masks, we can use them outside of the healthcare system too. Right now, it's better to keep them for healthcare workers. But if they weren't scarce, people should wear them in their daily lives, making it less likely that they infect other people when sick, and with proper training also reducing the likelihood that the wearers get infected. (In the meantime, wearing something is better than nothing.)
All of these are pretty cheap ways to reduce the transmission rate. The less this virus propagates, the fewer measures we'll need in the future to contain it. But we need time to educate people on all these measures and equip them.
We know very very little about the virus. But every week, hundreds of new papers are coming.
The world is finally united against a common enemy. Researchers around the globe are mobilizing to understand this virus better.
How does the virus spread?How can contagion be slowed down?What is the share of asymptomatic carriers?Are they contagious? How much?What are good treatments?How long does it survive?On what surfaces?How do different social distancing measures impact the transmission rate?What's their cost?What are tracing best practices?How reliable are our tests?
Clear answers to these questions will help make our response as targeted as possible while minimizing collateral economic and social damage. And they will come in weeks, not years.
Not only that, but what if we found a treatment in the next few weeks? Any day we buy gets us closer to that. Right now, there are already several candidates, such as Favipiravir, Chloroquine, or Chloroquine combined with Azithromycin. What if it turned out that in two months we discovered a treatment for the coronavirus? How stupid would we look if we already had millions of deaths following a mitigation strategy?
All of the factors above can help us save millions of lives. That should be enough. Unfortunately, politicians can't only think about the lives of the infected. They must think about all the population, and heavy social distancing measures have an impact on others.
Right now we have no idea how different social distancing measures reduce transmission. We also have no clue what their economic and social costs are.
Isn't it a bit difficult to decide what measures we need for the long term if we don't know their cost or benefit?
A few weeks would give us enough time to start studying them, understand them, prioritize them, and decide which ones to follow.
Fewer cases, more understanding of the problem, building up assets, understanding the virus, understanding the cost-benefit of different measures, educating the public... These are some core tools to fight the virus, and we just need a few weeks to develop many of them. Wouldn't it be dumb to commit to a strategy that throws us instead, unprepared, into the jaws of our enemy?
Now we know that the Mitigation Strategy is probably a terrible choice, and that the Suppression Strategy has a massive short-term advantage.
But people have rightful concerns about this strategy:
Here, we're going to look at what a true Suppression Strategy would look like. We can call it the Hammer and the Dance.
First, you act quickly and aggressively. For all the reasons we mentioned above, given the value of time, we want to quench this thing as soon as possible.
One of the most important questions is: How long will this last?
The fear that everybody has is that we will be locked inside our homes for months at a time, with the ensuing economic disaster and mental breakdowns. This idea was unfortunately entertained in the famous Imperial College paper:
Do you remember this chart? The light blue area that goes from end of March to end of August is the period that the paper recommends as the Hammer, the initial suppression that includes heavy social distancing.
If you're a politician and you see that one option is to let hundreds of thousands or millions of people die with a mitigation strategy and the other is to stop the economy for five months before going through the same peak of cases and deaths, these don't sound like compelling options.
But this doesn't need to be so. This paper, driving policy today, has been brutally criticized for core flaws: They ignore contact tracing (at the core of policies in South Korea, China or Singapore among others) or travel restrictions (critical in China), ignore the impact of big crowds...
The time needed for the Hammer is weeks, not months.
This graph shows the new cases in the entire Hubei region (60 million people) every day since 1/23. Within 2 weeks, the country was starting to get back to work. Within ~5 weeks it was completely under control. And within 7 weeks the new diagnostics was just a trickle. Let's remember this was the worst region in China.
Remember again that these are the orange bars. The grey bars, the true cases, had plummeted much earlier (see Chart 9).
The measures they took were pretty similar to the ones taken in Italy, Spain or France: isolations, quarantines, people had to stay at home unless there was an emergency or had to buy food, contact tracing, testing, more hospital beds, travel bans...
Details matter, however.
China's measures were stronger. For example, people were limited to one person per household allowed to leave home every three days to buy food. Also, their enforcement was severe. It is likely that this severity stopped the epidemic faster.
In Italy, France and Spain, measures were not as drastic, and their implementation is not as tough. People still walk on the streets, many without masks. This is likely to result in a slower Hammer: more time to fully control the epidemic.
Some people interpret this as ""Democracies will never be able to replicate this reduction in cases"". That's wrong.
For several weeks, South Korea had the worst epidemic outside of China. Now, it's largely under control. And they did it without asking people to stay home. They achieved it mostly with very aggressive testing, contact tracing, and enforced quarantines and isolations.
The following table gives a good sense of what measures different countries have followed, and how that has impacted them (this is a work-in-progress. Feedback welcome.)
This shows how countries who were prepared, with stronger epidemiological authority, education on hygiene and social distancing, and early detection and isolation, didn't have to pay with heavier measures afterwards.
Conversely, countries like Italy, Spain or France weren't doing these well, and had to then apply the Hammer with the hard measures at the bottom to catch up.
The lack of measures in the US and UK is in stark contrast, especially in the US. These countries are still not doing what allowed Singapore, South Korea or Taiwan to control the virus, despite their outbreaks growing exponentially. But it's a matter of time. Either they have a massive epidemic, or they realize late their mistake, and have to overcompensate with a heavier Hammer. There is no escape from this.
But it's doable. If an outbreak like South Korea's can be controlled in weeks and without mandated social distancing, Western countries, which are already applying a heavy Hammer with strict social distancing measures, can definitely control the outbreak within weeks. It's a matter of discipline, execution, and how much the population abides by the rules.
Once the Hammer is in place and the outbreak is controlled, the second phase begins: the Dance.
If you hammer the coronavirus, within a few weeks you've controlled it and you're in much better shape to address it. Now comes the longer-term effort to keep this virus contained until there's a vaccine.
This is probably the single biggest, most important mistake people make when thinking about this stage: they think it will keep them home for months. This is not the case at all. In fact, it is likely that our lives will go back to close to normal.
How come South Korea, Singapore, Taiwan and Japan have had cases for a long time, in the case of South Korea thousands of them, and yet they're not locked down home?
www.bbc.com
In this video, the South Korea Foreign Minister explains how her country did it. It was pretty simple: efficient testing, efficient tracing, travel bans, efficient isolating and efficient quarantining.
This paper explains Singapore's approach:
academic.oup.com
Want to guess their measures? The same ones as in South Korea. In their case, they complemented with economic help to those in quarantine and travel bans and delays.
Is it too late for countries with outbreaks to follow this model? No. By applying the Hammer, they're getting a new chance, a new shot at doing this right. The more they wait, the heavier and longer the hammer, but it can control the epidemics.
But what if all these measures aren't enough?
I call the months-long period between the Hammer and a vaccine or effective treatment the Dance because it won't be a period during which measures are always the same harsh ones. Some regions will see outbreaks again, others won't for long periods of time. Depending on how cases evolve, we will need to tighten up social distancing measures or we will be able to release them. That is the dance of R: a dance of measures between getting our lives back on track and spreading the disease, one of economy vs. healthcare.
How does this dance work?
It all turns around the R. If you remember, it's the transmission rate. Early on in a standard, unprepared country, it's somewhere between 2 and 3: During the few weeks that somebody is infected, they infect between 2 and 3 other people on average.
If R is above 1, infections grow exponentially into an epidemic. If it's below 1, they die down.
During the Hammer, the goal is to get R as close to zero, as fast as possible, to quench the epidemic. In Wuhan, it is calculated that R was initially 3.9, and after the lockdown and centralized quarantine, it went down to 0.32.
But once you move into the Dance, you don't need to do that anymore. You just need your R to stay below 1: a lot of the social distancing measures have true, hard costs on people. They might lose their job, their business, their healthy habits...
You can remain below R=1 with a few simple measures.
This is an approximation of how different types of patients respond to the virus, as well as their contagiousness. Nobody knows the true shape of this curve, but we've gathered data from different papers to approximate how it looks like.
Every day after they contract the virus, people have some contagion potential. Together, all these days of contagion add up to 2.5 contagions on average.
It is believed that there are some contagions already happening during the ""no symptoms"" phase. After that, as symptoms grow, usually people go to the doctor, get diagnosed, and their contagiousness diminishes.
For example, early on you have the virus but no symptoms, so you behave as normal. When you speak with people, you spread the virus. When you touch your nose and then open door knob, the next people to open the door and touch their nose get infected.
The more the virus is growing inside you, the more infectious you are. Then, once you start having symptoms, you might slowly stop going to work, stay in bed, wear a mask, or start going to the doctor. The bigger the symptoms, the more you distance yourself socially, reducing the spread of the virus.
Once you're hospitalized, even if you are very contagious you don't tend to spread the virus as much since you're isolated.
This is where you can see the massive impact of policies like those of Singapore or South Korea:
Only when all these fail do we need heavier social distancing measures.
If with all these measures we're still way above R=1, we need to reduce the average number of people that each person meets.
There are some very cheap ways to do that, like banning events with more than a certain number of people (eg, 50, 500), or asking people to work from home when they can.
Other are much, much more expensive economically, socially and ethically, such as closing schools and universities, asking everybody to stay home, or closing businesses.
This chart is made up because it doesn't exist today. Nobody has done enough research about this or put together all these measures in a way that can compare them.
It's unfortunate, because it's the single most important chart that politicians would need to make decisions. It illustrates what is really going through their minds.
During the Hammer period, politicians want to lower R as much as possible, through measures that remain tolerable for the population. In Hubei, they went all the way to 0.32. We might not need that: maybe just to 0.5 or 0.6.
But during the Dance of the R period, they want to hover as close to 1 as possible, while staying below it over the long term term. That prevents a new outbreak, while eliminating the most drastic measures.
What this means is that, whether leaders realize it or not, what they're doing is:
Initially, their confidence on these numbers will be low. But that's still how they are thinkingand should be thinking about it.
What they need to do is formalize the process: Understand that this is a numbers game in which we need to learn as fast as possible where we are on R, the impact of every measure on reducing R, and their social and economic costs.
Only then will they be able to make a rational decision on what measures they should take.
The coronavirus is still spreading nearly everywhere. 152 countries have cases. We are against the clock. But we don't need to be: there's a clear way we can be thinking about this.
Some countries, especially those that haven't been hit heavily yet by the coronavirus, might be wondering: Is this going to happen to me? The answer is: It probably already has. You just haven't noticed. When it really hits, your healthcare system will be in even worse shape than in wealthy countries where the healthcare systems are strong. Better safe than sorry, you should consider taking action now.
For the countries where the coronavirus is already here, the options are clear.
On one side, countries can go the mitigation route: create a massive epidemic, overwhelm the healthcare system, drive the death of millions of people, and release new mutations of this virus in the wild.
On the other, countries can fight. They can lock down for a few weeks to buy us time, create an educated action plan, and control this virus until we have a vaccine.
Governments around the world today, including some such as the US, the UK or Switzerland have so far chosen the mitigation path.
That means they're giving up without a fight. They see other countries having successfully fought this, but they say: ""We can't do that!""
What if Churchill had said the same thing? ""Nazis are already everywhere in Europe. We can't fight them. Let's just give up."" This is what many governments around the world are doing today. They're not giving you a chance to fight this. You have to demand it.
Unfortunately, millions of lives are still at stake. Share this articleor any similar oneif you think it can change people's opinion. Leaders need to understand this to avert a catastrophe. The moment to act is now.
If you want to receive more articles from me, subscribe to my newsletter.
If you are an expert in the field and want to criticize or endorse the article or some of its parts, feel free to leave a private note here or contextually and I will respond or address.
If you want to translate this article, do it on a Medium post and leave me a private note here with your link. Here are the translations currently available:
EuropeanSpanish (verified by author, full translation inc. charts)(alt. vs. 1, 2, 3, 4, 5, 6) French (translated by an epidemiologist)GermanPortuguese (alternative version, Brazilian Portuguese)RussianItalian (with graphs translated)(Alternative 1, Alternative version on Facebook)MacedonianTurkishPolishIcelandic (alternative translation)GreekCzech (alternative translation)DutchNorwegianUkrainian (alternative version)SwedishRomanian (alternative version)BulgarianCatalonianSlovakSlovenianHungarian (Alternative, with part 1, part 2)Finnish (alternative)BosnianCroatianLatvian
East & South AsianChinese Traditional (full translation including charts, alternative translation)Chinese Simplified (and version outside of Medium)JapaneseBahasa IndonesiaBahasa MalaysiaThai (alternative)VietnameseFilipinoTamilUrdu
Middle Eastern & Central AsianFarsi (alternative version outside of Medium)Hebrew (alternative through external link)Arabic (alternative)
This article has been the result of a herculean effort by a group of normal citizens working around the clock to find all the relevant research available to structure it into one piece, in case it can help others process all the information that is out there about the coronavirus.
Special thanks to Dr. Carl Juneau (epidemiologist and translator of the French version), Dr. Brandon Fainstad, Pierre Djian, Jorge Penalva, John Hsu, Genevieve Gee, Elena Baillie, Chris Martinez, Yasemin Denari, Christine Gibson, Matt Bell, Dan Walsh, Jessica Thompson, Karim Ravji, Annie Hazlehurst, and Aishwarya Khanduja. This has been a team effort.
Thank you also to Berin Szoka, Shishir Mehrotra, QVentus, Illumina, Josephine Gavignet, Mike Kidd, and Nils Barth for your advice. Thank you to my company, Course Hero, for giving me the time and freedom to focus on this.
",200
https://medium.com/the-shadow/what-happens-in-your-body-when-you-quit-drinking-e189f71f905c?source=tag_archive---------5-----------------------,What Happens in Your Body When You Quit Drinking,What would your body do if it could heal from poison?,Pedram Shojai,4,"What would your body do if it could heal from poison?
Depending on how much alcohol your body is used to filtering, abstaining can be trickier for some than for others.
So it's important to understand what goes on in your body when you first stop drinking.
Alcohol is a pretty ubiquitous mind-altering substance in most societies.
We drink for celebrations, for commemorations, memorials, athletic events, happy hours, on romantic dates, or even just because.
Whether you're taking a break for a couple of weeks, a month, or even longer-term, the body goes through changes when you remove alcohol from the picture.
Let's examine how your body reacts to the first four weeks of being booze-free.
When you remove alcohol from the equation, your body responds pretty quickly.
Within 24 hours, in fact.
One hour after your last alcoholic drink, your liver begins filtering the alcohol out of your bloodstream so that it doesn't poison you. It changes the chemical composition of alcohol to acetaldehyde, and then acetate.
Now, acetaldehyde is a carcinogen. It only exists in the body briefly until it becomes acetate, but it's worth knowing.
The pancreas also starts producing insulin, which causes an intense craving for carbs.
After 72 hours, most everyone, including very heavy drinkers, should no longer feel groggy, tired, achy, or foggy.
After a full week, you'll notice three main differences: better sleep, clearer skin, and a normalized eating pattern.
For every six glasses of wine, your body loses 19-24 glasses of water. Without the drain on your hydration reserves, your skin is brighter and clearer.
Without alcohol, your body experiences on average four to six REM cycles a night  with alcohol, you can typically expect between one and two REM cycles.
And when your hunger pangs aren't dictated by cycles of being drunk and hungover, your hunger is more in line with the circadian rhythm of the Earth. You also crave better food, and less of it.
After two weeks of improved hydration, you'll notice a few things. First, your body is releasing its water weight.
You see, while you're drinking, your body clings to any reserves of water because alcohol dehydrates you. When you stop drinking, your body releases that water and you start to pee more frequently and lose some of your water weight.
You also stop experiencing acid reflux, and your stomach lining has repaired itself from the damage that drinking alcohol does to it.
As you're not drinking pints of beer and glasses of wine anymore, you're saving majorly on calories and may start to show signs of weight loss.
Not to mention two weeks of clean, deep, and restful sleep makes you more alert, more productive, and less distracted.
Drinking alcohol causes higher blood pressure. Around week three, you should see your blood pressure levels return to normal.
Not only that, but your kidney health improves, and your risk of stroke or heart attack is significantly decreased.
Because alcohol raises your blood sugar levels, which causes the lens of your eyes to swell...
Your vision also gets better at around this stage of being sober!
Your sleep gets better every night, eating habits stabilize, skin clears, and immune function increases. And you may be able to notice some belly fat loss.
A whole month!
After a month without alcohol, your liver fat is reduced by 15% on average. That means that you can better filter out contaminants, store vitamins and minerals, fight infections, and more.
While your skin has been improving steadily over this month, you'll notice the most significant difference at this point.
Partially, it's because you're more likely to drink water when you're not drinking alcohol, but mainly it's because you're not actively robbing your skin of the water it needs by drinking alcohol.
Now, there are plenty of good changes happening in your body...
But if you've been a steady drinker for a long time, you should also expect to experience depression, anxiety, aggression, and a higher level of irritability during your first month without alcohol.
That's normal.
It's your body trying to get you to feed it booze  after all, people tend to drink alcohol when they're sad, angry, anxious, or stressed.
That's all okay.
Whether one week, two weeks, or one month is your goal, your emotional state evens out and adjusts to the absence of alcohol just like the body does, resulting in a brighter, healthier, more alert you.
If you enjoyed these thoughts and think we've got something in common, I have a feeling you're going to love the streaming service I launched last year  by wellness purveyors for wellness seekers. Here's two weeks free  on me.
Inspire and entertain
3.5K 
23
",201
https://ketoschool.com/the-science-behind-fat-metabolism-60f7a3f678d0?source=tag_archive---------6-----------------------,The Science Behind Fat Metabolism,"Per the usual disclaimer, always consult with your doctor before experimenting with your diet (seriously, go see a doctor, get data from...",Will Little,23,"Per the usual disclaimer, always consult with your doctor before experimenting with your diet (seriously, go see a doctor, get data from blood tests, etc.). Please feel free to comment below if you're aware of anything that should be updated; I'd appreciate knowing and I'll update the content quickly. My goal here is to help a scientifically curious audience know the basic story and where to dive in for further study. If I'm successful, the pros will say ""duh"", and everyone else will be better informed about how this all works.
[UPDATE #1: January 18, 2016. Based on a ton a helpful feedback and questions on the content below, I've written up a separate article summarizing the science behind ketogenic (low-carb) diets. Check it out. Also, the below content has been updated and is still very much applicable to fat metabolism on various kinds of diets. Thanks, everyone!]
[UPDATE #2 : December 6, 2019. I've edited/re-phrased topics below that I've learned more about in the last few years, especially in the sections regarding lipoproteins, exercise, and sleep.]
[UPDATE #3 : July 25, 2020. A more recent version of this article can be found over on my Satchel site here. Feel free to subscribe to my newsletter to stay posted as I publish more articles on health science topics. Thanks!]
The concentration of glucose in your blood is the critical upstream switch that places your body into a ""fat-storing"" or ""fat-burning"" state.
The metabolic efficiency of either state  and the time it takes to get into one from the other  depends on a large variety of factors such as food and drink volume and composition, vitamin and mineral balances, stress, hydration, liver and pancreas function, insulin sensitivity, exercise, mental health, and sleep.
Carbohydrates you eat, with the exception of indigestible forms like most fibers, eventually become glucose in your blood. Assuming your metabolism is functioning normally, if the switch is on you will store fat. If the switch is off, you will burn fat.
Therefore, all things being equal, ""diets"" are just ways of hacking your body into a sufficiently low-glycemic state to trigger the release of a variety of hormones that, in turn, result in a net loss of fat from long-term storage.
That pretty much sums it up.
If you want to lose weight from fat cells, those cells need to store fewer calories than they release. What most people do not understand, however, is that this is not as simple as ""calories consumed"" vs. ""calories burned""; your body responds very differently to 200 calories from a cookie, for example, verses 200 calories from broccoli.
Furthermore, as long as protein-intake is reasonably high enough, your body will not burn a significant amount of muscle protein (i.e. you won't lose muscle mass, but you will lose body fat).
If you're curious to read more about the details, I spend a fair amount of time below introducing the science of how carbs, fats, proteins, fibers, alcohol, caffeine, vitamins, minerals, water, gut microbiota, lipoproteins, cholesterol, sleep, exercise, mental health, etc... contribute to overall fat metabolism.
""Fat"", depending on the context, can mean different things (e.g. specific molecules, cells, tissue/food, and/or a physical body state). On a cellular level, there are a bunch of different types of fat cells and fat tissue. What most people are referring to when they are talking about the biology of ""fat"" is white fat cells, especially those located just under the skin (i.e. subcutaneous fat).
Below is a simple illustration from wikipedia that gives us a decent idea of the tissue and cells we're talking about:
Those large white blobs (termed ""lipid droplets"") in the white fat cells are what house our triglycerides (TAGs...get used to this acronym, I'm going to use it a lot). The lipid droplets are what grow and shrink as TAGs come and go. Chemically speaking, TAGs come in different varieties, but are always made up of a glycerol molecule plus three fat molecules, resulting in a structure that looks essentially like this:
The alpha () ""first"", omega () ""last"", and numbers on the bottom part there are labels (from a chemist's perspective) for specific carbon atoms. On the other hand, biologists, to make our lives easier (I'm joking), count from the other direction. You may have heard of Omega-3 Fatty Acids, which means the third carbon from the right has a double bond. Indeed, the keen observer will see that the bottom molecule in the figure above is an Omega-3. And yes, FYI, a ""fatty acid"" are those long squiggly things. So when you say the word ""fat"" to a biochemist, these are the molecules that come to mind.
As a final point of technical background here, let's clarify ""saturated"" versus ""unsaturated"" fat. This terminology has to do with the ""saturation"" of hydrogen atoms around the carbon atoms. Simply put, unsaturated fat has double bonds between one or more of of its carbon atoms (i.e. the double line things) and saturated fat are those that don't.
You've probably heard scary and opinionated things about saturated vs. unsaturated fat, and by all means, do what your doctor tells you. But for some peace of mind, know that your body needs both to function properly.
So, given the background above, the specific question we're curious about is what causes TAGs to accumulate in lipid droplets within white fat cells?
As you can imagine, the current answer (subject to change) is quite complex. I'm going to start the answer by categorizing the types of substances we consume into general buckets (there are, of course, more... but this provides sufficient coverage for our discussion here and helps us think carefully about what we eat and drink).
Now, given these general buckets, let's talk about how each one relates to TAGs filling up our fat cells.
First and foremost, out of all the buckets, it's important to note that glucose is by far the major player here. While on a normal ""well-balanced"" diet, the concentration of glucose over time in your blood is the key upstream factor to consider in the metabolism (both build-up and break-down) of fat.
When your blood has high concentration of glucose for a sufficient period of time, your pancreas kicks insulin into gear, which tells your cells to take in glucose, put it in a temp storage (like RAM in your computer) called glycogen, and put everything else into long-term storage inside your fat cells.
On the other hand, when your blood has a low concentration of glucose for a sufficient period of time, this process is reversed. Your pancreas releases a protein called glucagon, glycogen is depleted, a bunch of other hormones (epinephrine, cortisol, testosterone, etc..) are introduced into your system, and TAGs are thereby pulled from your fat cells and converted into Acetyl Co-A, which is the key precursor for the process your body uses to generate ""energy"" (i.e. ATP) for your cells.
So, as it relates to our digestion here, a key process to consider is the adsorption of glucose into (and across) your intestinal wall. It's a relatively short story. This is why when you eat/drink sugary stuff it goes quickly into your blood. These blue and green transporters (yes, it's active transport, not passive, which means it takes energy) are efficient at getting the job done:
It's important to note that the rate of getting free glucose into your bloodstream is quite high when you have, surprise surprise, free glucose in your lumen (intestinal tube). Once glucose gets into your blood, then insulin does its job. Here's a simple graphic that describes the basic process:
But before we get too far ahead of ourselves, let's talk a bit about what form glucose takes as we consume food and drink. Most of our glucose is packaged in starches (breads, pasta, corn, potatoes, and such), but we also get a fair amount via common table sugar, too (i.e. sucrose). This chart shows the basic breakdown pathways of starch, sucrose, and lactose into glucose:
As an important side note, High-Fructose Corn Syrup (HFCS, which is in everything these days) is our food industry's way to mimic table sugar (sucrose) for a lower price. Most HFCS in soft drinks is 55% fructose and 42% glucose, and HFCS in pretty much everything else is 42% fructose and 53% glucose. The rest of the percentage, for those doing the math, is maltose or maltotriose  that is, 2 or 3 glucose molecules bound together, respectively.
From a biochemistry standpoint, know that HFCS is essentially just fructose and glucose mixed together. It takes a bit more ""work"" for our body to breakdown table sugar (i.e. via sucrase, as noted above), which means glucose from HFCS goes into our blood ""faster"", depending on what you eat with it. Again, this consideration (i.e. the speed in which glucose gets into our blood and its concentration there) is the most important thing to talk about when it comes to regulating fat storage in our body.
Ok, so we know that glucose gets into our blood quickly and is used by all cells, stored temporarily in glycogen, and stored long-term in fat cells. But what about fructose?
Structurally, here is the exact difference between glucose and fructose:
As you can see, both have six carbons, but the fructose carbon ring is smaller (the other carbon is hanging off as an additional CH2OH group). But why does this matter? Well, for one, fructose tastes sweeter to us (which is nice, I suppose). Second, fructose is absorbed into our blood via a slightly different route than glucose, and is relatively slower to do so unless ratios of glucose to fructose are similar. Third, once in your blood, fructose can only be processed by your liver, where  when introduced in reasonably low concentrations  it is converted to glucose and mainly stored in glycogen:
However, in higher concentrations, and especially when there is plenty of glucose around, fructose can do unhelpful things, including a more direct route to storing TAGs in our fat cells and a propensity toward non-alcoholic fatty liver disease (NAFLD):
Fructose does not stimulate insulin like glucose does, so in small amounts fructose participates relatively harmlessly with our normal energy storage and supply. But, indeed, part of the reason why soft drinks with HFCS are so ""bad for us"" is because it's easy for us to drink too much, which thereby sends a significant percentage of it into our fat cells (especially around the liver).
Now that we know the basics of TAG production and storage from glucose and fructose, we can better understand why limiting the rate of adsorption of these molecules into our blood is important. This is  in part  where fiber comes in, and is a component of why the recent craze around eating ""whole foods"" is significant. Unprocessed foods generally contain more fiber (as well as more vitamins and minerals, which are often leached out during processing).
The below chart nicely sums up a ton of information about why fiber is important for overall fat metabolism.
It has been well documented that increased fiber in the diet reduces the concentration of glucose in the blood. The mechanisms behind this are not completely known, however, but it is likely because of a binding effect that prevents the degradation of carbs into free glucose.
If you've ever gone on a low-carb diet while still eating a decent amount (if not more) fat, you've probably been surprised that you actually lost weight. What's up with that? Doesn't eating fat make you fat?
Well, to answer this, let's go ahead and follow the same process we did above with glucose and fructose.
First, when you ingest fat, your digestive system breaks it down to its component parts, i.e. free fatty acids and monoglycerides. And, as you can guess, the rate of this degradation depends on what else you are eating (e.g. fiber). Once the molecules are freed up, they are adsorbed via a completely different mechanism than glucose and fructose. Here is a great summary:
Getting more specific on that last step there, here is how fatty acids are ultimately turned into energy:
In addition to the mechanisms it takes to get energy out of fat molecules, it's important to note (albeit somewhat tangentially) what else our body does with fat. Here is the basic structure of a Lipoprotein, which our cells produce to move fat, cholesterol, and other lipids around the body (T = TAGs, C = Cholesterol):
You may have heard of ""HDL"" and ""LDL"" when visiting the doctor and reviewing your blood work. There are actually five different types of lipoproteins that vary in size and density (H = ""High"", I = 'Intermediate', L = ""Low"", VL = ""Very Low"", D = ""Density"", L = ""Lipoprotein"") :
From large to small,
Cholesterol, FYI, is critical for our health since it's a key ingredient in our cell membranes, and it's the key precursor for vitamin D, bile salts, and a bunch of critical hormones. Our bodies produce all the cholesterol it needs (from, yes, our friend Acetyl Co-A), and  contrary to popular opinion  consumed cholesterol doesn't really effect our lipoprotein profiles or risk for heart disease.
The reason why our discussion of lipoproteins is important for fat metabolism is because HDLs are needed to properly respond to the call to get energy out of our fat cells. It's also better for us to simply be informed about how this all works, to be less afraid of ingesting fat.
Can protein make you fat? The short answer is yes, but not efficiently. When you eat protein, it gets broken down into ""peptides"" (i.e. chains of amino acids) and absorbed into the blood via mechanisms like this:
Free amino acids in the blood can then be used for energy, including getting converted into pyruvate and acetyl Co-A:
And as you can see, this is mainly for the purpose of getting energy out of our proteins, but it is possible that in excess, the Acetyl-CoA can get converted into fat (per the mechanisms described above).
At the end of the day, however, amino acids are ""best"" used for actually building and maintaining proteins in our body (including those in muscle tissue).
As we consume caffeine, an energy spike kicks in and  as we consume even more  most people get jittery and less coordinated. The basic mechanism here is to block your nervous system from getting drowsy and to increase hormones such as epinephrine (a.k.a. adrenaline). Caffeine itself is not the energy, despite what marketing from ""energy drink"" companies will tell you.
As it relates to fat metabolism, caffeine has been shown to promote TAG release from fat cells. While it's not clear exactly how this happens, it's thought to be a combination of the properties of caffeine itself along with the effects of epinephrine (i.e. calling for energy release).
Thus, diet pills are often packed with caffeine and, for some people, actually help the weight loss process. For others, the negative effects of caffeine (e.g. lack of good sleep) actually counteract the process and lead to a net weight gain.
So, in short, be careful with caffeine. I'll leave it at that. Again, always consult with your doctor on these issues if you have questions and/or are considering changing your diet.
When you consume alcohol, you are consuming ""calories"", but the relationship to fat gain/loss is highly debatable.
Looking at the basic science, alcohol can indeed be turned into Acetyl-CoA via:
...which in turn, similar to what we described downstream from fructose and protein above, can lead to fat production and storage (especially around the liver) with sufficient biochemical ""pressure"" (e.g. when blood glucose levels are high).
Another proposed effect of alcohol on weight control is blocking the oxidation of lipids (i.e. the process of getting energy from fat). One such study in the New England Journal of Medicine showed that alcohol can lead to weight gain if the body already has enough energy in place, saying:
the ingestion of ethanol as additional energy above nutritional requirements is a risk factor for obesity, because it decreases lipid oxidation and therefore favors lipid storage.
Finally, and perhaps more obviously, when you drink your judgement is impaired. Thus, you're less inclined to actually ""listen to your brain"" that's telling you to stop eating.
Vitamins (A, Bs, C, D, E, K) and minerals (calcium, magnesium, zinc, iron, etc...) are key components and regulators of many of the processes described above. Notable examples are B1, B2, and B5, which are all involved in the various stages of deriving energy from fat, proteins, and carbohydrates. Furthermore, Iron, Niacin, and B6 help metabolise L-carnitine, which is essential for the transport of fatty acid components to energy-producing areas of cells.
The list of similar examples is extensive, but what is key for our discussion here is that if you are deficient in any vitamin and/or mineral, it can mess up all your other efforts. For example, being low in Vitamin D or Calcium can affect your ability to stop eating, and being low in magnesium can seriously mess with your insulin sensitivity.
Therefore, it's a good idea to have your blood checked on a regular basis to see if you have any vitamin or mineral problems. Getting to a healthy baseline here is, obviously, an important component to overall health.
In the areas of appetite control, yet again we run into a ""highly complex"" set of overlapping systems. The basic elements are:
Underneath these factors are four important hormones that regulate appetite:
Sleep is extremely important regarding obesity in children and in adults. This topic merits an entire series of articles (and books!) on its own, so I'll refer you first to Matthew Walker's book Why We Sleep and his podcasts/AMAs with Peter Attia.
Long-story short, a lack of length, diversity, and ""quality"" of the various stages of sleep will have dramatic effects on a large number of biological systems, hormone levels, and cognitive functions related to energy metabolism. Focusing on improving all aspects of your sleep is essential to healthy fat loss.
Another area of fat metabolism research that is gaining popularity concerns the population of bacteria inside our intestines. Early research shows that its relationship to dietary fiber, the absorption of carbohydrates and fat, and effects on appetite are more significant than previously thought.
One of the concerns of taking strong antibiotics, for example, is that it can throw off your gut microbiota and possibly lead to weight gain. On the flip side, some strains of probiotics may actually lead to weight gain as well.
Thus, how your gut microbiota adjusts to your diet over time, and how food, drink, and medications may affect this composition will be, as many suspect, a significant area of research in the near future.
If you've read this far, you've hopefully realized that not all ""calories"" have the same effect on fat metabolism, and that counting calories can be an overly simplistic way to think about it. Here is a decent summary graphic of the basic pathways for proteins, carbs, and fat:
In other words, your body will get energy one way or another, but one of the best ways to effectively burn energy from fat-cell TAGs is to keep your carb consumption low. By a long shot (as anyone who has ever gone on a low-carb diets knows), our body strongly prefers to use the green route above for energy. If you tweak your diet to make your body use mostly the yellow route for energy, it  by definition  facilitates overall fat burning.
Everyone should exercise daily, mixing up high-intensity and low-intensity cardio, stability/flexibility training, and lifting weights. There really isn't any debate on this. Exercise burns energy stored in glycogen and fat, which helps your body regulate blood-sugar levels and overall energy metabolism.
However, unless you are an extreme athlete  or go through periods of extreme exercise  contrary to popular opinion, cardiovascular exercise rarely has a significant impact on the overall quantity of TAGs in your fat cells. While indeed you expend energy and burn through glycogen and fat when you workout for long enough  your body adjusts for this via increased hunger and consumption of carbs, fats, proteins, etc...
As you may have heard, building muscle does, indeed, raise your basal metabolic rate (so it's a good idea to lift weights!). But again, your body adjusts to this new rate by compelling you to eat more. In fact, in order to gain muscle you need to be in energy-positive situation and let insulin do its work of bringing in energy and amino acids in for ""muscle building.""
So, with regard to fat metabolism, this is a short section of the article. Everyone should exercise daily for overall muscular, cardiovascular, and mental health.
In order for our bodies to start pulling TAGs out of lipid droplets, the appropriate signals need to be sent to receptors on the surface of fat cells. When glucose resources are low, this activates a wide variety of hormones to help get the job done, including (to varying degrees and roles) epinephrine, norepinephrine, corticotropin (ATCH), thyroid stimulating hormone (TSH), cortisol, ghrelin, testosterone, human growth hormone (HGH), adiponectin, cholecystokinin (CCK), glucagon, and leptin. Here is a sample of the receptors that have been discovered on fat cells:
Not all are involved in telling adipocytes to release TAGs, however (though many, indeed, are, giving you an idea of the complexity here).
Below is the basic process by which leptin, as an example, stimulates the brain to signal the release of ""fight or flight"" (sympathetic) hormones that kick fat burning into gear.
And it's not just catecholamines (epinephrine and norepinephrine) that are involved, many of the hormones mentioned above also activate PKA (Protein Kinase A) in fat cells and work with ATGL, HSL, and MGL to break down TAGs and kick out fatty acids (FA) and glycerol.
So, as you can see, to lose weight all you need to do is keep glucose levels down, make sure that these hormones are triggered effectively, and don't eat more calories than you burn, right?
Everyone has a different ""metabolism"" (i.e. the rates and efficiencies of the metabolic pathways and biochemistry described above), and these factors always change as we get older. In other words, genetic and environmental factors will make it virtually impossible for some people to have a low body mass index (and vice versa), and most Americans are destined for an increasing BMI until the age of 60 or so.
This chart is especially disturbing since the Center for Disease Control indicates that a BMI of 25 or above is ""overweight."" Uh oh. If you have a relatively large amount of muscle mass it will throw your BMI calculations off, but it's an important metric for the overall population.
Now, there is the other end of the spectrum, as all of us suspect in the back of our heads. There are risks to low BMIs. One study found that ""adults who are underweight  with a BMI under 18.5 or less  have a 1.8 times higher risk of dying than those with a ""normal"" BMI of 18.5 to 24.9."" But, to put that in perspective, if you are 5'5"" you'd have to weigh less than 111 pounds, and if you are 6'0"" you'd have to weigh less than 137 pounds.
To actually get into a ""fat-burning"" state, the simplest explanation is to keep your blood-sugar level low for as long as possible and ensure nothing is getting in the way of your metabolism (e.g. vitamin/mineral imbalances, stress, lack of sleep, etc...). Obviously, people lose weight on diets all the time; and at the end of the day  biochemically speaking  this is how it always happens.
Whether you pick a low-fat or a low-carb diet, it's mostly a mental game. Maybe eating a small amount of carbs 4-5 times a day (along with a bit of fat and protein) makes you feel better and you can actually keep your overall glucose and insulin levels low. Or, maybe you prefer to essentially cut the carbs and keep eating reasonable amounts of fat and protein. You need to experiment yourself  and consult with your doctor  to figure out what works for your unique metabolism.
But, that being said, knowing ""the science"" behind all this can help us think through a few things:
In conclusion, the best game to play here is how to hack the mental process of staying reasonably ""healthy"" (however you and your doctor define that) while not freaking out about having a few TAGs in your fat cells. Knowing all the science above should help each of us, in concert with our health care providers, put together a customized plan for long-term health.
Author's note: I'd love to hear from anyone with suggestions on how to improve this article. I'm currently researching how various food combinations affect blood-glucose concentration, and how its specific rate-of-change and absolute value over time affects fat metabolism. Subscribe to my newsletter and I'll let you know when I get new content up (I write mostly about startup/tech stuff, but occasionally I'll write articles on topics like this). Let's keep the conversation going. Thanks!
image credit
We teach the biochemistry behind ketogenic dieting and...
1.5K 
10
1.5K claps
1.5K 
",202
https://medium.com/@billbarnwell/the-easiest-way-to-lose-125-pounds-is-to-gain-175-pounds-b4fca27ffcc5?source=tag_archive---------1-----------------------,The Easiest Way to Lose 125 Pounds Is to Gain 175 Pounds,"I let myself go for a few years and then, on a breezy spring afternoon in San Francisco, I found myself riding my bike down Market Street towards the Embarcadero. I stopped at a red light in the...",Bill Barnwell,18,"I let myself go for a few years and then, on a breezy spring afternoon in San Francisco, I found myself riding my bike down Market Street towards the Embarcadero. I stopped at a red light in the Tenderloin and a worn, reedy man panhandling for change headed in my direction. He glanced me up and down as I tried to avoid making eye contact. I was about to say that I couldn't give him any money, which was true, both because my first professional writing job wasn't paying me enough to actually cover the rent on my apartment, and because I was so uncoordinated that I was genuinely afraid I would fall off of my bike if I tried to reach into my pocket for change. Before I could say any of that, though, he chuckled to himself and smiled at me. ""Good for you, big man,"" he said. ""Keep riding that bike."" Then he walked away, having flipped my prepared pity on its head, and I wished I could have given him every dollar in my wallet to have not said anything at all.
That was back in 2008, when I weighed a mere 260 pounds. On January 1 of 2015, I lumbered onto a hotel scale which I was really hoping would be out of batteries in Austin and found that I weighed 334.7 pounds. I swore I would do something about it, and while that has been a quickly abandoned threat many times in the past, for some reason, this time it stuck. 364 days later, on New Year's Eve, I woke up and stepped onto a scale which delivered a much happier number: 206.0 pounds.
I lost just over 128 pounds this year, which is one of those things you're proud to do but not proud of having needed to do. In addition to trying to put some closure on a particularly dispiriting part of my past, I'm writing this in the hopes of helping people who find themselves in a similar boat, who have approached the New Year with the sort of drastic self-improvement plans that I held this time last year. I'm certainly not qualified to give professional advice on the topic, and I don't want to suggest that anybody of any shape or size should feel the need to change if they're happy with how they look. For those people who might want to radically change themselves in 2016, though, or those who are struggling with body image issues, I can tell my story of how I worked my way out of physical doldrums over the last 12 months. At the very least, I can write the piece I would have wanted to read at this time last year.
That starts with how I got to being 334.7 pounds in the first place. I was a fat kid growing up, eventually hitting 240 pounds by my junior year of high school. At 5-foot-11, I didn't wear that very well. Wanting something better for myself before I hit college, I lost 35 pounds over each of the next two summers, eventually making it to school in the fall of 2001 at 170 pounds, identifying as a quantitatively healthy adult. I spent the next three years fluctuating between 160 and 180 pounds. I was never truly satisfied with how I looked or felt, but it was easy to remember just how much worse it had been a few years earlier.
At the end of 2004, I was set upon by depression, the sort of overwhelming, crippling despair that is difficult to even fathom unless you've actually experienced it. I would try to get out of bed in the morning and be so overcome with anxiety that my heart would palpitate. When I did go out, I would be inexplicably anxious to walk through the main paths of my campus and quickly get exhausted without any obvious cause. I took an internship and regularly fell asleep in the middle of meetings without warning. I would look forward to the weekend for respite from letting people down during the week and count down the minutes until the weekend to be over because I needed the routine of the weekdays.
Eventually, the mental and emotional symptoms I had been struggling with faded and became functionally manageable. I was blessed to be set up with an incredible psychologist at my school. Even today, her level of compassion and capacity for challenging and improving my self-awareness informs my own personal empathy and reduces me to awe. In the process of dealing with the depression, though, I developed a coping mechanism which would present an equally intractable problem in the years to come. Somewhere along the way, I became addicted to eating.
It took years for me to actually believe that I had a real problem. I associated addiction with alcohol and hard drugs, vices which thankfully (slash luckily) held little interest for me. The idea of being addicted to food is, at least in most cases, played for a joke; everybody has that friend with a ridiculously unfair metabolism who eats whatever they want and laughs about how they're addicted to food. This, obviously, is not remotely the same thing. During those fleeting moments when I would recognize the need to change, I would look over this list of questions at Overeaters Anonymous and identify with far too many of the criteria, only to put off the genuine self-evaluation and improvement for some indeterminate date in the future.
My compulsive eating had little to do with hunger and was almost never enjoyable. Instead, it was like trying to chase a vague, indefinable comfort, some satisfaction that never arrived  or even could arrive. It was the fear of missing out, but for food and constantly. I would struggle to pick between two fast food places and just stop at each of their drive-thrus, hiding the soda cup and the wrappers from the first one so the second cashier wouldn't see, because being caught in my embarrassing act was somehow more shameful than the actual behavior I was committing. The idea of just waiting for another day just wasn't realistic to me; I absolutely, compulsively had to get that sandwich from Wendy's. The act of ordering the food, of making the conscious choice to indulge, was far more important than eating the stupid thing.
Most of the time, though, it would be deliveries, always heaping amounts of unmemorable food. Most people in the mood for pizza would go grab a slice. I would go to the Domino's website and order a medium pizza. When I got fatter, that medium became a large, and then a large with wings. I still name my fantasy football team to this day after the local wing delivery place in Allston. I ordered from them so frequently that the delivery drivers knew me by name, and even worse, I knew them so well that I recognized how one of them wore the same Queens of the Stone Age hoodie every time he stopped by. (It was a pretty cool hoodie.)
I contorted my life and the stories I told myself to fit a horrific eating schedule. As a football writer, I sat in front of the television all day and night on Sunday. That meant a couple of food deliveries, justified in my head because I was too busy to cook or otherwise eat healthy. I would stay up overnight and file my Monday morning column for Grantland at 5 a.m., then get up at 9 a.m. to prep and record our podcast. Exhausted from the lack of sleep, my treat for working so hard the day before would be something unhealthy for lunch and then another delivery during the Monday night game. Throw in more awful food to eat during the Thursday night game (thanks for the extra day of games, NFL) and then going out with friends on the weekends or justifying a relaxing night in after a long week, and there was never a respite. I was constantly able to convince myself that it was OK to put off taking control of my life for another day.
Of course, the same weird bargains I made with myself in my head didn't extend to working out. Squeezing any possibility of progress out of my schedule was the most exercise I would typically get. I'd take out a gym membership and go months without even thinking about attending. And then, when I'd finally pay lip service to working out, I would find the easiest possible excuse to be lazy. It would be too cold to make the trek in the middle of a Boston winter and too nice outside to justify going in the spring. When I didn't have a car, it was too much of a hassle to take the subway, and when I did get one, it would be too much of a nuisance to park. I'd sleep in and put off going in the afternoon, only to tell myself that it would be too crowded to go when the after-work crowd hit and too late to go in the evening. In part, it was laziness. Much of it, though, was that I was so far gone physically that I knew just how hard I would have to work to fix things, and how easy it was to just give up and pretend to try again tomorrow.
I recognized my behavior was idiotic and damaging, and there was some part of me which actually wanted to change, but I did nothing. That sentence makes no sense with any sense of perspective or reality, but it's a measure of how years of bad decisions had warped my brain. I'd eat something unhealthy for lunch and then write the rest of the day off and eat something worse for dinner because I had already wasted the day. I would lie in bed, tell myself I was going to do better the next day, and then inexplicably do the same thing over and over again. I'd go to the grocery store, buy the ingredients to make something reasonably healthy, and then go home and immediately order wings. It was Memento with fried food.
I would set some arbitrary point in the future and tell myself that I could eat whatever I wanted up to that date, only for it to never stick. I'd write off the final two months of a football season and swear to start eating better in February, only to then tell myself that I should relax during my offseason and swear to eat better once football season got me in a routine. I'd get healthy once I came back from vacation. Once I moved out of Boston. Then once I moved back to Boston. I'd save my life once I turned 25. 27. 30.
And so, unchecked by reason, I grew. Morbid obesity costs you some semblance of your agency as a person, and while it's probably not the worst thing in the world for a white dude in his twenties to see some of his privilege disappear, I can't pretend it didn't hurt. Friends who were happy to see me when I was thinner began to drift away. Bereft of confidence, it became increasingly difficult to meet new people, which made me less likely to go out, which reinforced how depressing it was to look terrible. I stopped caring about how I looked. After letting my facial hair grow for weeks out of sheer laziness before finally shaving, the very nice elderly doorman in my building in Vegas saw me and remarked, ""Wow, Bill, you look so much younger without your beard!"" I glowed for a moment before he followed that up by saying, ""It makes you look like you're still in your twenties!"" I was 27.
Outside of a few short sparks of self-assessment which never lasted more than a couple of weeks, I spent those twenties in a fog. I'd fall asleep and wander into a dream where I was some younger, thinner version of myself, recognize how much better I felt, and then wake up to reality, like some sort of phantom appendage in reverse. The writer side of me consciously wished for some moment of clarity that sliced through everything and made it obvious that I needed to change, but that really wasn't the case. Not that it stopped me from trying to manufacture one. I'd wake up the day after the Boston Marathon and decide that I was going to fix myself and somehow run a marathon one year later, despite the fact that I lacked the stamina to run around the block. I would take a meaningful life event  like the time a drunk driver totaled my car in Vegas  and try to use it as an anchor for change and a narrative into a new life. They served as neither.
All that eventually became my normal. Instead of feeling like the real me was the one who had gotten in shape before college and given way to a temporary weight gain, it felt like the years when I'd had control of my body image were an exception that I would never be able to re-create. I abandoned all hope that things would get better. I'd managed to build a reasonable career and found a level of emotional stability, but I couldn't fathom that there would be a way for me to ever drastically improve how I looked. Instead, change came from a more familiar process and unexpectedly stuck: I got out of an unhealthy relationship and swore to work on myself out of the fear that I would otherwise have to settle for being unhappy. That happened a few times over the last decade, but this time, for whatever reason, this time, I didn't retreat back towards my old habits.
I pieced together one healthy, productive weekend in January. I made it to the gym on consecutive days for the first time in years, even if I could barely gasp my way through a couple of abbreviated workouts. I watched and wrote through a weekend of football while my brain shrilly reminded me over and over again that it was time to feed myself shit. I'd open food delivery websites, read the menu, and close the window, or walk to the pantry to get snacks before shutting the door and walking away. I woke up Monday morning having strung together two consecutive healthy days and it felt, even for a moment, like I had found a possible way out.
Even with that modicum of success, I knowingly dismissed it as a hiccup, one that I would destroy with a bad decision within a matter of days. Thrillingly  surprising myself more than anybody else  I managed to keep it going. That weekend became a week. Then two. I hit a month and it was surreal, like I had somehow hopped into the ocean and caught a wave after ten years of getting smacked in the face by my surfboard. I was down 20 pounds and friends started noticing, which was enough to convince my brain that I couldn't go back on even that tiny bit of progress. And now, it's been a year and counting.
As I think about what I did to actually get in shape, the actual physical choices I made are a lot less important than they might seem. I knew that a healthy adult male ate about 2,000 calories per day, so I aimed for that figure, eventually settling into a range where I tried to eat between 1,800 and 2,400 calories each day. And I began going to the gym six days a week, almost exclusively using the elliptical machine, since I didn't think my joints could take much of a pounding after they had spent the last decade in energy saver mode.
Although I've adapted and expanded my knowledge and obviously developed a more robust game plan, those two simple concepts are at the core of what I've done all year. It was more important to show up every day and emulate that terrifyingly militaristic Tom Brady commercial than it was to worry about interval training and macronutrient ratios.
Instead, other aspects of my rebuild were far more meaningful and seem wholly more important to reiterate here as advice. That starts with a point I wish I had figured out much earlier: you simply have to build a plan which works for you in lieu of copying somebody else's. You can read a million stories of how people lost weight and drastically improved their self-image, but unless you can apply the principles of what they did to something that actually fits you, it's almost always going to fail.
There was one piece in this vein I read quite a few times, especially when I was getting started earlier this year. HardballTalk writer Aaron Gleeman, somebody I've been reading since I was in high school, wrote about losing 153 pounds over a one-year stretch between 2011 and 2012. (Go read it and come back. It's great.) When I was reading just about every Google hit for my ""lose [large number] of pounds in one year"" searches in January and February, it was easy to get discouraged. I'd read articles that seemed to contradict one another while expressing levels of certainty about how their process was the one true platonic ideal of how to get fit now. The simplicity of Aaron's piece really resonated with me as a stat nerd: burn off more calories than you need to sustain your current weight and you'll start dropping pounds.
Aaron's story was easy to relate to, given that we are both sports bloggers with a healthy appreciation for Ma$e's career, but his path towards losing weight  specifically eating the same meal on a daily basis  never would have worked for me. I strongly suspected that I wasn't going to be able to diet in the way that Aaron had, so I pieced together an eating plan which vaguely resembled a far healthier version of the things I used to enjoy. Desserts were cut out altogether. Never a big drinker, I took beer out of the equation and didn't have any alcohol in 2015. I loved wings more than I loved most family members, but I had to cut them out of my life and replace them with grilled chicken. (The one exception was when I inexplicably walked past a buffalo wing restaurant in Tokyo and genuinely thought I was hallucinating.)
In the process, I actually began to re-program my brain into remembering how food could be enjoyable. Those wings in Tokyo were pure pleasure. I'd stopped getting pizza delivered, but I would head out to Pino's for a slice and actually savor it in a way that simply didn't occur when I had been eating whatever I wanted. And then, instead of shutting down at the sign of something unhealthy and gorging myself with the idea that I needed to somehow get full before correcting things the next day, I planned the rest of my eating day around that slice of pizza and made my decisions accordingly. My relationship with food evolved into a philosophy which now rings through my head on a daily basis: Don't be a shithead.
I also found it incredibly valuable to log everything I did in a spreadsheet. In part, that's because I'm a spreadsheet fetishist who would write longform pieces and love letters in Excel if I could. I'm not sure any of this would have actually stuck if I hadn't started tracking what I was doing in a spreadsheet. I started really going out of my way to do so a couple of weeks into the process, and it's been both an invaluable resource and an incredible motivator. It's impossible to hide if you're actually counting calories every day, and there were so many times when I stopped myself from eating something because I knew how frustrated I would be if I had to type it into Excel. And likewise, the act of getting home and recording the calories I'd burned at the gym became a weirdly satisfying conclusion to my workout.
After a couple of months, when I realized that this wasn't just some blip of self-preservation, I built a model to estimate my weight loss, dynamically calculating my basal metabolic rate (BMR) and using a Monte Carlo simulation to predict how many calories I would take in during a typical day. Few creations have ever been simultaneously as nerdy and genuinely sad. I weighed myself far more frequently at first, when I was desperate for positive reinforcement, than I do now. At this point, I have enough data to reliably trust the process. The Sixers are a good rebuilding thing to emulate, right?
Having reliable data also allowed me to set short-term goals along the way to my long-term ideal, another thing I had hugely underestimated in years past, when I would get discouraged by the scope of what I had to accomplish and give up before even trying. This weight loss timeframe estimator wasn't perfectly accurate, but it was a reasonable estimate and gave me hope of how quickly I could turn things around, especially at the beginning.
Some of my benchmarks were downright embarrassing. I don't know that anybody should be as happy as I was about only weighing 300 pounds, but that was the first round number I hit. I was in Hong Kong and hopped on a scale which weighed me in kilograms and excitedly typed it into Google three or four times to make sure I hadn't messed up the conversion. Or there was the moment a couple of weeks ago when I went into a store in Brooklyn and tried on a medium-sized sweater. Having worn a medium at my thinnest before eventually expanding all the way to XXXL shirts by the beginning of 2015, it really meant something when the sweater fit and didn't look horrific. I started uncontrollably laughing out loud in the changing room (in part to avoid crying), which must have been incredibly confusing and a little terrifying to the tiny female employee on the other side of the door who didn't know why this lunatic was giggling as he tried on a shirt.
More than anything, though, the lesson I learned from all of this was how incredibly important is it to forgive yourself. Once I got stuck in that downward spiral of eating poorly and not taking care of myself, it was so incredibly easy to beat myself up and dismiss all hope. It's harder to escape, too, because during those brief moments where I felt motivated to actually try and change things, I'd feel like I needed to be perfect from then on to get out of my predicament as quickly as possible while I still had the, um, momentum. (This is awkward.) When I would inevitably screw up, it would feel like I'd failed a mission and had to start from the very beginning, and that would always lead to giving up altogether. At 25, I'd look back and wish I had done all this when I was 22, and then repeat the same cycle at 28.
It seems simple to say now, but it doesn't have to be that way! Now, I'm just happy I didn't wait until I was 40. It's easy to preach about sunk costs in football, but it's harder to actually recognize them in my own life. There have been days where I screwed up and ate like a shithead again. I've missed workouts. It happens. I've woken up the next day and gotten back on the wave. Even right now, I'm frustrated with myself because one of my biggest long-term goals was to get under 200 pounds by the end of 2015. I came up six pounds short. It's fine. I'll hit it sometime this month and be just as happy. Instead of being angry at myself for throwing away years of my life, I believe in myself and my ability to enact change in a way that I could not have fathomed this time last year.
Even then, I'll be far from finished. I'd like to settle somewhere between 160 and 170 pounds, and that will take a few more months. I'd like to transition from simply trying to get thinner to really focusing on trying to be healthier, which will be a more difficult process. There's the thorny issue of separating self-image and self-confidence from a number, an issue on which I'm making progress, but one that's always going to be easier as I get happier with the number.
Compared to a year ago, though, I'm in a much better place. I'm typing this in the middle seat on a cross-country flight, which would have been hours of hell for me and my seatmates years ago. My sides and hips are not spilling against and underneath the armrests. The seatbelt  one seatbelt  fits me with room to spare. I'll take the subway home from the airport and be able to sit down without being self-conscious, because I won't be afraid that I'm going to take up two seats. Happiness in those sorts of public situations is the ability to be anonymous. I don't feel any compulsion to eat something awful in the airport to tide me over during my flight or go home and order shit because I had a long day of traveling.
And honestly, I know that things are never going to be perfect, which is fine. My body's never going to look good. Right now, it looks roughly like a chubby old man was cryogenically frozen for a decade and then thrown in the microwave to thaw out. Even if I get to the number I want, it's going to be a life-long fight to actually keep that weight off and stay healthy. That's how addiction works. I haven't conquered or solved anything.
What I have managed to do is wipe away a lot of what went wrong over the last decade in the course of a year. More importantly, I feel like I've found a series of principles that I trust to keep me from falling into those same traps again. After years of being unable to get over my past, it's impossibly fulfilling to feel like I actually have control over my future.
",203
https://medium.com/@wilw/7-things-i-did-to-reboot-my-life-a4bab2d409e?source=tag_archive---------4-----------------------,7 things I did to reboot my life.,"About twenty years ago, I had a portable spa in the back yard of my first house. One day, the heater stopped working, so I called a...",Wil Wheaton,14,"About twenty years ago, I had a portable spa in the back yard of my first house. One day, the heater stopped working, so I called a repairman to come out and look at it. He told me that there would be an $85 charge no matter what, and I told him that was okay. When he got to my house, he opened up the access panel where the heater, pump, and filter lived. He looked inside, then looked back at me.
""Did you try pushing the reset button?"" He asked.
""Um. No,"" I said.
He pushed the reset button, and the heater came back to life.
""That'll be $85,"" he said. I paid him.
This post is about realizing that I was sitting in cold water, and not doing anything to turn the heater back on. This post is about how I hit the reset button.
I had this epiphany at the beginning of September: This thing that I'm doing? This series of choices I make every day? It isn't working. I don't like the way I feel, I don't like the way I look, I don't like the things I'm doing. Things need to change.
So I took a long, hard, serious look at myself, and concluded that some things needed to change.
All of these things are interconnected in ways that are probably obvious and non-obvious, and by making a commitment to do my best to accomplish these things, I've been able to do a soft reboot of my life.
The hardest part of this was not drink less beer, which surprised me. The hardest part has been writing more. The easiest part of this has been exercise more, which also surprised me. I thought watching more movies would be easy, but it turns out that time is not a renewable resource, nor can time be stretched out in any real way that lets me get, say, four hours of movie into an hour of linear time.
In fact, if I'm being honest with myself (and I have to be, because being honest with myself is the only way this is going to work; I'm making major life changes here, remember), all of these things are hard to a certain degree, and that's okay, because everything worth doing is hard.
So let's go down this list and talk about each thing a little bit:
Drink less beer.
I love beer. I mean, I really love it. I brew it, I write about it, I design recipes of my own, and I've structured entire meals around what food will pair with the beer I want to drink. The thing about beer, though, is that it's really easy to just keep on drinking it until it's all gone or your brain goes, ""um, hey, man, ithinkthat ... imean ... I mean, sorry, hold on. I th- think that youvehadneough."" Excessive drinking isn't just tough on my liver (which has been a fucking CHAMP for years), it's also tough on my brain because of my Depression. It's tough on my heart, too, it turns out, because alcohol is metabolized as sugar which drives up insulin which makes my cholesterol go up which is bad for my heart thanks a lot genetics.
So I just made a commitment to drink less, drink more responsibly, and keep the intoxicating effects of the beers I love to a minimum. The first two weeks of this were really tough because I was habitually drinking two or three beers a night, but once I got used to it and broke the habit, it became as easy as I think it's going to get (and that varies from day to day).
Read more. Write more.
These go together more closely than any other things on this list. Stephen King says that writers who don't make time to read aren't going to make time to write and holy shit is that exactly, perfectly true. I need to read so that my imagination is inspired. I need to read so I get an artistic and creative hunger that can only be fed by writing. I need to read so that I feel challenged to scrape ideas out of my skull and turn them into words and images. I need to read because if I don't, I'm not going to make time to write, and even though I've had a lot of success recently as an actor and host and Guy On The Internet, all of those things are ultimately in the hands of others. I became a writer ten years ago because I not only loved it, but because it was a way for me to express myself creatively in a way that ultimately gave me control over my own destiny and my own life.
I struggled as an actor for years (it's all in my book, Just A Geek! You should buy it!) and I was doomed to a future as a Former Child Star with occasional, humiliating, soul-crushing reality TV gigs if I was lucky. But I made a choice about twelve years ago to stop chasing the big film career I always felt I deserved (and maybe would have had, if I'd made different choices and had better advisers when I was younger) and start telling stories. By writing those stories and embracing the love I'd always had for creative writing, I made a second act in my life (take that, F. Scott Fitzgerald!).
Now this could sound like I'm complaining, or ungrateful, but I promise that isn't the case. I am totally aware that a combination of hard work, privilege, and luck have all come together for me and put me in a very good place. But I worry about things. A lot. When the night is darkest, and it seems like the sun may never rise, I worry about how long I can sustain this life. I worry about what will happen when the people who choose to hire actors decide that they don't want to hire me any more. I worry about how I'll support and provide for my family, and on and on and on.
I am profoundly and completely grateful for the success I had and continue to have, because of the hard work I did in those years. I'm so lucky and grateful for the things I've been able to do on TV, and I'm really proud of the things I've created for Geek & Sundry. But even Tabletop and Titansgrave aren't mine the same way a story I wrote is. In fact, the show I did with my name in the title was probably the least ""mine"" of anything I've ever done, and that didn't feel particularly good at the end of the day.
But writing and storytelling always feels good. It's truly mine, whether it's awesome or shit, and nobody can take it away from me. For all of this year and most of last year, I hardly wrote anything of consequence. A few blogs, a couple of columns, and some small creative things that were always well received by the audience, sure, but never consistently and never in a way that fed the creative hunger that constantly makes my stomach growl. Going all the way back to last August, I swore that I'd take more time away from other things to focus on writing and taking the pages and pages of story ideas I have in my little notebook and turning them into actual stories. The thing is, when I took that time off, my health and mana were so depleted, I couldn't find it in myself to do the work. Every few months, I'd take a week or two off, and instead of writing like I wanted to, I'd play video games and do nothing else, because I was just so goddamn tired. Then I would look up, realize a couple of weeks had passed, I hadn't done anything, and I needed to get back to ""real"" work. I would feel frustrated and empty, and the whole cycle would start all over again.
I've been reading this book called The War of Art. It gets a little churchy at points, but I can skip over that and focus on the stuff that helps me: identifying all the barriers we create to give us the excuses we crave to not create. It turns out that if I were as good at sitting down and writing as I am at coming up with all the reasons I just can't do it today, I'd have written ten novels this year.
So a big part of this Life Reset I'm doing is being honest with myself about why I want to write, why I need to write, and why I keep making excuses for not doing it. I won't go into it, because it's either too personal or too boring, but if you're reading this and starting to have the glimmer of an epiphany about your own creative process, maybe pick up this book and check it out. It's been very helpful for me.
Yesterday, I wrote over 3000 words. I only stopped because it was time to leave for a hockey game (there's that privilege again), and I felt so goddamn good about myself when I walked out my door. I couldn't wait to get back to work today, and I'm having so much fun writing this story, it may not even be fair to call it work.
Reading is an important component of this entire process for me, and making time to nourish my creative side has made a big and positive difference. Whether it's comic books, magazine articles, fiction or non-fiction, I'm reading every day, and finding inspiration in everything I read.
Watch more movies.
Thanks to Netflix, Hulu, Amazon Video, and a billion movie channels, we have access to more movies than we can ever watch, and that is amazing. Watching movies is, for me, similar to reading books. It's part homework (as an actor and writer) and part inspiration. I have the opportunity to make short films with Nerdist and Geek & Sundry, so watching movies  good movies, not disposable trash  just makes sense. It also engages my brain in creative and intellectual ways that are important. I need to watch things and feel like I can do that, too, or feel like if that thing got made, there's no reason I couldn't make one of my own things. Hell, in this room where I am right now, I have everything I need to take some sort of simple creative idea and turn it into a movie. So why not? Why not watch movies as varied as the true classics (Sunset Boulevard, Chinatown) to the weird 70s experimental (The Trip, Scorpio Rising) to the epic blockbusters (Star Wars, Jaws) and get inspired by them? This is something that I need to do, because sitting in my office all day and looking at the Internet isn't making me happier, more creative, more productive, or more inspired.
Related: listen to more podcasts like Lore, You Must Remember This, The Memory Palace, Welcome to Night Vale, The Black Tapes, and Unfictional. Those all inspire me to learn more, focus on pacing and storytelling and narrative, and make me want to be a more interesting person.
Get better sleep.
Maybe this is a strange one, but hear me out: for months  most of this year, in fact, now that I think of it  I haven't slept well. I've had the hardest time, ever, falling asleep and staying asleep. I've had nightmares that terrify me from the moment I finally fall asleep, well into the following day as they haunt my memories. I've lived with the constant dread of waking up in a panic attack, and then waking up in a panic attack. It's been terrible, and has contributed tremendously to my inability to get motivated, my reliance on alcohol to relax and go to sleep at night, and a general feeling of exhaustion, laziness, pessimism, and futility. I talked with my psychiatrist about it, and we tried sleep medication, which only made my body feel tired, while my brain was still racing along at the speed of terror. After several different attempts and failures, we tried changing up my brain meds, and about six weeks ago, I finally started feeling like a human again. I won't go into the deep science of it, but we think I just had too much of some chemicals in my brain, and not enough of others. And it turns out that drinking alcohol to help you go to sleep does not result in good sleep, but does result in feeling like shit when you wake up.
So by committing to getting better sleep, I have changed up my evening and nighttime routines. I eat earlier, I drink less, I read more, and I've been able to slowly adjust my circadian rhythm from wanting to sleep at 2am to wanting to sleep around 11pm. I've used some great apps on my Droid to help me monitor things and determine what patterns work best for me, and while it's still a work in progress  this entire thing is a work in progress, actually  it's definitely getting better. When I don't have nightmares all goddamn night, I'm more rested when I wake up. When I'm rested, I feel better in every aspect of my being. When I feel better, I am more creative and more willing to allow myself to take the risk of feeling good about myself.
Isn't that strange? It's a thing that I do, that I've done for my whole life: I don't want to take the risk of feeling good about myself, because I'm afraid that I'll get complacent, or arrogant, or someone will discover the Truth that my Depression tells me: I'm not that great and I don't deserve to feel good about myself. I'm reading another book, called Trapped in the Mirror, that's really helping me get through that, though.
So I take the risk of feeling good about myself, and more often than not, I actually do feel good about myself.
Eat better.
We all say we're going to do this, and we always make excuses for why we don't. Now I've been very lucky. My whole life, I've been able to stay at a healthy weight and body type without much effort ... then I turned 40. In the last couple of years, I gained almost 30 pounds, my cholesterol went through the roof, and my whole body began to hurt, all the time.
There's no excuse or rationalizing, here. I just wasn't taking care of myself. I wasn't eating right. I wasn't getting good nutrition. I wasn't thinking of food as fuel and nutrients. I wasn't making an effort to be smart about this, because I didn't particularly care about myself. I didn't really like myself, I didn't feel like I could do anything about getting tubby and slow and out of shape, because I was just getting older and that's the way it was.
That's all bullshit. Here's the thing, and this is pretty much the whole reason I made a choice almost three months ago to hit this reset button and really get my life together: I didn't like myself. I didn't care about myself. There's a bit more to it, but that's the two things that exist the most clearly and form the root of the last few years of my life. In fact, it took my wife, who is the most important person in my whole universe, telling me, ""I feel like you don't care about having a long life together with me, because you don't care about taking care of yourself.""
Well, that was ridiculous. Of course I cared about having a long life with her! Of course I cared ... didn't I?
I cared about her, and I cared about our kids, but I didn't really care about myself. And my Depression was doing a really good job of making sure that I didn't really think about my choices and my actions affected the people I love and care about the most in this world. That was what made me look up, take my head out of the darkness, and commit to doing all of these things.
It turns out that eating well, consistently, isn't as easy as just eating what I want, when I want. It turns out that it's really hard to break out of bad habits, but it also turns out that, once I made the commitment to do that, I started feeling better almost immediately. I got an app that makes it easy to track my nutrition, calories, and exercise, and in a very short amount of time, I lost almost all the weight I'd gained. I'd still like to lose five more pounds, and that five pounds is really keen on sticking around, but it's slowly coming off, it's staying off, and it feels great. Also, having a digital scale that lets me track precisely how much I just pooped is probably the pinnacle of technology in the 21st century.
And a super bonus that comes with this? My body doesn't hurt like it did, because I'm not schlepping around extra weight. My joints are healthy, my muscles are stronger, and my skeleton is just in better shape, and that leads me into the last part of this.
Exercise regulary.
My son, Nolan, is a personal trainer. He's in phenomenal shape, looks like Thor (for reals), and trains both me and his mom. So I should be in great shape, right?
Well, if I'd stuck to a good diet, taken better care of myself, and felt like I actually deserved to feel good about myself, then yes. But as we've seen, that just wasn't happening. I worked out a couple of times a week, I did good, hard workouts, but then I'd feel sore and tired and wasn't getting the results I wanted (mostly due to poor diet) and I'd come up with reasons not to do it. Then I'd make the mistake of seeing myself in the mirror when I got out of the shower, and I'd really hate myself.
I think that lifting weights and doing inside stuff just isn't for me. I get bored easily, and I don't fully participate in the workouts. But I love running. I used to run marathons and 5Ks all the time, and my entire self felt amazing when I did. So why not give running a try again? The excuses wrote themselves.
I was working on Con Man with Sean Astin. Sean told me about the Ironman Triathlon he was training for, and I complained that I couldn't run anymore, because my body always hurt. Sean talked to me about the diet aspect of it, and then suggested that I use a method of mixing running with walking to work myself back into shape.
So I bought an app called Zombies, Run 5K for my phone. It uses recording and all the various sensors in my phone to let me imagine that I'm a runner in the zombie apocalypse. While I am being trained by a doctor, I listen to my music (Taylor Swift and Tove Lo accompany me on most of my runs) and a story unfolds in my ears. I'm three weeks into the training, and while it's starting to ramp up from challenging to difficult, it feels great. I look forward to every run, I challenge myself to go as hard and as long as I can (that's what she said) and the benefits to my self-esteem, my physical and mental health, and overall quality of life are incredible. If I was looking to quantify the value of this, it would probably be like spending five bucks to get ten thousand dollars of awesome. (Does that make sense? I've been writing this for a long time and the words are starting to get blurry on the screen).
I deserve to be happy. I deserve to feel good about myself. I can do the work that I need to do to accomplish these things.
If I tried to do just one of these things, and I stayed committed to it, I'd probably feel better about myself. Doing all of them together isn't necessarily easier, but it isn't necessarily harder, either. Every one of these things supports the other in a sparkling geometric structure of awesome that is making my life significantly and consistently better.
I just did a word count on this, and I'm at nearly 3500 words. I didn't intend to write this much, and I suspect that most people will tl;dr it, but I'm glad that I spent the time thinking about these things and writing them down like this. I'm sure I'll revisit this, rewrite it, and revise it, but I'm going to publish it now without a lot of editing. This is raw, and I feel like it's supposed to be.
",204
https://medium.com/@shilpa_sn/how-i-lost-10kg-in-60-days-my-7-step-weight-loss-plan-ecd6c566c9ff?source=tag_archive---------4-----------------------,How I lost 10kg in 60 days: My 7-step weight loss plan,"When I turned 26 (nov 05) last year, I looked at the mirror and didn't recognize the person staring back at me. There and then, I decided I...",Shilpa S Nath,10,"When I turned 26 (nov 05) last year, I looked at the mirror and didn't recognize the person staring back at me. There and then, I decided I have had enough. Enough of wallowing in self-pity, enough of not staying committed to my fitness goals, enough of being disappointed in myself, and decided to be the change I wanted for myself. This is just the beginning of a life long journey and I am already excited for the next leg of it. #workit PS: yea, that's a captain America shirt and when civil war comes out, I will probably be on his team 
I shared my progress on Facebook last night, and some friends wanted to find out what I had done to lose weight. This post is just my experience over the last 60 days.
So here is my 7-step weight loss plan!
Being a mesomorph is one of the main reasons I am able to lose weight easily. The reverse is also true, I can gain up to 2kg in a week if I remain inactive. To maintain my weight, I need 20mins of moderate workout at least 4-5 times a week.
It is important to monitor your body fat percentage, and not just focus on how much weight you are losing. Weight loss can result from many reasons, water loss, muscle degradation, and you want to be sure that you are gaining muscles and losing body fats.
Calculate your body fat percentage here: http://www.bodybuilding.com/fun/becker11.htm
The amount of energy (in the form of calories) that the body needs to function while resting for 24 hours is known as the basal metabolic rate, or BMR. This number of calories reflects how much energy your body requires to support vital body functions if, hypothetically, you were resting in bed for an entire day. In fact, your BMR is the single largest component (upwards of 60 percent) of your total energy burned each day.While you can't magically change your BMR right away, knowing your personal number, how it's calculated, and which factors most influence your metabolism, can help you use this data point to create a smarter strategy for weight loss (or maintenance).
Check your basal metabolic rate here: http://dailyburn.com/life/health/how-to-calculate-bmr/
Calorie mindfulness: In order to lose weight, you need to have slightly less calories going in than you need, so your body will tap into the reserve energy stored in your body. Be sure, you don't starve yourself. This will result in your body going to ""starvation mode"" and it will do everything to hold on to the fats in your body because it thinks you will not be able to get nutrients on time. Losing weight is possible when you eat right. This will come in handy when you move on to the stage of determining your diet.
Check your optimal calorie intake for weight loss: http://www.calculator.net/calorie-calculator.html
You need to set goals for yourself. This can be intimidating. I had a total weight loss goal of 21kg, and wanted to loose 6"" off my hips (I started at 41"" when I came back from my holiday). This can seem very difficult and almost impossible to achieve.
I broke down my goals into mini goals, so that I would feel motivated every time I reached one of my mini goals. This kept me going especially during periods when I would hit my plateau.
Personally, my plateau points were 72kg and 69kg. The next plateau point I will reach is at 65kg. This means that for 1-2 weeks, I will not see any change in my weight. Don't freak out if this happens. Sometimes your weight might even go up. If you have been eating healthy, and consistently working out, this just means that your body is putting on muscle and soon you will notice the fats melting away. Everytime I break my plateau points, I lose another 1-1.5kg in 1 week.
Mental preparation for weight loss regime is critical for maintaining this as a lifestyle rather than just a phase that you go through to get a ""summer body."" You need to believe that you can shed the weight and this should be supported by why are you doing this. Is it to just look good for a holiday or are you making a commitment for life? This way, you won't be yo-yoing between your weight, which itself is bad for your body.
Hang on! We are halfway through the plan and we aren't even at the workout plan? There is a simple explanation for this, that you have to remember.
Losing weight is a strategic game plan. You need to have a plan in mind before you start hitting the gym aimlessly. Sure, you will lose some weight, but will you be able to keep it off? 80% of your body is shaped in the kitchen, and only 20% of is dependent on your workout itself.
So what does this mean? It simply means that you have to be mindful of your diet. Knowing more about your food, how it is being prepared, and the nutritional values of what you are putting into your mouth will help you change the way you eat.
I used to eat everything because I am such a food lover. I still eat what I love but in moderation. Previously, I used to have chocolates on a daily basis, and now I only have it once a week maximum.
I spent a lot of time researching on diets that will help me optimise my weight loss. I had done something similar in 2014, when I was even more of a health nut so I had a head start this time.
Start with small, gradual changes to your habits. This will help you commit to the new changes. Diving headfirst into a complete overhaul can be a shock to your system and you are more likely to switch back to your old habits. I enjoyed going onto pinterest and discovering new healthy recipes to try out.
My food plan breakdown:
The benefits of the alkaline diet are said to include higher energy levels, fat loss, increased concentration and clearer skin.
I love to work out. I love how I feel when I am doing weights and I can feel myself getting stronger.Every time I am able to lift a heavier weight, or increase my reps from the last time I did it, I would get an euphoric high that is kind of addictive.
Biggest myth that women must overcome: Using weights will make you into a ripped out hulk!
Girls, go pick the weights, the barbells, the kettle bells! You will not pack on muscles like a ripped body builder. We don't have enough testosterone to develop overly bulging muscles. You can lose fats faster when you have more muscles because they are the ones burning the fats in your body.
""You need to exercise at full intensity because the end goal is to burn more calories, and high intensity exercise does just that,"" says Natalie Jill, a San Diego, Calif.-based certified personal trainer. High intensity workouts mean you're going all out for as long as you can. If this sounds intimidating, think of it this way: You'll burn more calories in less time.
I usually go to pinterest to give me inspiration. I have saved all the workouts that I do here on Pinterest. When I started going back to the gym in November, I started with just 20mins in the gym. Sticking to going to the gym 3 times a week, working out with HIIT trainings helped to rev up my stamina and got me back into the headspace I needed to commit to the change.
""You need to do a combination of weights and cardiovascular training,"" says Sangeeta Kashyap, MD, an endocrinologist at Cleveland Clinic. Strength training increases muscle mass, which sets your body up to burn more fat. ""Muscle burns more calories than fat, and therefore you naturally burn more calories throughout the day by having more muscle,"" says Kate Patton, a registered dietitian at Cleveland Clinic. Patton recommends 250 minutes of moderate-intensity exercise or 125 minutes of high-intensity exercise a week.
On days that I don't have time to commute to my gym, I will work out from home. I have saved a list of workout videos that you can mix and match to create your own personal workout playlist.
You just have to commit to coming back 2-3 times a week,and soon you will find yourself doing more. If you are a beginner to working out, it would be better to join classes with a trainer or have a 1-on-1 trainer.
Form is everything in exercising. If you do it wrong, you are more prone to injury and your workout wouldn't be efficient anyway. I have worked with personal trainers who have guided me in getting my form right so I am able to work out without much guidance.
When it comes to workouts, you should do activities that you love. Because this will make you happy to do it, and you will look forward to the activity so you will keep going back. Think about it, aren't you more likely to do something that you love rather than something that you hate? Our subconscious mind is built to drive us towards pleasure. So try out different activities and do what gives you most happiness.
My weekly routine:
I plan to incorporate more HIIT trainings moving forward because they are amazing fat blasters!
This is a lifelong commitment. It's not a fad. So give yourself a break. When you are out, and you see a cake that you really really want to have, buy it. Share it. Or eat it by yourself. Research has shown that if you constantly deny yourself, you are bound to binge when you finally get what you want. So just savour the moment. There is no such thing as a ""guilty pleasure."" Just ""pleasure."" And you deserve it!
",205
https://medium.com/the-monthly/the-foo-fighters-aids-denialism-should-be-on-the-record-6e33666fdc3c?source=tag_archive---------8-----------------------,The Foo Fighters' AIDS denialism should be on the record,By Martin McKenzie-Murray in The Monthly,Schwartz Media,5,"By Martin McKenzie-Murray in The Monthly
A few weeks ago, coincidentally about the time the Foo Fighters were preparing to release a new album, a shaky video shot on a smartphone went viral. It showed a young British doctor intervening in a depressingly misguided rescue mission: relatives of an elderly man, suffering severe COVID-related pneumonia, had come to bust him out of hospital.
Why? As they aggressively and incoherently quiz the doctor, it becomes clear that they believe the virus is a hoax  even as their relative sits gasping on the bed. Then they remove his oxygen mask.
""You've taken his oxygen off!"" the doctor says, incredulous. ""He's going to die if we don't put it back on.""
""No, I'm not,"" the old man says.
""Yes, you are.""
Security guards are summoned, while the family protests  victims, they think, of a vast and malevolent hoax.
So what does this have to do with the Foo Fighters? Bear with me.
At 52, the Foo's Dave Grohl is rock's cool uncle, smiling evangelist and plain-spoken philosopher. Gregarious and media savvy, he recently engaged 10-year-old drumming prodigy Nandi Bushell in a cute online duel, and wrote a love letter to live music in the Atlantic. He jams with Paul McCartney, makes docos about recording studios, sent a hopeful message to the trapped miners of Beaconsfield and, even if his principal band have made insipid pap for the past two decades, is assured street cred by having written ""Everlong"" and bashed the skins for Nirvana. Everyone, it seems, loves Dave.
""When Kurt died, I had this whole new outlook at life, that we're all so lucky to be here,"" Grohl told Esquire in 2014. ""You can't take life for granted. It's short. It's fragile. And you don't know when it's going to be taken away from you. So the short time that you're here? You just have to kick ass the whole time and not look back.""
This heroic refusal to look back would seem to include not looking back upon his band's very public support for Alive and Well AIDS Alternatives, an AIDS denial group that argued HIV did not cause AIDS  and that HIV was, in fact, harmless and non-contagious. Alive and Well argued that AIDS was likely caused by recreational drug use and/or the very antiretroviral drugs used to treat HIV patients, and as such it discouraged people from taking HIV tests, practicing safe sex, or, for those living with HIV, accepting medication. In the late 1990s, its director was personally received by South African President Thabo Mbeki, a man who also denied the link between HIV and AIDS and, by banning antiretroviral drugs in public hospitals, is credibly blamed for hundreds of thousands of deaths.
Alive and Well was not your usual celebrity charity then, but it was nonetheless amplified by one of the biggest bands in the world. In early 2000, President Clinton's director of AIDS policy admonished them: ""For the Foo Fighters to be promoting this is extraordinarily irresponsible behaviour. There is no doubt about the link between HIV and AIDS in the respected scientific community and it's quite unfortunate that a band reads one book and then adopts this theory. To say [that HIV does not cause AIDS] is akin to saying the world is flat.""
That ""one book"" was What If Everything You Thought You Knew About AIDS Was Wrong?  self-published pseudo-science written by Alive and Well's founder, Christine Maggiore, a woman diagnosed with HIV in the early '90s  and it fell into the idle hands of the Foo Fighters' bassist, Nate Mendel. After devouring it, Mendel conscripted his bandmates in his advocacy for Maggiore's group.
The alliance lasted years. In January 2000, the group held a benefit concert for Alive and Well. ""I'm living in perfect health without any AIDS medicines,"" Maggiore told the cheering crowd between songs. For years, the band's website featured Alive and Well banners, and listed them as a favoured charity.
When Mother Jones ran an article critical of Mendel in 2000, the musician responded with an indignant letter: ""The story takes a decidedly derisive view of our efforts ... I am not a medical professional, and I am relatively new to these questions, but I am convinced that those who have tested HIV positive and those sick with AIDS are being done a disservice by not having all the information available to them.""
The founder of Alive and Well is long dead. So is her daughter. Both succumbed to AIDS. Christine Maggiore fell pregnant in 2001, spurned antiretroviral drugs, breastfed her daughter and refused to have her tested for the virus. As her child became increasingly ill, Maggiore attributed her sickness to other things and sought quacks to reaffirm her delusions. Eliza was three when she died  after an autopsy, a coroner ruled that the ""unequivocal"" cause of her death was AIDS-related Pneumocystis pneumonia.
The death of her daughter did not free Maggiore from her destructive theories  or the Foo Fighters from their endorsement of them. Instead, Maggiore doubled down. She accused the coroner of political bias and continued to refuse treatment for herself. She died three years after her daughter, of pneumonia as well, and we are left with the very essence of tragedy: a denial of extreme and self-annihilating sophistication.
You might say that this is old news, except that it was never really news in the first place  and there's almost no cultural memory of it now. Regarding the band's destructive foolishness, the internet resembles a hastily cleaned crime scene. Professionally rinsed, however, are Paul Brannigan's ""in-depth"" and best-selling 2011 biography of Grohl, and Mick Wall's 2015 band biography Learning to Fly. Many years of interviews and magazine profiles  accounting for hundreds of thousands of words  have never referenced it. A 5000-word profile of Grohl in Rolling Stone in 2017 refers to the ""crazy"" early years of the band, which included the 2001 overdose and subsequent coma of its drummer, Taylor Hawkins, but the band's long and destructive support of HIV denialism is, typically, never mentioned.
There's a dark irony here, or perhaps just ghastly resonance: Grohl personally knows the harm of conspiracy theories. For decades, the suicide of Grohl's bandmate Kurt Cobain has been loudly reinterpreted by twisted hyper-fans as a murder coordinated by Cobain's wife, Courtney Love  a baseless claim that has greatly compounded suffering.
""I'm absolutely confident that I'm doing the right thing,"" Nate Mendel once said. ""No, I wouldn't feel responsible for possibly harming somebody. I [feel] I'm doing the opposite.""
Misinformation kills. It impairs health campaigns and incites fatal insurrections. It erodes public faith in its institutions and emboldens militancy and extremism. It encourages families to remove their grievously sick from hospital. The Foo Fighters' amplification of fatal misinformation should be on the record, as should the band's silent and cowardly retreat from it, a successful vanishing act that was enabled by a vapid and transactional music press that cherishes access  however qualified  above all else. Grohl loves talking about rock's power of soulful confession, but on this there's only silence.
If you enjoy reading our articles on Medium, consider subscribing to unlock full access to The Monthly archive and the best in politics and culture each week.
Originally published at https://www.themonthly.com.au on February 19, 2021.
",206
https://historyofyesterday.com/the-uncut-history-of-male-circumcision-f5f86b33fa16?source=tag_archive---------9-----------------------,The Uncut History of Male Circumcision,Why is cutting the foreskin off of a penis a thing?,Ben Kageyama,5,"The male genitalia comes in all forms, shapes, and sizes. There are fat penises, tiny weenies, thin dicks, and tall rods. But one fascinating variety (from a strictly historical perspective) is the circumcised penis.
Male circumcision is an ancient practice from various cultures that has survived to this day. It is the surgical removal of the foreskin from the penis, with some anesthesia applied to relieve the pain and stress. It's estimated that around one-third of all men are circumcised for various reasons  some men for religious or cultural, others for medical reasons.
There are many theories about how male circumcision began  none of them absolute. Some have suggested that it started as a rite of passage marking the transition from boyhood to manhood. Others had posited that it was a way to reduce and control sexual pleasure, as well as aid hygiene in a time when bathing was irregular and often impractical.
Ancient Egyptian records point to practicing circumcision for all of the above-stated reasons. The earliest recorded form of male circumcision is dated around the 5th century BCE, when the Greek historian Herodotus noted that the Egyptians ""practice circumcision for the sake of cleanliness, considering it better to be cleanly than comely.""
Performed primarily on the upper classes and those seeking to enter the priesthood, it was said that circumcision allowed men to access and witness certain mysteries. It was one of their many sexually rooted traditions.
While the Greeks shunned the practice and considered it mutilation of a ""perfect organ,"" the Egyptians, Phoenicians, Ethiopians, and Jews continued to perform circumcisions. Gradually, the Greeks' distaste for circumcision caused other countries to follow suit, and by the 2nd century, the practice was confined largely to Jews, Jewish Christians, Nabatean Arabs, and Egyptian priests.
The Jewish practice of circumcision has religious significance. In the Book of Genesis, God commanded the prophet Abraham to circumcise himself and all the men in his household as part of an ""everlasting covenant."" To this day, modern Jewish theologians believe that the cutting was symbolic of the covenant's sealing. After all, the Hebrew term ""karat berit,"" which means ""sealing a covenant,"" is literally translated into ""cutting a covenant.""
Christians who previously adopted the practice began to take a negative view of circumcision. St. Paul, also known as Paul the Apostle, appeared to praise Jewish circumcision in Romans 3:2. He then shifted to indifference towards the practice in 1 Corinthians 7:19, before turning to condemnation in Galatians 6:11-13 and saying outright to Christians to ""beware the mutilation.""
As a result, most Europeans  except for Jews  were uncircumcised. But by the 20th century, circumcision grew in popularity once more due to medical rather than religious reasons.
Around the late 1800s, circumcision was redefined as a medical rather than a religious practice. From the said period, medical journals called it a procedure ""done as a preventive measure in the infant"" and ""performed chiefly for purposes of cleanliness.""
This redefinition fit well with the conservative views of the time. The medical community started to believe that circumcision could reduce the risk of getting sexually transmitted diseases. It was also said to help curb the urge to masturbate  an ""unfavored practice"" for such a mainly traditional society.
Nineteenth-century doctor Jonathan Hutchinson can be credited for circumcision's sudden acceptance as a medical procedure. The British physician advocated its practice, publishing a study where he compared the incidence of venereal disease in London's Jewish and gentile (non-Jewish) communities. While it's more likely that the Jewish community's lower incidence was due to cultural factors, the study asserted that circumcision made you less vulnerable to sexually transmitted diseases.
Calling the foreskin a ""harbor for filth"" and claiming it increased the risk of contracting syphilis or getting cancer in old age, Hutchinson advocated circumcision as a necessary health measure. Another British doctor, Nathaniel Heckford, even claimed that it would help prevent epilepsy and chorea cases.
These largely unfounded claims were even applied to women, with doctor Isaac Baker Brown controversially advocating clitoridectomies  the removal of the clitoris  as providing similar health benefits. He performed numerous clitoridectomies without his patients' consent, causing him to be expelled from the Obstetrical Society of London. Nevertheless, his ideas became popular in America, where clitoridectomies were used to ""cure"" hysteria, nymphomania, and otherwise ""unfeminine"" behavior.
Doctors increasingly and disturbingly began to try circumcision as a treatment for all kinds of diseases. Asthma, insanity, skin cancer, and bladder stones were just some of the illnesses they linked to the foreskin. They attributed this to the smegma that collected under the foreskin. It was seen as an unhealthy, dirty waste product, with circumcision a surgical practice that promoted good penile hygiene.
By the 1920s, many of these baseless claims were debunked. However, circumcision is still seen today as a proper corrective surgery for certain penile conditions, such as phimosis. With this condition, the foreskin can't be stretched to pull it back from the tip of the penis, making erections painful  and circumcision is a quick solution to this medical problem. It's also done to prevent the inflammation and infection of the penis tip and foreskin.
Are there really any health benefits to circumcision apart from those special conditions? Some studies say there are in order to prevent urinary tract infection for infants.
But having things cut definitely won't do anything about your asthma, and there's no evidence whatsoever that it can curb masturbation. So, at least historically, the presence or absence of foreskin won't make your wang that different from anyone else's.
From the times that the pyramids were raised to the end of...
2.5K 
51
2.5K claps
2.5K 
",207
https://medium.com/personal-growth/10-things-you-can-do-this-morning-to-heal-your-anxiety-d1e320aef4b1?source=tag_archive---------6-----------------------,10 Things You Can Do This Morning To Heal Your Anxiety,"For the past 33 years, I have looked in the mirror every morning and asked myself: 'If today were the last day of my life, would I want to...",Benny Glick,14,"For the past 33 years, I have looked in the mirror every morning and asked myself: 'If today were the last day of my life, would I want to do what I am about to do today?' And whenever the answer has been 'No' for too many days in a row, I know I need to change something.""
-Steve Jobs
If you are anything like I used to be, you probably put mornings right up there with death on the scale of things you most dread. I used to wake up in a fog, feeling just as tired as when I went to bed, and immediately I began to fear the inevitable feeling that creeps into your stomach and throat...
...here it is again, I would think to myself as I pulled the covers over my head. Fearing that this is how I would feel every single morning for the rest of my life. No confidence that I could ever make myself feel better.
I had always heard about how the most successful people like Gary Vaynerchuk, Tim Ferriss, and Ben Franklin had daily, morning routines that helped them wake up energized and ready to take on the day. But in the midst of my anxiety, the thought of waking up at 4 am to hustle was the last thing on my mind. I needed to find a way to model the morning routines of the most successful people in business in a way that was focused on decreasing my stress & anxiety.
I realized the ""morning routine"" I used to follow was setting me up for increased anxiety and failure throughout my day.
I would wake up. Snooze my alarm a couple of times. Then finally open my eyes to face the day. Immediately, I would check all of my ""socials"" to assure myself that I had not missed anything. After realizing that no one had liked me throughout the night, I would get out of bed, take a shower, down a cup of coffee, and run off to work.
No structure. No purpose. No peace.
It was no wonder why I would arrive to work completely stressed out. I was setting myself up for failure by not being intentional with how I operated my day.
After reading an incredible article by Benjamin P. Hardy on Medium, I decided to finally try out a morning routine. My morning routine would be focused solely on lowering my anxiety, instead of increasing it.
Because It Can Massively Reduce Anxiety
When I implemented an effective morning routine and stuck to it, I was able to massively reduce my anxiety. In addition, it also helped with my productivity, energy, relationships, and a host of other areas of my life. It truly was life-changing.
I wish there had been a list like this when I was first battling with anxiety during my senior year of college. On most nights I was going to bed after 2. I would procrastinate anything that felt remotely difficult in fear it may trigger more panic. And my mornings were always a nightmare. It was my daily reminder of that I had a problem.
However, something happened once I began working on my personal development, I started to view my mornings as a gift rather than a nightmare. I begin to trigger my brain, from the moment I first opened my eyes, to see the beauty in my life.
Creating a morning routine has been the single most important strategy for lowering my stress and anxiety that I have implemented over the last year. It has allowed me to get more done than I ever thought possible, while also helping to keep me grounded throughout the day.
Once you begin starting your day off on a positive, structured, and intentional note, you will be amazed by the reduction in your anxiety as well as the other benefits you will receive.
With this short morning routine, your anxiety will start to drop and your life will become enhanced.
Let's Do This.
According to Hannah Hepworth, an expert on natural anxiety relief, ""when you wake up early you can have plenty of time to get where you need to go. Instead of rushing and yelling...you can work calmly.""
Getting up early to simply ""crush it"" is not a good idea. But when you wake up early you have more time to focus on things like self-care and reading that you will often not focus on if you are rushing out of the door every single morning.
Start small and build up.
The reason making your bed is so powerful is that it allows you to successfully complete a task first thing in the morning, which then builds momentum to continue doing more for the rest of the day.
Making my bed has taught me that how you do anything will be how you do everything. No matter how bad or stressful your day becomes, you can always make your bed. And if that is all that you complete in the day; it is still a success.
Growing up I hated making my bed, now I relish it. In fact, I sometimes get upset when my fiance makes it before I have a chance to do it.
Like some of you, I used to be very skeptical of meditation. Just the word itself has an aura of incense and ommming. I didn't want to lose my edge or be one of those hippies sitting cross-legged playing the banjo and singing Kumbaya. But as I dug deeper into the research, I found that mindfulness meditation (devoid of religious ties) can have massive, positive effects on your brain and help decrease your anxiety and depression, substantially.
According to an article written by the Harvard Medical School, ""Mindfulness meditation can help ease psychological stresses like anxiety, depression, and pain.""
Well...if Harvard said it worked, I better try it. So, I gave it a shot using the free 10-day trial from Headspace. And the results have been amazing. I started to feel calmer and had more clarity in my thoughts and emotions after five days. However, It took me several months to make this a recurring habit, but now that it is a staple in my morning routine, the benefits have been less anxiety, more clarity in business decisions, and overall increased happiness.
I have furthered my exploration into mindfulness and meditation and it has brought so much more depth and healing into my life. By being present to the moment you start to wake up to how much chatter you have going on in your head on a day-to-day basis.
The goal is not to stop these thoughts but rather to be aware of them and act on them skillfully. When you can do that it changes your life.
Best Resources on Getting Started:
Headspace
Waking Up By Sam Harris
Disclaimer: This sounds awful. In fact, when I first heard about it I didn't try it for months because I didn't think it could help and I loathed the idea of taking a freezing, cold shower. However, after 3-months of consistently taking a cold shower every morning, I can ensure you the benefits are enormous.
The science behind cold exposure is not new science. Cold shower therapy is an ancient Ayurvedic remedy that has numerous health benefits such as treating anxiety and depression, improving circulation and toning skin. The use of coldness as a 'good stressor' on the body can help to trigger several helpful responses within the human body. It allows the controlled elicitation of the body's natural cell repairing, pain & inflammation-reducing and metabolic processes.
A study by Researcher Nikolai Shevchuk of the Department of Radiation Oncology at Virginia Commonwealth University School of Medicine found that cold showers can alleviate, and even prevent depression and anxiety. Shevchuk makes the claim that short, cold showers may stimulate the locus ceruleous, or ""blue spot,"" which is the brain's primary source of noradrenaline  a biochemical that could help mediate depression and anxiety. The body is stressed by a hostile factor  in this case, icy water  that stimulates a healing response in the body and can lead to lower levels of anxiety and depression as well as a plethora of other benefits.
The easiest recipe to get the psychological lift is by taking a cold shower for 2 to 3 minutes once or twice daily, preceded by a five-minute gradual adaptation to the temperature (i.e. start your shower hot and then finish it with 2-3 minutes of pure icy goodness). Only taking a cold shower can strengthen your body's parasympathetic and sympathetic nervous systems, increase proper circulation of blood through your body, and contract your muscles to eliminate toxins and poisonous wastes.
Cold Therapy Experts:
Dr. Ronda Patrick
I wrote a full post on the benefits of cold showers. Check it out here.
Until recently, my mornings would always start with a cup of coffee. I have been consuming the beverage ever since I can remember, and I never wanted to lose that, but there came a point where I knew that healing my anxiety was much more important than the benefits of coffee. So I went cold turkey.
However, I needed to replace this habit with another, more helpful one. I decided that an early morning thirty-minute walk would be an amazing way to start the day. Not only does it get me out of the house and allow me to enjoy nature, it also helps increase blood flow and mood.
Some of the most famous men and women have been known to be huge meanderers (I think I made that up, but that's okay). I have been extremely pleased with this new update to my routine.
Give it a try, but leave your phone at home. Just embrace your surrounds and be grateful that you woke up this morning.
""Could bitching and moaning on paper for 5 minutes each day change your life? As crazy as it may seem, I believe the answer is yes."" -Tim Ferriss
Are they all crazy? How am I supposed to find the time to write out my thoughts every morning? I am not a writer. How could writing down my anxious ruminations help me overcome anxiety?
That was my initial barrage of fears when I first heard about the power of journaling. And if you are not someone who is already journaling for growth, your reaction is going to be the same.
But I am happy to say that I was dead wrong
Over the past 12-months, journaling has been one of the four cornerstone habits (the other being meditation, exercise, and healthy
diet) that I have implemented in my daily routine that has changed my life.
For me, the purpose of journaling is to create a vessel for clarity and resilience. A mode of transportation that takes my thoughts from anxious ruminations to empowered actions. It is an amazing way to trap your thoughts on paper and give you a heightened view of your internal dialogue; not a hack that will generate wealth and success simply by writing about it.
Some of the ways it has changed my life:
The Two Main Reasons I Journal:
1. Brain Clarity
2. Detachment from thoughts
See my full write-up on how journaling can heal your anxiety here.
I write the three things I am most grateful for today:
The key here is not to repeat that you are grateful for your family, life, and god. The key is to focus on being aware of the smaller things in life that you would miss if you were gone. This is a very powerful practice that has been utilized by the Stoics, Billionaires, and monks to help them appreciate life and reduce anxiety. Dr. Emmons, a gratitude researcher, confirms that practicing gratitude daily can reduce anxiety and depression.
The three ""topics"" I find easiest to channel are:
1. Person  I write one thing about my fiance that I am grateful for every morning. But it could be anyone for you and probably helps if you change it every day to realize how many people you are grateful for.
2. Small object close by  The wind blowing on your face, the warmth of the coffee mug, the silence of your bedroom. This is a stoic practice to realize that even if everything you owned was taken from you there are still small pleasures in life.
3. Something I would miss if it were gone  running water, heat, the ability to run, etc.
1. Affirmation: By stating three affirmations in the morning I am able to put myself into a charged state. It may seem hokey, but it has been hugely beneficial to my mental state.
2. What do I get to enjoy today  By starting my day thinking about what I get to enjoy today, I put my mind into a positive mode and trigger my brain to see the upside of the day.
3. Daily Intention  I start out each day with intention. Whether it's as simple as ""I will be present today"" or ""I will choose to see the beauty in everything that happens to me today."" It doesn't really matter, but I have found it extremely helpful for lowering my daily anxiety to be intentional about what I want my day's purpose to be,
Even a basic plan of attack for your day can drastically reduce your anxiety by decreasing the cognitive load that comes with increased decision making. Each morning we wake up with a finite amount of brainpower and every decision we make detracts from it. By having a basic structure that decreases the number of decisions you have to make about what you are going to do next, you will be able to take control of your day and calm your restless mind.""
Every morning, I write down the 3-5 things that are making me the most anxious or stressed out. They tend to be things that I have pushed off for days on end. And more often than not, they are the most difficult or uncomfortable tasks that I need to do in order to move forward.
Once I have written out the 3-5 MOST important tasks  and no more  I ask myself the following questions to help me prioritize which to focus on first:
1. What task, if completed successfully, will make all of the others obsolete?
2. What task do I have the most anxiety/fear about?
3. What task will move me closest to accomplishing my number 1 goal?
""If it's your job to eat a frog, it's best to do it first thing in the morning. And If it's your job to eat two frogs, it's best to eat the biggest one first.""  Mark Twain
Once I have prioritized my top MIT for the day, I block off 90 minutes of time to focus exclusively on it. This is a common strategy known as the 90-90-1 rule.
Why is it so important to get the top MIT done first thing in the morning?
Well, according to psychologist Ron Friedman, the first three hours of your day are your most precious for maximized productivity. ""Typically, we have a window of about three hours where we're focused. We're able to have some strong contributions regarding planning, regarding thinking, regarding speaking well,"" Friedman told Harvard Business Review.
I understand we all have different schedules and responsibilities, but if we want to overcome anxiety and move our goals forward truly we must protect our mornings. If we don't take control of our mornings, something else will.
Don't check your email or social media until you have spent at least 30-90 minutes of uninterrupted time on your number 1 MIT. Use your mornings for output, not more meaningless input. As productivity expert Benjamin Hardy says, ""[p]rotecting your mornings means you are unreachable during certain hours. Only in the case of serious emergency can you be summoned from your focus-cave.""
Not only will this help you complete your number 1 MIT, but it will also do wonders for your anxiety throughout the day.
The purpose of this list is to be a roadmap, a blueprint, to help you reduce your anxiety by creating an empowering morning routine. It is packed with strategies and loads of scientific research about what routines and habits are the most effective to lower your anxiety and increase your life. But the most important strategy of all is to remember that when you are creating your own confident morning it needs to be sustainable for you. The best routine is the one you actually stick to.
If you are anything like I used to be, you need to listen to this part. I used to spend so much time worrying about the ""perfect"" morning routine. Should I wake up at 5:45 or 6:00. Should I meditate before or after, I shower? Should I exercise or not exercise? This constant need to be perfect kept my mornings stressful and prevented me from forming habits with my morning.
So, my hope is that you try out some of the ones that worked wonders for decreasing my anxiety and test if they make a difference for you. If they don't then drop them and try something else. The key is to keep testing what works and what doesn't for you and your life. The more you test; the closer you will get to your confident morning.
An Empowered Morning isn't something that just falls into your lap  it's created consciously. I hope you are able to implement some changes in your morning and more importantly, I hope it lowers your anxiety and gives you more excitement in life!
The Morning Routine Experts
Chris Winfield
Taylor Pearson
Benjamin P. Hardy
Are you ready to wake up, get more focused, and find more happiness in your life?
If so, sign up for my free 5 Day Mindfulness Email Course. I'll be sending you an email every day that will help you reduce stress, increase focus, and find more happiness!
If you are ready to take back control of your life and start living above stress and overwhelm...
Sign Up Here!
If you liked this article, please give it a few Claps so other people will see it here on Medium.
Sharing our ideas and experiences.
6.7K 
59
",208
https://medium.com/gethealthy/i-just-lost-100-pounds-here-s-why-almost-nobody-else-will-c9d7de4f64a7?source=tag_archive---------2-----------------------,I just lost 100 pounds. Here's whyalmost nobody else will!,"What I've learned over 20 months in losing 100 pounds, recovering from diabetes, high blood pressure and sleep apnea.",Noel Dickover,37,"By Noel Dickover, August 2015
In late 2013, I was pushing close to 300 pounds, and was suffering from diabetes, high blood pressure, sleep apnea. I had to use a CPAP machine at night to prevent myself from waking up in a panic, gasping for breath. Worse, my feet were going numb due to back problems, muscle issues or blood sugar problems  the doctors were not able to identify the cause, so they simply prescribed medication to take away my nerve sensation. I began to question whether I would get to see my own children graduate from college, get married and have grandchildren of their own.
I knew I had to do something, but I felt that yet another diet would lead me right back to where it always did  failure. Instead, I decided to try a different route  I began to think about a gradual approach of changing my movement and eating habits toward a more healthier alternatives that I still enjoyed.
Since my high of 297 pounds in October 2013, I've lost over 100 pounds, and my belt size has gone from 48 to 36. I no longer have diabetes, and am proud to be one of less than 2% of those who have been able to quit taking Metformin after starting it. My blood pressure at one point was 139 over 97  my last reading was 112 over 74! My sleep apnea is long gone, and I've gone from stress eating to using creative movement routines to alleviate stress.
Throughout this journey, I never counted calories, fat grams, water intake, nor have I regularly stabbed my finger to see my sugar level. The only external measures I use to measure progress are a scale and my belt size. Even more incredible, I've accomplished all this while sustaining a broken foot in July 2014 that did not heal well enough to remove my cast until April 2015!
This is not a story about dieting, and I don't have a product to sell. I can eat what I want, whenever I want, in whatever quantities I want. I do make lots better decisions than I used to, and my life is no longer defined by uncontrollable food cravings or binges.
This is a story about how I learned to get healthy gradually, without guilt, and without unrealistic goals that everyone seems to think are necessary to lose weight. Hopefully you will find ways to apply the approach I have taken into your own journey.
Over the last 30 years, my weight has risen progressively higher, usually spiking after another failed diet. Like almost all morbidly obese folk, I've experimented with my fair share of weight loss schemes, from nutty food regimens like Atkins and SouthBeach, to deranged 7 day cleanse diets, to cocktails of chemically-based metabolism boosters and appetite suppressants that caused jitters and bizarre mood swings. Like most dieters, I never had confidence that diets would amount to anything other than short term gains. Through dieting, my weight yo-yo'd more up than down for 20 years. With each attempt I would lose 10 to 20 pounds, only to gain it all back and more  this cycle was endlessly repeated, only with new and improved gimmicks that I hoped would be the magical ticket I was seeking. After each attempt ended in failure, intense guilt and loathing soon followed. It seemed clear I just didn't have the willpower that skinny folk have.
Unfortunately I'm not alone. A recent report on obesity that followed over 150,000 people concluded that if you are overweight, you are likely to remain that way regardless of the actions you take to get healthy. Of the morbidly obese patients in the study, meaning those in excess of 100 pounds or more  just one of every 1,290 men, and one of every 677 women returned to normal weight. The full range of solutions offered up for weight loss, regardless of the cost, simply do not work.
In 1980, 15% of the population was obese, with 1.3% morbidly obese. By 2008, obesity had grown to more than 34% of the population, with 6% now morbidly obese. Meanwhile, spending on diet products has skyrocketed. In dietary supplements, spending has more than doubled since 2002. Estimates vary  in 2006, $35 billion was spent on weight loss products, but most estimates now put that figure over $60 billion by 2014! People are spending money in increasing quantities on diets, while we continue to get fatter as a population. By 2010, a full 63% of Americans were overweight.
While there is a clearly a correlation, I think its possible a causative link exists between dieting and weight gain  meaning diets cause the average person to gain more weight than they would have otherwise. Diets ask us to adopt a strange set of tools and behaviors that look nothing like what healthy people do. Have you ever seen a skinny person keep a food journal? How many of them are meticulously counting calories, fat and all the rest? When is this magical switch supposed to occur whereby people who follow all of the experts' advice for weight loss make that transformation toward doing all the things healthy people do as a normal course? The truth is for the vast majority of us, there is no transformation;  as the study above shows, there is no light at the end of the dieting tunnel! Dieting simply doesn't work.
Bottom line, if you're looking for the best way to become morbidly obese for the rest of your natural born life, go on a diet! Set weight loss goals! Count your calories, fat and sugar, and keep daily records of your food intake. And then, a few months later, after you've failed, self loathed and binged a bit, go on another. This time, try kicking it off with a gym membership that you've given no thought about how to integrate into your life. You'll learn all the behaviors necessary for long term, sustained weight gain. You'll received advanced training on eating binges and intense craving, and will regularly stress over fruitless expenditures and delusional decisions, along with the self loathing that accompanies it.
Like real prison, you are forced to go to diet-induced ""psychic prison"" because you're bad! You've done something wrong, and now need to atone through misery!
When you decide start a diet, you have literally agreed to create your own self-imposed psychic prison for your mind. This causes you to assume contradictory roles of both the inmate and jailor. You are responsible for both policing your activity, but like all who are incarcerated, feel the intense longing for what you cannot have  intense and constant food cravings are at the core of your existence. This creates perpetual internal conflict which endlessly fuels your weight gain. Implicit is the notion that your behavior and eating habits are the root cause  that until you make a radical change, you cannot be rehabilitated. Like real prison, diet-induced psychic prisons imbue people with crippling behavior patterns which will lead to cycles of repeated negative outcomes.
Each diet foists unfamiliar, unpalatable food choices you are forced to eat in some capacity for the rest of your natural born life, assuming you stay with the plan. They require a radical break from your current eating habits, usually through a controlled set of phases or steps that can be outlined and followed in a plan. A magical gimmick often drives the diet  a radical health insight, pill, or purchased food product that forms the basis of the unique offering. Coupled with this are the resplendent exercise gizmos of all flavors that constantly seek to assure you your sit-ups will be easier, and that your magical workout to Skinnyville won't last more than 10 minutes a day.
I've done my share of diet psychic prison time, on and off for the last 30 years.I'm not going back. I can eat what I want, when I want, however much I want.
Phase I  Solitary Confinement: Diets often commence with a solitary confinement phase of food choices  choices which we tolerate for a few days, but are forced to endure for weeks if not months. The rationale is that your behavior is so catastrophic, you must detox completely before any progress can be achieved. The stated reward for completing the cataclysmic beginning? You're offered the equivalent of food furlough which gives you somewhat edible, but certainly not enjoyable, choices for the rest of your life. You like to eat spaghetti or bread? Too bad, never again! Its only broccoli and tofu from now on, but eventually we'll let you have low sodium soy sauce and skim cheese with your tofu!
Time for a Psychic Jail Break! At some point, your diet finally becomes unbearable and you snap. You just can't take it the regimen any more. It might be triggered by a life event, or just daily stress, but inevitably culminates in a psychic jail break.
And like all good jail breaks, you'll want to make the most of it! You might start with a chocolate shake and chicken wings at 10:00 pm while watching the game with friends. But fuck-it, you've blown it, so you might as well go to IHOP in the morning to devour that huge steak burrito for breakfast you've been craving.
Intense guilt follows shortly afterwards...
Shortly after the jail break, you might return to diet prison, or you might not. But at some point you will return, and will repeat the cycle endlessly, each time with false hope some new found magic. If you're like me, years of yo-yo dieting will leave you significantly overweight and in increasingly poor health.
Make a Permanent Escape! I've done my share of diet prison time on and off for the better part of 30 years  I'm not going back. I can eat what I want, when I want, and however much I want. I just make healthier decisions now. When I don't, it's usually on my terms. If a friend unexpectedly drops by at night, I'll go out and have a good time, even though it results in waking up a few pounds heavier. It will slow my metabolism a little, and perhaps set me back a few days. That's the impact. There is no failure or guilt here, just life choices.
When I began thinking seriously about how to change my life, I didn't know where to start, but I knew I wasn't going back to diet prison. I had a bevy of health issues, so it wasn't hard to find small places to start experimenting. I choose both a movement idea to experiment with. This resulted in attempts to stretch my feet out each evening, and make healthier eating decisions late at night. I was evaluating my food choices, but had no real compass guiding my behavior. Frankly I was still flailing  some days doing well, and others poorly, with crushing failure each time I faltered.
My job takes me to some fascinating places  in the beginning of 2015 I traveled to Yangon, Myanmar. After work wrapped up, I found myself in Shwedagon Pagoda, a stunning 2600 year-old gold structure surrounded by an elaborate set of ornate temples and buildings. I used to meditate regularly up until my early 20s, and was hoping to recapture that thirst in this historic and holy place. I had this notion that meditation could help me gain insight on how to get healthy. While positioning myself in front of a lavishly adorned deity statue with gaudy flashing colored lights, I sat, closed my eyes and meditated for what felt like a half hour.
When I opened my eyes, I was presented with this monk (pictured above), patently waiting for more than 20 minutes for me to open my eyes. He wanted to know my story, and we had the most incredible conversation. He told me he left to become a monk at 3 years old because ""family life was hard."" I'm assuming hardships involved food or conflict issues, but never asked. He was very animated in talking about Shwedagon Pagoda and spent the bulk of the conversation relaying his journey to town every Saturday to perform a series of chores and activities at the Pagoda. He revealed how his efforts and the perpetual efforts of his fellow monks over more than two thousand years cultivated the life energy and nurtured the harmony of the pagoda. Shwedegon Pagoda had embraced monks just like him for the duration of its existence, and would do so well into the future. In short the change this monk was trying to achieve, involving deeply meaningful tasks, took place in timescales that well exceeded his own lifespan! He wasn't interested in getting anywhere  he wasn't focused on a short term goal  it was pure journey.
After the monk and I parted ways, I seriously contemplated what it would mean if I looked at my potential recovery as a journey with clear direction, but without a clear end point. It would not be a quick journey. I spent 30 years developing my failing health profile, it made sense to give myself significant time  many years perhaps  to get healthy. Nobody is delusional enough to believe they could stay on a diet more than a year  we try to last just long enough to depart with at least a shred of dignity so we have a story to share about another flawed diet.
I looked for small changes  changes that I couldsuccessfully and permanently integrate into my daily routine.
Thinking about change in multi-year timescales implies a gradual approach. Numerous life events that impact weight gain, a key source of tension for dieters, will regularly and repeatedly occur during this time  unexpected visits by relatives leading to late night food excursions, holidays, graduations, and all the rest. Gradual change implies living your life in a normal and happy manner. I was not ""planning for maximal weight loss."" Instead, I looked for small changes to make  changes that I could successfully and permanently integrate into my daily routine.
Plan for Multiple Years: Changing the time horizon to a multi-year path fundamentally altered my approach to getting healthy. I decided to dedicate 3 years to this path toward gradually becoming healthy. I didn't imagine I'd be healthy or even normal weight by then, but thought it was long enough that I wouldn't stress about meeting short term weight loss targets. No stressing whether I am ""on course"" or behind. A time span 3 years gave enough time to evaluate whether my journey was successful.
Here's the first concrete step I took:
If I make one small change, like going from ""inhaling my food,""to thoroughly chewing it, then if I keep that change for the rest of my life,I will have taken a huge step toward getting healthy.
All my life I've inhaled food. Just a few short bites and down it goes. Where's the harm in that, right? (more on this below). Each of us is uniquely different. We arrived at our current condition through our own exclusive path, so why would our journey to getting healthy all be the same? Your journey towards a long term, sustained healthy lifestyle will not be like mine. You may not inhale food, but you ""know"" of behaviors and activities to regularly engage in that lead to poor health. Choose one  any one  a behavior, a new form of movement, a change in eating habits.
Don't choose right now! Choose when you're ready. This is not a ""you must get up off your chair right now or else"" type gung-ho solicitation. Experiment with it find a way to integrate this change into your life. If what you are trying doesn't work, try something else, until you find a change you can happily lock in.
Gradual change means finding small, potential changes that you experiment with, and either integrate in your life or discard. This is a deliberate process, one without stress or grades. Look to integrate new changes carefully, with no more than one, perhaps two at a time, if you include both movement and eating options. Each attempt leads either to a new small but permanent change, or more learning on what doesn't work for you. The path you choose will be uniquely your own, and will be based on your timeline, your life circumstances and your health profile.
A journey toward gradual health means:
When you're overweight and exhibit negative health habits, they tend to compound. My weight gain led to lower back pain, and crippling ankle and foot problems. My weight gain also led to diabetes, high blood pressure, and sleep apnea. Bad behaviors for health are similar to bad financial behaviors that lead to spiraling debt, but the reverse is also true.
Here's the good news  small, positive behaviors for improving your health compound as well or better than negative behaviors!
When I started my journey I didn't even fantasize I would be anywhere near 50 pounds lighter a year later, but in reality I flew by 250 pounds. Here's the good news  small, positive behaviors for improving your health compound as well or better than negative behaviors! Stretching regularly will lead to more flexibility, which leads to more movement, and better food choices. Over time, each of these behaviors leads to weight loss at a rate far faster than I could have dreamed!
You will not be successful if you attempt to change everything at once, nor do you need to. That's a recipe for failure and guilt. Instead, focus on integrating one new change at a time. Do so slowly and deliberately. You may not be able to give up eating primarily pizza, beer, hamburgers, fries and soda all in one week. Dietary changes should be gradual and deliberate, as should exercise routines, and overall behavior changes.
Most professional advice on weight loss will emphasize the critical need to have clear weight loss goals. The Mayo clinic recommends you make goals that are ""specific, measurable, attainable, realistic and trackable""  realistic in this sense means between 5-10% body weight. If you are morbidly obese, why would you only want to lose 5% of your body weight? I've lost over 1/3 of my body weight, but I've never had a weight loss goal that I inevitably end up fixating over. This is a short term goal, which perpetuates the quick fix mindset towards health. I strongly recommend you throw away your weight loss goals, along with your calorie counters.
While I argue against clear, measurable, attainable, realistic and trackable weight loss goals, I do agree its necessary to have a clear process for gauging progress, one which focuses energy toward a goal. With the help of a gamification expert, I devised a process of gamifying my weight loss.
Gamifying your weight loss in 10 pound increments with steady state breaks removes the ""always on"" stress that diets require
Lose weight in 10 pound increments but not all the time: Aside for the first 12 pounds or so, I've lost my weight in 10 pound increments. I accomplished this by undertaking a series of ""health sprints"" followed by steady state ""breaks"". Diets require you to be close to perfect all the time  we don't work that way. We are more successful varying our routine. I give myself time to prepare for a health sprint, and then give myself time to go easy in the interim rest periods. In my rest period, or steady state break as I call it, I'm probably not going to ""go hog wild"" with hamburger binges at 11:30 at night. But I am more likely to have an occasional healthy hamburger (grass-fed, all natural burger on a good bun, not McDonalds) at 1:00 pm during my steady state, then I am during a health sprint. I will still be exercising and generally eating well according to the small changes I've made, but I won't be optimizing. Here's how the gamification process works:
Whenever I reach the ""2,"" in the 10 pound increment, such as when I reached 282, 272, 262, 252, 242, 232, 222, 212, or 202, I ""lock in"" on my eating and movement to perform a ""health sprint."" This means I am generally engaging in movement every day (exercise or stretching), and am trying my best to eat reasonable, healthy foods until my weight gets to the next lower ""7"", such as when I went from 282 down to 277. Even if I eat poorly at 277, chances are good I'll still be below 280, which keeps me in the 10 pound increment. In game terms, once I reach the ""7"", I tell myself I have successfully reached the 270s! This is in effect attaining a ""level up"" in my personal health game.
Once I get to the ""7"", such as 287, 277, or 197 as in my current situation, I slowly ease up and settle into a healthy steady state. This may last weeks, or even longer in the case of lower weight levels like 227, 217, and 207. But eventually, I slowly start drifting down to the ""2"" again, and get ready for another health sprint. This drifting is most often caused by the newer behavior or activity changes I've added, or because I've started slowly locking down my habits to get ready for another sprint.
This approach toward gamifying your weight loss in 10 pound increments removes the ""always on"" stress that diets require. It also removes a time component  you don't really know when you'll leave the 260s, which is just fine. Your time horizon is such that you plan on getting healthy eventually, over time  not next week! Gamification also provides a clear signal when you gone off the rails in a significant way. Its never fun going up a level in a game  the same thing applies here. The days I went back up a weight level  and I did multiple times  had me seriously focusing my actions to return.
Gamifying your weight loss in 10 pound increments mimics normal game dynamics. Like most games, the levels get harder as you progress in lower weight and improve your skills. It's relatively easy to drop from 282 to 277, but is significantly harder to go from 222 to 217. The gamification works because you too are improving. The actions you take at 222 to get healthy will be significantly different than what you did at 282.
In my journey, there were times when my body seemed determined to stay around a particular weight. Looking back, the 270s, 220s and the 200s were especially difficult to overcome. But if you generally stay on your path, eventually you will find yourself ready to do another sprint, and with success, will level up to your new weight class. This provides a sense of joy and accomplishment that helps sustain your journey. Over time, you will be amazed how far you've come!
Deluxe, often expensive Internet of Things (IoT) personal tracking devices like Fitbit or the new Apple watch are the rage right now. Just pay $99.99 and get your calories counted, heart rate monitored, your blood pressure measured, your steps counted, your sleeping patterns graphed, and bloody sugar levels tracked regularly. I really have an aversion to the entire product set, because the focus is wrong. These products get you to think of your body as an mysterious black box that requires external scientific measures to generate reliable data.
Your body is a living system. It doesn't operate in ""per day"" increments.Your body operates in the here and now, whether or not you're noticing.You can become aware of it...will you?
Further, these products seem to buy into the delusion that external goals should drive the pace of your recovery, not the actual and current condition of your body. You want to lose 30 pounds in 6 weeks? You can now measure your progress in real time! These products optimize your time, and our bodies just have get with the program! To be clear, I am not opposed to datasets or sensors for data collection my day job involves finding aggregating and applying data from harsh environments in really creative ways. I just think the costs for using it personally far outweight the benefits.
That said, I do have friends who are using IoT products to successfully lose weight. If you're using Fitbit, cell phone apps, Apple watch or some other hip device and its helping you get healthy, congrats and keep rocking it! If its working over the long term, use it! The section below provides an alternative approach to external measures  one that is at odds with the current external measures trend. So if you've settled into a happy partnership with your Fitbit and are afraid of offending it, feel free to skip down to the ""Movement and Eating"" section below.
Experts say you should drink X cups of water per day, Y calories per day, that you should limit your carb and fat intake to Z grams  you know the routine. With no data other than my own experience in losing over 100 pounds, I've decided that's all bullshit  the whole approach is flawed. As we see above, this advice is not helping people lose weight. Worse, the reliance on external measures solidifies the view of the human body as a near undecipherable black box  one that causes us to quickly lose confidence in our own perceptions. And in doing so, we stop paying attention to clear signals and rich feedback our bodies are constantly providing. Not surprisingly, the new innovations in personal tracking devices line up perfectly with new and improved, data-enabled 5 step diet plans, that if you only follow to perfection will allow you to...its all bullshit.
Your body is an organic, living system. This means it doesn't operate in ""per day"" increments. Your body operates in the here and now. Whether not you're pay attention, your body continually regulates itself in real-time. Diets are designed as if your body was an engineered machine that is missing a gasket or two, but living systems simply don't function that way  they aren't analogous. Engineered machines like precise instructions and reliable measurements, and constant improvement. But your body is constantly adjusting to a whole series of internal and external purturbations  this implies prescribed totals quickly lose their utility. There is a tenuous connection at best between what the prescribed daily totals for someone like you ""should be"" and what your body actually requires at this very moment.
Your body's actual functioning in the present and next few hours are critical, but daily totals at the end of the day are near meaningless. I'm suggesting that like most skinny people, it is important for the rest of us to develop a deeper awareness of our body's functioning  one that will inform our decisions on food and movement choices far more effectively than any professional health plan.
To paraphrase Darth Vader,
""The power of the Fitbit pales in comparison to power of your own ability to understand what is occurring within you in the ever living present. I find your lack of faith in yourself disturbing.""
You either ""do"" or ""do not"" need water right now, but chances are, if you're exercising regularly and regulating your metabolism you'll be drinking water heavily. Put simply, guidelines to drink 12 cups of water per bullshit period time in absence of actual movement will not improve your health or disposition.
As a living system, your body still abides by the laws of nature  if health goals are disconnected from your actual state of health, they won't succeed. You might lose that fast 5-20 pounds, but we both know you'll have more weight than you started a few months later. Long term change comes from understanding your body's current state  this gives you the knowledge necessary to make the right changes for you, one by one, gradually and permanently over time. Understanding your body's current state requires learning to pay attention to what is happening inside. This is a very knowable thing  something a large percentage of skinny people do naturally.
The Measures that Matter: If I'm not counting calories or fat, or poking myself daily for sugar levels, how can I gauge progress? My approach to getting healthy has as its core developing deep awareness of how my body functions. Like all gradual change, this is a long term effort that yields greater dividends the more skill you attain. Understanding both my internal motor and physical condition has been critical to my journey. For measures directly affecting weight loss and diabetes recovery, I pay attention to three key things:
 Energy level  Metabolism  Sugar level
ENERGY LEVEL: Start with your energy level. Everyone can feel if they are energetic or not. I'm asking you to pay regular and explicit attention to your energy level, not unlike your awareness of the room temperature. This takes practice  for me it took well over a month to really lock this in.
I would try to ask myself almost hourly, ""On a scale of 1 to 10, what is my current energy level?
Once you do, you'll begin notice a clear relationship between food you eat, both type and quantity and its impact on your energy level. For me, this was the key to changing my eating habits. When I looked at a lunch menu and fixated on something large and heavy, once I knew intimately and repeatedly what happens to my energy the immediate enjoyment factor drops  I started selecting better choices because of it. Again I stress that this is a not a day revelation  this takes weeks if not longer.
Over time, you can use your awareness of energy levels to make small changes in your food and movement choices. Awareness of your current energy level will prompt you to find reasons to get up and move around once an hour, or you might find yourself eating more snacks along with smaller sized meals far more often. This approach is VERY different from keeping track of calories. In essence, you are paying attention to the internal impact of the calories in the here and now. That's the only time they calories are relevant. The fat either will or will not have been added to your gut by the time you calculate your caloric intake the end of the day.
Once you are comfortably keeping track of energy levels and regularly notice the impact of your food choices, you are ready to gain better awareness of your body's motor  your metabolism.
METABOLISM: Energy awareness is the gateway to gaining awareness of your metabolism. Your metabolism is the engine that powers the living system that is you. Its processing can be felt, and eventually optimized in the sense that there are natural ways to speed it up. But in the beginning, start paying attention to it in the same way you do your energy level. This will take you a few months of effort, so don't be discouraged if you aren't successful after a day or two.
If you've ever lost weight, at times you can ""feel"" almost a tingly sensation when you perceive you are losing weight  this is usually accompanied by higher energy levels. Similarly if you pay attention, you can ""feel"" when you are gaining weight. This is not magic  its a knowable thing that lots of skinny people take for granted. If you gain awareness of your body's engine, you can gain control over your weight loss. If you understand what your metabolism is doing, you have the knowledge necessary to guide your eating and movement choices.
Get to Know Your Stomach: The key for sustained attention to your metabolism is your stomach. If you're like I was, you have no idea what's happening in your stomach. I rarely paid attention to how my eating habits directly led to mood swings or energy spikes. As a consequence, I used to ingest 2-3 Pepsid ACs a day  now I take less than that in a month! If acid reflux is a regular part of your day, chances are good that your stomach seldom processes your food properly, and has long since resorted to just ""sending it on"" to the small intestine where your food sits and is digested slowly and poorly.
Chew Your Food! If you're interested in developing a deeper awareness of your stomach, try learning chew your food very thoroughly. I used to decide I was full when my taste buds were satiated. By this point, if the food was tasty, my stomach was usually stuffed to the point of breaking. To stop the cycle, the stomach eventually initiates its ""nuclear option"" stomach acid starts shooting up into my mouth to ruin my taste. Acid reflux is the stomach's way of getting the mouth to stop overeating. Like all nuclear options, the side effects aren't good. If you chew your food thoroughly, you are pre-digesting it for the stomach in a way that it can take the next step in the digestion process. Food that's properly chewed leads to quicker, better faster processing by the stomach, which leads to a faster metabolism rate. More importantly, it allows you to pay attention to your body's engine.
I went from chowing down three large meals a day to consuminglots of small meals all day long
Over time, I began looking for ways to optimize my metabolism. I did this through food choices, supplements and most importantly, my eating frequency. I went from chowing down three large meals a day to consuming somewhere between 10-20 small meals all day long. In keeping my metabolism at a high burn rate, I usually try to keep my stomach just above empty. As soon as I feel hunger pangs, I eat a small snack. I have lots of healthy, but mostly delicious things to eat. I now keep snacks everywhere and am constantly eating. As a consequence, my stomach has shrank so much that even if I wanted to binge, I can only eat a small amount before feeling bloated.
SUGAR LEVEL: After a few months of paying deep attention to my metabolism, I began to regularly sense of my sugar level. At first, it was most easily felt when I had something overly sweet. Most people can feel a sugar rush if they eat large quantities of candy. This is no different, except in the sense I became hyper-aware of it, even when I wasn't indulging in sweets. As I ate food, I became aware of its near term impact on my sugar level. My goal was to stop the spikes in sugar (which often correspond to spikes in energy levels), and to slowly lower the overall sugar levels over time. As I mentioned earlier, I applied this approach without regularly poking my finger, so I can't give you any statistics of my rate of decline. I will say that over a period of 6 months, my doctor slowly lowered and then eliminated my drug reminen.
If you're diabetic and on Metformin, the challenge in getting off is that Metformin is designed to keep your sugar levels in the normal range (this is a good thing). Once I developed better awareness of my sugar level, I attempted to keep my sugar levels regularly slightly below what I perceived as normal. This led to doctor-approved reductions in Metformin (from 2 pills to one, to a half, and then finally the doctor removed them altogether). If you try this approach and start getting faint spells (this definitely happened to me), it's probably a sign your sugar level is too low. If so, this is where you want to talk with your doctor about reductions in Metformin intake.
The measures that matter  the ones that will help you get healthy  are internal
Use of Personal Tracking Devices Sap Confidence in Internal Measures: The Fitbit approach will give you exact measures (we hope), but at the cost of reducing confidence in your ability to understand your internal operations. By relying on external measures, you give more credence to its ""rightness"" and less to internal measures that are inexact and untrackable. But the measures that matter  the ones that will help you get healthy, are all internal. I've only listed those critical to metabolism, but your perception in engaging in movement and exercise is just as critical the two are connected.
Sure, you could poke your finger 20 minutes after eating each meal  this would work as a method for understanding your spikes. But it's not necessary- there's nothing magical about becoming aware of your internal workings. Large numbers of skinny people manage themselves naturally by paying attention to their bodies. This is a learned skill  one you can develop and improve on over time. If you learn to pay deep awareness to your internal measures, you will find they are transformative in their ability to change your future in ways that external devices simply cannot touch.
Movement and eating healthy are both critical to weight loss. If your diet consists of pizza multiple times a week, sports bars late at night, pop tarts in the morning and 2-3 cans of soda a day, you will need to make some changes in your eating habits. Likewise, if you pride yourself on your couch potato ways, you will also need to make some changes in your movement. This is different from saying you will never eat pizza again, and will have to do a full circuit routine every day. Make the changes gradually, in ways you enjoy. Getting healthy is not about pain and force of will. It's about finding enjoyable but healthier alternatives to eating, and enjoyable ways of movement. In my case, over the course of my journey, I gradually went from being a stress eater to a stress exerciser  this from someone who never enjoyed exercising before.
I have a lot lessons to share, and decided to give a sampling below. If these are of interest, let me know in the comments section below, and I will create videos or something to go into more detail.
We all have different eating habits, but it's clear that processed foods aren't food. Processed foods are made to look edible, but your body has limited ability to process them. They slow your body's metabolism and leave long term residues. Processed foods have significant, long term negative impacts on your health.
If you want to improve your metabolism and develop deep awareness to what is happening inside of you, you'll need to slowly ween yourself off of processed foods. Processed foods are designed to be addictive, and usually contain a potent cocktail of sweet, salty, and fatty flavors. The combination of the three removes your ability to control your cravings. Lays potato chips really do have truth in advertising  they are designed to ensure that you ""just can't eat one!""
This doesn't mean you'll never be able to eat all the foods you love. But it does mean you find yourself paying more for better quality alternatives. I'm a chocoholic, and MUST have chocolate every day. Now I go with all natural 65-70% cocoa (80% enters the no taste zone) with delicious additive ingredients like berries, coconut or nuts. I still want hamburgers and fries every so often, but it's never at McD's or Wendys. I go to upscale, grass-fed beef places, and often substitute sweet potato fries. You'll still be eating most of the things you enjoy, but hopefully, you'll do so in moderation, and not at 10:30 at night!
When I was 270+ pounds (for the majority of the last 15-20 years), every so often I would attain the willpower to try exercising as a regular routine. I usually started out well, but soon after I would end up pulling a muscle so severely that I was forced to stop my exercise for a week, which get me off track, often resulting in ceasing my efforts altother. I've also tried gym memberships that, after an initial crazy and unsustainable burst, I stopped going but still kept paying; a personal trainer who regularly pushed me to exhaustion, and various other delusional fits and starts. I haven't tried exercise boot camps, but can already see the futility there  yet another short term quick fix approach that will ultimately lead you weighing more than you started.
Start with What You Have: Purchasing a gym membership is not a ""first step"" toward getting healthy. It's a loss of $50 bucks a month. If you don't use it, it doesn't exist. Instead of wasting money on something you may not use, start with what you have. In my case, I had some small free weights, a chair, a sitting bench and a floor in my bedroom. This was more than enough to get started.
Movement is Fun First, Useful Second: So many sales pitches for exercise products talk about optimizing your time. Basically they say, ""You only have to suffer 20 minutes a day. But its ultra comfortable suffering!"" (jail break alert). I optimize enjoyment in my movement routines. I don't ""try to get through them"" as quickly as possible, and I don't really schedule them. I often integrate movement routines in small bits throughout the day. If I have time for an early morning workout, terrific! I love that, but it doesn't kill my day from a movement standpoint if I miss it. And at this point in my journey, I NEVER exercise because I have to in order to get healthy. I do my routines because my body now craves it, and really, because they're really fun! If movement is fun, as opposed to exercise you need to endure, you're more likely to sustain your efforts for life.
Injuries Do Not Have to Kill Your Progress: Unfortunately for me, after I was in better shape to exercise, I broke my foot in a freak accident. After a short period of self-pity, I found new methods of movement ones that would allow me to continue improving my movement and health. I gravitated toward exercising wooden swords on my son's old, sparsely used plastic Tae Kwon Do kicking post. Because my foot was broken, I focused on my core, with various routines using four different sized wooden swords  I always had one foot always planted. This exercise regimen had the amazing side benefit of bestowing significantly increased flexibility in my back. For me, what could have been a setback to getting healthy became a reason to try something new and different  to get creative and see what worked for me.
Stretching Over Exercise: If you've been sedentary for an extended duration, you probably aren't ready for a full exercise regime. In my case, my foot pain restricted my exercise options. Instead, consider a goal of increasing your flexibility through stretching. For my first five months of weight loss I exercised sparsely, but engaged in stretching on a daily basis. Over time, stretching became a meditative release for me, something I looked forward to performing.
Quality Stretches Last Minutes, not Seconds: The purpose of the stretch is to slowly get the muscles to release their tension  to become looser. This doesn't happen quickly. In practice, my stretches can easily last 4 or 5 minutes. For me, it's a meditative thing. I don't go for as hard a stretch as possible. Instead I stretch to the point that it's definitely uncomfortable, and keep it there until I feel the muscle ""release"" a little. I normally stretch until the muscle releases, and then keep the stretch until it stabilizes in its new looser position. Over time, I gradually increased my flexibility throughout my body to the point that I looked forward to regular exercise.
Stop Counting Reps and Listen to your Body! I used to buy into the optimization thing, which includes tracking reps and weight, with the intent of improving it over time. This is pretty much the accepted way most think of exercise. I always tried to beat my previous score well before I was ready. In practice, I focused on reaching ""the number"", such as a set of 10 squats. But by focusing on reaching 10 squats, I wasn't paying attention to how my body was responding. For me, this is a recipe for disaster. I usually end up pulling something at rep 7, but still power through until 10. If I had stopped a rep or two earlier, it would have been a minor thing, but instead I'd end up with a debilitating tear. In exercising, I now listen to my body first and foremost.
My approach for the past year has been to perform my current exercise for as long as I can, without keeping track of the number. My focus is on my body so if something feels wrong, I stop immediately and stretch out the area. If not, I go as long as I can before exhaustion or lack of will to continue stops me. Sometimes I like to go all tough guy and push myself, while other times, my body just doesn't feel like it. Instead of ""pushing through"", now I simply do a lesser workout when my body pushes back.
My routine, if you can call it that, is never regular, but is usually both fun and stress relieving. I'll hang out on a stationary bike if I want to meditate, but far more often I'll have lots of different activities to try. This includes 4 sizes of wooden swords that I jazz on, various forms of plank exercises, to recently experimenting with running. Bottom line, I'm don't optimize my exercise time  I optimize my enjoyment. Slow improvement works fine for me, especially if its enjoyable enough to do it regularly.
Good posture is a critical to living a healthy life. As I started to lose weight I became increasingly interested in examining my posture. For the better part of at least 20 years, I had what I affectionately refer to as the ""zombie walk."" Because of my immense belly size, I ended up balancing my weight on my frame on the stress points  the lower back and ankles in my case, the knees in others. I also developed significant stress on my neck, which was caused by moving my head back in order to counter-balance the weight of my stomach. Walking for me involved ""falling forward"" not too unlike a zombie with my legs locked. The zombie walk as the two-fold negative benefit of damaging your stress points, while expending as little energy as possible to move around. Go to any public gathering where people are milling about and you'll see the zombie walk practiced in abundance!
I've since changed my posture to maintain the majority of weight on my upper legs and core. This took many months of practice to make the shift, and requires lots of strength exercises (squats) to adequately support the weight. The advantage of my new posture is my stress points no longer holds the majority of my weight throughout the day . Moreso, by shifting my weight bearing to my legs and core, I burn up the calories just by walking around!
Bottom line, if you are significantly overweight your current posture is may be the result of your body attempting to mitigate your weight gain. Your current stance and gait may work for you, but it may also be leading to long term debilitations in your movement and flexibility. If so, experiment with changes to your posture, especially as you lose weight. As your body type changes, so too should your posture.
This is a new discovery for me, but I'm beginning to understand that our stress and tension can be transferred from the mind to the body. I add this section especially for those suffering chronic foot pain  up until a few weeks ago, my toes were still regularly going numb, and pain still persisted. Over time, tension and stress, either from physical or mental ailments can end up being transferred and stored in the body's fascia, a thin tissue that covers all our muscles throughout the body. I developed what I refer to as ""human barnacles,"" where masses of tissue have solidified to almost feel like bone. This dynamic exists all around my body from the fascia in my chest releasing tension in my shoulders.
A few weeks back, I found myself in Neredu Valley, as remote a location as you can find in India, and where many of the pictures you see here originated. There I had a chance encounter with Nath Yogi named Ashwin Mohan, who shared this massage technique with me. I think Ashwin calls it yogamukti therapy, which incorporates really empowering ways of mind-body healing. In a cursory look, in the US, I think the non-spiritual part of what Ashwin talks about is referred to myofacial release.
Even after losing over 100 pounds, and getting in the best physical shape of my life, I still had numbness in my feet. I have a drawer full of wool socks I wear starting in late fall due to them always feeling cold. But by softly working out the fascia tightly coupled to my shins, for instance, tension is immediately released around my big toe. After like 10 minutes of Ashwin massaging my feet and legs, I had this strange sensation flooding my feet  something new and wonderful. I asked Ashwin what it was. He said, ""That's the blood returning to my toes."" The fascia had tightened to the point that it was restricting the muscles and blood flow! Since then, I have spent the better part of the past week slowly working out fascia tension in my legs, which immediately loosens my feet! I showed my chiropractor this technique  he applied it to my back, which led to significant easing of tension in my legs, feet and neck. I understand its also effective for migrains and headaches.
Up until last week, I still pictured body image as being that of someone far heavier. Once I've started working on alleviating and removing the tension in my fascia, it has changed my body image completely. By getting rid of all my stored tension in my toes and shoulders and back, I now feel like a normal sized person!
I'm over 20 months in to my 3 year journey to get healthy. The fact that I've lost over 100 pounds does not mean I've finished. Currently I'm sitting between 197 and 192 pounds in a steady state. I still intend to lose more, but realistically, my focus has shifted towards other health goals like continuing to heal my lower back, feet and ankles. I'm learning ways to use my body's energy for self-healing, but that is another post.
Each personal health journey is unique. I hope you start yours!
Note on the Inclusion of Nature Photos: A number of the photos you see here are from a very recent trip I made to a healing and meditation center Neredu Valley, India, a very remote part of Andhra Pradesh, India. In addition to serving as a capstone experience to my health journey, I add these photos because the guidance in this story is very eastern in flavor. To me, the photos totally add to the vibe. The leopard track is an external measure of its existance. The woman serving coconut water epitomizes healthy movement and eating, and so on. Apologies if they don't work for you.
And regarding that capstone experience in Neredu Valley, in addition to Ashwin, special thanks to Arjun, Deepta, and especially my son, Justin for that magical time in Neredu (and to my wife Nam for all those yummy food experiments). Also thanks to Vuaneeta strives to live her life in Neredu always in the present; but who will never read this as she is disconnected from that ""thing."" Also, special thanks to Gigi for the edits!
If you liked this, click the below so other people will see it here on Medium.
Gradual change approach toward getting healthy, with...
1.8K 
44
",209
https://medium.com/@drjasonfung/type-2-diabetes-reversal-the-quick-start-guide-6187210f14ce?source=tag_archive---------4-----------------------,Type 2 Diabetes Reversal  The Quick Start Guide,How to Reverse Type 2 Diabetes  The Quick Start Guide,Dr. Jason Fung,10,"How to Reverse Type 2 Diabetes  The Quick Start Guide
Twenty years ago, when you bought a brand sparkly new VCR machine, you would also get a thick instruction manual. Read this thoroughly before you start, the manufacturer would implore. There would be detailed setup procedures and troubleshooting guides.
Most of us ignored the manual, just plugged it in and tried to figure out the rest. That's why we all had the blinking 12:00 on. Today, most new electronics now come with a quick start guide which has the most basic 4 or 5 steps to get your machine working and then anything else you needed, you could reference the detailed instruction manual. Instruction manuals are just so much more useful this way.
Well, I don't know much about VCRs, but I do know about type 2 diabetes. I could write an entire book about obesity (oh, wait, I did that already), or fasting (oh, wait, done too) or type 2 diabetes (next up for 2018). But many of you will not want to go through the entire instruction manual. So this is your quick start guide for reversing your type 2 diabetes.
A Fully Reversible Disease
Most doctors, dietitians and diabetes specialists claim that type 2 diabetes is a chronic and progressive disease. The American Diabetes Association, for example, almost proudly proclaims this on its website. Once you get the diagnosis, it's a life sentence. But, it's actually a great big lie. Type 2 diabetes is almost always reversible and this is almost ridiculously easy to prove. This is great news for the more than 50% of American adults who have been diagnosed with pre-diabetes or diabetes. Recognizing this truth is the crucial first step in reversing your diabetes or pre-diabetes. Actually, it something that most people already instinctively recognized to be true.
Suppose your friend is diagnosed with type 2 diabetes, then works hard to lose 50 pounds. He takes himself off all his medications and his blood sugars are now normal. What would you say to him? Probably something like ""Great job. You're really taking care of yourself. Keep it up!"" What you wouldn't say is something like ""You're such a dirty, filthy liar. My doctor says this is a chronic and progressive disease so you must be lying "". It seems perfectly obvious that diabetes reversed because your friend lost all that weight. And that's the point. The disease is reversible.
We've known all this along. But only diet and lifestyle changes will reverse it. NOT medications. The most important thing, of course, is to lose weight. But the diabetes medications don't do this. Quite the contrary. Insulin, for example is notorious for causing weight gain. Patients intuitively sense that they are heading down the wrong path.
They would often say to me, ""Doctor. You've always said that weight loss is the key to reversing diabetes. Yet you prescribed me a drug that made me gain 25 pounds. How is that good?"" I never had a good answer, because none existed. The truth was that insulin was not good for type 2 diabetes  it was only good for reducing blood glucose. The key was weight loss, whereupon the diabetes often goes away or at least gets significantly better. So, logically, insulin does not help reverse the disease, but actually worsens it.
Other medications such as metformin or the DPP4 drug class are weight neutral. While this won't make things worse, they won't make things better either. Since weight loss is the key to reversing type 2 diabetes, medications won't make things better. Medications make blood sugars (the symptom) better, but not the diabetes (the actual disease). We've been pretending that the symptom is the disease.We can pretend the disease is better, but that doesn't make it true. That's the reason most doctors think type 2 diabetes a chronic and progressive disease. We've been using the wrong treatment. We've been prescribing drugs for a dietary disease. No wonder it doesn't work.
So, how can you reverse your type 2 diabetes?
The Sugar Bowl
The essential feature of type 2 diabetes and pre-diabetes is that our bodies are completely filled with sugar. It's not just too much sugar in the blood. That's only part of the problem. There's too much sugar in our entire body. Imagine our bodies to be a sugar bowl. A bowl of sugar. When we are young, our sugar bowl is empty. Over decades, we eat too much of the wrong things  sugary cereals, desserts and white bread. The sugar bowl gradually fills up with sugar until completely full. The next time you eat, sugar comes into the body, but the bowl is full, so it spills out into the blood.
Insulin is a normal hormone produced when we eat and its job is to allow glucose into the cells. When it is no longer able to do it, glucose piles up outside the cell in the blood, and it is called insulin resistance.
But why does this happen? The cells are already over-filled with glucose (see previous post  A New Paradigm, and Insulin Resistance is Good?). Like trying to blow air into an over-inflated balloon, it simply takes more force. The cell resists the glucose because it's completely full. insulin resistance is an overflow phenomenon.
It's like packing your clothes into a suitcase. At first, the clothes go without any trouble. After a certain point, though, it is just impossible to jam in those last 2 T-shirts. You can't close the suitcase. The luggage is now 'resistant' to the clothes. It's waaayyy harder to put those last 2 T-shirts than the first 2. It's the same overflow phenomenon. The cell is filled to bursting with glucose, so trying to force more in is difficult and requires much higher doses of insulin.
When the insulin levels are unable to keep up with the increasing resistance, blood sugars rise and your doctor diagnoses you with type 2 diabetes and starts you on a pill, such as metformin. But metformin does not get rid of the sugar. Instead, it simply takes the sugar from the blood and rams it back into the liver. The liver doesn't want it either, so it ships it out to all the other organs  the kidneys, the nerves, the eyes, the heart. Much of this extra sugar will also just get turned into fat.
The problem, of course, has not been solved  the sugar bowl is still overflowing. You've only moved sugar from the blood (where you could see it) into the body (where you couldn't see it). It's putting a band-aid over a bullet hole. So, the very next time you eat, the exact same thing happens. Sugar comes in, spills out into the blood and you take medication to cram the sugar back into the body. This works for a while, but eventually, the body fills up with sugar, too. Now, that same dose of medication cannot force any more sugar into the body.
So you go to your doctor. What does he do? Instead of getting rid of the toxic sugar load, he doubles the dose of the medication. If the luggage doesn't close, the solution is to empty it out, not use more force to . The higher dose of medication helps, but only for a time. Blood sugars go down as you force your body to gag down even more sugar. But eventually, this dose fails as well. So then your doctor gives you a second medication, then a third one and then eventually insulin injections.
Over a period of years, you went from pre-diabetes, to diabetes, to taking one medication, then two then three and then finally large doses of insulin. Here's the thing. If you are taking more and more medications to keep your blood sugars at the same level, your diabetes is getting worse! Even if your blood sugars get better, your diabetes is getting worse. This is unfortunately what happens to virtually every patient. The body is already overflowing with sugar. The medications only hide the blood sugar by cramming it into the engorged body.
The diabetes looks better, since you can only see the blood sugars. Doctors can congratulate themselves on a illusion of a job well done, even as the patient gets continually sicker. Patients require ever increasing doses of medications and yet still suffer with heart attacks, congestive heart failure, strokes, kidney failure, amputations and blindness. ""Oh well"" the doctor tells himself, ""It's a chronic, progressive disease"".
Imagine that you hide your kitchen garbage under the rug instead throwing it outside in the trash. You can't see it, so you can pretend your house is clean. When there's no more room underneath the rug, you throw the garbage into your bedroom, and bathroom, too. Anywhere where you don't have to see it. Eventually, it begins to smell. Really, really bad. You needed to throw out the garbage, not hide it away. If we understand that too much sugar in the blood is toxic, why can't we understand that too much sugar in the body is toxic too?
The End Game
What happens over time  10, 20 years?
Every single part of the body just starts to rot. This is precisely why type 2 diabetes, unlike virtually any other disease, affects every part of our body. Every organ suffers the long term effects of the excessive sugar load. Your eyes rot  and you go blind. Your kidneys rot  and you need dialysis. You heart rots  and you get heart attacks and heart failure. Your brain rots  and you get Alzheimers disease. Your liver rots  and you get fatty liver disease. Your legs rot  and you get diabetic foot ulcers. Your nerves rot  and you get diabetic neuropathy. No part of your body is spared.
Medications and insulin do nothing to slow down the progression of this organ damage, because they do not eliminate the toxic sugar load from our body. We've known this inconvenient fact since 2008. No less than 7 multinational, multi-centre, randomized controlled trials of tight blood glucose control with medications (ACCORD, ADVANCE, VADT, ORIGIN, TECOS, ELIXA, SAVOR) failed to demonstrate reductions in heart disease, the major killer of diabetic patients. We pretended that using medications to lower blood sugar makes people healthier. But it's only been a lie. You can't use drugs to cure a dietary disease.
How to Reverse Diabetes
Once we understand type 2 diabetes, then the solution becomes pretty bloody obvious. If we have too much sugar in the body, then get rid of it. Don't simply use medications to hide it away so we can't see it. There are really only two ways to get rid of the excessive sugar in the body.
That's it. That's all we need to do. The best part? It's all natural and completely free. No drugs. No surgery. No cost.
Step 1  Don't put sugar in
The first step is to eliminate all sugar and refined starches from your diet. Sugar has no nutritional value and can therefore be eliminated. Starches are simply long chains of sugars. Highly refined starches such as flour or white rice are quickly broken down by digestion into glucose. This is quickly absorbed into the blood and raises blood sugar. For example, eating white bread increases blood sugars very quickly. Doesn't it seem self-evident that we should avoid foods that raise blood sugars because they will eventually be absorbed into the body? The optimum strategy is to eat little or no refined carbohydrates.
Most importantly, stick to eating whole, natural, unprocessed foods.
Step 2  Burn it off
Fasting is the simplest and fastest method to force your body to burn sugar for energy. Glucose in the blood is the most easily accessible source of energy for the body. Fasting is merely the flip side of eating  if you are not eating you are fasting. When you eat, your body stores food energy. When you fast, your body burns food energy. If you simply lengthen out your periods of fasting, you can burn off the stored sugar.
Since type 2 diabetes is merely excessive glucose in the body, burning it off will reverse the disease. While it may sound severe, fasting has been practiced for at least 2000 years. It is the oldest dietary therapy known. Literally millions of people throughout human history have fasted without problems. If you are taking prescription medications, you should seek the advice of a physician. But the bottom line comes to this.
If you don't eat, will your blood sugars come down? Of course.
If you don't eat, will you lose weight? Of course.
So, what's the problem? None that I can see.
We can reverse type 2 diabetes and pre-diabetes today, right now, immediately. All without cost, without drugs, without surgery, with an all natural, time-tested healing method. We only need to lead our bodies down the healing pathway and have the courage to apply our hard-won knowledge. Type 2 diabetes is reversible. A new hope arises.
",210
https://medium.com/@realkniels/how-a-22-day-water-fast-changed-my-life-84b48287144e?source=tag_archive---------9-----------------------,How a 22 Day Water Fast Changed My Life,"Last year, I was trying to come up with novel ways to raise money for MacMillan Cancer who were a huge help to our family when my dad...",Karin Nielsen,7,"Last year, I was trying to come up with novel ways to raise money for MacMillan Cancer who were a huge help to our family when my dad became terminally ill with a rare form of lung cancer. I was conscious of how desensitised I had become by the sheer volume of charitable requests on my facebook feed and figured I'd have to come up with something radical to make my plea for donations stand out.
Already aware of the growing trend in intermittent fasting for weight loss, I began to wonder what would happen to my body if I did not eat anything for a prolonged period of time. The more I researched extended water fasting, the more excited I became about its potential to help me raise money and benefit my overall health at the same time.
First I had to settle on a duration to aim for. While most fasting practitioners recommend fasts between 5 and 14 days, I found some detailed accounts of people who had fasted for as long as 45 days and lived to tell the tale. I had a holiday booked and did not want to ruin it so I settled on 22 days as this felt long enough to get some meaningful data. I always think the measure of a goal is how intimidated you feel at the thought of achieving it and this scared the shit out of me so I knew it was ambitious enough.
My initial enthusiasm was quickly quashed by well meaning colleagues, friends and family members who tried to talk me out of taking on such a radical challenge.
""Nobody can survive that long without food""
""You'll do irreparable damage to your body""
""You won't be able to work""
By the time I started discussing my plans with other people, I'd done enough research to reassure myself that none of these doomsday prophecies were likely to materialise. Moreover, I trusted that my body would send me warning signals I was heading towards danger at any point.
Frustrated that water fasting seemed to have such a bad rep I decided to take pre- and post-fast blood tests that would prove unequivocally that I was not permanently damaging my health. This made it easier to field the daily onslaught of concern as I was able to deflect futile debate by reminding people that the results of my post-fast tests would settle the argument once and for all.
I know from experience that doing any kind of 'diet' is close to impossible for me if I am not closely tracking my progress. I was also fascinated by the plethora of claims made about the impact of fasting so I decided to track as much of my experience as possible to motivate myself and help others who consider embarking on a similar journey.
There are many parameters you could track and the extent to which you engage with this depends on how inquisitive you are and how much (if anything) you want to spend. I'm very health conscious anyway so I decided to go for the following:
To avoid pestering my GP, I arranged my own blood tests immediately before and after the fast through home health testing startup Thriva.
I weighed myself using my Withings Bodi Cardio scales every 7 days.
I measured ketones and pH balance daily using urine test strips to ensure I remained within healthy ranges.
I took body measurements and pictures of my progress every 7 days.
Many purists in the fasting community (yes, that is a thing) will claim that you are not doing it properly unless you drink only filtered water for the duration of the fast. That seemed like a lot of hassle to me and I was not convinced that it would make a big difference so I chose to ignore it. Instead, I decided to allow myself herbal teas and 4tbs of Braggs Organic Apple Cider Vinegar each day. The latter is highly regarded by many fasters as it's packed with vitamins, minerals and enzymes that help to control your body's pH levels.
There are lots of excellent and very detailed resources about how best to prepare for an extended water fast (see 'Further Reading' at the end of this post) so I won't bore you with the detail here. I did not prepare at all as I already restricted my sugar and carbohydrate intake. If you do eat a lot of junk it may be an idea to start with a low sugar/carb diet for a couple of weeks to reduce the withdrawals and minimise the risk of keto flu which is extremely unpleasant. If you have any pre-existing medical conditions whatsoever you should definitely consult your GP before you fast for any period of time.
I'm not a doctor or fasting expert so I'm not here to make claims about what is right and wrong. I can only share my own experience and it's up to you how you interpret the results.
A lot of blogs I read recommended getting as much rest as possible and ideally, taking some time off work. This just was not an option for me so I decided to do my fast while going about my normal daily life.
That means working 12+ hour days, 22km round-trip commutes on my bike and continuing to work out in the gym 2-3 times per week. I did reduce cardio intensity to ease the pressure on my heart but was able to lift as much weight as before without noticing much difference in form for the duration of the fast.
Fasting is insanely hard but it does get a lot easier as your body adjusts to the new 'normal'.
During the first three days I felt like I could have fallen off the wagon at any time. The smell of food drove me wild and I started to obsess about all the things I would devour as soon as I could eat again. By the third day I was in full ketosis and experienced pounding headaches, disturbed sleep and felt light headed when standing up too quickly.
At the two week mark, the uncontrollable hunger pangs and headaches were completely gone. My blood pressure was still lower than usual which explains why I felt light-headed and freezing cold at times. I was able to handle and cook food for my boyfriend without feeling tempted and found the smell of junk and sugar laden snacks repulsive.
Week three was surprisingly easy compared to weeks one and two. I suspect this is due to the body adjusting to the new 'normal' and the fact that I had purged most of the toxins in my system at this point which is said to significantly reduce unpleasant and flu-like side effects.
Physically and mentally, I felt in the shape of my life.
Don't believe me? Let's take a look at the data:
As you can see, the results are astonishing. Far from causing irreparable damage to my health, my extended fast had a profound impact on my vital health stats.
The values highlighted in orange were classified as suboptimal by the good doctors at Thriva. That is to say that while I was not in immediate danger, my pre-fast test results indicated abnormalities that were a cause for concern.
The values highlighted in green indicate a return to 'healthy' levels in the areas where results were previously in the unhealthy range. Most notably, my liver function and cholesterol bounced right back to well within the healthy zone.
Naturally, I expected to lose weight on a diet of water and herbal tea but I never expected such wide-ranging improvements across the board.
I embarked on this journey believing that the only positive side effects would be weight loss + money raised for charity. I was wrong!
Extended fasting is ultimately a test of determination, will power and grit. Completing such a tough personal challenge did not only improve my physical wellbeing but also boosted my self belief and fine tuned my self awareness.
I now believe that the '3 meals per day' lifestyle we have all become used to is nothing more than a social construct. Our hunter gatherer forefathers did not have the luxury of cornflakes on waking followed by a Boots meal deal at lunch and deliveroo for dinner. I no longer think we 'need' this either.
6 months on from breaking my fast, I have managed to maintain my post-fast weight (with the exception of a small amount of water gain). I fast intermittently each day (no food between 9pm and noon the following day) which helps me achieve razor sharp focus on difficult tasks first thing in the morning. I try to avoid refined carbohydrates and sugar in favour of complex carbs, healthy fats and proteins.
Above all, I trust my body to let me know when it's time to eat.
If you appreciate the time and effort it has taken to conduct this 'experiment' why not make a small donation to my favourite charity?
You can donate and learn more about this cause on my JustGiving Page
A guide to water fasting: https://www.natural-health-zone.com/water-fasting.html
Is fasting the secret to longer life: http://www.menshealth.co.uk/food-nutrition/does-fasting-make-you-live-longer
Water fasting benefits and dangers: https://www.healthline.com/nutrition/water-fasting
If you found this essay insightful, I'd massively appreciate you recommending to friends and colleagues.
Really good read? Share the  on Twitter!
Follow me here: https://twitter.com/realkniels
I've teamed up with a data scientist to help me dig even deeper on my next fast.
",211
https://medium.com/zero-fasting/how-to-break-your-fast-6097a98cf911?source=tag_archive---------8-----------------------,Breaking Your Fast,If you've embarked upon one of the many varieties of fasts  where you take in little or no food for a stretch of time  at some point...,Jordan Rosenfeld,5,"If you've embarked upon one of the many varieties of fasts  where you take in little or no food for a stretch of time  at some point you'll eventually need to break your fast and begin eating again. Experts say you shouldn't let your hunger or cravings drive you, but rather go slowly and thoughtfully into what you eat first.
When to break your fast?
When you start eating again depends upon the fasting protocol you've taken on. If you're doing time restricted feeding, many people choose to stop eating around 7 or 8pm, fast overnight, skip breakfast, and have their first meal around 11am or 12pm, at lunch essentially. Generally, you don't want to break a fast at night since you'll be using fewer calories as you head to bed.
Go slow
No matter the kind of fast you've done, be it time restricted feeding for some hours, or a water fast for a day or more, it's most important that you ease back into eating so as not to overwhelm your digestion or undo some of the good effects you've achieved by fasting. A loaded cheeseburger might sound fantastic, but it probably won't feel so good going down.
One expert recommends you halve the number of days you fasted and used that as a guide for reintroducing foods. So, if you fasted for four days, take two days to ease your way back into eating.
Consider starting with broths and liquids for the first meal, and possibly the first day, if you've been on a longer fast. The less sugar in these liquids, the better, as those can create bloating and digestive irritation. Once you introduce food, expect to gain back a certain amount of water weight due to adding back in some carbs and replenishing your glycogen stores. However, be sure to continue to drink lots of water as well. You might have become dehydrated while fasting, which is common if you don't stay on top of your fluid intake.
Post-fasting is also not a great time to experiment with new foods or recipes you haven't tried before. Your body might not be prepared to properly digest these new foods, so it is best to first reintroduce something that is familiar to you to ease the transition. Wait until you are back into a normal eating routine to start experimenting with new foods.
Gradually introduce foods
When you start eating again, choose foods that are low on the glycemic index because too many carbs, especially carbohydrates that are easily digested and quickly absorbed, can spike insulin levels. One fasting expert recommends eggs, avocado, nuts and spinach as good examples to introduce first.
Another nutrition expert recommends that you start with soft-cooked foods like vegetables, since these are easier to digest. Slowly move up to foods with whole grains and high fiber because too much of these after not eating can lead to constipation. Add in raw foods last, as these can irritate your digestive tract.
When transitioning off a prolonged fast, it may be better to start with small servings and have no more than 500 calories per meal. This slow transition will help your body adapt to having higher amounts of nutrients again. In rare cases, people who have gone a long time without eating  usually more than 10 days  or who are malnourished prior to starting a fast can be at higher risk of a dangerous condition known as ""refeeding syndrome."" This is a potentially fatal syndrome that can occur due to shifts in fluids and electrolytes after macronutrients are too rapidly reintroduced into the body. As long as you are in good health going into your fast, transition slowly back to your normal diet and are under the care of a health professional before, during, and after fasting, this is unlikely to occur.
If you start craving sugar again after a fast and think you'll quell your sweet tooth with fruit, go carefully, particularly if you are fasting for weight loss or to balance your blood sugar. Even too much natural fructose can negatively impact insulin sensitivity. If you must eat fruit, choose those that are lower on the glycemic index and higher in fiber, such as berries, apples, or pears.
To help overeating, have a plan in place for breaking your fast. For prolonged fasts, map out your meals for the week after your fast. Knowing what and how much you will be eating the days following your fast will help reduce your risk of binge eating and choosing poor quality foods. Meal planning apps or meal prep kits are great tools to help with this.
Other considerations
A small portion of protein at your first meal such as eggs or chicken can be a good option, as they provide essential amino acids to help rebuild and repair the body. When you fast, growth pathways such as IGF-1 and mTOR are suppressed. Amino acids stimulate both of these, so once you reintroduce amino acids from protein sources post fast, you then activate this beneficial growth period.
What you eat isn't the only consideration after fasting; another expert recommends you chew your food very thoroughly, as many as 30 times per bite, to make sure your food is easily digested.
If you're anxious about digestion, consider adding in a probiotic, either in the form of a supplement or by eating foods that have naturally occurring probiotics in them, such as sauerkraut, miso, and, when you feel ready for dairy, yogurt.
Beyond just breaking your fast, also consider your overall eating habits. If fasting is your only answer to getting healthier, but you don't eat well the rest of the time, fasting is unlikely to give you the results you're seeking.
The official blog of the Zero Fasting app
822 
1
",212
https://betterhumans.pub/11-ninja-tips-on-how-to-wake-up-early-58d63d3972f3?source=tag_archive---------1-----------------------,11 Unusual Tips for How to Wake Up Early,"If you're a night owl and you've tried waking up early, you know it's one of the most difficult habits. It's a pain in the ass to deal with...",George Halachev,11,"If you're a night owl and you've tried waking up early, you know it's one of the most difficult habits. It's a pain in the ass to deal with grogginess in the morning and to be in bed on time with all the digital distractions nowadays.
You probably already read a few articles (or books) on sleep and early rising. In those, you see the usual advice:
All these points are important and they've been discussed to death everywhere on the internets. Instead, I want to talk about some less known, ""ninja"" tips that will make you an early riser faster. I've been coaching people on the waking up early habit for more than two years now, and there are few of those that pop up constantly.
The usual advice with the alarm is to keep it far away and immediately jump out of bed when it goes off.
That approach works well in the military, but what if you don't want to have that stressful schedule in your every-day life? What if you want to enjoy your morning and spend some time in bed before jumping into work?
If you want to do a habit long term it has to feel good. You're not going to make it very far if your new habit feels horrible, and that's exactly how it feels when you jump out of bed groggy.
So how can you spend time in bed without falling back asleep?
Have a two-alarm setup.
The first one is to wake you up, the second one is your cue to get out of bed. The first one should be within arm's reach and the second one should be away from your bed.
That way, you can give your body some time to gently awaken and you can spend some quiet time in bed doing something you love, like reading your favorite novel, writing in your journal, or doing affirmations.
When the second alarm goes off your time is up and you have to get out of bed. A 10 or 15-minute period before the first and the second alarm works well. By that time your body will feel much better and you would have gotten some inspiration by doing your favorite thing in the morning.
My setup is a silent Fitbit alarm that wakes me up and my phone alarm goes off 10 minutes later. I use that 10 minutes to cuddle with my girlfriend  a great way to start the day.
We've been conditioned by the productivity movement that everything should be about getting things done. Do more, faster, increase efficiency. Most of our morning routines are filled with activities that require willpower and discipline.
But getting out of bed is much easier if you have something that you're looking forward to. Something that gives you joy and excites you.
It might be doing a morning walk in the park, walking your dog, getting a cup of coffee at your favorite cafe, spending time with your loved ones.
It's different for everybody but whatever it is, make sure there's at least one activity every morning that is just to excite you and improve your mood. That indirectly will make you more productive for the rest of the day.
""I heard that early risers are happier and more productive,"" is not a good reason. It's too general and will not inspire you to take action.
Changing this habit is hard, and if you want to endure the difficulty, you will need a good reason for it. Be really clear about what you want to get out of the extra morning time.
Do you want to use it to work more on your business? To improve your fitness and health? To get some extra time with your friends and loved ones? To make more time for learning and reading?
If you don't come up with a good way to spend your mornings, they will automatically be allocated for sleeping in.
Planning it in advance is also very important. Coming up with the right thing to do at 6 AM when you're feeling groggy isn't going to work. At that time your mind will always come up with the same priority: sleep more.
Before you even start waking up early, come up with a great plan about how you're going to use that extra morning time.
The statistics of one of the top habit tracking apps Lift showed that people using a coach had a 300% better chance to master a new habit than the ones trying to do it on their own  that's how important accountability is.
The problem is that it's hard to find a friend that cares enough to hold us accountable, at least not long-term.
Well, thanks to technology, nowadays it's easy to find and hire a professional coach that specializes in the habit or goal you want to achieve. And believe it or not, you can do so for as little as $25/week using the Coach.me coaching platform.
You can even sign up for a 3-day free trial and see if it's the right fit for you before you commit here: www.coach.me/coaching
The more specific you are with your morning routine, the easier it's going to be to execute it. I'm talking about the really small details.
Where do you put your alarm? Do you get dressed before going to the bathroom? Do you shave first or brush your teeth first? Do you take a shower in the morning or in the evening?
Also, the better defined your routine is the more efficient it's going to be. Since you're doing the same thing every day, you will find many ways to optimize it.
After writing down my routine in detail, I figured out that laying out my clothes for the next day is much better than doing it when I wake up. It saves time and it feels nice to have everything ready when you wake up. I imagine it's going to save even more time for the ladies.
When I saw the routine written on paper I also noticed how many unnecessary trips I was making back and forth to different rooms. First, because I didn't do it in the right order and second because I would forget something and then have to go back.
Here's my morning routine in detail:
It seems like a big list, but since I do exactly the same thing every morning I've optimized it so it takes just 15 minutes.
6 years ago, when I was really struggling with waking up early, something happened that made me an early riser in just 3 days  my computer died. It was as easy as that.
I had no TV, smartphone or tablet at the time, so there was nothing keeping me up late at night. The only electronic I had was an iPod which I used to listen to audio books and that only helped me fall asleep faster. Since I fell asleep early, I started getting enough sleep and naturally started waking up early.
Going through that period without a computer and effortlessly becoming an early riser, helped me realize that waking up early is our natural state. We don't have to do anything extra to be early birds, we just need to eliminate the obstacles.
If you're committed to succeeding at this habit, get rid of all the electronics. Don't just turn them off, that never works. There is nothing stopping you from turning everything back on when you're feeling bored at night and can't fall asleep.
Lend your tablet to a friend for a few weeks. Get rid of the TV. Switch to an old school phone, where the only distraction is playing the Snake game.
It's an extreme step and getting rid of all electronics will be challenging. Also, it's not realistic to make it a permanent change. But doing it for a few weeks will help you get a great start with this habit.
It's like learning to ride the bike with training wheels, it eliminates the chance to fall down. Getting rid of the electronics eliminates the chance to be tempted and stay up late.
Many people manage to get out of bed early, but an hour later they still feel groggy and go back to sleep.
Changing your wake-up time to a few hours earlier is hard. While your body gets used to the new timing, you will feel sleepy in the first few hours and going back to sleep will be tempting. Especially if you're still at home with your cozy bed seemingly doing a come-hither gesture, just like in a Disney movie.
Even coffee doesn't help in that case. I've tried getting two strong cups of coffee when feeling groggy and I can still keep snoozing for a few more hours.
So what's the solution?
Go outdoors as soon as possible. Do a quick morning routine to refresh yourself and hit the door immediately.
Something about being outdoors makes it easy to stay awake. Feeling the cool air on your skin, smelling the grass and flowers, hearing the rustling leaves. Nature tends to melt away all the grogginess.
It's a great opportunity to do some exercise too, which is one of the best ways to start your day. Get your heart rate up.
Accountability is becoming mainstream. Getting friends and family to keep you in check helps a lot. Hiring a coach or making a financial commitment on sites like stikK.com is great too. But there is no better accountability than a hungry cat in the morning.
If you get a cat and show her that 6 AM is food time, she will make sure you're preparing breakfast at 5:55 AM every morning. No exceptions.
The downside to that approach is that she might also decide she wants some play time at 3 am, but oh well... cats will be cats.
Have you had one of those days where you wake up early, but you don't feel sleepy or groggy? You can fall back asleep easily but you can also get up and start your day. Feels great, doesn't it?
Of course, you also know about the other mornings. Mornings where waking up early is terrible: your mind is foggier than the Golden Gate bridge and your body feels like it's been run over by an 18 wheeler. Several times.
The difference between those two cases is the sleep cycles. When we sleep at night we experience a few cycles that our bodies go through. Each cycle passes through different stages, illustrated on the graph below.
Stage 4 is the deepest one and Stage 1 (REM) is the lightest one, meaning the closest one to the awake state. The closer to the awake state you are when the alarm goes off the better you will feel. The deeper you are the worse you will feel.
So how can you use that to your advantage?
Figure out at which time in the morning you're in REM sleep. If you feel terrible when the alarm goes off at 7 am, try 7:30 instead. If that doesn't work, try 8 am. Eventually, you'll find the sweet spot and you'll be able to get up much more easily.
Once you find that sweet spot, you can begin gradually moving your alarm back by 10-15 minutes earlier, and shift your sleep cycles until you hit your target wake-up time.
This approach works only if you have consistent bedtimes. If you change the bedtimes by 1-2 hours every day, the sleep cycles will change too, and you won't be able to find that stability in the morning.
One of the most frequent questions that I get is, ""How long does it take to become an early riser?""
It only takes your body 4-5 days of waking up and going to bed at the same time to adjust to the new schedule. It works even if it's a big change, like moving in a different time zone.
However, getting yourself to do those 4-5 consistent days is a different story. Being able to do it depends on your current habits.
If you're used to watching Netflix while overeating until 2 AM, and you try shifting your bedtime to 10 pm, it's going to be a big challenge. In that case, you have to change two additional habits  the overeating and the late TV watching. That takes additional time and discipline.
Becoming an early riser is difficult because it's not just one habit but a combination of many tiny ones. That's why shifting the sleep schedule gradually works better than cold turkey. It allows you to gradually improve the other prerequisite habits at the same time.
You already know that consistent wake-up and bedtimes are crucial to becoming an early riser. However, we don't live in a perfect world. Sometimes our priorities change and we have to stay up late.
In those cases, we have two choices for the morning after: 1) keep the same alarm time even though we'll get less sleep, or 2) turn off the alarm and get enough sleep.
The best choice depends on how late you go to bed.
1) If you go to bed late but still get at least 5-6 hours, then it's better to maintain the same alarm time. You will feel a bit sleepy during the day, but you can always get a power nap in the afternoon to help the process. You can also go to bed a little earlier the following evening to catch up even more.
Maintaining the wake-up time will make it easier to stay on track for the next few days, even though you had one late night.
On the other hand, if you were to change your alarm every time just to make sure you're getting enough sleep, you will have a much more inconsistent schedule. You might end up sleeping 2-3 hours longer. Then on the following evening, you'll not feel sleepy enough at the usual time, and you'll stay up late again. The whole thing turns into a negative spiral.
A good rule of thumb is, ""Regulate the amount of sleep by adjusting the bedtime, not the wake-up time.""
2) The second scenario is when you go to bed very late and keeping the typical alarm time means you'll only get 1-2 hours of sleep.
In that case, you'll be better off sleeping in.
Even if you woke up on time, with so little sleep you'll end up spending the day like a zombie, struggling to stay awake. So instead, turn off the alarm and let your body wake you up naturally. Then make sure you put some extra effort to be in bed on time on the following evening.
It's likely that you won't feel sleepy at your usual bedtime, so you can get some melatonin to ease the falling asleep.
Originally published at georgehalachev.com on April 18, 2017.
The Better Humans publication is a part of a network of personal development tools. For daily inspiration and insight, subscribe to our newsletter, and for your most important goals, find a personal coach.
",213
https://medium.com/@drstephanie/the-3-biggest-mistakes-people-make-on-the-ketogenic-diet-and-how-to-fix-them-ea554f38ff0d?source=tag_archive---------1-----------------------,The 3 Biggest Mistakes Women Make On The Ketogenic Diet (And How To Fix Them),Discover the difference between 'dirty' keto and 'clean' keto to keep the weight off and succeed in the long-term.,Dr. Stephanie Estima,12,"Discover the difference between 'dirty' keto and 'clean' keto to keep the weight off and succeed in the long-term.
Most women are doing Keto wrong.
The ketogenic diet, when it is properly formulated, can (and should) be sustained over the long term.
In order to do that, you need to be consuming foods that feed the microbiome (the billions of bacteria that live in your gut), that are inherently healthy for you and are anti-inflammatory.
The keto diet, when properly formulated, fulfills all these requirements, but there are common pitfalls most people fall into.
The reason why most people fail at keto in the long term is due to the fact that HOW and WHAT they are eating cannot be sustained very long.
NOTE: This article is 2,773 words. If you want the simple supplementary checklist with weekly recipes on How to Lose Weight by using your Menstrual Cycle as a PDF download, get it right here. It's free.
""The human body is a miraculous self-healing machine, but those self-repair systems require a nutrient-dense diet.""  Joel Fuhrman
Most people, when they first start eating keto, use an approach I call 'dirty' keto.
A properly formulated ketogenic diet should have high fat, moderate amount of protein and low carbohydrate intake.
Most people ""get"" this intellectually, but the practical application of it oft falls short.
This is why many people will start off getting their macros on target, but after 2 or 3 weeks are absolutely starving, no matter how much fat they eat.
There are many reasons why this happens, and one of the most common reasons is that your microbiome is starving, and it will direct all your attention to lusting after pizza, or burgers, or both.
Enter the binge. The carb coma. The falling off the wagon. And the feeling of failure once again.
This is how most of the weight loss industry works.
Dirty keto is no exception.
The weight loss industry is rigged to keep you on the yo-yo routine so that you're constantly buying more 'stuff' and feeling like you're not good enough.
You might get a quick win, but the weight always comes back, because you have not been taught the skills to keep it going on your own.
Have you ever thought ""I'm not good enough?""
Have you ever worried that there will never be an ""after"" photo?
That someone having a body you are proud of is not in cards for you?
I work from a different vantage point.
I always start with the premise that your body WANTS to be healthy, no matter what label you have been given.
Essentially, dirty keto, is what is commonly available in the market right now.
It is when people will eliminate all carbs from their diet, including vegetables, and then proceed to eat equally nutritiously devoid toxic garbage.
I may be breaking some hearts here, but you can't just eat bacon, lard, and greasy burgers forever.
Dirty keto is a substitution of one bad habit for another.
Sure, getting rid of processed carbohydrates is awesome, but substituting it ONLY for bacon, butter and burgers is not.
Reducing your carbohydrate load has a direct impact on mortality ... but are you really going to eat bacon fat fudge as a snack forever?
What are we, 6 years old with no executive brain function? How is this a reasonable long term solution, or considered healthy by any means??
Like I said...play the long game. Bacon fudge, butter, and lard all day err day is not taking into consideration longevity or vitality.
Yes macros are important, but so are micronutrients  the minerals and vitamins derived from plants and other living things.
On a cellular level, eating a properly formulated, clean ketogenic diet will support enhanced mitochondrial function, reduce ROS that cause oxidative damage, prevent cellular senescence, and increase the production of butyrate, B-hydroxybutyrate.
It amplifies autophagy, prevents cellular aging and precancerous pathways like mTOR.
Now, if you are a meat eater, I'm not saying you can never eat bacon.
Of course you can.
And I'm not AGAINST bacon and butter  I love them both.
But I am always going to look at things from both a practical perspective, and from a longevity and vitality lens.
Bacon all day every day (which is what a lot of keto ""experts"" will recommend) is not a long term solution for optimal health and vitality.
A properly formulated ketogenic diet is primarily plant based.
Meaning we are eating a large amount of vegetables (like green leafy ones with a high fiber content), and then layering appropriate fat and protein on top of that.
What's nice about a ""clean"" ketogenic diet is that vegetarians can also be successful in becoming fat-adapted, and also profit from the benefits from a fasting-mimicking diet.
When you layer fasting along with a fasting-mimicking diet  you've hit the jackpot.
When we think about playing the long game with your health, we should always be asking the following questions:
What are the things I can eat that are anti  inflammatory?
What are foods I can create that will nourish and feed my cells?
What will help with regular and consistent elimination?
What will make me feel good and stabilize my mood, increase my energy, focus, and clarity?
How can I keep my brain happy and healthy?
The answer my friends, are your vegetables and your fats. Just like mom used to tell you.
I realize, as someone who spends most of her time educating people about brain health and optimization, that the brain is often forgotten.
We cannot see the brain, and changes in function are often not noticed because they are so subtle, and because the way the brain is is the way we are.
It is immensely difficult to detect changes unless they are severe.
It is hard to detect changes in your brain, because you ARE your brain.
We need to be thinking about the brain, and optimizing the brain's performance, because your body, in all its glory, is simply a reflection of brain health and function.
One of the ways in which you can protect your brain is by the fuel you feed your brain in order to perform its best.
Everything follows the brain.
In fact, when we look at your body ,all parameters of your body  HRV, blood pressure, respiratory rate, forced vital capacity, grip strength, hormones, lab markers  will give you a good picture of the health and vitality of your brain, too.
Hormones out of whack? We need to look at the brain.
Overweight? We need to look at the brain
Low energy? We need to look at the brain
Cannot sleep through the night? We need to look at the brain.
Poor focus? We need to look at the brain.
Mood swings? We need to look at the brain.
We need to stop looking at the end organ as the primary focus, and instead look a level or two up to find why downstream changes are happening at the organ or behavioural level.
So, when we are talking about the best fuel you need to feed your brain (and therefore your body), we need to be thinking about the best possible foods for lowering inflammation, improving autophagy, removing cellular senescence, enhance energy production with whole, nutrient dense foods.
Because all these things help your brain.
We need to be thinking about a clean ketogenic diet.
Not dirty keto.
Not bacon, burgers, and butter.
Just because it fits the macros, doesn't mean you should do it.
""Go vegetable heavy. Reverse the psychology of your plate by making meat the side dish and vegetables the main course.""  Bobby Flay
The big pattern I have found most often with people doing dirty keto engage in, is unknowingly they consume too much protein.
Day over day, week over week, a dirty keto diet will predictably lead to an increase in protein consumption.
This is because eating protein, if you are not feeding your microbiome, can feel more ""satisfying"" than fat, and with time, you want to eat it more and more.
Excess protein consumption is insulinergic, just as carbohydrates are.
Meaning, consuming protein will raise insulin in the same manner carbohydrates do. The excess protein will be converted into glucose, thereby raising insulin levels, and fat storage.
The insulin will try to scurry the glucose into the liver, muscles, and fat cells.
Protein also has the double edge sword of also activating mTOR  a pathway that is involved in aging, cancer, and brain cell dysregulation.
I have written about how protein may actually be making you fat here, and discuss the mTOR pathway in greater detail.
Again, with my longevity glasses on, excess protein consumption is also related to creating more cancer cells.
This is from chronic mTOR activation which leads to increased cell proliferation, cell division, and it inhibits autophagy.
Autophagy is the cleanup process that gets rid of the mutated, damaged bits of cellular debris that results from natural metabolism.
The only two populations that can and should benefit from mTOR activation are children and pregnant women  as they require cell proliferation in order to grow.
A properly formulated ketogenic diet should be plant-based. Meaning, that the majority of your plate should be green leafy vegetables.
All carbohydrates are not created equal. Green leafy veg like kale, spinach, swiss chard, bok choy, broccoli, and fermented foods like sauerkraut and kimchi all have an extraordinary amount of insoluble fiber.
Along with the phytonutrients derived from these vegetables, the fiber plays an important role in becoming fat-adapted, or ketogenic.
Insoluble fiber means that it cannot be broken down by the gut bacteria, and does not dissolve in water.
It is indigestible, an ""anti-nutrient"" if you will, because it does not break down and become part of you.
It passes through you, cleaning up the debris, sopping up excess hormones, and cleaning out the gut track like a pipe cleaner.
Fiber is also great at retaining a lot of water, thereby making stool bulky and softer.
Consuming fiber does not affect blood sugar levels and therefore will have a negligible effect on insulin.
Congratulations, you are about halfway through my geeky magic carpet ride. The nerd in me loves and honors the nerd in you. Just a reminder if you want the simple supplementary checklist with weekly recipes on How to Lose Weight by using your Menstrual Cycle as a PDF download, get it right here. It's free.
""There are receptors to these molecules in your immune system, in your gut and in your heart. So when you say, 'I have a gut feeling' or 'my heart is sad' or 'I am bursting with joy,' you're not speaking metaphorically. You're speaking literally."" Deepak Chopra
Often forgotten is looking at the relationship between your gut and your brain.
When you are on a dirty keto program, one of the most common complaints is by week 2 or 3 you are starving. More correctly, it is your microbiome that is starving.
Part of the way we blunt this hunger response is via resistant starches. In other words, starches that ""resist"" digestion. These are abundantly found in oats, green banana flour, legumes, and beans.
On a traditional ketogenic diet, with a standard Fat:Protein:Carb ratio of 70-20-10, what I have often found is you are absolutely starving by about week 3 and will overeat and binge on protein, and carbs.
This is why most people ""fail"" at keto.
However, when we integrate fiber and resistant starches into the picture, this hunger response is blunted.
Look at this experiment on consuming resistant starches and the blunting effect on post prandial blood glucose levels.
In this graph, they are looking at the effects of taking resistant starches, followed by a high glycemic food. When taking resistant starches, the effect on blood glucose levels are outstanding compared to the baseline  a 28 point rise compared to almost 170.
When in a state of ketosis (either through a fasted state or via a fasting mimicking diet like keto), consuming resistant starches does not disturb this state:
Resistant starches are great for a ketogenic diet  they blunt your hunger response, and also blunt elevated blood glucose levels when carbohydrates are consumed.
Part of why resistant starches are so great is instead of being digested, they are selectively feeding the ""good"" microbiota in our gut.
As these microbiota chow down on the resistant starches, they will produce short-chain fatty acids as a by-product of their meal. Of particular importance here is the short-chain fatty acid known as butyrate.
Butyrate has a whole host of beneficial effects.
One important one is decreasing gut permeability.
When the lining of the gut is more permeable, we are more susceptible to developing food allergies, food intolerances, and an overall immune system on high alert all the time.
Having an increased permeability of the gut has been associated with almost all proinflammatory conditions, including most autoimmune conditions, fatty liver, and heart disease.
Inflammation is the linchpin of most lifestyle diseases.
One of the primary objectives we must look at in any health protocol is to heal the gut.
The obvious reason here is it makes digesting food a more pleasant and efficient experience. We all want to be able to eat without gas, bloating, indigestion, or diarrhea.
But it is the knock-on positive benefits to our neurology that is of particular long term importance.
The gut has its own nervous system  the enteric nervous system  which has strong communication pathways with the brain and central nervous system.
A little nerding out for my neuronerds: Developmentally, the enteric nervous system is made from neural crest cells. Specifically the vagal neural crest cells...which is why we have such strong gut reactions to stimuli. A ""gut feeling"" is the communication between the central and enteric nervous systems via the vagus nerve.
Ever feel nervous about something and then immediately have to go to the bathroom, or feel like you were going to throw up? It is because of the strong vagal tone and communication between a perceived stimulus in your brain, and the enteric nervous system in the gut.
I know. So cool, right?!
The communication between the gut and the brain is paramount, and making sure your enteric nervous system is aligned with your autonomic nervous system should be a goal in any nutrition program.
So when we feed the microbiome and they produce butyrate levels are high, you allow for gut healing to occur...but it is also important in brain health.
The butyrate effect is the gift that keeps on giving.
Butyrate helps keep the brain healthy by upping the expression of neurotrophic factors.
Butyrate and the structurally similar B-hydroxybutyrate (which is produced in a ketogenic state) help increase BDNF (Brain-Derived Neurotrophic Factor), GDNF (Glial-cell derived Neurotrophic Factor) and NGF (Nerve Growth Factor).
All of these help in the maintenance of nerve cells that you already have, but also help grow new, healthy nerve cells.
In other words  butyrate, and B-hydroxybutyrate keep your brain big and juicy.
""The hormonal interplay inside a woman's head creates her reality. Her hormones tell her day to day what's important. They mold her desires and values.""  Abhijit Naskar
Once you are fat-adapted by following a clean keto protocol, the best way to maintain your weight loss results is to begin to harness the power of your menstrual cycle.
Meaning, you can begin to vary the percentage of fats, proteins, and carbohydrates week by week, depending on where you are in your cycle.
You are not, as a woman, meant to be in ketosis forever.
During certain times of the month, you can and should be eating more carbohydrates, and protein.
Yes, that includes chocolate, too!
Once we have become metabolically flexible, we can use insulin, growth factors to our advantage. Insulin is not the enemy once you are fat-adapted.
When done smart, we can build lean muscle mass, continue to lose weight, suppress cravings, and support some of the moodiness that can come in the week before your period.
Serotonin gets gobbled up in the weeks leading up to your period. Trying to push through and eat low carb during this time is a recipe for failure.
Not to mention the self-talk, doubt, and inner critic that is going to go along with it.
Women are not little men.
We cannot eat the same way every day of the month or stay in ketosis forever.
I have written a week by week guide on eating in accordance with your menstrual cycle here.
Bottom line is this: Use your menstrual cycle to lose weight and keep it off over the long term.
I have created a quick start guide, including meals and nutrient breakdown for how to eat during each stage of your cycle.
If you liked this article, click the below, and share it with others so they can enjoy it as well.
",214
